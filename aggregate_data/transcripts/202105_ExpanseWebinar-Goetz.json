[{"text": "okay", "start": 6.64, "duration": 3.2}, {"text": "thanks everyone for joining we have a", "start": 7.359, "duration": 4.641}, {"text": "great presentation this morning", "start": 9.84, "duration": 5.44}, {"text": "by uh andreas getz on gpu computing and", "start": 12.0, "duration": 4.4}, {"text": "programming on", "start": 15.28, "duration": 4.72}, {"text": "expense before andy gets started", "start": 16.4, "duration": 5.28}, {"text": "i want to bring your attention to the", "start": 20.0, "duration": 3.119}, {"text": "exceed code of conduct", "start": 21.68, "duration": 3.759}, {"text": "everyone's generally on good behavior", "start": 23.119, "duration": 3.361}, {"text": "but if you do", "start": 25.439, "duration": 3.521}, {"text": "observe any inappropriate behavior", "start": 26.48, "duration": 4.32}, {"text": "please let us know", "start": 28.96, "duration": 5.439}, {"text": "by reporting it to exceed.org", "start": 30.8, "duration": 6.64}, {"text": "code of conduct and exceed is also", "start": 34.399, "duration": 5.601}, {"text": "trying to be sensitive to terminology", "start": 37.44, "duration": 5.84}, {"text": "used in these training events if you do", "start": 40.0, "duration": 4.079}, {"text": "observe any", "start": 43.28, "duration": 4.0}, {"text": "inappropriate terminology or materials", "start": 44.079, "duration": 6.441}, {"text": "please send an email to terminology", "start": 47.28, "duration": 6.16}, {"text": "exceed.org and with that", "start": 50.52, "duration": 6.519}, {"text": "i will stop sharing and hand it off", "start": 53.44, "duration": 6.959}, {"text": "to andy take it away andy all right", "start": 57.039, "duration": 4.32}, {"text": "thank you jeff", "start": 60.399, "duration": 4.08}, {"text": "welcome everybody thanks for joining so", "start": 61.359, "duration": 12.481}, {"text": "let me share my screen", "start": 64.479, "duration": 9.361}, {"text": "all right can you see my presentation", "start": 77.04, "duration": 3.92}, {"text": "okay yes it looks good", "start": 78.56, "duration": 5.36}, {"text": "okay awesome", "start": 80.96, "duration": 2.96}, {"text": "all right so uh welcome everybody so", "start": 85.36, "duration": 4.32}, {"text": "this is a very brief introduction we", "start": 88.0, "duration": 4.4}, {"text": "only have one hour unfortunately um", "start": 89.68, "duration": 4.479}, {"text": "and so i hope i can give you a good", "start": 92.4, "duration": 3.6}, {"text": "overview of gpu computing and", "start": 94.159, "duration": 3.121}, {"text": "programming so i have programming in", "start": 96.0, "duration": 2.64}, {"text": "there so i want to take you briefly", "start": 97.28, "duration": 3.04}, {"text": "through", "start": 98.64, "duration": 4.08}, {"text": "also how to program gpus and then of", "start": 100.32, "duration": 3.2}, {"text": "course", "start": 102.72, "duration": 3.759}, {"text": "we have expanded sdsc our new", "start": 103.52, "duration": 4.48}, {"text": "supercomputer", "start": 106.479, "duration": 4.161}, {"text": "and to just briefly show you how to use", "start": 108.0, "duration": 4.88}, {"text": "expand", "start": 110.64, "duration": 5.2}, {"text": "gpu nodes", "start": 112.88, "duration": 2.96}, {"text": "okay um so what i want to do", "start": 117.52, "duration": 5.919}, {"text": "with you is um to quickly go over the", "start": 120.799, "duration": 4.241}, {"text": "following topics so give you a brief", "start": 123.439, "duration": 4.96}, {"text": "overview of gpu hardware", "start": 125.04, "duration": 5.12}, {"text": "short introduction to some examples of", "start": 128.399, "duration": 4.401}, {"text": "rtp accelerated software", "start": 130.16, "duration": 4.079}, {"text": "and then i want to give you a short", "start": 132.8, "duration": 3.04}, {"text": "introduction about programming gpus", "start": 134.239, "duration": 2.0}, {"text": "we're", "start": 135.84, "duration": 3.759}, {"text": "using libraries computer c computer", "start": 136.239, "duration": 6.401}, {"text": "programming and then open acc and then", "start": 139.599, "duration": 3.601}, {"text": "we", "start": 142.64, "duration": 3.12}, {"text": "will have a look at uh sds61 gpu nodes", "start": 143.2, "duration": 4.16}, {"text": "how to get into those nodes how to run", "start": 145.76, "duration": 2.24}, {"text": "gpu", "start": 147.36, "duration": 4.159}, {"text": "jobs and um well developing gpu software", "start": 148.0, "duration": 5.519}, {"text": "basically you know a", "start": 151.519, "duration": 4.481}, {"text": "few quick examples on how to compile", "start": 153.519, "duration": 3.681}, {"text": "code and similar things", "start": 156.0, "duration": 3.84}, {"text": "now i want to say um i i don't know", "start": 157.2, "duration": 4.319}, {"text": "exactly what all your expectations are", "start": 159.84, "duration": 3.2}, {"text": "in coming in here so please give us", "start": 161.519, "duration": 3.521}, {"text": "feedback you know if you want to", "start": 163.04, "duration": 4.8}, {"text": "hear more about expands and how you know", "start": 165.04, "duration": 4.32}, {"text": "practically get us to the computer notes", "start": 167.84, "duration": 2.8}, {"text": "and similar things and less about", "start": 169.36, "duration": 3.599}, {"text": "um overview of gpus or if you want to", "start": 170.64, "duration": 3.76}, {"text": "have more of the other or if you", "start": 172.959, "duration": 4.481}, {"text": "want to have multiple um webinars um", "start": 174.4, "duration": 7.04}, {"text": "any feedback would be good for us um", "start": 177.44, "duration": 6.64}, {"text": "so just briefly know what is a gpu", "start": 181.44, "duration": 4.159}, {"text": "effectively it's just an accelerator in", "start": 184.08, "duration": 4.879}, {"text": "some sense you know in the past um", "start": 185.599, "duration": 4.72}, {"text": "you know we had different types of", "start": 188.959, "duration": 3.601}, {"text": "accelerators and we still do have that", "start": 190.319, "duration": 3.761}, {"text": "it's just a component that speeds up", "start": 192.56, "duration": 3.679}, {"text": "some aspect of a computing workload and", "start": 194.08, "duration": 3.28}, {"text": "um", "start": 196.239, "duration": 2.801}, {"text": "you know in the past we would have had", "start": 197.36, "duration": 3.2}, {"text": "even floating point code processors and", "start": 199.04, "duration": 2.559}, {"text": "older pcs", "start": 200.56, "duration": 3.679}, {"text": "for floating point operations um and so", "start": 201.599, "duration": 4.161}, {"text": "it's a specialized chip um", "start": 204.239, "duration": 2.801}, {"text": "more recently you know there are a few", "start": 205.76, "duration": 3.44}, {"text": "programmable gate arrays um especially", "start": 207.04, "duration": 3.119}, {"text": "as chips", "start": 209.2, "duration": 2.959}, {"text": "but um so the important piece here about", "start": 210.159, "duration": 4.481}, {"text": "um the graphics processing units is that", "start": 212.159, "duration": 5.36}, {"text": "you know it's been driven by a a massive", "start": 214.64, "duration": 3.599}, {"text": "investment", "start": 217.519, "duration": 3.201}, {"text": "in in in the gaming industry which is", "start": 218.239, "duration": 5.841}, {"text": "approximately 150 billion dollar", "start": 220.72, "duration": 6.0}, {"text": "per year i think it's 180 billion last", "start": 224.08, "duration": 4.32}, {"text": "year so it's there's a lot of money and", "start": 226.72, "duration": 3.2}, {"text": "so that goes into development of these", "start": 228.4, "duration": 2.8}, {"text": "gpus", "start": 229.92, "duration": 5.2}, {"text": "and um originally they had six function", "start": 231.2, "duration": 4.959}, {"text": "pipelines so", "start": 235.12, "duration": 3.839}, {"text": "some um people who are older like myself", "start": 236.159, "duration": 3.36}, {"text": "they will", "start": 238.959, "duration": 3.761}, {"text": "remember um", "start": 239.519, "duration": 5.041}, {"text": "early gpus that you actually couldn't", "start": 242.72, "duration": 3.519}, {"text": "program and then at some point so the", "start": 244.56, "duration": 4.64}, {"text": "first really programmable gpu", "start": 246.239, "duration": 5.441}, {"text": "uh was geforce gtx card i don't remember", "start": 249.2, "duration": 4.0}, {"text": "if it was a 280 might have been another", "start": 251.68, "duration": 2.32}, {"text": "one but", "start": 253.2, "duration": 4.48}, {"text": "around that time by nvidia and you know", "start": 254.0, "duration": 5.68}, {"text": "they don't look any different nowadays", "start": 257.68, "duration": 3.76}, {"text": "but you know you have gpus of course not", "start": 259.68, "duration": 3.519}, {"text": "only that go into pci express", "start": 261.44, "duration": 3.199}, {"text": "slots but also they are sitting as", "start": 263.199, "duration": 4.56}, {"text": "processors directly in the motherboard", "start": 264.639, "duration": 4.56}, {"text": "important piece is that they have a", "start": 267.759, "duration": 4.401}, {"text": "simplified core design compared to cpu", "start": 269.199, "duration": 4.401}, {"text": "which means limited architectural", "start": 272.16, "duration": 3.28}, {"text": "features like branch caches", "start": 273.6, "duration": 3.76}, {"text": "and a partially exposed memory hierarchy", "start": 275.44, "duration": 3.6}, {"text": "that you actually have to should be", "start": 277.36, "duration": 2.96}, {"text": "aware of", "start": 279.04, "duration": 3.52}, {"text": "when you program gpus but also even if", "start": 280.32, "duration": 4.24}, {"text": "you just use gpus you should know", "start": 282.56, "duration": 3.919}, {"text": "how the data moves between your cpu and", "start": 284.56, "duration": 4.48}, {"text": "the gpu", "start": 286.479, "duration": 5.921}, {"text": "now why is such an interest in gpus it", "start": 289.04, "duration": 6.879}, {"text": "has to do um you know with a", "start": 292.4, "duration": 4.72}, {"text": "microprocessor", "start": 295.919, "duration": 3.601}, {"text": "design over the trends that we have seen", "start": 297.12, "duration": 3.359}, {"text": "over there", "start": 299.52, "duration": 4.0}, {"text": "uh last 20 years i would say um so", "start": 300.479, "duration": 4.881}, {"text": "this is this is not fully up to date but", "start": 303.52, "duration": 3.6}, {"text": "effectively what's on this plot is still", "start": 305.36, "duration": 3.76}, {"text": "holding today the microprocessor trend", "start": 307.12, "duration": 2.4}, {"text": "data", "start": 309.12, "duration": 3.519}, {"text": "and um you're probably all aware of", "start": 309.52, "duration": 4.88}, {"text": "moore's law that the transistor count in", "start": 312.639, "duration": 3.361}, {"text": "integrated circuits doubles about every", "start": 314.4, "duration": 2.799}, {"text": "two years and", "start": 316.0, "duration": 3.52}, {"text": "that exponential growth still holds so", "start": 317.199, "duration": 3.601}, {"text": "that's that yellow", "start": 319.52, "duration": 4.56}, {"text": "brownish dots here as you can see", "start": 320.8, "duration": 7.6}, {"text": "right but um the way that that that", "start": 324.08, "duration": 6.399}, {"text": "processes are implemented has changed", "start": 328.4, "duration": 3.68}, {"text": "simply because we have been shrinking", "start": 330.479, "duration": 2.16}, {"text": "down the", "start": 332.08, "duration": 3.36}, {"text": "um die features so much the clock", "start": 332.639, "duration": 4.321}, {"text": "frequency in particular has not been", "start": 335.44, "duration": 2.0}, {"text": "going up", "start": 336.96, "duration": 3.76}, {"text": "anymore since the early 2000s", "start": 337.44, "duration": 6.24}, {"text": "which means that the serial execution", "start": 340.72, "duration": 4.479}, {"text": "performance is roughly constant that's", "start": 343.68, "duration": 3.12}, {"text": "not entirely true of course but", "start": 345.199, "duration": 5.121}, {"text": "um it's you know not automatically much", "start": 346.8, "duration": 5.04}, {"text": "faster or it's not getting much faster", "start": 350.32, "duration": 3.36}, {"text": "your code if you have a serial code", "start": 351.84, "duration": 3.199}, {"text": "you can't just wait two years and it", "start": 353.68, "duration": 3.519}, {"text": "gets faster so the performance increases", "start": 355.039, "duration": 4.081}, {"text": "really due to the increase of cpu cores", "start": 357.199, "duration": 3.521}, {"text": "per processor right", "start": 359.12, "duration": 5.12}, {"text": "and um you can't simply wait to get", "start": 360.72, "duration": 5.28}, {"text": "your code run faster you must write", "start": 364.24, "duration": 3.92}, {"text": "parallel code that's basically the gist", "start": 366.0, "duration": 3.28}, {"text": "of it", "start": 368.16, "duration": 4.479}, {"text": "um the frequency has here um", "start": 369.28, "duration": 5.919}, {"text": "plateaued and you have many many many", "start": 372.639, "duration": 4.241}, {"text": "more cores now whatever you do whether", "start": 375.199, "duration": 3.921}, {"text": "it's cpus or gpus you have to", "start": 376.88, "duration": 5.36}, {"text": "write parallel code and", "start": 379.12, "duration": 4.799}, {"text": "so this is also a little bit outdated i", "start": 382.24, "duration": 3.76}, {"text": "don't have a new data", "start": 383.919, "duration": 4.481}, {"text": "for this unfortunately but it really", "start": 386.0, "duration": 3.36}, {"text": "shows you", "start": 388.4, "duration": 3.28}, {"text": "uh trends that have been going on and", "start": 389.36, "duration": 3.76}, {"text": "they're still true so what you see on", "start": 391.68, "duration": 2.4}, {"text": "the left and on the right", "start": 393.12, "duration": 2.72}, {"text": "is the floating point performance of", "start": 394.08, "duration": 3.6}, {"text": "cpus and gpus", "start": 395.84, "duration": 4.32}, {"text": "so blue is a representative like intel", "start": 397.68, "duration": 4.079}, {"text": "xeon cpus", "start": 400.16, "duration": 6.0}, {"text": "and red and green is amd", "start": 401.759, "duration": 4.401}, {"text": "and nvidia gpus and so you can see that", "start": 406.24, "duration": 3.76}, {"text": "you have a", "start": 409.039, "duration": 3.681}, {"text": "significantly higher um floating point", "start": 410.0, "duration": 4.24}, {"text": "performance", "start": 412.72, "duration": 4.64}, {"text": "on gpus um that is true for 32-bit", "start": 414.24, "duration": 3.84}, {"text": "floats so", "start": 417.36, "duration": 2.239}, {"text": "of the referred to a single precision", "start": 418.08, "duration": 3.6}, {"text": "floating point arrivals", "start": 419.599, "duration": 5.521}, {"text": "or data um also for gaming gpu so cheap", "start": 421.68, "duration": 4.239}, {"text": "gaming", "start": 425.12, "duration": 3.04}, {"text": "gpus that you see in here like uh gtx", "start": 425.919, "duration": 3.201}, {"text": "titan and", "start": 428.16, "duration": 2.8}, {"text": "these are older ones right now our data", "start": 429.12, "duration": 3.199}, {"text": "would have a lot of gaming features", "start": 430.96, "duration": 2.799}, {"text": "they're hard to get by now because", "start": 432.319, "duration": 2.961}, {"text": "they're all", "start": 433.759, "duration": 5.28}, {"text": "basically bought up on the market by", "start": 435.28, "duration": 5.68}, {"text": "cryptocurrency miners as far as i", "start": 439.039, "duration": 3.841}, {"text": "understand but um", "start": 440.96, "duration": 4.079}, {"text": "if you get your hand on those you get a", "start": 442.88, "duration": 3.84}, {"text": "lot of floating point performance for", "start": 445.039, "duration": 2.241}, {"text": "cheap", "start": 446.72, "duration": 2.96}, {"text": "at least 32 bits if you need a 64-bit", "start": 447.28, "duration": 5.039}, {"text": "float you need to go to data center gpus", "start": 449.68, "duration": 4.799}, {"text": "uh like here's the tetler p100 that's", "start": 452.319, "duration": 4.32}, {"text": "what's in comet the newer newer gpus and", "start": 454.479, "duration": 3.761}, {"text": "comet and we have newer ones the", "start": 456.639, "duration": 6.201}, {"text": "walter v-100 and in expands", "start": 458.24, "duration": 7.84}, {"text": "and um it's not only the floating point", "start": 462.84, "duration": 5.72}, {"text": "performance so on the left you see", "start": 466.08, "duration": 4.239}, {"text": "the memory bandwidth and you see there's", "start": 468.56, "duration": 3.52}, {"text": "a pretty big gap between", "start": 470.319, "duration": 4.481}, {"text": "cpus and gpus and that means you know", "start": 472.08, "duration": 4.559}, {"text": "the speed with which you can", "start": 474.8, "duration": 5.04}, {"text": "move data between the gpu uh", "start": 476.639, "duration": 5.521}, {"text": "memory so the gpu has its own ram its", "start": 479.84, "duration": 3.919}, {"text": "own memory", "start": 482.16, "duration": 4.96}, {"text": "to the gpu processor versus the cpu ram", "start": 483.759, "duration": 5.921}, {"text": "to the cpu processor and that's very", "start": 487.12, "duration": 5.04}, {"text": "important because many many", "start": 489.68, "duration": 5.359}, {"text": "or i would say probably the major part", "start": 492.16, "duration": 3.439}, {"text": "of", "start": 495.039, "duration": 3.84}, {"text": "algorithms uh implemented", "start": 495.599, "duration": 6.241}, {"text": "are not floating point performance bound", "start": 498.879, "duration": 4.801}, {"text": "but memory bandwidth bonds which means", "start": 501.84, "duration": 3.28}, {"text": "it doesn't help you if the process was", "start": 503.68, "duration": 2.959}, {"text": "faster if you can't get this data to", "start": 505.12, "duration": 2.88}, {"text": "compute", "start": 506.639, "duration": 4.641}, {"text": "or store the data back output data", "start": 508.0, "duration": 5.44}, {"text": "and in terms of of power consumption you", "start": 511.28, "duration": 4.239}, {"text": "know it's sort of like comparable", "start": 513.44, "duration": 3.76}, {"text": "it depends on the gpu type in the", "start": 515.519, "duration": 3.52}, {"text": "processor type but you know", "start": 517.2, "duration": 4.88}, {"text": "a gpu consumes approximately", "start": 519.039, "duration": 8.161}, {"text": "1.5 to 2 times as much as a a", "start": 522.08, "duration": 9.439}, {"text": "cpu so you have to practice that end", "start": 527.2, "duration": 5.68}, {"text": "because", "start": 531.519, "duration": 3.361}, {"text": "of course power is very expensive as", "start": 532.88, "duration": 3.44}, {"text": "well right", "start": 534.88, "duration": 4.16}, {"text": "uh so that's a comparison between cpus", "start": 536.32, "duration": 3.84}, {"text": "and gpus um", "start": 539.04, "duration": 3.359}, {"text": "so that that's basically a state of the", "start": 540.16, "duration": 3.92}, {"text": "art to dual circuit through", "start": 542.399, "duration": 5.041}, {"text": "leon uh type um", "start": 544.08, "duration": 7.439}, {"text": "um uh cpus each each of those cpus has", "start": 547.44, "duration": 6.0}, {"text": "28 cores and here you would have to envy", "start": 551.519, "duration": 3.76}, {"text": "the tesla v100", "start": 553.44, "duration": 6.0}, {"text": "um these type of gpus we have", "start": 555.279, "duration": 7.281}, {"text": "on expands right so expands actually has", "start": 559.44, "duration": 5.44}, {"text": "four of those gpus per node and they are", "start": 562.56, "duration": 5.04}, {"text": "not like this uh pci express", "start": 564.88, "duration": 5.76}, {"text": "connected i'll show you that later", "start": 567.6, "duration": 4.48}, {"text": "and so you can see that you know if you", "start": 570.64, "duration": 3.759}, {"text": "compare like a dual focal", "start": 572.08, "duration": 6.48}, {"text": "socket um um yeah here 456 cores per", "start": 574.399, "duration": 6.241}, {"text": "per node you get around uh four", "start": 578.56, "duration": 4.08}, {"text": "teraflops of performance it's versus", "start": 580.64, "duration": 6.08}, {"text": "you know 3.5 times that with a two gpus", "start": 582.64, "duration": 7.44}, {"text": "and uh the peak for the signal precision", "start": 586.72, "duration": 7.04}, {"text": "is of course the factors the same um", "start": 590.08, "duration": 6.48}, {"text": "i i've seen many codes that make good", "start": 593.76, "duration": 4.96}, {"text": "use of the signal precision", "start": 596.56, "duration": 5.76}, {"text": "floating point performance of the gpus", "start": 598.72, "duration": 4.32}, {"text": "less so", "start": 602.32, "duration": 3.759}, {"text": "than cpus there's something else in the", "start": 603.04, "duration": 4.72}, {"text": "newer gpus that's half precision and", "start": 606.079, "duration": 3.361}, {"text": "that's often used in machine learning", "start": 607.76, "duration": 2.72}, {"text": "applications", "start": 609.44, "duration": 2.56}, {"text": "uh specifically deep learning", "start": 610.48, "duration": 3.44}, {"text": "applications that can actually", "start": 612.0, "duration": 3.92}, {"text": "effect efficiently use half precision so", "start": 613.92, "duration": 3.919}, {"text": "that means 16-bit floats that's often", "start": 615.92, "duration": 4.159}, {"text": "not possible for", "start": 617.839, "duration": 3.841}, {"text": "you know scientific applications where", "start": 620.079, "duration": 2.821}, {"text": "you need higher um", "start": 621.68, "duration": 2.48}, {"text": "[Music]", "start": 622.9, "duration": 5.1}, {"text": "larger precision for higher accuracy but", "start": 624.16, "duration": 6.0}, {"text": "many deep learning applications can", "start": 628.0, "duration": 4.079}, {"text": "exploit that", "start": 630.16, "duration": 3.6}, {"text": "and you see that then you can get a", "start": 632.079, "duration": 3.121}, {"text": "really huge performance so", "start": 633.76, "duration": 4.8}, {"text": "that's why gpus are very um", "start": 635.2, "duration": 5.52}, {"text": "popular for for machine learning deep", "start": 638.56, "duration": 4.56}, {"text": "learning applications in particular", "start": 640.72, "duration": 3.84}, {"text": "and then the memory bandwidth of course", "start": 643.12, "duration": 4.56}, {"text": "is much higher and i put on here the", "start": 644.56, "duration": 7.519}, {"text": "pci express bandwidth um so you see that", "start": 647.68, "duration": 5.52}, {"text": "you know if you have to go through the", "start": 652.079, "duration": 3.281}, {"text": "pci express bus to get data from your", "start": 653.2, "duration": 3.04}, {"text": "cpu", "start": 655.36, "duration": 3.919}, {"text": "memory to the gpu um it's much slower", "start": 656.24, "duration": 4.64}, {"text": "right so that's something to keep in", "start": 659.279, "duration": 2.56}, {"text": "mind", "start": 660.88, "duration": 2.72}, {"text": "and the code you write is it portable i", "start": 661.839, "duration": 4.641}, {"text": "would say yes um", "start": 663.6, "duration": 6.08}, {"text": "you know that open acc in opencl", "start": 666.48, "duration": 5.12}, {"text": "i'm not going to talk about opencl", "start": 669.68, "duration": 4.399}, {"text": "though um i'm", "start": 671.6, "duration": 4.4}, {"text": "going to introduce briefly cuda that's", "start": 674.079, "duration": 3.601}, {"text": "nvidia specific so in that sense it's", "start": 676.0, "duration": 2.72}, {"text": "not portable but", "start": 677.68, "duration": 3.68}, {"text": "um you know it's been around for so long", "start": 678.72, "duration": 3.44}, {"text": "that and", "start": 681.36, "duration": 3.2}, {"text": "and there's like a software out there", "start": 682.16, "duration": 4.08}, {"text": "that helps you also to translate cuda", "start": 684.56, "duration": 4.24}, {"text": "code into", "start": 686.24, "duration": 4.719}, {"text": "programming languages that work with amd", "start": 688.8, "duration": 3.52}, {"text": "gpus or", "start": 690.959, "duration": 4.801}, {"text": "intel gpus now as well so in some sense", "start": 692.32, "duration": 5.44}, {"text": "you know there's portability", "start": 695.76, "duration": 3.68}, {"text": "out there it's not too much to worry", "start": 697.76, "duration": 4.8}, {"text": "about and um", "start": 699.44, "duration": 6.16}, {"text": "i just want to point out you know how", "start": 702.56, "duration": 5.68}, {"text": "super computers or you know processes in", "start": 705.6, "duration": 5.44}, {"text": "general have developed over the last um", "start": 708.24, "duration": 4.32}, {"text": "20 years here right so if you look at", "start": 711.04, "duration": 4.64}, {"text": "the number one of the top 500 list", "start": 712.56, "duration": 5.2}, {"text": "that was at lawrence livermore national", "start": 715.68, "duration": 3.12}, {"text": "lab ascii vice", "start": 717.76, "duration": 2.72}, {"text": "the supercomputer it had a total", "start": 718.8, "duration": 5.92}, {"text": "performance of 12.3 teraflops", "start": 720.48, "duration": 6.88}, {"text": "um the machine cost 110 million us", "start": 724.72, "duration": 5.679}, {"text": "dollars in 2001 so 20 years ago right so", "start": 727.36, "duration": 4.64}, {"text": "really expensive now if you compare that", "start": 730.399, "duration": 4.641}, {"text": "to expanse um", "start": 732.0, "duration": 5.12}, {"text": "it's a much smaller machine in size you", "start": 735.04, "duration": 4.64}, {"text": "know doesn't fill the entire", "start": 737.12, "duration": 6.88}, {"text": "data center it has 728 cpu nodes that", "start": 739.68, "duration": 4.88}, {"text": "give us", "start": 744.0, "duration": 3.6}, {"text": "3.4 petaflops right so that's from", "start": 744.56, "duration": 4.719}, {"text": "teraflops to petaflops right", "start": 747.6, "duration": 3.6}, {"text": "the aggregate performance and there are", "start": 749.279, "duration": 4.721}, {"text": "52 gpu nodes each with four nvidia v100", "start": 751.2, "duration": 3.84}, {"text": "gpus", "start": 754.0, "duration": 4.16}, {"text": "those 52 nodes right um give us 1.6", "start": 755.04, "duration": 4.88}, {"text": "petaflops in double precision or three", "start": 758.16, "duration": 3.2}, {"text": "point three petaflops and single", "start": 759.92, "duration": 2.24}, {"text": "precision", "start": 761.36, "duration": 3.12}, {"text": "gpu performance and that's all for of", "start": 762.16, "duration": 3.04}, {"text": "course a 10", "start": 764.48, "duration": 3.359}, {"text": "million us dollars right if you want to", "start": 765.2, "duration": 3.52}, {"text": "build your own", "start": 767.839, "duration": 3.201}, {"text": "little super computer you can do that", "start": 768.72, "duration": 4.96}, {"text": "you know you buy these gaming gpus", "start": 771.04, "duration": 5.28}, {"text": "um and that costs you around four", "start": 773.68, "duration": 3.839}, {"text": "thousand dollars", "start": 776.32, "duration": 4.4}, {"text": "um if you get them at retail price and", "start": 777.519, "duration": 4.961}, {"text": "you see you get ten percent of the", "start": 780.72, "duration": 3.359}, {"text": "double precision performance of the", "start": 782.48, "duration": 3.599}, {"text": "entire supercomputer here", "start": 784.079, "duration": 4.56}, {"text": "from 20 years ago so it's pretty", "start": 786.079, "duration": 3.681}, {"text": "impressive right", "start": 788.639, "duration": 3.2}, {"text": "and you get 10 times the performance in", "start": 789.76, "duration": 4.879}, {"text": "single precision", "start": 791.839, "duration": 2.8}, {"text": "as the supercomputer so if you have an", "start": 794.8, "duration": 3.039}, {"text": "algorithm that you can implement", "start": 796.079, "duration": 4.481}, {"text": "efficiently in single precision you can", "start": 797.839, "duration": 3.841}, {"text": "run", "start": 800.56, "duration": 3.12}, {"text": "10 times as many simulations on this", "start": 801.68, "duration": 3.68}, {"text": "single", "start": 803.68, "duration": 4.24}, {"text": "you know desktop workstation as compared", "start": 805.36, "duration": 5.68}, {"text": "to that supercomputer 20 years ago so", "start": 807.92, "duration": 4.96}, {"text": "i hope i hope you all are going to make", "start": 811.04, "duration": 3.44}, {"text": "great discovery scientific specific", "start": 812.88, "duration": 4.0}, {"text": "discoveries with all that", "start": 814.48, "duration": 4.32}, {"text": "computing power that you have at your", "start": 816.88, "duration": 5.12}, {"text": "hands um", "start": 818.8, "duration": 6.32}, {"text": "gpu accelerator software now goes", "start": 822.0, "duration": 6.0}, {"text": "you know covers literally all the fields", "start": 825.12, "duration": 4.399}, {"text": "nvidia has a lot of information on their", "start": 828.0, "duration": 3.199}, {"text": "website it's actually quite good so if", "start": 829.519, "duration": 2.961}, {"text": "you go there and you want to explore a", "start": 831.199, "duration": 3.281}, {"text": "little bit you can find uh", "start": 832.48, "duration": 3.919}, {"text": "you know a software in your field you", "start": 834.48, "duration": 3.44}, {"text": "might find something new or some", "start": 836.399, "duration": 4.44}, {"text": "inspirations an example", "start": 837.92, "duration": 5.52}, {"text": "um i just wanted to briefly talk about", "start": 840.839, "duration": 4.281}, {"text": "machine learning a gpu is just a single", "start": 843.44, "duration": 2.639}, {"text": "slide and and", "start": 845.12, "duration": 4.8}, {"text": "and why um um that actually", "start": 846.079, "duration": 6.401}, {"text": "is such a big thing for gpus at the", "start": 849.92, "duration": 4.64}, {"text": "moment um so really so what you do", "start": 852.48, "duration": 4.479}, {"text": "in machine learning you have what you", "start": 854.56, "duration": 4.16}, {"text": "want to build is a predictive model so", "start": 856.959, "duration": 2.32}, {"text": "you have", "start": 858.72, "duration": 2.96}, {"text": "you have", "start": 859.279, "duration": 2.401}, {"text": "some data here that you train on right", "start": 863.519, "duration": 6.081}, {"text": "and you have a model that you implement", "start": 866.72, "duration": 3.28}, {"text": "and", "start": 869.6, "duration": 2.56}, {"text": "so that gives you your machine learning", "start": 870.0, "duration": 3.68}, {"text": "algorithm and then with some", "start": 872.16, "duration": 3.119}, {"text": "input data that's unknown you make a", "start": 873.68, "duration": 3.599}, {"text": "prediction and then you actually check", "start": 875.279, "duration": 3.041}, {"text": "whether the prediction", "start": 877.279, "duration": 3.36}, {"text": "so during the training you check whether", "start": 878.32, "duration": 4.16}, {"text": "the prediction comes out okay and once", "start": 880.639, "duration": 3.361}, {"text": "you're satisfied", "start": 882.48, "duration": 3.52}, {"text": "you can then use that algorithm to", "start": 884.0, "duration": 4.0}, {"text": "predict um", "start": 886.0, "duration": 5.6}, {"text": "of course results for unknown data", "start": 888.0, "duration": 6.959}, {"text": "and so deep learning is is the thing", "start": 891.6, "duration": 6.64}, {"text": "that works so well on gpus", "start": 894.959, "duration": 5.201}, {"text": "and that's basically a neural network", "start": 898.24, "duration": 3.68}, {"text": "with many hidden layers simplified", "start": 900.16, "duration": 3.359}, {"text": "and so so you have a neural network", "start": 901.92, "duration": 3.919}, {"text": "where you have like this input layer", "start": 903.519, "duration": 3.921}, {"text": "and an output layer and then you have", "start": 905.839, "duration": 3.521}, {"text": "these nodes in between", "start": 907.44, "duration": 4.0}, {"text": "um and this this is an example of a", "start": 909.36, "duration": 3.12}, {"text": "fully connected", "start": 911.44, "duration": 3.92}, {"text": "neural network and if you write um if", "start": 912.48, "duration": 4.799}, {"text": "you look at a deep neural network", "start": 915.36, "duration": 3.68}, {"text": "you know simplified basically all it is", "start": 917.279, "duration": 3.201}, {"text": "you know you have many hidden layers", "start": 919.04, "duration": 2.799}, {"text": "right and with that you can", "start": 920.48, "duration": 2.719}, {"text": "get many more variables and you", "start": 921.839, "duration": 3.12}, {"text": "potentially you know", "start": 923.199, "duration": 5.281}, {"text": "have a new high dimensional", "start": 924.959, "duration": 5.921}, {"text": "function that can fit practically", "start": 928.48, "duration": 4.24}, {"text": "anything that's the idea essentially", "start": 930.88, "duration": 3.199}, {"text": "right so if you have the right", "start": 932.72, "duration": 5.039}, {"text": "feature vectors here and", "start": 934.079, "duration": 5.44}, {"text": "if you write down the equations and look", "start": 937.759, "duration": 3.361}, {"text": "at how it looks like there's a lot of", "start": 939.519, "duration": 3.361}, {"text": "matrix multiplications the tensor", "start": 941.12, "duration": 3.68}, {"text": "operations", "start": 942.88, "duration": 3.92}, {"text": "involved and gpus are just very", "start": 944.8, "duration": 3.44}, {"text": "efficient at these because if you go", "start": 946.8, "duration": 3.279}, {"text": "back to the history of where gpus come", "start": 948.24, "duration": 3.76}, {"text": "from", "start": 950.079, "duration": 4.081}, {"text": "and then you think about how 3d graphics", "start": 952.0, "duration": 3.519}, {"text": "is implemented", "start": 954.16, "duration": 4.08}, {"text": "um there's a lot of 4x4 matrix algebra", "start": 955.519, "duration": 3.601}, {"text": "so they have", "start": 958.24, "duration": 3.279}, {"text": "very efficient hardware to do these", "start": 959.12, "duration": 3.839}, {"text": "matrix multiplication", "start": 961.519, "duration": 3.68}, {"text": "and in addition as i said you know gpu", "start": 962.959, "duration": 3.761}, {"text": "is now implemented", "start": 965.199, "duration": 4.481}, {"text": "in hardware half precision arithmetic um", "start": 966.72, "duration": 4.239}, {"text": "that can be used for many machine", "start": 969.68, "duration": 2.719}, {"text": "learning applications at least at the", "start": 970.959, "duration": 3.201}, {"text": "inference stage so when you", "start": 972.399, "duration": 3.44}, {"text": "make your predictions uh for the", "start": 974.16, "duration": 4.08}, {"text": "training often you have to use the", "start": 975.839, "duration": 5.521}, {"text": "uh at least 32-bit flows um it depends a", "start": 978.24, "duration": 4.719}, {"text": "little bit on the problem", "start": 981.36, "duration": 3.839}, {"text": "and then the newer architectures like", "start": 982.959, "duration": 3.921}, {"text": "the nvidia volta", "start": 985.199, "duration": 3.521}, {"text": "and the successors like the ampere", "start": 986.88, "duration": 3.28}, {"text": "architectures", "start": 988.72, "duration": 4.08}, {"text": "um they introduced tensor cores and", "start": 990.16, "duration": 4.08}, {"text": "that's dedicated hardware for mixed", "start": 992.8, "duration": 3.2}, {"text": "precision matrix multiplies", "start": 994.24, "duration": 4.399}, {"text": "so really super efficient um and if you", "start": 996.0, "duration": 4.24}, {"text": "have the machine learning framework then", "start": 998.639, "duration": 3.281}, {"text": "it's actually really simple", "start": 1000.24, "duration": 4.159}, {"text": "um because you know pytorch or", "start": 1001.92, "duration": 3.279}, {"text": "tensorflow", "start": 1004.399, "duration": 2.56}, {"text": "you just have to tell them to use the", "start": 1005.199, "duration": 3.921}, {"text": "gpus and they will generate the code", "start": 1006.959, "duration": 5.921}, {"text": "for the gpus and execute on the gpus as", "start": 1009.12, "duration": 5.12}, {"text": "of course in the cpu so it's pretty", "start": 1012.88, "duration": 2.56}, {"text": "straightforward so you don't really need", "start": 1014.24, "duration": 2.88}, {"text": "to know much about the", "start": 1015.44, "duration": 5.199}, {"text": "gpu programming and better", "start": 1017.12, "duration": 3.519}, {"text": "i want to quickly show you some", "start": 1022.56, "duration": 3.359}, {"text": "benchmarks um but", "start": 1024.4, "duration": 3.519}, {"text": "you like benchmarks or not you know you", "start": 1025.919, "duration": 5.52}, {"text": "can you can uh", "start": 1027.919, "duration": 3.52}, {"text": "often show data that", "start": 1032.559, "duration": 5.12}, {"text": "that you can tweak speak wentworth in a", "start": 1035.76, "duration": 3.679}, {"text": "way that you know whatever you want to", "start": 1037.679, "duration": 2.16}, {"text": "show", "start": 1039.439, "duration": 2.64}, {"text": "looks good for you um so that you have", "start": 1039.839, "duration": 3.281}, {"text": "to be a little bit careful with", "start": 1042.079, "duration": 2.72}, {"text": "benchmarks um but", "start": 1043.12, "duration": 4.959}, {"text": "um i i'll i just picked uh two examples", "start": 1044.799, "duration": 4.961}, {"text": "two three examples from my field i'm a", "start": 1048.079, "duration": 2.401}, {"text": "chemist", "start": 1049.76, "duration": 2.48}, {"text": "by background so quantum chemistry", "start": 1050.48, "duration": 3.199}, {"text": "molecular dynamics um", "start": 1052.24, "duration": 4.319}, {"text": "background and so this is an example", "start": 1053.679, "duration": 3.841}, {"text": "this was the", "start": 1056.559, "duration": 3.601}, {"text": "earliest real code and it's still around", "start": 1057.52, "duration": 3.519}, {"text": "you know that", "start": 1060.16, "duration": 3.28}, {"text": "high performance code that does um", "start": 1061.039, "duration": 4.401}, {"text": "quantum chemistry calculations on gpus", "start": 1063.44, "duration": 2.88}, {"text": "and", "start": 1065.44, "duration": 3.68}, {"text": "you can see you know they they this is", "start": 1066.32, "duration": 4.64}, {"text": "an old comparison from", "start": 1069.12, "duration": 3.76}, {"text": "from many years ago from their website", "start": 1070.96, "duration": 4.079}, {"text": "but you know a single gaming gpu versus", "start": 1072.88, "duration": 3.919}, {"text": "the single cpu core and", "start": 1075.039, "duration": 3.921}, {"text": "back then there were not so many cores", "start": 1076.799, "duration": 4.081}, {"text": "in each pixel in any case but you see", "start": 1078.96, "duration": 3.36}, {"text": "you know", "start": 1080.88, "duration": 3.12}, {"text": "in particular for larger calculations", "start": 1082.32, "duration": 2.88}, {"text": "larger molecules you can have", "start": 1084.0, "duration": 2.559}, {"text": "significant speed of the", "start": 1085.2, "duration": 4.0}, {"text": "you know showing speed ups here around", "start": 1086.559, "duration": 5.841}, {"text": "30 to 176", "start": 1089.2, "duration": 5.599}, {"text": "and so even if you have a multi-core cpu", "start": 1092.4, "duration": 3.519}, {"text": "you can't beat", "start": 1094.799, "duration": 4.24}, {"text": "a single gpu here um yes", "start": 1095.919, "duration": 4.561}, {"text": "similar so we have our own quantum", "start": 1099.039, "duration": 3.121}, {"text": "chemistry code", "start": 1100.48, "duration": 3.52}, {"text": "trick that we're developing and and you", "start": 1102.16, "duration": 4.399}, {"text": "see the same you know these are", "start": 1104.0, "duration": 4.4}, {"text": "a certain piece to the quantum", "start": 1106.559, "duration": 3.681}, {"text": "mechanical hamiltonian that we compute", "start": 1108.4, "duration": 4.88}, {"text": "and um for different parts of the", "start": 1110.24, "duration": 5.439}, {"text": "calculation you know we also speed up", "start": 1113.28, "duration": 4.48}, {"text": "anywhere between like 50 and 200 and the", "start": 1115.679, "duration": 4.0}, {"text": "important part is here on the right", "start": 1117.76, "duration": 3.48}, {"text": "plot where you see the time", "start": 1119.679, "duration": 3.841}, {"text": "logarithmically and the number of", "start": 1121.24, "duration": 5.16}, {"text": "cpu cores right so this was a benchmark", "start": 1123.52, "duration": 3.76}, {"text": "that was run", "start": 1126.4, "duration": 3.12}, {"text": "on up to 40 cpu cores with an mpi", "start": 1127.28, "duration": 4.48}, {"text": "parallel implementation", "start": 1129.52, "duration": 5.12}, {"text": "and so you see this is these are two", "start": 1131.76, "duration": 4.72}, {"text": "pieces of the calculation that are", "start": 1134.64, "duration": 3.84}, {"text": "computationally expensive", "start": 1136.48, "duration": 3.68}, {"text": "the important part is the dashed line", "start": 1138.48, "duration": 4.88}, {"text": "that's the time to solution on the gpu", "start": 1140.16, "duration": 6.16}, {"text": "and it's faster than than running on a", "start": 1143.36, "duration": 3.439}, {"text": "on a", "start": 1146.32, "duration": 2.96}, {"text": "full compute node right so our energy q", "start": 1146.799, "duration": 4.161}, {"text": "is faster than", "start": 1149.28, "duration": 5.92}, {"text": "our computer um", "start": 1150.96, "duration": 4.24}, {"text": "molecular dynamics is another example", "start": 1155.36, "duration": 3.36}, {"text": "where you know we have to simulate", "start": 1157.12, "duration": 5.12}, {"text": "you know the interactions of particles", "start": 1158.72, "duration": 4.319}, {"text": "you know", "start": 1162.24, "duration": 3.04}, {"text": "in in and there's thousands and", "start": 1163.039, "duration": 4.721}, {"text": "thousands of atoms in these", "start": 1165.28, "duration": 4.56}, {"text": "simulations that we are doing and the", "start": 1167.76, "duration": 3.68}, {"text": "relevant time scales that we want to", "start": 1169.84, "duration": 2.4}, {"text": "achieve", "start": 1171.44, "duration": 3.84}, {"text": "is somewhere here in the um micro to", "start": 1172.24, "duration": 4.48}, {"text": "millisecond second", "start": 1175.28, "duration": 3.68}, {"text": "time scale but the time steps we have to", "start": 1176.72, "duration": 4.959}, {"text": "take is in the sample second time scale", "start": 1178.96, "duration": 4.64}, {"text": "and you know one times that step has to", "start": 1181.679, "duration": 3.441}, {"text": "be done after another one so if you want", "start": 1183.6, "duration": 3.439}, {"text": "to finish with the simulations within", "start": 1185.12, "duration": 5.28}, {"text": "a reasonable amount of uh time", "start": 1187.039, "duration": 4.801}, {"text": "you know the simulations have to be", "start": 1190.4, "duration": 3.92}, {"text": "really fast at each time step", "start": 1191.84, "duration": 5.76}, {"text": "and uh this is performance data um", "start": 1194.32, "duration": 6.32}, {"text": "you know a benchmarked um for the amber", "start": 1197.6, "duration": 4.4}, {"text": "and d codes that", "start": 1200.64, "duration": 4.48}, {"text": "we have been developing um", "start": 1202.0, "duration": 6.48}, {"text": "on on cpus here so this is up to 36", "start": 1205.12, "duration": 5.84}, {"text": "cores and this is on a single gpu", "start": 1208.48, "duration": 5.52}, {"text": "so you see it can a single gpu this is", "start": 1210.96, "duration": 6.64}, {"text": "this is the v100 that's the gpu that's", "start": 1214.0, "duration": 6.559}, {"text": "deployed in expands we effectively get", "start": 1217.6, "duration": 3.52}, {"text": "like", "start": 1220.559, "duration": 3.681}, {"text": "25 times as much um computational", "start": 1221.12, "duration": 5.36}, {"text": "throughput as on this you know cpu right", "start": 1224.24, "duration": 5.76}, {"text": "there's probably some some leeway some", "start": 1226.48, "duration": 5.28}, {"text": "room to improve the cpu code but not", "start": 1230.0, "duration": 2.799}, {"text": "much", "start": 1231.76, "duration": 3.36}, {"text": "and this is because also partially we", "start": 1232.799, "duration": 5.12}, {"text": "exploit single precision", "start": 1235.12, "duration": 4.4}, {"text": "uh now what's the problem with that and", "start": 1237.919, "duration": 3.441}, {"text": "that's why you are all here is", "start": 1239.52, "duration": 4.08}, {"text": "literally you know you know you want to", "start": 1241.36, "duration": 3.6}, {"text": "use gpus because", "start": 1243.6, "duration": 2.88}, {"text": "you want to cut down on your electricity", "start": 1244.96, "duration": 3.199}, {"text": "costs and finish the simulation faster", "start": 1246.48, "duration": 2.16}, {"text": "and", "start": 1248.159, "duration": 3.121}, {"text": "you know um and then the manager asks", "start": 1248.64, "duration": 3.12}, {"text": "you why is", "start": 1251.28, "duration": 3.6}, {"text": "your part taking so long", "start": 1251.76, "duration": 3.12}, {"text": "and it has to do with the gpu", "start": 1255.12, "duration": 3.36}, {"text": "architecture you can't just", "start": 1256.96, "duration": 3.04}, {"text": "you know take your code and compile it", "start": 1258.48, "duration": 4.96}, {"text": "for the gpu you have to actually develop", "start": 1260.0, "duration": 5.6}, {"text": "algorithms that have run effectively on", "start": 1263.44, "duration": 4.08}, {"text": "efficiently on the gpu", "start": 1265.6, "duration": 4.0}, {"text": "and so what's depicted here on the left", "start": 1267.52, "duration": 3.76}, {"text": "is schematically", "start": 1269.6, "duration": 3.92}, {"text": "and on the right you see for a cpu and", "start": 1271.28, "duration": 3.84}, {"text": "for a gpu on the right", "start": 1273.52, "duration": 3.92}, {"text": "on the left for the cpu this is the real", "start": 1275.12, "duration": 3.919}, {"text": "estate so the", "start": 1277.44, "duration": 5.2}, {"text": "uh depicted the", "start": 1279.039, "duration": 6.081}, {"text": "um area of your chip of the of the", "start": 1282.64, "duration": 4.0}, {"text": "processor right", "start": 1285.12, "duration": 5.039}, {"text": "um and so in a cpu you have a you know", "start": 1286.64, "duration": 6.96}, {"text": "um a lot of uh cache memory you have a", "start": 1290.159, "duration": 5.681}, {"text": "lot of control units", "start": 1293.6, "duration": 4.24}, {"text": "with so you have lots of sophisticated", "start": 1295.84, "duration": 3.52}, {"text": "hardware in there and then only a few", "start": 1297.84, "duration": 3.36}, {"text": "processing course arithmetic logical", "start": 1299.36, "duration": 3.52}, {"text": "units here", "start": 1301.2, "duration": 3.28}, {"text": "so the cpu does a lot of things for you", "start": 1302.88, "duration": 3.6}, {"text": "like multi-level caching pre-fashion", "start": 1304.48, "duration": 3.439}, {"text": "branch predictions all these things that", "start": 1306.48, "duration": 2.96}, {"text": "happen under the hood", "start": 1307.919, "duration": 3.841}, {"text": "um that make your code run as fast as", "start": 1309.44, "duration": 3.599}, {"text": "possible on a single", "start": 1311.76, "duration": 5.08}, {"text": "core now that's very different on a gpu", "start": 1313.039, "duration": 6.801}, {"text": "um there's much less control logic but", "start": 1316.84, "duration": 5.4}, {"text": "in exchange you have many more uh", "start": 1319.84, "duration": 3.839}, {"text": "compute cores that are much more", "start": 1322.24, "duration": 2.559}, {"text": "simplistic", "start": 1323.679, "duration": 3.081}, {"text": "um those are packaged into individual", "start": 1324.799, "duration": 3.12}, {"text": "multiprocessors", "start": 1326.76, "duration": 3.48}, {"text": "so it's a little bit as if you would", "start": 1327.919, "duration": 3.601}, {"text": "have like", "start": 1330.24, "duration": 4.88}, {"text": "several multi-core processors on each", "start": 1331.52, "duration": 6.639}, {"text": "in each gpu and", "start": 1335.12, "duration": 6.08}, {"text": "these processors they their cores they", "start": 1338.159, "duration": 4.321}, {"text": "operate in lockstep", "start": 1341.2, "duration": 3.44}, {"text": "so there's a single instruction issued", "start": 1342.48, "duration": 3.679}, {"text": "for multiple threads those", "start": 1344.64, "duration": 4.24}, {"text": "simp architecture and it does also", "start": 1346.159, "duration": 4.64}, {"text": "vectorize loads and stores to memory and", "start": 1348.88, "duration": 3.2}, {"text": "and you need to manage the memory", "start": 1350.799, "duration": 2.961}, {"text": "hierarchy uh", "start": 1352.08, "duration": 3.599}, {"text": "now operating in lockstep that means you", "start": 1353.76, "duration": 4.08}, {"text": "need to write a parallel algorithm that", "start": 1355.679, "duration": 4.801}, {"text": "exploits that and connects like that so", "start": 1357.84, "duration": 3.12}, {"text": "here", "start": 1360.48, "duration": 2.72}, {"text": "i'm showing you a very very old graphic", "start": 1360.96, "duration": 4.64}, {"text": "from nvidia so this is like 2008 and you", "start": 1363.2, "duration": 3.52}, {"text": "wonder oh", "start": 1365.6, "duration": 3.6}, {"text": "this is very old what he's showing but", "start": 1366.72, "duration": 4.16}, {"text": "um so this was the", "start": 1369.2, "duration": 4.959}, {"text": "first data center gpu server", "start": 1370.88, "duration": 5.6}, {"text": "from nvidia and i'm showing this because", "start": 1374.159, "duration": 4.4}, {"text": "there's not really not much in", "start": 1376.48, "duration": 3.84}, {"text": "principle that has changed on the very", "start": 1378.559, "duration": 3.681}, {"text": "basics right so this is your", "start": 1380.32, "duration": 4.56}, {"text": "gpu processor and you know they have the", "start": 1382.24, "duration": 4.72}, {"text": "same type of processor either in a pci", "start": 1384.88, "duration": 2.56}, {"text": "express", "start": 1386.96, "duration": 4.079}, {"text": "card or uh in in chips", "start": 1387.44, "duration": 6.32}, {"text": "um yeah no in this case yeah they were", "start": 1391.039, "duration": 4.481}, {"text": "connected by a pci expresso so you can", "start": 1393.76, "duration": 3.36}, {"text": "see here two graphics", "start": 1395.52, "duration": 5.68}, {"text": "cards in there as far as i remember um", "start": 1397.12, "duration": 7.36}, {"text": "and each of those gpu processes has", "start": 1401.2, "duration": 5.28}, {"text": "several multiprocessors and each of", "start": 1404.48, "duration": 3.36}, {"text": "these multiprocessors", "start": 1406.48, "duration": 4.079}, {"text": "right has its own uh instruction cache", "start": 1407.84, "duration": 3.28}, {"text": "and the", "start": 1410.559, "duration": 3.841}, {"text": "um and there's a multi-thread issuer", "start": 1411.12, "duration": 4.32}, {"text": "that issues", "start": 1414.4, "duration": 4.48}, {"text": "you know um the instructions to the", "start": 1415.44, "duration": 5.04}, {"text": "processors you have the single precision", "start": 1418.88, "duration": 3.2}, {"text": "compute cores you have double precision", "start": 1420.48, "duration": 2.64}, {"text": "compute cores", "start": 1422.08, "duration": 2.88}, {"text": "special function units and shared memory", "start": 1423.12, "duration": 3.84}, {"text": "that is shared for all those", "start": 1424.96, "duration": 3.28}, {"text": "processors that are sitting in one of", "start": 1426.96, "duration": 4.079}, {"text": "those multiprocessors", "start": 1428.24, "duration": 5.679}, {"text": "and so one of the important things this", "start": 1431.039, "duration": 4.24}, {"text": "architecture can handle many more", "start": 1433.919, "duration": 2.961}, {"text": "threats than processing cores and that's", "start": 1435.279, "duration": 2.081}, {"text": "one way", "start": 1436.88, "duration": 3.679}, {"text": "actually to hide latency", "start": 1437.36, "duration": 3.199}, {"text": "you know the hardware has changed over", "start": 1442.159, "duration": 3.361}, {"text": "the years but the basics have remained", "start": 1443.919, "duration": 3.521}, {"text": "the same so you know", "start": 1445.52, "duration": 3.36}, {"text": "if i say the hardware character is", "start": 1447.44, "duration": 3.04}, {"text": "exchanged across gpu models and for", "start": 1448.88, "duration": 2.399}, {"text": "generations", "start": 1450.48, "duration": 3.12}, {"text": "that's right but that doesn't have to", "start": 1451.279, "duration": 4.0}, {"text": "affect the way with which you write your", "start": 1453.6, "duration": 2.48}, {"text": "code so yeah", "start": 1455.279, "duration": 2.88}, {"text": "of course gpus has improved similar to", "start": 1456.08, "duration": 3.44}, {"text": "gpus", "start": 1458.159, "duration": 2.88}, {"text": "but one thing that's important is you", "start": 1459.52, "duration": 3.039}, {"text": "know memory hierarchy", "start": 1461.039, "duration": 4.081}, {"text": "is exposed and it doesn't have to be", "start": 1462.559, "duration": 4.081}, {"text": "there ways to get around that but", "start": 1465.12, "duration": 4.32}, {"text": "uh partially but you know uh you have to", "start": 1466.64, "duration": 4.8}, {"text": "be available cpu and gpu memory and then", "start": 1469.44, "duration": 2.56}, {"text": "there's", "start": 1471.44, "duration": 2.32}, {"text": "other types of memories in the gpu that", "start": 1472.0, "duration": 3.039}, {"text": "you can use", "start": 1473.76, "duration": 3.039}, {"text": "um so unipod memory is something that", "start": 1475.039, "duration": 4.321}, {"text": "exists that helps you um", "start": 1476.799, "duration": 3.76}, {"text": "you don't have to distinguish between", "start": 1479.36, "duration": 3.199}, {"text": "cpu and gpu memory but of course behind", "start": 1480.559, "duration": 3.681}, {"text": "the scenes the data still needs to be", "start": 1482.559, "duration": 3.6}, {"text": "copied for them back", "start": 1484.24, "duration": 3.28}, {"text": "and then there's of course nvidia and", "start": 1486.159, "duration": 3.361}, {"text": "amd and we have nvidia processors on", "start": 1487.52, "duration": 4.0}, {"text": "expand so just we'll talk about those", "start": 1489.52, "duration": 4.96}, {"text": "and maybe i'll just skip this because we", "start": 1491.52, "duration": 4.56}, {"text": "don't have much time", "start": 1494.48, "duration": 5.6}, {"text": "um different types of gaming gpus", "start": 1496.08, "duration": 6.4}, {"text": "and in any case so what what you want to", "start": 1500.08, "duration": 4.0}, {"text": "do is you never write code with any", "start": 1502.48, "duration": 3.04}, {"text": "assumption for how many threads it will", "start": 1504.08, "duration": 3.36}, {"text": "use there are functions in tutor", "start": 1505.52, "duration": 3.68}, {"text": "to query your hardware and and you", "start": 1507.44, "duration": 3.28}, {"text": "launch more threads than processing", "start": 1509.2, "duration": 1.92}, {"text": "cores", "start": 1510.72, "duration": 3.52}, {"text": "and to hide memory latency and if you", "start": 1511.12, "duration": 4.48}, {"text": "can avoid double precision", "start": 1514.24, "duration": 3.919}, {"text": "to avoid it um because then you get", "start": 1515.6, "duration": 3.84}, {"text": "higher performance but of course you", "start": 1518.159, "duration": 2.88}, {"text": "need to make sure that", "start": 1519.44, "duration": 3.119}, {"text": "this doesn't affect the accuracy of your", "start": 1521.039, "duration": 3.52}, {"text": "numerical simulation", "start": 1522.559, "duration": 4.961}, {"text": "um a brief overview there's opens opencl", "start": 1524.559, "duration": 3.441}, {"text": "that you can", "start": 1527.52, "duration": 2.48}, {"text": "use it works for nvidia and amd gpus", "start": 1528.0, "duration": 4.159}, {"text": "there's cuda that's um proprietary", "start": 1530.0, "duration": 4.4}, {"text": "but um you know open source so when we", "start": 1532.159, "duration": 3.201}, {"text": "can", "start": 1534.4, "duration": 4.0}, {"text": "just you know nothing's hidden there uh", "start": 1535.36, "duration": 4.96}, {"text": "it works only for nvidia gpus but it's", "start": 1538.4, "duration": 3.44}, {"text": "the defacto center for high performance", "start": 1540.32, "duration": 3.52}, {"text": "code", "start": 1541.84, "duration": 4.0}, {"text": "and then there's open acc that's", "start": 1543.84, "duration": 4.8}, {"text": "accelerator directive", "start": 1545.84, "duration": 4.719}, {"text": "when video and amd efficient", "start": 1548.64, "duration": 3.84}, {"text": "implementation at the moment only for", "start": 1550.559, "duration": 4.161}, {"text": "nvidia", "start": 1552.48, "duration": 4.48}, {"text": "it works the b c plus stuff in fortran", "start": 1554.72, "duration": 4.319}, {"text": "similar to openmp and then openmp also", "start": 1556.96, "duration": 3.44}, {"text": "supports gpus but", "start": 1559.039, "duration": 3.601}, {"text": "um it's not really mature yet so you", "start": 1560.4, "duration": 3.68}, {"text": "might want to", "start": 1562.64, "duration": 4.72}, {"text": "um you know follow the developments", "start": 1564.08, "duration": 3.839}, {"text": "there", "start": 1567.36, "duration": 3.52}, {"text": "if you're interested now if we go to", "start": 1567.919, "duration": 5.041}, {"text": "nvidia we have nvidia gpus you know you", "start": 1570.88, "duration": 3.279}, {"text": "have", "start": 1572.96, "duration": 3.12}, {"text": "a lot of libraries available for deep", "start": 1574.159, "duration": 3.041}, {"text": "learning qdn", "start": 1576.08, "duration": 3.68}, {"text": "class fourier transforms you know linear", "start": 1577.2, "duration": 4.24}, {"text": "algebra and so on", "start": 1579.76, "duration": 4.24}, {"text": "um and you know their support for", "start": 1581.44, "duration": 3.92}, {"text": "basically all", "start": 1584.0, "duration": 3.12}, {"text": "well you know many programming languages", "start": 1585.36, "duration": 3.919}, {"text": "you know natively cc plus plus um", "start": 1587.12, "duration": 4.799}, {"text": "because q does an expansion to c and uh", "start": 1589.279, "duration": 4.081}, {"text": "but also fortune and", "start": 1591.919, "duration": 4.081}, {"text": "and then their wrap has for java pipe", "start": 1593.36, "duration": 3.52}, {"text": "and", "start": 1596.0, "duration": 2.799}, {"text": "of course directive phrase programming", "start": 1596.88, "duration": 3.12}, {"text": "and", "start": 1598.799, "duration": 6.081}, {"text": "yeah basically any nvidia gpu is", "start": 1600.0, "duration": 6.48}, {"text": "cute enabled so if you write code that", "start": 1604.88, "duration": 4.88}, {"text": "runs on expands it will run also on your", "start": 1606.48, "duration": 6.16}, {"text": "desktop computer or laptop depending on", "start": 1609.76, "duration": 5.68}, {"text": "what type of gpu you have", "start": 1612.64, "duration": 4.48}, {"text": "development tools there is a so-called", "start": 1615.44, "duration": 3.599}, {"text": "filter toolkit that's free", "start": 1617.12, "duration": 5.039}, {"text": "contains computer c compiler libraries", "start": 1619.039, "duration": 6.24}, {"text": "debugging tools there's a debugger just", "start": 1622.159, "duration": 4.4}, {"text": "qh gtv and", "start": 1625.279, "duration": 3.921}, {"text": "amp qr mem check for memory leaks then", "start": 1626.559, "duration": 5.041}, {"text": "there's profiling tools and we prob nvbt", "start": 1629.2, "duration": 3.68}, {"text": "so and we process the", "start": 1631.6, "duration": 3.84}, {"text": "command line profiler that's a visual", "start": 1632.88, "duration": 5.2}, {"text": "profile that gives you a visual overview", "start": 1635.44, "duration": 5.2}, {"text": "these are being slowly phased out um by", "start": 1638.08, "duration": 3.36}, {"text": "nvidia", "start": 1640.64, "duration": 2.56}, {"text": "in favor of their enzyme systems and", "start": 1641.44, "duration": 5.68}, {"text": "computer inside systems is the um", "start": 1643.2, "duration": 3.92}, {"text": "high level profiling tool it gives an", "start": 1647.84, "duration": 4.16}, {"text": "overview over the entire runtime of the", "start": 1650.159, "duration": 2.561}, {"text": "program", "start": 1652.0, "duration": 3.279}, {"text": "shows you you know when cpu code is", "start": 1652.72, "duration": 4.559}, {"text": "running when memory copies happen", "start": 1655.279, "duration": 4.801}, {"text": "which kernels run at which time and then", "start": 1657.279, "duration": 4.561}, {"text": "with inside compute you can get", "start": 1660.08, "duration": 3.76}, {"text": "more statistics and data about", "start": 1661.84, "duration": 3.04}, {"text": "individual kernels", "start": 1663.84, "duration": 3.28}, {"text": "if you get down into actually optimizing", "start": 1664.88, "duration": 3.44}, {"text": "individual", "start": 1667.12, "duration": 2.799}, {"text": "functions so kernels or functions that", "start": 1668.32, "duration": 4.56}, {"text": "you've written for your um", "start": 1669.919, "duration": 6.0}, {"text": "gpu and this this i always like very", "start": 1672.88, "duration": 4.399}, {"text": "much in particular in particular", "start": 1675.919, "duration": 2.561}, {"text": "beginner", "start": 1677.279, "duration": 4.161}, {"text": "code samples um which are contained in", "start": 1678.48, "duration": 4.559}, {"text": "there uh", "start": 1681.44, "duration": 4.479}, {"text": "and uh i hope i'll get to that once i", "start": 1683.039, "duration": 3.441}, {"text": "get there", "start": 1685.919, "duration": 4.24}, {"text": "um that's where you can get them now if", "start": 1686.48, "duration": 5.679}, {"text": "you are on expanse if you are on a gpu", "start": 1690.159, "duration": 2.961}, {"text": "node what you can", "start": 1692.159, "duration": 2.721}, {"text": "what you have to do all you have to do", "start": 1693.12, "duration": 3.039}, {"text": "is you have to load the module", "start": 1694.88, "duration": 4.48}, {"text": "um i always recommend to say module", "start": 1696.159, "duration": 4.961}, {"text": "purge first to remove all the modules", "start": 1699.36, "duration": 3.28}, {"text": "and then module reset because we have", "start": 1701.12, "duration": 3.439}, {"text": "different types of modules on the cpu", "start": 1702.64, "duration": 3.36}, {"text": "nodes and the gpu node so you want to", "start": 1704.559, "duration": 3.041}, {"text": "make sure there's nothing left over from", "start": 1706.0, "duration": 3.2}, {"text": "your configuration from the docking node", "start": 1707.6, "duration": 2.72}, {"text": "etc so just", "start": 1709.2, "duration": 2.64}, {"text": "it's safe to put these two things in", "start": 1710.32, "duration": 2.8}, {"text": "there and then explicitly load the", "start": 1711.84, "duration": 3.199}, {"text": "modules that you want to have", "start": 1713.12, "duration": 5.2}, {"text": "um so module not computer will load cuda", "start": 1715.039, "duration": 4.64}, {"text": "11.", "start": 1718.32, "duration": 4.4}, {"text": "uh qr10.2 is also available and", "start": 1719.679, "duration": 5.281}, {"text": "the the the code samples unfortunately", "start": 1722.72, "duration": 4.319}, {"text": "are included and work on this computer", "start": 1724.96, "duration": 4.319}, {"text": "10.2 in the installation.com", "start": 1727.039, "duration": 6.081}, {"text": "expands um then there's also the nvidia", "start": 1729.279, "duration": 4.4}, {"text": "hpc", "start": 1733.12, "duration": 3.12}, {"text": "sdk that's also available this will", "start": 1733.679, "duration": 4.401}, {"text": "replace the qr toolkit in the future", "start": 1736.24, "duration": 4.64}, {"text": "and it's basically the cuda toolkit plus", "start": 1738.08, "duration": 4.479}, {"text": "some extras and those extras", "start": 1740.88, "duration": 5.76}, {"text": "are the nvidia t citrus plus and fortran", "start": 1742.559, "duration": 5.84}, {"text": "compilers", "start": 1746.64, "duration": 4.96}, {"text": "those used to be pgi compilers", "start": 1748.399, "duration": 5.52}, {"text": "there's also and the pgi compilers are", "start": 1751.6, "duration": 4.959}, {"text": "also available on", "start": 1753.919, "duration": 4.721}, {"text": "but in going forward those those are", "start": 1756.559, "duration": 3.921}, {"text": "being merged because nvidia brought pgi", "start": 1758.64, "duration": 3.2}, {"text": "many years ago and now", "start": 1760.48, "duration": 3.04}, {"text": "you know all the software infrastructure", "start": 1761.84, "duration": 3.199}, {"text": "and software development infrastructure", "start": 1763.52, "duration": 3.6}, {"text": "is being", "start": 1765.039, "duration": 4.961}, {"text": "put into a single package and very", "start": 1767.12, "duration": 4.72}, {"text": "simple you just say module load any hpc", "start": 1770.0, "duration": 4.72}, {"text": "and then it's available to you", "start": 1771.84, "duration": 6.4}, {"text": "and yeah so okay so how would you", "start": 1774.72, "duration": 5.439}, {"text": "write or develop applications so that's", "start": 1778.24, "duration": 3.039}, {"text": "what i'm going to do i'm going to show", "start": 1780.159, "duration": 1.601}, {"text": "you", "start": 1781.279, "duration": 4.161}, {"text": "uh quickly libraries uh then cuda then", "start": 1781.76, "duration": 4.96}, {"text": "open acc and", "start": 1785.44, "duration": 4.16}, {"text": "and um then we'll look at how to get", "start": 1786.72, "duration": 4.0}, {"text": "into compute nodes", "start": 1789.6, "duration": 4.4}, {"text": "on expand um", "start": 1790.72, "duration": 5.6}, {"text": "so if you have libraries available if", "start": 1794.0, "duration": 3.919}, {"text": "you have a code that calls library save", "start": 1796.32, "duration": 4.32}, {"text": "last functions for instance or ssts or", "start": 1797.919, "duration": 4.161}, {"text": "you know you can just use those", "start": 1800.64, "duration": 3.6}, {"text": "libraries and and that should be pretty", "start": 1802.08, "duration": 2.959}, {"text": "straightforward", "start": 1804.24, "duration": 3.28}, {"text": "it's a drop-in replacement because these", "start": 1805.039, "duration": 4.081}, {"text": "follow standard apis", "start": 1807.52, "duration": 4.0}, {"text": "so minimal code changes are required and", "start": 1809.12, "duration": 4.32}, {"text": "of course do use those because they are", "start": 1811.52, "duration": 3.039}, {"text": "very performant", "start": 1813.44, "duration": 3.44}, {"text": "tuned and high quality implementations", "start": 1814.559, "duration": 3.521}, {"text": "so don't write your", "start": 1816.88, "duration": 3.039}, {"text": "own matrix multiplication apart from", "start": 1818.08, "duration": 4.079}, {"text": "learning i have a matrix multiplication", "start": 1819.919, "duration": 4.64}, {"text": "example in here that i'll", "start": 1822.159, "duration": 4.561}, {"text": "very quickly go over with you just so", "start": 1824.559, "duration": 3.681}, {"text": "you get an idea how cuter code looks", "start": 1826.72, "duration": 3.28}, {"text": "like", "start": 1828.24, "duration": 5.039}, {"text": "if you go to this website there's a list", "start": 1830.0, "duration": 5.36}, {"text": "and lots of explanations of all sorts of", "start": 1833.279, "duration": 5.12}, {"text": "libraries that are available", "start": 1835.36, "duration": 6.48}, {"text": "um so you know if you have", "start": 1838.399, "duration": 6.16}, {"text": "a c plus plus a fortune code and and you", "start": 1841.84, "duration": 4.24}, {"text": "want to use the library it's pretty", "start": 1844.559, "duration": 2.24}, {"text": "simple right", "start": 1846.08, "duration": 2.719}, {"text": "imagine you have a stack speed that's a", "start": 1846.799, "duration": 3.6}, {"text": "glass function that's a signal precision", "start": 1848.799, "duration": 2.081}, {"text": "a times", "start": 1850.399, "duration": 4.721}, {"text": "x plus y um it's a vector addition", "start": 1850.88, "duration": 5.84}, {"text": "uh you just replace it with a called the", "start": 1855.12, "duration": 3.76}, {"text": "cube velocity and and then you have to", "start": 1856.72, "duration": 3.679}, {"text": "do some additional things", "start": 1858.88, "duration": 3.279}, {"text": "managing data locality meaning you know", "start": 1860.399, "duration": 4.321}, {"text": "you need to allocate memory on your gpu", "start": 1862.159, "duration": 3.36}, {"text": "you need to", "start": 1864.72, "duration": 3.679}, {"text": "transfer the input data to the gpu and", "start": 1865.519, "duration": 4.321}, {"text": "there are cube loss functions for that", "start": 1868.399, "duration": 2.64}, {"text": "as well", "start": 1869.84, "duration": 4.4}, {"text": "um so you can use cuda or wrappers", "start": 1871.039, "duration": 6.321}, {"text": "specifically for vectors or arrays or", "start": 1874.24, "duration": 6.48}, {"text": "um by q class and then you just rebuild", "start": 1877.36, "duration": 7.199}, {"text": "your code and link against the library", "start": 1880.72, "duration": 5.6}, {"text": "and so imagine you have this function", "start": 1884.559, "duration": 4.0}, {"text": "here a call to fax view", "start": 1886.32, "duration": 5.12}, {"text": "and it's the size of your problem so you", "start": 1888.559, "duration": 3.681}, {"text": "compute r", "start": 1891.44, "duration": 3.92}, {"text": "y equals a times x plus y so y and x are", "start": 1892.24, "duration": 3.679}, {"text": "input", "start": 1895.36, "duration": 4.159}, {"text": "y is also output um and", "start": 1895.919, "duration": 5.841}, {"text": "yeah this is just the strike in the", "start": 1899.519, "duration": 3.441}, {"text": "arrays and the", "start": 1901.76, "duration": 4.24}, {"text": "last function call now all you do is", "start": 1902.96, "duration": 5.439}, {"text": "um you have to create it with a uh", "start": 1906.0, "duration": 4.08}, {"text": "replace it with the call to q plus", "start": 1908.399, "duration": 4.481}, {"text": "black screen um and pass it a handle", "start": 1910.08, "duration": 4.0}, {"text": "that basically has", "start": 1912.88, "duration": 4.56}, {"text": "the um uh you know", "start": 1914.08, "duration": 5.04}, {"text": "some memory allocations and similar", "start": 1917.44, "duration": 3.68}, {"text": "things on the gpu", "start": 1919.12, "duration": 3.84}, {"text": "you can you can create that handle and", "start": 1921.12, "duration": 5.12}, {"text": "then destroy it again", "start": 1922.96, "duration": 3.28}, {"text": "then you call the allocation of memory", "start": 1926.96, "duration": 3.12}, {"text": "allocation", "start": 1929.12, "duration": 2.48}, {"text": "where you need to know how much memory", "start": 1930.08, "duration": 3.28}, {"text": "you need and you", "start": 1931.6, "duration": 3.6}, {"text": "store that in some pointers here on the", "start": 1933.36, "duration": 3.84}, {"text": "device it's often you", "start": 1935.2, "duration": 5.199}, {"text": "see that prefix where i will see the d", "start": 1937.2, "duration": 5.599}, {"text": "underscore to tell the programmer to", "start": 1940.399, "duration": 4.241}, {"text": "understand that okay b is actually", "start": 1942.799, "duration": 5.201}, {"text": "a pointer to memory on the gpu right", "start": 1944.64, "duration": 6.8}, {"text": "and when you transfer the data from the", "start": 1948.0, "duration": 5.039}, {"text": "cpu so you have it in", "start": 1951.44, "duration": 4.32}, {"text": "x right uh you transfer it to the", "start": 1953.039, "duration": 3.441}, {"text": "pointer", "start": 1955.76, "duration": 2.56}, {"text": "to the memory address on the gpu and you", "start": 1956.48, "duration": 3.52}, {"text": "use this two plus that vector function", "start": 1958.32, "duration": 2.8}, {"text": "for that", "start": 1960.0, "duration": 3.84}, {"text": "then you call your function and then you", "start": 1961.12, "duration": 4.08}, {"text": "copy the data back", "start": 1963.84, "duration": 4.16}, {"text": "and and that's it so as simple as that", "start": 1965.2, "duration": 3.599}, {"text": "you can get", "start": 1968.0, "duration": 2.399}, {"text": "you know gpu acceleration if you have a", "start": 1968.799, "duration": 3.841}, {"text": "computer-intensive part of your code", "start": 1970.399, "duration": 6.561}, {"text": "um that you want to replace with um", "start": 1972.64, "duration": 7.36}, {"text": "library implementation", "start": 1976.96, "duration": 3.04}, {"text": "um that's", "start": 1980.159, "duration": 5.36}, {"text": "that's to use libraries now how what", "start": 1983.2, "duration": 5.359}, {"text": "about writing your own code okay", "start": 1985.519, "duration": 6.081}, {"text": "there's c um", "start": 1988.559, "duration": 5.201}, {"text": "here's again a link for you to explore", "start": 1991.6, "duration": 3.36}, {"text": "um", "start": 1993.76, "duration": 3.039}, {"text": "it's a solution to run c seamlessly on", "start": 1994.96, "duration": 4.4}, {"text": "gpus right as i said this is the factor", "start": 1996.799, "duration": 3.521}, {"text": "standard", "start": 1999.36, "duration": 2.88}, {"text": "microphone is called nvidia gpus but of", "start": 2000.32, "duration": 4.0}, {"text": "course you know while the extensions to", "start": 2002.24, "duration": 2.88}, {"text": "c are", "start": 2004.32, "duration": 4.32}, {"text": "you know modest uh it requires major", "start": 2005.12, "duration": 4.88}, {"text": "rewriting of code so it", "start": 2008.64, "duration": 4.159}, {"text": "can be easy in some cases but you know", "start": 2010.0, "duration": 3.36}, {"text": "it's", "start": 2012.799, "duration": 1.921}, {"text": "in particular if you have a larger code", "start": 2013.36, "duration": 4.0}, {"text": "space it's a lot of can be a lot of work", "start": 2014.72, "duration": 5.679}, {"text": "um there's also cuda fortune", "start": 2017.36, "duration": 4.64}, {"text": "uh that has two-day extensions for", "start": 2020.399, "duration": 4.721}, {"text": "fortune and was developed by pgi and is", "start": 2022.0, "duration": 3.919}, {"text": "available", "start": 2025.12, "duration": 3.439}, {"text": "in the pgi fortran compiler but also now", "start": 2025.919, "duration": 5.6}, {"text": "in the nvidia quantum compiler", "start": 2028.559, "duration": 5.761}, {"text": "and there's some recommended reading", "start": 2031.519, "duration": 3.841}, {"text": "here", "start": 2034.32, "duration": 3.839}, {"text": "um you know about the nvidia hpc sdk", "start": 2035.36, "duration": 4.64}, {"text": "then q2c the programming guide is", "start": 2038.159, "duration": 4.081}, {"text": "actually pretty good um", "start": 2040.0, "duration": 4.64}, {"text": "what kind of fortran and then here this", "start": 2042.24, "duration": 4.88}, {"text": "website the gpuhacathons.org there is", "start": 2044.64, "duration": 3.759}, {"text": "there's", "start": 2047.12, "duration": 2.719}, {"text": "a lot of resources we just got a", "start": 2048.399, "duration": 3.44}, {"text": "hacker's unorganized also at sdsc but", "start": 2049.839, "duration": 3.84}, {"text": "there are many throughout the", "start": 2051.839, "duration": 3.76}, {"text": "year so that must be something you could", "start": 2053.679, "duration": 3.361}, {"text": "be interested in if you have code that", "start": 2055.599, "duration": 4.56}, {"text": "you want to port um", "start": 2057.04, "duration": 6.72}, {"text": "and heterogeneous computing is called", "start": 2060.159, "duration": 5.44}, {"text": "heterogeneous because you have two types", "start": 2063.76, "duration": 3.119}, {"text": "of processes in your", "start": 2065.599, "duration": 4.641}, {"text": "system cpus and gpus so that's literally", "start": 2066.879, "duration": 5.2}, {"text": "how the flow looks like you have", "start": 2070.24, "duration": 5.359}, {"text": "code on there on the cpu serial code", "start": 2072.079, "duration": 6.721}, {"text": "could be parallel code to mpi or openmp", "start": 2075.599, "duration": 5.04}, {"text": "um and then and then you have this", "start": 2078.8, "duration": 3.599}, {"text": "parallel code in a kernel that launches", "start": 2080.639, "duration": 2.96}, {"text": "on the device and", "start": 2082.399, "duration": 5.28}, {"text": "just depicted here a grid of blocks", "start": 2083.599, "duration": 6.8}, {"text": "of threads right so that's uh cuda", "start": 2087.679, "duration": 4.081}, {"text": "nomenclature", "start": 2090.399, "duration": 3.601}, {"text": "that then those blocks get mapped onto", "start": 2091.76, "duration": 4.319}, {"text": "the different multiprocessors in the gpu", "start": 2094.0, "duration": 3.68}, {"text": "and they run in parallel and then you", "start": 2096.079, "duration": 3.52}, {"text": "know you turn control back to the host", "start": 2097.68, "duration": 3.36}, {"text": "and then you launch maybe another kernel", "start": 2099.599, "duration": 3.52}, {"text": "that does some different computation", "start": 2101.04, "duration": 3.68}, {"text": "similar to the library function call", "start": 2103.119, "duration": 4.561}, {"text": "that we just looked at", "start": 2104.72, "duration": 2.96}, {"text": "so serial parallel code and that's how", "start": 2108.48, "duration": 4.8}, {"text": "the processing flow looks like", "start": 2111.839, "duration": 3.921}, {"text": "right you have your cpu it's connected", "start": 2113.28, "duration": 4.64}, {"text": "via pci bus or some other interconnect", "start": 2115.76, "duration": 2.72}, {"text": "to the", "start": 2117.92, "duration": 2.64}, {"text": "gpu now you have to be aware that", "start": 2118.48, "duration": 3.599}, {"text": "transferring data from here to here", "start": 2120.56, "duration": 4.4}, {"text": "is relatively slow and that's the device", "start": 2122.079, "duration": 4.081}, {"text": "that's your gpu", "start": 2124.96, "duration": 5.6}, {"text": "with all these multi processors on there", "start": 2126.16, "duration": 7.52}, {"text": "that i talked about before and so you", "start": 2130.56, "duration": 5.84}, {"text": "copy data from the cpu to the gpu", "start": 2133.68, "duration": 7.679}, {"text": "and then you run your program on the gpu", "start": 2136.4, "duration": 7.12}, {"text": "and then you copy the results back okay", "start": 2141.359, "duration": 4.401}, {"text": "so that's basically how", "start": 2143.52, "duration": 4.48}, {"text": "what you have to do now how to implement", "start": 2145.76, "duration": 3.2}, {"text": "that", "start": 2148.0, "duration": 5.28}, {"text": "um first of all unified memory you you", "start": 2148.96, "duration": 6.56}, {"text": "you know you don't have to do this", "start": 2153.28, "duration": 3.6}, {"text": "distinction so this", "start": 2155.52, "duration": 3.68}, {"text": "the concept of unified memory that you", "start": 2156.88, "duration": 3.36}, {"text": "can use that", "start": 2159.2, "duration": 3.2}, {"text": "the cool of shared memory a memory that", "start": 2160.24, "duration": 3.76}, {"text": "is shared between host and device and", "start": 2162.4, "duration": 1.92}, {"text": "that", "start": 2164.0, "duration": 3.119}, {"text": "helps your programming faster right um", "start": 2164.32, "duration": 4.4}, {"text": "the memory copies still happen under the", "start": 2167.119, "duration": 4.161}, {"text": "hood so you need to be aware of that", "start": 2168.72, "duration": 4.8}, {"text": "and that's also a way to improve code", "start": 2171.28, "duration": 3.28}, {"text": "after you've done our first", "start": 2173.52, "duration": 2.559}, {"text": "implementation of unified memory", "start": 2174.56, "duration": 2.799}, {"text": "potentially", "start": 2176.079, "duration": 4.401}, {"text": "to explicitly manage this memory um", "start": 2177.359, "duration": 6.48}, {"text": "now if we look think about cuda you see", "start": 2180.48, "duration": 3.76}, {"text": "of", "start": 2183.839, "duration": 2.721}, {"text": "a lot of nomenclature and so i mentioned", "start": 2184.24, "duration": 3.839}, {"text": "kernel before", "start": 2186.56, "duration": 4.559}, {"text": "what is a kernel a kernel is a piece of", "start": 2188.079, "duration": 4.721}, {"text": "code typically a function that can be", "start": 2191.119, "duration": 3.521}, {"text": "executed on the gpu so it's a function", "start": 2192.8, "duration": 3.2}, {"text": "that you write that then is", "start": 2194.64, "duration": 4.479}, {"text": "supposed to be executed on the gpu and", "start": 2196.0, "duration": 4.88}, {"text": "this kernel code operates in lockstep on", "start": 2199.119, "duration": 4.321}, {"text": "the multiprocessors of the gpu", "start": 2200.88, "duration": 5.92}, {"text": "in parallel right so many uh compute", "start": 2203.44, "duration": 4.08}, {"text": "cores", "start": 2206.8, "duration": 3.76}, {"text": "do the same operation at the same time", "start": 2207.52, "duration": 5.28}, {"text": "and on nvidia gpus that happens in", "start": 2210.56, "duration": 4.88}, {"text": "so-called warps", "start": 2212.8, "duration": 4.64}, {"text": "are threads currently consisting of 32", "start": 2215.44, "duration": 4.48}, {"text": "threads right", "start": 2217.44, "duration": 3.679}, {"text": "and that's basically a single", "start": 2219.92, "duration": 4.32}, {"text": "instruction multiple thread architecture", "start": 2221.119, "duration": 5.041}, {"text": "and the thread is just the execution of", "start": 2224.24, "duration": 3.68}, {"text": "a kernel with a given index right so", "start": 2226.16, "duration": 4.0}, {"text": "that kernel is basically launched", "start": 2227.92, "duration": 4.64}, {"text": "many times that function is executed", "start": 2230.16, "duration": 3.28}, {"text": "many times", "start": 2232.56, "duration": 4.24}, {"text": "and each each each instance of that", "start": 2233.44, "duration": 4.639}, {"text": "function", "start": 2236.8, "duration": 3.84}, {"text": "knows in this grid block that i alluded", "start": 2238.079, "duration": 3.361}, {"text": "to before", "start": 2240.64, "duration": 3.68}, {"text": "where it is it has an index for that and", "start": 2241.44, "duration": 4.56}, {"text": "with that you can access subsets of the", "start": 2244.32, "duration": 3.44}, {"text": "data", "start": 2246.0, "duration": 4.32}, {"text": "now the block that i showed before is", "start": 2247.76, "duration": 4.0}, {"text": "just a collection of the threads that", "start": 2250.32, "duration": 3.039}, {"text": "are grouped into blocks and that's done", "start": 2251.76, "duration": 2.16}, {"text": "because", "start": 2253.359, "duration": 2.081}, {"text": "they're guaranteed to execute on the", "start": 2253.92, "duration": 4.159}, {"text": "same multiprocessor", "start": 2255.44, "duration": 4.0}, {"text": "and with that the threads can", "start": 2258.079, "duration": 3.201}, {"text": "synchronize and share data and so you", "start": 2259.44, "duration": 3.36}, {"text": "can exploit this in the algorithms that", "start": 2261.28, "duration": 3.36}, {"text": "you implement", "start": 2262.8, "duration": 3.12}, {"text": "while blocks that run on different", "start": 2264.64, "duration": 3.12}, {"text": "multi-processors they can't see they", "start": 2265.92, "duration": 3.04}, {"text": "don't know what the other", "start": 2267.76, "duration": 5.52}, {"text": "processor has is currently operating on", "start": 2268.96, "duration": 6.8}, {"text": "and the grid is just an entirety of all", "start": 2273.28, "duration": 4.4}, {"text": "the blocks so that's just", "start": 2275.76, "duration": 5.2}, {"text": "geodynamic later and so", "start": 2277.68, "duration": 4.8}, {"text": "you know you know the size of the grid", "start": 2280.96, "duration": 4.0}, {"text": "and the size of each block", "start": 2282.48, "duration": 3.84}, {"text": "you know the number of total running", "start": 2284.96, "duration": 3.68}, {"text": "threads", "start": 2286.32, "duration": 5.36}, {"text": "yes depicted again you have the grid", "start": 2288.64, "duration": 6.719}, {"text": "with your blocks of threads and um", "start": 2291.68, "duration": 5.439}, {"text": "like i said the blocks mapped to the", "start": 2295.359, "duration": 3.121}, {"text": "processors", "start": 2297.119, "duration": 3.361}, {"text": "so block zero zero could execute on one", "start": 2298.48, "duration": 3.84}, {"text": "multiprocessor while at the same same", "start": 2300.48, "duration": 3.84}, {"text": "time block one zero could execute on a", "start": 2302.32, "duration": 6.96}, {"text": "second multiprocessor and so forth um", "start": 2304.32, "duration": 7.039}, {"text": "and the threads then you know in within", "start": 2309.28, "duration": 3.92}, {"text": "those blocks there's always 32 that", "start": 2311.359, "duration": 3.601}, {"text": "execute the same operation at the same", "start": 2313.2, "duration": 2.96}, {"text": "time", "start": 2314.96, "duration": 3.92}, {"text": "yeah in a so-called warp right so in", "start": 2316.16, "duration": 4.48}, {"text": "this example here", "start": 2318.88, "duration": 4.8}, {"text": "um yeah let me talk first about the", "start": 2320.64, "duration": 4.64}, {"text": "built-in variables right so", "start": 2323.68, "duration": 5.12}, {"text": "um you have the variables", "start": 2325.28, "duration": 7.52}, {"text": "here uh that they can query", "start": 2328.8, "duration": 7.039}, {"text": "um in order to understand which id", "start": 2332.8, "duration": 5.36}, {"text": "each of those threads has right so", "start": 2335.839, "duration": 4.401}, {"text": "that's the grid dimension and", "start": 2338.16, "duration": 4.64}, {"text": "this is these are three dimensional", "start": 2340.24, "duration": 4.08}, {"text": "variables we have an x y and z", "start": 2342.8, "duration": 5.84}, {"text": "component um uh z component for block", "start": 2344.32, "duration": 6.32}, {"text": "for crit dimension is unused so you can", "start": 2348.64, "duration": 3.28}, {"text": "have a two dimensional grid and a three", "start": 2350.64, "duration": 3.439}, {"text": "dimensional block lock", "start": 2351.92, "duration": 3.28}, {"text": "and the reason for having a", "start": 2354.079, "duration": 3.28}, {"text": "three-dimensional block is because", "start": 2355.2, "duration": 3.919}, {"text": "this way you can map it easier in your", "start": 2357.359, "duration": 3.121}, {"text": "algorithms you know", "start": 2359.119, "duration": 2.881}, {"text": "you can think about you know maybe you", "start": 2360.48, "duration": 3.359}, {"text": "want to use a two-dimensional block", "start": 2362.0, "duration": 4.32}, {"text": "indices if you have a problem with a", "start": 2363.839, "duration": 4.161}, {"text": "two-minute dimensional matrix", "start": 2366.32, "duration": 3.12}, {"text": "right matrix multiplication something", "start": 2368.0, "duration": 4.079}, {"text": "like that or if you have volumetric data", "start": 2369.44, "duration": 4.56}, {"text": "then maybe it's easier for you to use x", "start": 2372.079, "duration": 3.601}, {"text": "y and z to um", "start": 2374.0, "duration": 3.68}, {"text": "index points in a three-dimensional grid", "start": 2375.68, "duration": 4.399}, {"text": "for instance", "start": 2377.68, "duration": 4.8}, {"text": "and you know then so we know the block", "start": 2380.079, "duration": 3.921}, {"text": "dimensions", "start": 2382.48, "duration": 2.879}, {"text": "oh yeah sorry i didn't talk about so", "start": 2384.0, "duration": 3.28}, {"text": "this is a block id right so this", "start": 2385.359, "duration": 4.081}, {"text": "block here would have an id 0 0 on this", "start": 2387.28, "duration": 4.319}, {"text": "one id 1 0", "start": 2389.44, "duration": 4.32}, {"text": "and then within that block you have the", "start": 2391.599, "duration": 3.681}, {"text": "thread ids for each thread", "start": 2393.76, "duration": 4.72}, {"text": "so this right here has id 0 0 this", "start": 2395.28, "duration": 6.559}, {"text": "has an id 1 0 this one here 0 1 and so", "start": 2398.48, "duration": 5.44}, {"text": "forth", "start": 2401.839, "duration": 3.921}, {"text": "so with that you can compute the size", "start": 2403.92, "duration": 3.679}, {"text": "the number of all your threads", "start": 2405.76, "duration": 4.4}, {"text": "and you can compute your individual each", "start": 2407.599, "duration": 3.281}, {"text": "thread knows", "start": 2410.16, "duration": 3.76}, {"text": "where in this grid it's sitting", "start": 2410.88, "duration": 6.32}, {"text": "in this example um just for depicting", "start": 2413.92, "duration": 5.52}, {"text": "here", "start": 2417.2, "duration": 2.24}, {"text": "you know you have three times two blocks", "start": 2419.76, "duration": 4.4}, {"text": "so six blocks", "start": 2421.839, "duration": 4.801}, {"text": "three five times two and then each block", "start": 2424.16, "duration": 4.0}, {"text": "has four times three threads", "start": 2426.64, "duration": 3.12}, {"text": "so 12 you have a total of them two", "start": 2428.16, "duration": 3.12}, {"text": "threads running of course", "start": 2429.76, "duration": 3.44}, {"text": "you want to launch many more and you", "start": 2431.28, "duration": 4.079}, {"text": "should always use at least 32 or a", "start": 2433.2, "duration": 3.76}, {"text": "multiple of 32", "start": 2435.359, "duration": 4.96}, {"text": "threads in each block because you know", "start": 2436.96, "duration": 6.32}, {"text": "32 is the magic number of cores that", "start": 2440.319, "duration": 3.52}, {"text": "always", "start": 2443.28, "duration": 2.079}, {"text": "will do the same operation at the same", "start": 2443.839, "duration": 3.76}, {"text": "time", "start": 2445.359, "duration": 2.24}, {"text": "then you know if you write a function", "start": 2447.68, "duration": 3.52}, {"text": "you have the", "start": 2449.68, "duration": 4.32}, {"text": "underscore global underscore underscore", "start": 2451.2, "duration": 3.919}, {"text": "keyword", "start": 2454.0, "duration": 4.48}, {"text": "that is a function now that executes on", "start": 2455.119, "duration": 4.801}, {"text": "the device", "start": 2458.48, "duration": 3.839}, {"text": "but can be called from the host and so", "start": 2459.92, "duration": 3.84}, {"text": "that's basically a kernel so this is", "start": 2462.319, "duration": 5.28}, {"text": "just like c code that you see here and", "start": 2463.76, "duration": 5.839}, {"text": "you see you have here a simple while", "start": 2467.599, "duration": 3.681}, {"text": "loop for instance", "start": 2469.599, "duration": 4.401}, {"text": "that would be a vector addition imagine", "start": 2471.28, "duration": 4.559}, {"text": "you're passing in", "start": 2474.0, "duration": 4.96}, {"text": "a and b and c as pointers", "start": 2475.839, "duration": 6.161}, {"text": "and those point to arrays right to", "start": 2478.96, "duration": 4.159}, {"text": "vector", "start": 2482.0, "duration": 2.8}, {"text": "and you want to do vector addition and", "start": 2483.119, "duration": 3.601}, {"text": "now what you do is you compute your", "start": 2484.8, "duration": 2.48}, {"text": "index", "start": 2486.72, "duration": 3.52}, {"text": "my thread index as an offset in that", "start": 2487.28, "duration": 3.92}, {"text": "grid block", "start": 2490.24, "duration": 2.599}, {"text": "using those variables that i showed you", "start": 2491.2, "duration": 3.76}, {"text": "before and i know that this is", "start": 2492.839, "duration": 3.561}, {"text": "this is hard to follow now because i'm", "start": 2494.96, "duration": 3.2}, {"text": "going very fast but i just want to show", "start": 2496.4, "duration": 2.64}, {"text": "you the basics", "start": 2498.16, "duration": 4.56}, {"text": "and um and then you the stride right so", "start": 2499.04, "duration": 5.44}, {"text": "that's basically the total number of all", "start": 2502.72, "duration": 3.28}, {"text": "the threads that are running and all you", "start": 2504.48, "duration": 1.92}, {"text": "do is", "start": 2506.0, "duration": 3.44}, {"text": "each thread computes that", "start": 2506.4, "duration": 6.48}, {"text": "sum a plus b for a different element of", "start": 2509.44, "duration": 4.399}, {"text": "that vector", "start": 2512.88, "duration": 3.68}, {"text": "right and stores it back in there in the", "start": 2513.839, "duration": 4.0}, {"text": "result vector", "start": 2516.56, "duration": 2.96}, {"text": "so that means you're doing this in", "start": 2517.839, "duration": 3.361}, {"text": "parallel with all the threads that you", "start": 2519.52, "duration": 2.559}, {"text": "have been launching", "start": 2521.2, "duration": 4.32}, {"text": "right with on as many processes", "start": 2522.079, "duration": 4.641}, {"text": "and compute cores that you have", "start": 2525.52, "duration": 3.36}, {"text": "available on the gpu", "start": 2526.72, "duration": 3.92}, {"text": "and then so each thread at some point", "start": 2528.88, "duration": 3.92}, {"text": "you know needs to", "start": 2530.64, "duration": 5.36}, {"text": "increase its counter and to operate on", "start": 2532.8, "duration": 4.72}, {"text": "the on the elements of the vector that", "start": 2536.0, "duration": 3.44}, {"text": "have not been operated on and will stop", "start": 2537.52, "duration": 3.76}, {"text": "once we reach the end of the vector", "start": 2539.44, "duration": 3.919}, {"text": "and that's the very basics of how you", "start": 2541.28, "duration": 4.079}, {"text": "implement like um", "start": 2543.359, "duration": 4.881}, {"text": "parallel kernels on a gpu and then of", "start": 2545.359, "duration": 5.841}, {"text": "course you have to call them from the", "start": 2548.24, "duration": 5.68}, {"text": "ppu code right in addition to that you", "start": 2551.2, "duration": 4.56}, {"text": "have the", "start": 2553.92, "duration": 4.399}, {"text": "memory functions like allocation screen", "start": 2555.76, "duration": 4.0}, {"text": "memory memory copies", "start": 2558.319, "duration": 4.52}, {"text": "is equivalent to cuda c malloc p and", "start": 2559.76, "duration": 4.64}, {"text": "memco", "start": 2562.839, "duration": 3.321}, {"text": "and so if you have a kernel like that", "start": 2564.4, "duration": 3.6}, {"text": "then you can launch it from your cpu", "start": 2566.16, "duration": 2.8}, {"text": "code", "start": 2568.0, "duration": 2.72}, {"text": "and all you have to do is you have to", "start": 2568.96, "duration": 3.52}, {"text": "add this launch configuration here in", "start": 2570.72, "duration": 3.92}, {"text": "this triple angle bracket", "start": 2572.48, "duration": 5.04}, {"text": "and where you tell basically with this", "start": 2574.64, "duration": 4.0}, {"text": "method where i will", "start": 2577.52, "duration": 5.28}, {"text": "type in three um", "start": 2578.64, "duration": 7.92}, {"text": "but you can use um very where you", "start": 2582.8, "duration": 5.6}, {"text": "specify the grid dimensions the number", "start": 2586.56, "duration": 3.84}, {"text": "of blocks in x y and z dimension and the", "start": 2588.4, "duration": 3.12}, {"text": "number of threads", "start": 2590.4, "duration": 3.84}, {"text": "so the block lies in x y and z dimension", "start": 2591.52, "duration": 3.12}, {"text": "and that", "start": 2594.24, "duration": 1.44}, {"text": "of course has to match the", "start": 2594.64, "duration": 4.959}, {"text": "implementation of the kernel here right", "start": 2595.68, "duration": 3.919}, {"text": "this is again the memory hierarchy", "start": 2602.48, "duration": 6.48}, {"text": "depicting again what i've been talking", "start": 2607.28, "duration": 3.12}, {"text": "about you have", "start": 2608.96, "duration": 3.76}, {"text": "so called global memory that is the gpu", "start": 2610.4, "duration": 3.199}, {"text": "ramp", "start": 2612.72, "duration": 2.72}, {"text": "that can be seen by all your threads", "start": 2613.599, "duration": 3.201}, {"text": "from the gpu", "start": 2615.44, "duration": 4.48}, {"text": "and then you have the shared memory here", "start": 2616.8, "duration": 4.799}, {"text": "this is visible by all the threads in", "start": 2619.92, "duration": 4.0}, {"text": "each block", "start": 2621.599, "duration": 5.121}, {"text": "so the thread 0 0 and thread 1 0 can", "start": 2623.92, "duration": 4.48}, {"text": "store data and read from the same shared", "start": 2626.72, "duration": 2.639}, {"text": "memory", "start": 2628.4, "duration": 3.679}, {"text": "but they cannot do that for thread 0 0", "start": 2629.359, "duration": 4.641}, {"text": "and 1 0 and this other block here right", "start": 2632.079, "duration": 5.121}, {"text": "so that's that's", "start": 2634.0, "duration": 4.72}, {"text": "that's something you have to be aware of", "start": 2637.2, "duration": 3.04}, {"text": "and we want to use shared memory because", "start": 2638.72, "duration": 2.56}, {"text": "it's fast", "start": 2640.24, "duration": 3.28}, {"text": "uh much faster than global maps it's", "start": 2641.28, "duration": 4.079}, {"text": "like a cache on the chip", "start": 2643.52, "duration": 3.839}, {"text": "um there's also automatic cache and then", "start": 2645.359, "duration": 3.921}, {"text": "there's registers and local memory", "start": 2647.359, "duration": 5.041}, {"text": "um and", "start": 2649.28, "duration": 6.24}, {"text": "constant memory is important where you", "start": 2652.4, "duration": 3.52}, {"text": "can", "start": 2655.52, "duration": 2.319}, {"text": "store for instance like you know if you", "start": 2655.92, "duration": 5.36}, {"text": "have natural constantly", "start": 2657.839, "duration": 3.441}, {"text": "and when you write code what you want to", "start": 2662.0, "duration": 3.119}, {"text": "do is you want to", "start": 2663.44, "duration": 3.52}, {"text": "avoid data transfers between the cpu and", "start": 2665.119, "duration": 4.321}, {"text": "the gpu because these are slow", "start": 2666.96, "duration": 4.399}, {"text": "um you want to minimize access to global", "start": 2669.44, "duration": 3.04}, {"text": "memory", "start": 2671.359, "duration": 3.121}, {"text": "um for instance you know caching data in", "start": 2672.48, "duration": 4.879}, {"text": "the chat memory space", "start": 2674.48, "duration": 5.92}, {"text": "by tiling data right", "start": 2677.359, "duration": 4.48}, {"text": "that fits into the shared memory and", "start": 2680.4, "duration": 4.0}, {"text": "then so each each data subset", "start": 2681.839, "duration": 4.48}, {"text": "will be operated on by a single thread", "start": 2684.4, "duration": 4.24}, {"text": "block so you decompose your problem into", "start": 2686.319, "duration": 4.481}, {"text": "smaller problems but then", "start": 2688.64, "duration": 4.4}, {"text": "you know after you operate in parallel", "start": 2690.8, "duration": 4.08}, {"text": "only within those smaller problems you", "start": 2693.04, "duration": 2.72}, {"text": "also", "start": 2694.88, "duration": 2.479}, {"text": "work in parallels with your threads in a", "start": 2695.76, "duration": 3.359}, {"text": "block", "start": 2697.359, "duration": 3.681}, {"text": "and so you load that subset of data from", "start": 2699.119, "duration": 3.281}, {"text": "global to shared memory and then you", "start": 2701.04, "duration": 2.64}, {"text": "perform the computations", "start": 2702.4, "duration": 3.52}, {"text": "and then you copy the results back to", "start": 2703.68, "duration": 5.76}, {"text": "the global memory now", "start": 2705.92, "duration": 3.52}, {"text": "i put too much into this presentation so", "start": 2709.92, "duration": 3.52}, {"text": "i will skip", "start": 2712.319, "duration": 3.52}, {"text": "over the matrix multiplication example", "start": 2713.44, "duration": 3.76}, {"text": "because it's just too much", "start": 2715.839, "duration": 3.28}, {"text": "to follow and i want to show you and if", "start": 2717.2, "duration": 3.6}, {"text": "you've seen of course expands", "start": 2719.119, "duration": 4.801}, {"text": "but just to give you an idea um", "start": 2720.8, "duration": 6.799}, {"text": "uh sorry this was in you have a matrix", "start": 2723.92, "duration": 4.64}, {"text": "multiplication", "start": 2727.599, "duration": 2.321}, {"text": "kernel so that's what you would do", "start": 2728.56, "duration": 2.48}, {"text": "imagine if you do a matrix", "start": 2729.92, "duration": 2.72}, {"text": "multiplication", "start": 2731.04, "duration": 4.559}, {"text": "um the way you'd implement that if you", "start": 2732.64, "duration": 3.84}, {"text": "were to do this", "start": 2735.599, "duration": 4.0}, {"text": "is and here i i'm actually simplifying", "start": 2736.48, "duration": 4.72}, {"text": "something here in this scheme and in the", "start": 2739.599, "duration": 5.041}, {"text": "code that's on those slides", "start": 2741.2, "duration": 3.44}, {"text": "you make an assumption that you have a", "start": 2745.119, "duration": 4.24}, {"text": "block size here", "start": 2747.2, "duration": 4.24}, {"text": "you know and for each block you have", "start": 2749.359, "duration": 3.281}, {"text": "several threads running", "start": 2751.44, "duration": 3.6}, {"text": "and that block size must be a multiple", "start": 2752.64, "duration": 4.56}, {"text": "or the size of that result matrix must", "start": 2755.04, "duration": 4.079}, {"text": "be a multiple of the block size in x and", "start": 2757.2, "duration": 3.28}, {"text": "y dimension", "start": 2759.119, "duration": 3.2}, {"text": "and then we launch a single thread to", "start": 2760.48, "duration": 3.28}, {"text": "compute each element so you have one", "start": 2762.319, "duration": 2.401}, {"text": "thread", "start": 2763.76, "duration": 3.52}, {"text": "that does this dot product between you", "start": 2764.72, "duration": 3.2}, {"text": "know this", "start": 2767.28, "duration": 4.0}, {"text": "row vector and this column vector", "start": 2767.92, "duration": 5.28}, {"text": "and so what that means is that each day", "start": 2771.28, "duration": 3.2}, {"text": "threat computes one of those dot", "start": 2773.2, "duration": 2.639}, {"text": "products", "start": 2774.48, "duration": 3.52}, {"text": "and in order to have good performance", "start": 2775.839, "duration": 3.841}, {"text": "for a kernel like that what you need to", "start": 2778.0, "duration": 2.72}, {"text": "do", "start": 2779.68, "duration": 4.24}, {"text": "is um you load these", "start": 2780.72, "duration": 5.92}, {"text": "sub matrices sub blocks here basically", "start": 2783.92, "duration": 4.159}, {"text": "into shared memory", "start": 2786.64, "duration": 3.84}, {"text": "so this stop block and this sub block", "start": 2788.079, "duration": 3.361}, {"text": "and then you do the matrix", "start": 2790.48, "duration": 2.48}, {"text": "multiplication", "start": 2791.44, "duration": 2.879}, {"text": "and then you store the data back and", "start": 2792.96, "duration": 3.44}, {"text": "then you load the next stop block", "start": 2794.319, "duration": 3.361}, {"text": "and the next stop lock and you do the", "start": 2796.4, "duration": 3.28}, {"text": "matrix modification we do this cache", "start": 2797.68, "duration": 2.8}, {"text": "blocking and", "start": 2799.68, "duration": 2.48}, {"text": "um you know i have to see on this fight", "start": 2800.48, "duration": 3.359}, {"text": "but this was you know", "start": 2802.16, "duration": 3.52}, {"text": "sorry i should take these things off", "start": 2803.839, "duration": 3.201}, {"text": "because it's very it's a little bit hard", "start": 2805.68, "duration": 2.56}, {"text": "to go through", "start": 2807.04, "duration": 3.039}, {"text": "quickly it's not a difficult example but", "start": 2808.24, "duration": 3.599}, {"text": "you can find that also in the um", "start": 2810.079, "duration": 4.961}, {"text": "cuda documentation where i actually took", "start": 2811.839, "duration": 5.201}, {"text": "also that", "start": 2815.04, "duration": 5.279}, {"text": "figure from right and that would be an", "start": 2817.04, "duration": 4.72}, {"text": "example of why you want to have two", "start": 2820.319, "duration": 3.121}, {"text": "dimensional grids and blocks because you", "start": 2821.76, "duration": 2.16}, {"text": "can", "start": 2823.44, "duration": 3.12}, {"text": "index easier into a matrix right makes", "start": 2823.92, "duration": 4.72}, {"text": "it easier", "start": 2826.56, "duration": 3.759}, {"text": "in reality we would not want to write a", "start": 2828.64, "duration": 3.52}, {"text": "matrix multiplication because we have a", "start": 2830.319, "duration": 3.52}, {"text": "clearer class", "start": 2832.16, "duration": 4.159}, {"text": "function implementation in our linear", "start": 2833.839, "duration": 5.361}, {"text": "algebra annotation is very efficient", "start": 2836.319, "duration": 6.481}, {"text": "um so so so this this was now very", "start": 2839.2, "duration": 5.04}, {"text": "complex but", "start": 2842.8, "duration": 3.519}, {"text": "what about something simpler and", "start": 2844.24, "duration": 4.079}, {"text": "something simpler open acc is this open", "start": 2846.319, "duration": 3.52}, {"text": "standard for", "start": 2848.319, "duration": 3.681}, {"text": "uh expressing accelerated parallelism", "start": 2849.839, "duration": 4.161}, {"text": "and this designed to make port into gpus", "start": 2852.0, "duration": 2.559}, {"text": "easy", "start": 2854.0, "duration": 3.76}, {"text": "create and portable right because um in", "start": 2854.559, "duration": 4.401}, {"text": "principle", "start": 2857.76, "duration": 3.44}, {"text": "you don't write code that is specific", "start": 2858.96, "duration": 3.92}, {"text": "for", "start": 2861.2, "duration": 3.84}, {"text": "a certain vendor like like cuda for", "start": 2862.88, "duration": 3.36}, {"text": "nvidia", "start": 2865.04, "duration": 4.64}, {"text": "um so openmp like compiler directive", "start": 2866.24, "duration": 6.64}, {"text": "works with fortran c c and it's fully", "start": 2869.68, "duration": 4.72}, {"text": "supported by nvidia", "start": 2872.88, "duration": 4.64}, {"text": "and craig compilers on craze there's", "start": 2874.4, "duration": 4.8}, {"text": "partial support by new compilers like i", "start": 2877.52, "duration": 2.559}, {"text": "mentioned before", "start": 2879.2, "duration": 5.2}, {"text": "um and so it's pretty", "start": 2880.079, "duration": 7.52}, {"text": "yeah i would say future proof as well", "start": 2884.4, "duration": 6.24}, {"text": "and openmp also works but it's not", "start": 2887.599, "duration": 4.561}, {"text": "not there yet and you know these things", "start": 2890.64, "duration": 4.479}, {"text": "might just merge at some point the", "start": 2892.16, "duration": 8.32}, {"text": "standard there's the pgi compilers", "start": 2895.119, "duration": 10.0}, {"text": "that we can use on on xbars", "start": 2900.48, "duration": 4.639}, {"text": "they also come with a pd prof", "start": 2905.44, "duration": 4.48}, {"text": "performance compiler", "start": 2908.0, "duration": 5.359}, {"text": "and also open acc code samples", "start": 2909.92, "duration": 5.919}, {"text": "you can also use the nvidia hpc sdk", "start": 2913.359, "duration": 3.841}, {"text": "right um", "start": 2915.839, "duration": 3.041}, {"text": "since i mentioned that before that you", "start": 2917.2, "duration": 3.919}, {"text": "know computer toolkit and and the ptr", "start": 2918.88, "duration": 2.88}, {"text": "compilers", "start": 2921.119, "duration": 3.44}, {"text": "are merging effectively in future", "start": 2921.76, "duration": 3.44}, {"text": "releases", "start": 2924.559, "duration": 2.641}, {"text": "um there's probably only going to be", "start": 2925.2, "duration": 4.48}, {"text": "only the nvidia hpc sdk", "start": 2927.2, "duration": 4.48}, {"text": "and again you can do the same module", "start": 2929.68, "duration": 3.84}, {"text": "purge module reset and then you say", "start": 2931.68, "duration": 3.6}, {"text": "module load pgi you get the", "start": 2933.52, "duration": 3.76}, {"text": "pgi compiler this loads by default", "start": 2935.28, "duration": 3.839}, {"text": "version 20.4 at the moment but", "start": 2937.28, "duration": 4.0}, {"text": "if for some whatever reason you need an", "start": 2939.119, "duration": 3.2}, {"text": "older version", "start": 2941.28, "duration": 4.4}, {"text": "the older versions don't as well on xbar", "start": 2942.319, "duration": 7.361}, {"text": "um and", "start": 2945.68, "duration": 6.08}, {"text": "literally i only have this one slide", "start": 2949.68, "duration": 4.8}, {"text": "here to show you some code", "start": 2951.76, "duration": 5.04}, {"text": "but you know we have that example of sax", "start": 2954.48, "duration": 4.32}, {"text": "py in this function", "start": 2956.8, "duration": 4.96}, {"text": "um and and now if you were to implement", "start": 2958.8, "duration": 4.96}, {"text": "this function yourself manually", "start": 2961.76, "duration": 3.12}, {"text": "of course you don't want to do that", "start": 2963.76, "duration": 2.64}, {"text": "because you know you have the blast", "start": 2964.88, "duration": 3.12}, {"text": "functions for that but imagine you had", "start": 2966.4, "duration": 3.199}, {"text": "implemented that", "start": 2968.0, "duration": 3.28}, {"text": "this is your function on the left is", "start": 2969.599, "duration": 3.441}, {"text": "just in c in the writing fortune i'll", "start": 2971.28, "duration": 2.16}, {"text": "just", "start": 2973.04, "duration": 4.48}, {"text": "go over here it is a vector addition so", "start": 2973.44, "duration": 7.119}, {"text": "you have this for loop right", "start": 2977.52, "duration": 6.559}, {"text": "now the vector addition like we've seen", "start": 2980.559, "duration": 6.0}, {"text": "before is very well suited to run in", "start": 2984.079, "duration": 4.561}, {"text": "parallel on the gpu because you can", "start": 2986.559, "duration": 4.0}, {"text": "add each element of that vector", "start": 2988.64, "duration": 3.84}, {"text": "independently of each other", "start": 2990.559, "duration": 5.04}, {"text": "and you have this this for loop here or", "start": 2992.48, "duration": 3.76}, {"text": "two loop", "start": 2995.599, "duration": 3.201}, {"text": "in fortran and all you have to do in", "start": 2996.24, "duration": 3.839}, {"text": "principle is in", "start": 2998.8, "duration": 4.559}, {"text": "insert this pragma acc kernel and", "start": 3000.079, "duration": 3.841}, {"text": "there's", "start": 3003.359, "duration": 2.321}, {"text": "of course much more to this but this is", "start": 3003.92, "duration": 4.0}, {"text": "the most first and simple thing you can", "start": 3005.68, "duration": 2.56}, {"text": "do", "start": 3007.92, "duration": 2.96}, {"text": "and this will instruct the openacc", "start": 3008.24, "duration": 3.839}, {"text": "compiler to generate", "start": 3010.88, "duration": 4.56}, {"text": "a kernel for this um loop", "start": 3012.079, "duration": 5.601}, {"text": "that executes in parallel on the gpu so", "start": 3015.44, "duration": 3.36}, {"text": "you don't have to", "start": 3017.68, "duration": 3.6}, {"text": "do all this programming yourself right", "start": 3018.8, "duration": 4.0}, {"text": "allocating memory moving data", "start": 3021.28, "duration": 3.2}, {"text": "implementing the kernel", "start": 3022.8, "duration": 4.96}, {"text": "launching the kernel from the uh cpu", "start": 3024.48, "duration": 5.2}, {"text": "it also has the advantage that this code", "start": 3027.76, "duration": 4.24}, {"text": "just keeps compiling only with your cpu", "start": 3029.68, "duration": 3.439}, {"text": "compiler because", "start": 3032.0, "duration": 2.72}, {"text": "you know you've just inserted", "start": 3033.119, "duration": 2.801}, {"text": "effectively programmers that are", "start": 3034.72, "duration": 2.32}, {"text": "comments into the", "start": 3035.92, "duration": 4.159}, {"text": "code right and uh", "start": 3037.04, "duration": 4.72}, {"text": "and that's that's all you have to do in", "start": 3040.079, "duration": 3.201}, {"text": "principle right um", "start": 3041.76, "duration": 3.76}, {"text": "of course you'll have to help the", "start": 3043.28, "duration": 3.6}, {"text": "compiler and that's much more to it", "start": 3045.52, "duration": 3.839}, {"text": "similar to openmp", "start": 3046.88, "duration": 6.0}, {"text": "um so in fortran so you have these um", "start": 3049.359, "duration": 6.401}, {"text": "directives tips that are often paired", "start": 3052.88, "duration": 4.239}, {"text": "with a matching end directive", "start": 3055.76, "duration": 3.839}, {"text": "surrounding a structured code block", "start": 3057.119, "duration": 4.641}, {"text": "right like this for loop you have a", "start": 3059.599, "duration": 3.601}, {"text": "structured code block following this", "start": 3061.76, "duration": 4.16}, {"text": "pragma acc directive for instance", "start": 3063.2, "duration": 5.52}, {"text": "that we had here on the slide before is", "start": 3065.92, "duration": 4.72}, {"text": "drag my acc kernel", "start": 3068.72, "duration": 4.08}, {"text": "in this case and so you have this", "start": 3070.64, "duration": 3.199}, {"text": "kernel's constructed", "start": 3072.8, "duration": 2.72}, {"text": "instructs the compiler to generate a", "start": 3073.839, "duration": 4.72}, {"text": "kernel that computes on the gpu", "start": 3075.52, "duration": 4.96}, {"text": "then you have clauses that you can add", "start": 3078.559, "duration": 3.04}, {"text": "like if", "start": 3080.48, "duration": 3.68}, {"text": "a certain condition is match and so on", "start": 3081.599, "duration": 4.081}, {"text": "and then you have for instance like data", "start": 3084.16, "duration": 2.88}, {"text": "clauses", "start": 3085.68, "duration": 4.8}, {"text": "like copy copy in list here's a list of", "start": 3087.04, "duration": 5.039}, {"text": "variables right", "start": 3090.48, "duration": 4.72}, {"text": "copy out create present and", "start": 3092.079, "duration": 4.881}, {"text": "you know like i've shown you before the", "start": 3095.2, "duration": 4.32}, {"text": "gpu has its own memory and", "start": 3096.96, "duration": 4.8}, {"text": "um this is exactly how you tell the", "start": 3099.52, "duration": 4.0}, {"text": "compiler", "start": 3101.76, "duration": 4.799}, {"text": "how to handle memory copies right", "start": 3103.52, "duration": 4.64}, {"text": "and and sometimes the compiler does the", "start": 3106.559, "duration": 3.121}, {"text": "right thing but more often than not it", "start": 3108.16, "duration": 2.48}, {"text": "will not and you", "start": 3109.68, "duration": 2.32}, {"text": "get bad performance and you have to", "start": 3110.64, "duration": 3.12}, {"text": "actually tell the compiler", "start": 3112.0, "duration": 5.839}, {"text": "um that you want to copy in the um", "start": 3113.76, "duration": 6.16}, {"text": "data to the gpu or the data is already", "start": 3117.839, "duration": 3.921}, {"text": "on the gpu and so on and with that you", "start": 3119.92, "duration": 2.399}, {"text": "can use", "start": 3121.76, "duration": 2.0}, {"text": "data clauses we have to help the", "start": 3122.319, "duration": 4.0}, {"text": "compiler and there's an example that i", "start": 3123.76, "duration": 5.44}, {"text": "want to show you here", "start": 3126.319, "duration": 2.881}, {"text": "is a stencil operation effectively um", "start": 3129.52, "duration": 7.76}, {"text": "it's it's an algorithm and laplace", "start": 3133.44, "duration": 6.159}, {"text": "sorry for the laplace equation by jacobi", "start": 3137.28, "duration": 3.2}, {"text": "iteration", "start": 3139.599, "duration": 4.0}, {"text": "um where it you have", "start": 3140.48, "duration": 6.56}, {"text": "um multiple iterations for time steps in", "start": 3143.599, "duration": 4.48}, {"text": "your algorithm", "start": 3147.04, "duration": 4.4}, {"text": "so at step k plus one right", "start": 3148.079, "duration": 5.601}, {"text": "you have the data so here stored in the", "start": 3151.44, "duration": 3.84}, {"text": "two-dimensional area imagine you have a", "start": 3153.68, "duration": 2.56}, {"text": "huge grid so", "start": 3155.28, "duration": 2.72}, {"text": "here i'm looking only at one grid point", "start": 3156.24, "duration": 3.28}, {"text": "and it's neighbors but imagine you have", "start": 3158.0, "duration": 3.68}, {"text": "thousands of you know two-dimensional", "start": 3159.52, "duration": 5.2}, {"text": "uh uh grid um you know you have", "start": 3161.68, "duration": 4.96}, {"text": "the data and you just compute the", "start": 3164.72, "duration": 3.68}, {"text": "average from the previous time step and", "start": 3166.64, "duration": 3.439}, {"text": "that gives you the result at the next", "start": 3168.4, "duration": 3.199}, {"text": "time step and that that will slowly", "start": 3170.079, "duration": 2.081}, {"text": "converge", "start": 3171.599, "duration": 3.681}, {"text": "um and this is", "start": 3172.16, "duration": 6.72}, {"text": "this is the code right written", "start": 3175.28, "duration": 7.68}, {"text": "in for the cpu right so you have a loop", "start": 3178.88, "duration": 7.6}, {"text": "where you so what you do is you iterate", "start": 3182.96, "duration": 5.2}, {"text": "until basically", "start": 3186.48, "duration": 4.16}, {"text": "you the the matrix elements don't change", "start": 3188.16, "duration": 3.84}, {"text": "anymore between the steps", "start": 3190.64, "duration": 4.88}, {"text": "right so then the algorithm is converged", "start": 3192.0, "duration": 5.119}, {"text": "that's the outer while loop here and so", "start": 3195.52, "duration": 3.839}, {"text": "you compute the error that's that's your", "start": 3197.119, "duration": 4.801}, {"text": "threshold for convergence so you first", "start": 3199.359, "duration": 4.561}, {"text": "you set it to zero and then here we do", "start": 3201.92, "duration": 4.24}, {"text": "our update so we have this loop", "start": 3203.92, "duration": 5.6}, {"text": "we go through x and y dimensions i and j", "start": 3206.16, "duration": 7.76}, {"text": "and then so we have you know the array a", "start": 3209.52, "duration": 6.16}, {"text": "and we just do the sum of the", "start": 3213.92, "duration": 3.679}, {"text": "neighboring elements i plus one i minus", "start": 3215.68, "duration": 2.48}, {"text": "one", "start": 3217.599, "duration": 4.0}, {"text": "j minus one j plus one quarter of that", "start": 3218.16, "duration": 5.04}, {"text": "you know so the average installs it in", "start": 3221.599, "duration": 3.681}, {"text": "the new matrix and then we check", "start": 3223.2, "duration": 3.76}, {"text": "the difference between the old matrix", "start": 3225.28, "duration": 3.279}, {"text": "and the new matrix and", "start": 3226.96, "duration": 3.44}, {"text": "store the maximum error and we basically", "start": 3228.559, "duration": 3.201}, {"text": "stop", "start": 3230.4, "duration": 5.679}, {"text": "uh here when our error is", "start": 3231.76, "duration": 6.64}, {"text": "smaller than a tolerance or once we hit", "start": 3236.079, "duration": 4.161}, {"text": "a maximum number of iterations", "start": 3238.4, "duration": 3.52}, {"text": "so now we want to do this in the gpu in", "start": 3240.24, "duration": 3.76}, {"text": "parallel right", "start": 3241.92, "duration": 4.639}, {"text": "um oh yeah here we just swap the array", "start": 3244.0, "duration": 4.4}, {"text": "so that's not so important", "start": 3246.559, "duration": 3.841}, {"text": "um and what you want to do is if we just", "start": 3248.4, "duration": 3.28}, {"text": "say okay", "start": 3250.4, "duration": 4.32}, {"text": "do this on the gpu", "start": 3251.68, "duration": 3.04}, {"text": "and then you can compile this code with", "start": 3254.88, "duration": 3.84}, {"text": "a port portland group compiler", "start": 3256.559, "duration": 4.641}, {"text": "and you just tell it okay minus acc this", "start": 3258.72, "duration": 6.72}, {"text": "is an icc it's opening the c code", "start": 3261.2, "duration": 8.08}, {"text": "and this is pgf94 fortune code", "start": 3265.44, "duration": 6.879}, {"text": "you could use pgcc for a c code etcetera", "start": 3269.28, "duration": 5.2}, {"text": "and then you get some information here", "start": 3272.319, "duration": 3.681}, {"text": "minus", "start": 3274.48, "duration": 3.359}, {"text": "m in four equals x well it tells you", "start": 3276.0, "duration": 3.2}, {"text": "actually what it does you know to", "start": 3277.839, "duration": 3.121}, {"text": "generate those copy operations", "start": 3279.2, "duration": 5.44}, {"text": "as it generates the reduction operation", "start": 3280.96, "duration": 5.44}, {"text": "here's some information about you know", "start": 3284.64, "duration": 3.84}, {"text": "blocks and threats and so on so you see", "start": 3286.4, "duration": 3.6}, {"text": "it uses the same nomenclature so", "start": 3288.48, "duration": 3.119}, {"text": "internally it translates it basically", "start": 3290.0, "duration": 3.52}, {"text": "into a qr code", "start": 3291.599, "duration": 4.081}, {"text": "and this is a performance we get for", "start": 3293.52, "duration": 3.52}, {"text": "this code", "start": 3295.68, "duration": 5.04}, {"text": "on expands gpu node", "start": 3297.04, "duration": 5.2}, {"text": "you know with a corresponding compiler", "start": 3300.72, "duration": 3.839}, {"text": "flags", "start": 3302.24, "duration": 3.92}, {"text": "if it's limited i don't remember ten", "start": 3304.559, "duration": 3.121}, {"text": "thousand iterations of certain", "start": 3306.16, "duration": 3.679}, {"text": "size but um i'm executing literally the", "start": 3307.68, "duration": 3.84}, {"text": "same code in the cpu", "start": 3309.839, "duration": 4.881}, {"text": "and the gpu and this is using openmp", "start": 3311.52, "duration": 5.2}, {"text": "parallelization instead of openacc", "start": 3314.72, "duration": 2.56}, {"text": "there's uh", "start": 3316.72, "duration": 3.92}, {"text": "openmp programmers in there and you see", "start": 3317.28, "duration": 4.72}, {"text": "we get a decent speed up", "start": 3320.64, "duration": 4.24}, {"text": "goes down from 40 to 20 seconds to 10 to", "start": 3322.0, "duration": 3.28}, {"text": "seven", "start": 3324.88, "duration": 2.32}, {"text": "you know speed up gets worse and worse", "start": 3325.28, "duration": 3.68}, {"text": "of course the more threats you have", "start": 3327.2, "duration": 4.0}, {"text": "now if i use open acc it gets really", "start": 3328.96, "duration": 5.119}, {"text": "slow and so that's really bad so", "start": 3331.2, "duration": 5.52}, {"text": "what happened is you can switch on the", "start": 3334.079, "duration": 3.921}, {"text": "pgi", "start": 3336.72, "duration": 4.0}, {"text": "acc time environment variable receptors", "start": 3338.0, "duration": 3.839}, {"text": "to one and you've got", "start": 3340.72, "duration": 3.92}, {"text": "um profiles you know you need to profile", "start": 3341.839, "duration": 4.401}, {"text": "your code and what you see it says 20", "start": 3344.64, "duration": 3.679}, {"text": "seconds spent on something", "start": 3346.24, "duration": 5.2}, {"text": "that is just data copy transfers right", "start": 3348.319, "duration": 5.361}, {"text": "and then 1.5 seconds here on the on the", "start": 3351.44, "duration": 4.0}, {"text": "compute it's actually not", "start": 3353.68, "duration": 4.72}, {"text": "the profile is not from from expands but", "start": 3355.44, "duration": 3.6}, {"text": "you know", "start": 3358.4, "duration": 2.56}, {"text": "you get the idea and what's happening is", "start": 3359.04, "duration": 3.68}, {"text": "in this loop that we have in each of", "start": 3360.96, "duration": 2.879}, {"text": "those iterations which", "start": 3362.72, "duration": 4.399}, {"text": "copy the data to the gpu", "start": 3363.839, "duration": 4.881}, {"text": "and then after that we copy it back", "start": 3367.119, "duration": 3.68}, {"text": "because the compiler has no way of", "start": 3368.72, "duration": 3.04}, {"text": "knowing that", "start": 3370.799, "duration": 2.641}, {"text": "you know once it's done with this loop", "start": 3371.76, "duration": 3.52}, {"text": "it can keep the data on the gpu so we", "start": 3373.44, "duration": 4.32}, {"text": "have to help the compiler", "start": 3375.28, "duration": 4.559}, {"text": "and tell it here with a data clause that", "start": 3377.76, "duration": 3.92}, {"text": "you know copy", "start": 3379.839, "duration": 5.601}, {"text": "a to the gpu allocate memory for a new", "start": 3381.68, "duration": 6.56}, {"text": "right so now the compiler knows off", "start": 3385.44, "duration": 3.84}, {"text": "while i'm not done", "start": 3388.24, "duration": 2.879}, {"text": "i always just keep the data on the gpu", "start": 3389.28, "duration": 3.92}, {"text": "for my iteration right", "start": 3391.119, "duration": 4.561}, {"text": "and if i do that now all the magic", "start": 3393.2, "duration": 3.68}, {"text": "happened", "start": 3395.68, "duration": 3.679}, {"text": "and you know it executes in one second", "start": 3396.88, "duration": 3.439}, {"text": "so it's", "start": 3399.359, "duration": 3.521}, {"text": "it's almost seven times as fast as eight", "start": 3400.319, "duration": 3.681}, {"text": "cores right and that's", "start": 3402.88, "duration": 3.28}, {"text": "that's with this little change so it was", "start": 3404.0, "duration": 4.319}, {"text": "a huge speed up with respect to the", "start": 3406.16, "duration": 5.199}, {"text": "you see like a 42 time speed of body", "start": 3408.319, "duration": 3.601}, {"text": "speed up", "start": 3411.359, "duration": 3.601}, {"text": "x speed up versus the cpu code the zero", "start": 3411.92, "duration": 4.48}, {"text": "cpu code with a", "start": 3414.96, "duration": 4.56}, {"text": "very very little um effort", "start": 3416.4, "duration": 6.0}, {"text": "um there's more to open acc but i'll", "start": 3419.52, "duration": 4.799}, {"text": "skip that because what i want to show", "start": 3422.4, "duration": 2.56}, {"text": "you", "start": 3424.319, "duration": 3.601}, {"text": "is expand um almost at the end of the", "start": 3424.96, "duration": 4.159}, {"text": "hour um", "start": 3427.92, "duration": 3.52}, {"text": "you give me maybe two or three minutes", "start": 3429.119, "duration": 4.561}, {"text": "i'll show you very quickly how to", "start": 3431.44, "duration": 4.96}, {"text": "how to get onto advanced gpu nodes and", "start": 3433.68, "duration": 4.08}, {"text": "and", "start": 3436.4, "duration": 4.64}, {"text": "how to use nvidia computer compilers and", "start": 3437.76, "duration": 6.4}, {"text": "open acc um the important thing is we", "start": 3441.04, "duration": 5.12}, {"text": "have the 52 gpu nodes", "start": 3444.16, "duration": 5.76}, {"text": "a total of 208 gpus", "start": 3446.16, "duration": 3.76}, {"text": "and that's a little bit here some some", "start": 3450.24, "duration": 3.28}, {"text": "more details", "start": 3452.559, "duration": 4.401}, {"text": "um about how the", "start": 3453.52, "duration": 6.079}, {"text": "expanse looks like i think all of these", "start": 3456.96, "duration": 4.08}, {"text": "things have been explained already in a", "start": 3459.599, "duration": 3.601}, {"text": "different um", "start": 3461.04, "duration": 5.279}, {"text": "webinar uh", "start": 3463.2, "duration": 4.72}, {"text": "important piece here is you know that's", "start": 3466.319, "duration": 4.561}, {"text": "that schematically how a compute node", "start": 3467.92, "duration": 4.0}, {"text": "looks like so you see", "start": 3470.88, "duration": 3.36}, {"text": "these gpus they're not sitting on a pci", "start": 3471.92, "duration": 4.48}, {"text": "express bus they are", "start": 3474.24, "duration": 5.599}, {"text": "like this v100 sxm2 gpus they're like", "start": 3476.4, "duration": 5.52}, {"text": "regular processors like you know there's", "start": 3479.839, "duration": 4.96}, {"text": "four gpus here and the compute", "start": 3481.92, "duration": 5.84}, {"text": "um appear in the back it has the cpus", "start": 3484.799, "duration": 3.841}, {"text": "and then", "start": 3487.76, "duration": 2.799}, {"text": "these have special interconnects where", "start": 3488.64, "duration": 3.52}, {"text": "envy links so that's actually really", "start": 3490.559, "duration": 3.76}, {"text": "fast interconnect among the gpus", "start": 3492.16, "duration": 3.36}, {"text": "that you can export if you write", "start": 3494.319, "duration": 3.441}, {"text": "parallel gpu code there's more", "start": 3495.52, "duration": 3.52}, {"text": "information at the", "start": 3497.76, "duration": 3.839}, {"text": "user guide to expand user guide now if", "start": 3499.04, "duration": 3.12}, {"text": "you log", "start": 3501.599, "duration": 5.441}, {"text": "in so you get onto expanse", "start": 3502.16, "duration": 4.88}, {"text": "trust ssh into expands the gpu node", "start": 3507.92, "duration": 7.36}, {"text": "if you write a batch shell script", "start": 3511.839, "duration": 6.081}, {"text": "you access them by two different types", "start": 3515.28, "duration": 3.44}, {"text": "of partitions", "start": 3517.92, "duration": 3.28}, {"text": "you have a partition that's called gpu", "start": 3518.72, "duration": 4.16}, {"text": "then you get an entire node", "start": 3521.2, "duration": 4.48}, {"text": "with four gpus or you can use a gpu", "start": 3522.88, "duration": 3.439}, {"text": "shared", "start": 3525.68, "duration": 3.04}, {"text": "queue that you know you can request one", "start": 3526.319, "duration": 3.361}, {"text": "two three", "start": 3528.72, "duration": 2.72}, {"text": "gpus for instance if you don't need four", "start": 3529.68, "duration": 3.04}, {"text": "gpus", "start": 3531.44, "duration": 3.359}, {"text": "you do have to always specify the number", "start": 3532.72, "duration": 3.76}, {"text": "of gpus", "start": 3534.799, "duration": 5.04}, {"text": "um and so this here is an example", "start": 3536.48, "duration": 5.44}, {"text": "where you use s1 to get interactive", "start": 3539.839, "duration": 4.561}, {"text": "access to a compute node and like you", "start": 3541.92, "duration": 4.72}, {"text": "see here on a shared gpu node with a", "start": 3544.4, "duration": 3.28}, {"text": "single", "start": 3546.64, "duration": 3.04}, {"text": "node i want to have with a single gpu", "start": 3547.68, "duration": 4.399}, {"text": "and then i'm asking for a single task", "start": 3549.68, "duration": 2.8}, {"text": "and", "start": 3552.079, "duration": 4.0}, {"text": "and a 10 cpus per task so that's", "start": 3552.48, "duration": 5.119}, {"text": "important if you", "start": 3556.079, "duration": 4.561}, {"text": "are running a openmp code", "start": 3557.599, "duration": 5.601}, {"text": "in combination with a gpu i'm asking", "start": 3560.64, "duration": 4.32}, {"text": "here for 80 gigabytes of memory don't", "start": 3563.2, "duration": 2.159}, {"text": "ask", "start": 3564.96, "duration": 3.599}, {"text": "for more than a quarter of the memory", "start": 3565.359, "duration": 5.121}, {"text": "per gpu otherwise you will be charged", "start": 3568.559, "duration": 4.081}, {"text": "for 2 gpus or three or four depending on", "start": 3570.48, "duration": 3.839}, {"text": "how much memory you choose right", "start": 3572.64, "duration": 4.959}, {"text": "proportionally and the time and then you", "start": 3574.319, "duration": 3.601}, {"text": "know i", "start": 3577.599, "duration": 3.121}, {"text": "just want to have a bash shell and then", "start": 3577.92, "duration": 4.879}, {"text": "drop starts i'm on a on the gpu node", "start": 3580.72, "duration": 3.359}, {"text": "here", "start": 3582.799, "duration": 4.721}, {"text": "because i'm on a gpu note um andy can i", "start": 3584.079, "duration": 4.881}, {"text": "jump in here just for a moment", "start": 3587.52, "duration": 4.0}, {"text": "we're at top of the hour um so some", "start": 3588.96, "duration": 4.08}, {"text": "people may need to drop off i just", "start": 3591.52, "duration": 2.24}, {"text": "wanted to", "start": 3593.04, "duration": 2.88}, {"text": "remind people we are recording this", "start": 3593.76, "duration": 3.44}, {"text": "recording available", "start": 3595.92, "duration": 4.08}, {"text": "and there was one question in the chat", "start": 3597.2, "duration": 3.52}, {"text": "make sure", "start": 3600.0, "duration": 2.64}, {"text": "we've got that covered before if he has", "start": 3600.72, "duration": 4.0}, {"text": "to drop off", "start": 3602.64, "duration": 4.0}, {"text": "but yeah okay yeah keep going because", "start": 3604.72, "duration": 4.079}, {"text": "i'm gonna i'm i'm almost at the end", "start": 3606.64, "duration": 3.36}, {"text": "there's not much more that it's", "start": 3608.799, "duration": 2.481}, {"text": "literally one or two minutes and i'm", "start": 3610.0, "duration": 3.359}, {"text": "done so if people want to hang on", "start": 3611.28, "duration": 6.16}, {"text": "i'd be happy and i apologize for uh", "start": 3613.359, "duration": 4.081}, {"text": "um so this is what i wanted to say don't", "start": 3618.64, "duration": 4.0}, {"text": "ask for more than", "start": 3620.64, "duration": 3.679}, {"text": "you know proportional resources you have", "start": 3622.64, "duration": 3.12}, {"text": "four gpus per node", "start": 3624.319, "duration": 4.561}, {"text": "40 cpu cores and 374 gigabyte of ram so", "start": 3625.76, "duration": 5.12}, {"text": "if you ask for 40 cpu course it will be", "start": 3628.88, "duration": 2.64}, {"text": "charged for", "start": 3630.88, "duration": 3.199}, {"text": "four gpus because nobody else can get on", "start": 3631.52, "duration": 4.48}, {"text": "that node anymore", "start": 3634.079, "duration": 3.681}, {"text": "and then if you want to load the modules", "start": 3636.0, "duration": 4.24}, {"text": "you know it's a module purge module", "start": 3637.76, "duration": 3.12}, {"text": "reset", "start": 3640.24, "duration": 3.68}, {"text": "um sorry this one you can load you don't", "start": 3640.88, "duration": 3.919}, {"text": "have to", "start": 3643.92, "duration": 3.679}, {"text": "um but important piece you can load cuda", "start": 3644.799, "duration": 3.841}, {"text": "pgi", "start": 3647.599, "duration": 4.321}, {"text": "and or nvh pc so this either these two", "start": 3648.64, "duration": 5.76}, {"text": "together or any one of those two or", "start": 3651.92, "duration": 4.48}, {"text": "alternatively in the hpc depending on", "start": 3654.4, "duration": 4.08}, {"text": "which compilers you want to use", "start": 3656.4, "duration": 4.08}, {"text": "and so if you do that and you have the", "start": 3658.48, "duration": 3.44}, {"text": "extruder compiler loaded", "start": 3660.48, "duration": 3.76}, {"text": "you should be available you type nvcc", "start": 3661.92, "duration": 3.6}, {"text": "version it should show you the version", "start": 3664.24, "duration": 2.319}, {"text": "of the compiler", "start": 3665.52, "duration": 4.16}, {"text": "so uh the same for the pgi compilers", "start": 3666.559, "duration": 5.76}, {"text": "like pgcc pgf 90 and", "start": 3669.68, "duration": 5.84}, {"text": "pte keep forgetting how it's called the", "start": 3672.319, "duration": 6.321}, {"text": "c plus plus compiler", "start": 3675.52, "duration": 3.12}, {"text": "this is the interactive access um i'm on", "start": 3679.92, "duration": 4.32}, {"text": "a gpu node now", "start": 3683.119, "duration": 3.601}, {"text": "um nvda smi system management interface", "start": 3684.24, "duration": 3.28}, {"text": "will give you", "start": 3686.72, "duration": 3.359}, {"text": "information about the gpu you'll see", "start": 3687.52, "duration": 5.039}, {"text": "that the v100 s6 and 2 and so on and so", "start": 3690.079, "duration": 4.72}, {"text": "on", "start": 3692.559, "duration": 2.24}, {"text": "it will tell you if there are any", "start": 3694.88, "duration": 3.12}, {"text": "processes running", "start": 3697.119, "duration": 2.321}, {"text": "um at the moment i don't have anything", "start": 3698.0, "duration": 3.44}, {"text": "running the no running processes", "start": 3699.44, "duration": 3.84}, {"text": "found there should be no other jobs", "start": 3701.44, "duration": 3.119}, {"text": "running because you get", "start": 3703.28, "duration": 4.319}, {"text": "um individual access to each gpu um so", "start": 3704.559, "duration": 4.481}, {"text": "they are configured such a way that the", "start": 3707.599, "duration": 3.361}, {"text": "queued or runtime sees only the gpu that", "start": 3709.04, "duration": 3.039}, {"text": "belongs to you", "start": 3710.96, "duration": 5.92}, {"text": "okay the cuda toolkit samples", "start": 3712.079, "duration": 6.881}, {"text": "they are also available here in in this", "start": 3716.88, "duration": 4.719}, {"text": "directory for this you have to load the", "start": 3718.96, "duration": 7.28}, {"text": "cuda 10.2 version to work", "start": 3721.599, "duration": 6.161}, {"text": "you if you copy those into your home", "start": 3726.24, "duration": 3.04}, {"text": "directory that's what i've done here", "start": 3727.76, "duration": 3.2}, {"text": "there's different types of samples from", "start": 3729.28, "duration": 2.799}, {"text": "simple examples", "start": 3730.96, "duration": 3.44}, {"text": "etc so the simple examples and utilities", "start": 3732.079, "duration": 3.841}, {"text": "are useful", "start": 3734.4, "duration": 3.28}, {"text": "you can then just compile those if you", "start": 3735.92, "duration": 3.36}, {"text": "have loaded the cuda", "start": 3737.68, "duration": 4.96}, {"text": "toolkit and this will take a while", "start": 3739.28, "duration": 5.36}, {"text": "or you compile them individually so you", "start": 3742.64, "duration": 4.08}, {"text": "can go into one of those sub directories", "start": 3744.64, "duration": 4.88}, {"text": "and compile them so here if i compile", "start": 3746.72, "duration": 5.119}, {"text": "the device query function", "start": 3749.52, "duration": 3.68}, {"text": "and you can always look at those source", "start": 3751.839, "duration": 2.801}, {"text": "codes and that's why these examples are", "start": 3753.2, "duration": 1.839}, {"text": "so", "start": 3754.64, "duration": 3.12}, {"text": "nice it queries information about the", "start": 3755.039, "duration": 3.28}, {"text": "gpu", "start": 3757.76, "duration": 3.039}, {"text": "right so for instance if i start this", "start": 3758.319, "duration": 4.081}, {"text": "called the device query it tells me i", "start": 3760.799, "duration": 3.841}, {"text": "have a single v100 gpu there and", "start": 3762.4, "duration": 3.84}, {"text": "gives information about the memory the", "start": 3764.64, "duration": 4.479}, {"text": "acute of course available and so forth", "start": 3766.24, "duration": 5.599}, {"text": "and that's the very last thing this is", "start": 3769.119, "duration": 5.121}, {"text": "the last slide um", "start": 3771.839, "duration": 4.401}, {"text": "if you go to the simple examples because", "start": 3774.24, "duration": 3.599}, {"text": "i mentioned matrix multiplication", "start": 3776.24, "duration": 2.799}, {"text": "there's a handwritten matrix", "start": 3777.839, "duration": 2.561}, {"text": "multiplication that has all these", "start": 3779.039, "duration": 3.841}, {"text": "optimizations in there", "start": 3780.4, "duration": 4.959}, {"text": "and you see if you run that it gives you", "start": 3782.88, "duration": 5.0}, {"text": "a performance of", "start": 3785.359, "duration": 7.121}, {"text": "3.2 3.3 teraflops approximately", "start": 3787.88, "duration": 6.439}, {"text": "now there's another example that uses q", "start": 3792.48, "duration": 3.76}, {"text": "plus the q plus function for matrix", "start": 3794.319, "duration": 3.52}, {"text": "multiplication and that gives you like", "start": 3796.24, "duration": 3.44}, {"text": "7.6 teraflops", "start": 3797.839, "duration": 4.48}, {"text": "so you see that you know if you want to", "start": 3799.68, "duration": 3.359}, {"text": "use um", "start": 3802.319, "duration": 3.04}, {"text": "libraries you always you know these are", "start": 3803.039, "duration": 4.56}, {"text": "highly optimized but um", "start": 3805.359, "duration": 4.48}, {"text": "much faster than the handwritten matrix", "start": 3807.599, "duration": 4.161}, {"text": "multiplication here", "start": 3809.839, "duration": 4.641}, {"text": "and yeah with that i'm i'm at the end", "start": 3811.76, "duration": 3.839}, {"text": "and", "start": 3814.48, "duration": 2.879}, {"text": "thank you for bearing with me now if", "start": 3815.599, "duration": 3.841}, {"text": "there was a question um i don't know i", "start": 3817.359, "duration": 3.44}, {"text": "don't see", "start": 3819.44, "duration": 3.2}, {"text": "jeff could you tell me what the question", "start": 3820.799, "duration": 3.76}, {"text": "is and i can try to answer", "start": 3822.64, "duration": 4.56}, {"text": "yeah gregory wolf was asking he's an", "start": 3824.559, "duration": 4.8}, {"text": "experienced programmer of single gpu", "start": 3827.2, "duration": 2.96}, {"text": "machines", "start": 3829.359, "duration": 3.2}, {"text": "using cuda what does he need to look out", "start": 3830.16, "duration": 3.12}, {"text": "for", "start": 3832.559, "duration": 4.961}, {"text": "or consider when developing for expanse", "start": 3833.28, "duration": 4.24}, {"text": "okay um so if you're experienced writing", "start": 3837.599, "duration": 7.841}, {"text": "let me go here to the last slide um if", "start": 3842.48, "duration": 3.28}, {"text": "you're", "start": 3845.44, "duration": 3.359}, {"text": "if you are an experienced programmer so", "start": 3845.76, "duration": 4.24}, {"text": "it depends on", "start": 3848.799, "duration": 2.721}, {"text": "on the language that you've been using", "start": 3850.0, "duration": 3.039}, {"text": "but so the different ways", "start": 3851.52, "duration": 4.559}, {"text": "to program multiple gpus um", "start": 3853.039, "duration": 6.56}, {"text": "one thing is to use cuda streams um", "start": 3856.079, "duration": 5.601}, {"text": "where you know if you're using if you", "start": 3859.599, "duration": 3.2}, {"text": "have um", "start": 3861.68, "duration": 3.6}, {"text": "computer code you can use cuda streams", "start": 3862.799, "duration": 3.681}, {"text": "and then", "start": 3865.28, "duration": 5.12}, {"text": "you can launch code", "start": 3866.48, "duration": 5.839}, {"text": "or kernels execute kernels on different", "start": 3870.4, "duration": 3.6}, {"text": "gpus you know you can transfer part of", "start": 3872.319, "duration": 3.361}, {"text": "your data to one gpu part of the other", "start": 3874.0, "duration": 3.599}, {"text": "gpu and then there are", "start": 3875.68, "duration": 3.84}, {"text": "of course also communication that has to", "start": 3877.599, "duration": 4.081}, {"text": "happen between the gpus and their", "start": 3879.52, "duration": 3.839}, {"text": "libraries for communication between the", "start": 3881.68, "duration": 3.679}, {"text": "gpus from nvidia", "start": 3883.359, "duration": 3.68}, {"text": "the other option obviously is to use", "start": 3885.359, "duration": 3.68}, {"text": "openmp", "start": 3887.039, "duration": 4.721}, {"text": "you write a parallel code cpu parallel", "start": 3889.039, "duration": 5.121}, {"text": "code and you have individual threads", "start": 3891.76, "duration": 5.52}, {"text": "handling individual gpus um", "start": 3894.16, "duration": 6.08}, {"text": "and finally you can use mpi and what you", "start": 3897.28, "duration": 3.6}, {"text": "always have", "start": 3900.24, "duration": 4.4}, {"text": "to watch out for is is", "start": 3900.88, "duration": 5.52}, {"text": "you know whether you can decompose your", "start": 3904.64, "duration": 3.76}, {"text": "problem without having to communicate", "start": 3906.4, "duration": 3.919}, {"text": "between the gpus or not and", "start": 3908.4, "duration": 4.56}, {"text": "you know that you don't similar things", "start": 3910.319, "duration": 3.52}, {"text": "that if you write", "start": 3912.96, "duration": 4.32}, {"text": "parallel code that uses", "start": 3913.839, "duration": 7.2}, {"text": "multiple nodes because because", "start": 3917.28, "duration": 5.519}, {"text": "you know the communication stuff at some", "start": 3921.039, "duration": 4.241}, {"text": "point might be", "start": 3922.799, "duration": 5.52}, {"text": "limiting and if you copy", "start": 3925.28, "duration": 5.039}, {"text": "data for them back between the cpu and", "start": 3928.319, "duration": 4.081}, {"text": "the gpu memory you know that could be", "start": 3930.319, "duration": 4.881}, {"text": "just just rate limiting those are some", "start": 3932.4, "duration": 4.399}, {"text": "of the considerations to consider", "start": 3935.2, "duration": 5.68}, {"text": "um and um", "start": 3936.799, "duration": 5.76}, {"text": "yeah i hope that answers the question", "start": 3940.88, "duration": 2.959}, {"text": "but i don't know if there are further", "start": 3942.559, "duration": 2.321}, {"text": "questions that we have", "start": 3943.839, "duration": 3.601}, {"text": "to discuss", "start": 3944.88, "duration": 2.56}, {"text": "yeah if anyone uh wants to unmute that's", "start": 3948.16, "duration": 4.08}, {"text": "fine as well there was another question", "start": 3950.799, "duration": 2.481}, {"text": "compared to cuda", "start": 3952.24, "duration": 2.879}, {"text": "how much performance is lost when using", "start": 3953.28, "duration": 4.079}, {"text": "open acc", "start": 3955.119, "duration": 5.521}, {"text": "oh good question um", "start": 3957.359, "duration": 6.561}, {"text": "huh it's it it it opens your acc can be", "start": 3960.64, "duration": 6.399}, {"text": "very efficient um", "start": 3963.92, "duration": 4.639}, {"text": "i wouldn't say how much performance is", "start": 3967.039, "duration": 3.441}, {"text": "lost because usually if you don't", "start": 3968.559, "duration": 4.161}, {"text": "translate a cuda code into open acc", "start": 3970.48, "duration": 3.52}, {"text": "usually you go the other way", "start": 3972.72, "duration": 3.2}, {"text": "the the big question is how much more", "start": 3974.0, "duration": 3.92}, {"text": "effort is it to write a qr code and open", "start": 3975.92, "duration": 3.919}, {"text": "acc because the most expensive piece", "start": 3977.92, "duration": 3.28}, {"text": "typically is you know", "start": 3979.839, "duration": 4.641}, {"text": "developer time um unless you get to some", "start": 3981.2, "duration": 4.08}, {"text": "code that", "start": 3984.48, "duration": 2.96}, {"text": "you know like our anti-simulation code", "start": 3985.28, "duration": 4.72}, {"text": "is literally earning i don't know", "start": 3987.44, "duration": 4.0}, {"text": "a few percent of all the available", "start": 3990.0, "duration": 3.039}, {"text": "compute power", "start": 3991.44, "duration": 4.96}, {"text": "on on the exceed resources so that makes", "start": 3993.039, "duration": 4.961}, {"text": "sense to actually really put a lot of", "start": 3996.4, "duration": 3.679}, {"text": "effort into porting", "start": 3998.0, "duration": 4.64}, {"text": "so open acc is a good way to start", "start": 4000.079, "duration": 3.76}, {"text": "porting code if you", "start": 4002.64, "duration": 3.439}, {"text": "have only cpu code and it can be high", "start": 4003.839, "duration": 3.921}, {"text": "performance", "start": 4006.079, "duration": 3.441}, {"text": "but it's not a magical bullet so you", "start": 4007.76, "duration": 3.279}, {"text": "often have to restructure the code and", "start": 4009.52, "duration": 2.48}, {"text": "once you", "start": 4011.039, "duration": 3.04}, {"text": "get into doing that you could", "start": 4012.0, "duration": 3.76}, {"text": "potentially write cuda kernels", "start": 4014.079, "duration": 4.72}, {"text": "that can be faster here", "start": 4015.76, "duration": 4.72}, {"text": "but it's it's it's not a bad place to", "start": 4018.799, "duration": 4.961}, {"text": "start it's a good place to start", "start": 4020.48, "duration": 3.28}, {"text": "stop sharing here", "start": 4028.559, "duration": 2.881}, {"text": "don't know if there are any other", "start": 4032.24, "duration": 3.599}, {"text": "questions", "start": 4033.359, "duration": 2.48}, {"text": "no more in the chat um okay do you have", "start": 4036.799, "duration": 4.8}, {"text": "any other questions", "start": 4040.16, "duration": 4.8}, {"text": "if not going once going twice", "start": 4041.599, "duration": 5.281}, {"text": "um well thanks very much andy this was", "start": 4044.96, "duration": 3.119}, {"text": "very uh", "start": 4046.88, "duration": 4.239}, {"text": "interesting and useful and hopefully the", "start": 4048.079, "duration": 4.24}, {"text": "audience got", "start": 4051.119, "duration": 3.601}, {"text": "a lot out of it we will be making the", "start": 4052.319, "duration": 4.0}, {"text": "recording available as soon as possible", "start": 4054.72, "duration": 2.0}, {"text": "after", "start": 4056.319, "duration": 3.52}, {"text": "zoom finishes the transcription", "start": 4056.72, "duration": 4.8}, {"text": "other than that thanks everyone for", "start": 4059.839, "duration": 4.561}, {"text": "joining have a great rest of your day", "start": 4061.52, "duration": 4.72}, {"text": "and we'll call it a day thanks again", "start": 4064.4, "duration": 3.04}, {"text": "andy", "start": 4066.24, "duration": 4.319}, {"text": "thank you very much everybody", "start": 4067.44, "duration": 3.119}, {"text": "bye everyone goodbye", "start": 4070.72, "duration": 6.24}]