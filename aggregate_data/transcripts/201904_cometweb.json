[{"text": "and just you know Jeff and I do answer", "start": 15.719, "duration": 4.221}, {"text": "all right hello good morning everybody", "start": 34.34, "duration": 5.32}, {"text": "um this is Andreas Gertz for speaking", "start": 37.8, "duration": 3.36}, {"text": "from the San Diego Super Computing", "start": 39.66, "duration": 2.34}, {"text": "Center", "start": 41.16, "duration": 4.26}, {"text": "and uh so welcome to the webinar", "start": 42.0, "duration": 6.12}, {"text": "and I will talk today about GPU", "start": 45.42, "duration": 6.06}, {"text": "Computing and programming and uh a", "start": 48.12, "duration": 6.72}, {"text": "little bit on how to get along for GPU", "start": 51.48, "duration": 6.18}, {"text": "Computing on our supercomputer here at", "start": 54.84, "duration": 6.539}, {"text": "sdsc so sdsc comment", "start": 57.66, "duration": 7.2}, {"text": "so if you have any questions during the", "start": 61.379, "duration": 5.821}, {"text": "webinar um you can use the chat and we", "start": 64.86, "duration": 5.22}, {"text": "have um Mary Thomas and Jeff sale here", "start": 67.2, "duration": 4.739}, {"text": "who would be able to either answer some", "start": 70.08, "duration": 4.02}, {"text": "of the questions or maybe um refer them", "start": 71.939, "duration": 3.72}, {"text": "to me so that I could answer questions", "start": 74.1, "duration": 4.44}, {"text": "that you might have along the way", "start": 75.659, "duration": 3.901}, {"text": "um the webinar is going to be", "start": 78.54, "duration": 3.0}, {"text": "approximately an hour um I'll try to", "start": 79.56, "duration": 4.559}, {"text": "stick with the time and we'll see how it", "start": 81.54, "duration": 3.84}, {"text": "goes", "start": 84.119, "duration": 2.221}, {"text": "um", "start": 85.38, "duration": 3.9}, {"text": "and go from there so", "start": 86.34, "duration": 5.459}, {"text": "let me see if I can advance the slides", "start": 89.28, "duration": 3.72}, {"text": "yeah", "start": 91.799, "duration": 3.36}, {"text": "so the overview is basically what I will", "start": 93.0, "duration": 4.14}, {"text": "cover is going a little bit over GPU", "start": 95.159, "duration": 4.201}, {"text": "Hardware overview", "start": 97.14, "duration": 4.019}, {"text": "um quickly talk about some GPU", "start": 99.36, "duration": 4.2}, {"text": "accelerated software examples", "start": 101.159, "duration": 4.621}, {"text": "um and then how to program gpus using", "start": 103.56, "duration": 5.22}, {"text": "libraries that are GPU enabled give a", "start": 105.78, "duration": 5.04}, {"text": "very very quick introduction to qdc", "start": 108.78, "duration": 3.72}, {"text": "programming so what you have to know to", "start": 110.82, "duration": 2.88}, {"text": "understand a little bit what happens", "start": 112.5, "duration": 4.2}, {"text": "when you run a GPU code and how you", "start": 113.7, "duration": 4.62}, {"text": "could get started yourself", "start": 116.7, "duration": 4.919}, {"text": "a short introduction to open ACC and", "start": 118.32, "duration": 4.92}, {"text": "give you a quick overview on how to", "start": 121.619, "duration": 4.381}, {"text": "access GPU notes on Comet and run GPU", "start": 123.24, "duration": 5.939}, {"text": "drops on stlc comic", "start": 126.0, "duration": 6.239}, {"text": "so to go back what is a GPU it's uh", "start": 129.179, "duration": 5.161}, {"text": "basically you might have heard the term", "start": 132.239, "duration": 4.86}, {"text": "accelerator and an accelerator is simply", "start": 134.34, "duration": 4.38}, {"text": "a specialized Hardware component that", "start": 137.099, "duration": 3.72}, {"text": "speeds up certain aspects of a Computing", "start": 138.72, "duration": 4.04}, {"text": "workload", "start": 140.819, "duration": 4.14}, {"text": "some of you who are a little bit older", "start": 142.76, "duration": 4.9}, {"text": "like myself might remember floating", "start": 144.959, "duration": 4.201}, {"text": "Point core processor for instance in", "start": 147.66, "duration": 3.84}, {"text": "older PCS that were specialized chips to", "start": 149.16, "duration": 3.84}, {"text": "perform floating point marks in Hardware", "start": 151.5, "duration": 4.26}, {"text": "rather than software so that could speed", "start": 153.0, "duration": 4.739}, {"text": "up mathematical operations", "start": 155.76, "duration": 3.96}, {"text": "more recently there are also a field", "start": 157.739, "duration": 6.241}, {"text": "programmable gate arrays fpgas and of", "start": 159.72, "duration": 6.42}, {"text": "course Graphics processing units are are", "start": 163.98, "duration": 4.74}, {"text": "very widely used and very popular and", "start": 166.14, "duration": 4.319}, {"text": "the GPU is basically a specialist", "start": 168.72, "duration": 3.96}, {"text": "processor to accelerate the rendering of", "start": 170.459, "duration": 5.28}, {"text": "computer graphics and that development", "start": 172.68, "duration": 5.699}, {"text": "has been driven by a very big gaming", "start": 175.739, "duration": 3.841}, {"text": "industry this was a lot of money", "start": 178.379, "duration": 2.961}, {"text": "involved in there", "start": 179.58, "duration": 4.439}, {"text": "which explains some of the developments", "start": 181.34, "duration": 4.96}, {"text": "that have happened over the last decade", "start": 184.019, "duration": 5.821}, {"text": "originally uh that GPU Hardware", "start": 186.3, "duration": 5.76}, {"text": "was using fixed function pipelines so", "start": 189.84, "duration": 3.899}, {"text": "they were not easily programmable for", "start": 192.06, "duration": 3.2}, {"text": "general purpose", "start": 193.739, "duration": 4.561}, {"text": "but nowadays modern gpus basically since", "start": 195.26, "duration": 5.14}, {"text": "a decade approximately are programmable", "start": 198.3, "duration": 4.14}, {"text": "for general purpose computation so that", "start": 200.4, "duration": 4.8}, {"text": "you can effectively use them", "start": 202.44, "duration": 3.42}, {"text": "um", "start": 205.2, "duration": 3.36}, {"text": "for any Computing tasks that otherwise", "start": 205.86, "duration": 4.68}, {"text": "you would be using a central processing", "start": 208.56, "duration": 3.78}, {"text": "unit for", "start": 210.54, "duration": 5.279}, {"text": "um however the in comparison to a CPU", "start": 212.34, "duration": 5.28}, {"text": "there's a simplified Core Design if you", "start": 215.819, "duration": 4.381}, {"text": "want so so um with limited architectural", "start": 217.62, "duration": 4.979}, {"text": "features for instance launch caches and", "start": 220.2, "duration": 3.66}, {"text": "there's a partially exposed memory", "start": 222.599, "duration": 3.06}, {"text": "hierarchy that you should know about", "start": 223.86, "duration": 5.82}, {"text": "when you're programming uh gpus", "start": 225.659, "duration": 7.561}, {"text": "and just to give you a background of why", "start": 229.68, "duration": 5.46}, {"text": "there's such an interest in gpus so on", "start": 233.22, "duration": 4.079}, {"text": "the right you see a plot that plots the", "start": 235.14, "duration": 4.679}, {"text": "years and then a number of different", "start": 237.299, "duration": 3.3}, {"text": "um", "start": 239.819, "duration": 2.84}, {"text": "properties of", "start": 240.599, "duration": 4.56}, {"text": "integrated ships", "start": 242.659, "duration": 4.72}, {"text": "um so what you can see is for instance", "start": 245.159, "duration": 4.021}, {"text": "in yellow the number of transistors and", "start": 247.379, "duration": 3.42}, {"text": "thousands and it's a logarithmic scale", "start": 249.18, "duration": 3.419}, {"text": "that's basically Moore's law that you", "start": 250.799, "duration": 3.901}, {"text": "have probably heard about which states", "start": 252.599, "duration": 3.86}, {"text": "as a transistor count approximately", "start": 254.7, "duration": 4.379}, {"text": "doubles approximately every two years", "start": 256.459, "duration": 5.201}, {"text": "and that still holds nowadays", "start": 259.079, "duration": 5.161}, {"text": "this exponential growth however since", "start": 261.66, "duration": 4.62}, {"text": "the mid-2000s what you can see in black", "start": 264.24, "duration": 4.56}, {"text": "for instance the number of compute cores", "start": 266.28, "duration": 5.28}, {"text": "we started to get like multi-core CPUs", "start": 268.8, "duration": 6.0}, {"text": "and the reason is simply", "start": 271.56, "duration": 5.48}, {"text": "um because the performance of a single", "start": 274.8, "duration": 5.349}, {"text": "[Music]", "start": 277.04, "duration": 3.109}, {"text": "process as a threat single processors", "start": 283.34, "duration": 6.1}, {"text": "see basically it's been constant and the", "start": 286.259, "duration": 5.041}, {"text": "single threat performance has not", "start": 289.44, "duration": 4.02}, {"text": "increased as dramatically as it used to", "start": 291.3, "duration": 4.26}, {"text": "so while in the past you could just wait", "start": 293.46, "duration": 3.36}, {"text": "two years to double the execution", "start": 295.56, "duration": 3.12}, {"text": "performance of your code that's not the", "start": 296.82, "duration": 3.599}, {"text": "case anymore in system it 2000s we must", "start": 298.68, "duration": 4.68}, {"text": "write parallel code and why is there", "start": 300.419, "duration": 4.621}, {"text": "such an interest in particularly in gpus", "start": 303.36, "duration": 3.54}, {"text": "if you look at these plots what they're", "start": 305.04, "duration": 4.2}, {"text": "plotting here is the floating Point", "start": 306.9, "duration": 3.6}, {"text": "performance", "start": 309.24, "duration": 4.62}, {"text": "on the y-axis versus the year", "start": 310.5, "duration": 6.56}, {"text": "and in blue you see um into Xeon CPUs", "start": 313.86, "duration": 6.779}, {"text": "and in red and green you see", "start": 317.06, "duration": 7.66}, {"text": "um AMD gpus and Nvidia gpus and what you", "start": 320.639, "duration": 6.301}, {"text": "can see is the gpus offer significantly", "start": 324.72, "duration": 3.9}, {"text": "higher floating Point performance than", "start": 326.94, "duration": 4.02}, {"text": "CPUs again so that's a logarithmic scale", "start": 328.62, "duration": 4.38}, {"text": "and that Gap has", "start": 330.96, "duration": 2.82}, {"text": "um", "start": 333.0, "duration": 3.479}, {"text": "so the gpus have maintained that", "start": 333.78, "duration": 5.1}, {"text": "performance gun", "start": 336.479, "duration": 6.121}, {"text": "it holds both for gaming gpus on and as", "start": 338.88, "duration": 4.62}, {"text": "well as", "start": 342.6, "duration": 4.08}, {"text": "um data center gpus in particular if you", "start": 343.5, "duration": 5.52}, {"text": "are interested or need 64-bit floating", "start": 346.68, "duration": 5.16}, {"text": "performance you have to use um so-called", "start": 349.02, "duration": 4.619}, {"text": "data center gpus which are more", "start": 351.84, "duration": 5.22}, {"text": "expensive than the cheap gaming gpus", "start": 353.639, "duration": 6.421}, {"text": "um but it's that's not all gpus also", "start": 357.06, "duration": 4.56}, {"text": "have a significantly higher memory", "start": 360.06, "duration": 3.419}, {"text": "bandwidth than CPU so that's important", "start": 361.62, "duration": 4.019}, {"text": "because if you can't get the data from", "start": 363.479, "duration": 3.121}, {"text": "Ram", "start": 365.639, "duration": 3.421}, {"text": "through the processing course", "start": 366.6, "duration": 5.099}, {"text": "us effectively wasting flops so what we", "start": 369.06, "duration": 5.699}, {"text": "say nowadays is that flops are free so", "start": 371.699, "duration": 5.581}, {"text": "um and and most uh codes and most", "start": 374.759, "duration": 4.5}, {"text": "algorithms are actually memory bandwidth", "start": 377.28, "duration": 6.419}, {"text": "bound and also here gpus outperform CPUs", "start": 379.259, "duration": 7.021}, {"text": "in terms of the memory bandwidth so in", "start": 383.699, "duration": 4.261}, {"text": "principles a combination of the high", "start": 386.28, "duration": 3.9}, {"text": "memory bandwidth and the high floating", "start": 387.96, "duration": 3.959}, {"text": "Point performance should lead to a", "start": 390.18, "duration": 4.92}, {"text": "higher um performance of your code if it", "start": 391.919, "duration": 5.161}, {"text": "has been written in a proper way so if", "start": 395.1, "duration": 3.36}, {"text": "you're using algorithms that can make", "start": 397.08, "duration": 4.739}, {"text": "use of the GPU architecture", "start": 398.46, "duration": 4.679}, {"text": "um then again if you look at the power", "start": 401.819, "duration": 3.66}, {"text": "consumption to have a fair comparison", "start": 403.139, "duration": 6.961}, {"text": "so a single um CPU current version on", "start": 405.479, "duration": 6.06}, {"text": "the right hand side on the plot you see", "start": 410.1, "duration": 3.96}, {"text": "that consumes approximately", "start": 411.539, "duration": 5.521}, {"text": "half the power of a single high-end GPU", "start": 414.06, "duration": 5.1}, {"text": "so a fair comparison in terms of like", "start": 417.06, "duration": 4.02}, {"text": "performance", "start": 419.16, "duration": 3.659}, {"text": "um given a certain power consumption", "start": 421.08, "duration": 4.02}, {"text": "would be a single GPU for instance a", "start": 422.819, "duration": 6.481}, {"text": "Tesla p100 versus two high-end", "start": 425.1, "duration": 5.3}, {"text": "um", "start": 429.3, "duration": 6.08}, {"text": "CPUs in a dual socket CPU server", "start": 430.4, "duration": 4.98}, {"text": "now here's a summary again just to give", "start": 436.139, "duration": 5.161}, {"text": "you again an idea about a comparison of", "start": 438.72, "duration": 6.84}, {"text": "CPUs versus gpus so we have a table here", "start": 441.3, "duration": 7.08}, {"text": "with a high-end dual soccer socket Intel", "start": 445.56, "duration": 4.4}, {"text": "8180", "start": 448.38, "duration": 5.7}, {"text": "Skylake so 28 core CPUs that was 56", "start": 449.96, "duration": 6.22}, {"text": "cores in total", "start": 454.08, "duration": 3.959}, {"text": "um and in such a server you could stick", "start": 456.18, "duration": 5.04}, {"text": "in for instance a two to win V100 Nvidia", "start": 458.039, "duration": 6.421}, {"text": "Tesla gpus so these are the high-end um", "start": 461.22, "duration": 6.96}, {"text": "data center gpus by Nvidia and so you", "start": 464.46, "duration": 5.82}, {"text": "can see that the Peak Performance and", "start": 468.18, "duration": 3.84}, {"text": "both in double precision and in single", "start": 470.28, "duration": 6.359}, {"text": "Precision is approximately a factor of", "start": 472.02, "duration": 7.799}, {"text": "3.5 higher this is now comparing two", "start": 476.639, "duration": 6.96}, {"text": "gpus to the two CPUs in addition what", "start": 479.819, "duration": 6.72}, {"text": "gpus have available in Hardware now is", "start": 483.599, "duration": 5.22}, {"text": "so called half Precision so if you have", "start": 486.539, "duration": 3.6}, {"text": "an algorithm typically that's machine", "start": 488.819, "duration": 2.94}, {"text": "learning nowadays", "start": 490.139, "duration": 3.0}, {"text": "um you can get very high performance", "start": 491.759, "duration": 4.44}, {"text": "also with a 16-bit floating Point", "start": 493.139, "duration": 5.881}, {"text": "numbers and the memory bandwidth of", "start": 496.199, "duration": 4.741}, {"text": "course is also much higher", "start": 499.02, "duration": 4.14}, {"text": "and then", "start": 500.94, "duration": 4.8}, {"text": "I have one row here that is the PCI", "start": 503.16, "duration": 4.2}, {"text": "Express bandwidth", "start": 505.74, "duration": 3.72}, {"text": "so that's something that you don't have", "start": 507.36, "duration": 3.779}, {"text": "to worry about in a CPU but since the", "start": 509.46, "duration": 3.66}, {"text": "gpus they are basically accelerators", "start": 511.139, "duration": 2.941}, {"text": "that are", "start": 513.12, "duration": 3.299}, {"text": "stuck in in most of the cases into a PCI", "start": 514.08, "duration": 3.899}, {"text": "Express Bus you have to transfer data", "start": 516.419, "duration": 3.54}, {"text": "from the ram that's attached to the CPU", "start": 517.979, "duration": 3.841}, {"text": "to the GPU through the PCI Express plus", "start": 519.959, "duration": 3.96}, {"text": "and that has a relatively low bandwidth", "start": 521.82, "duration": 4.139}, {"text": "so that can be a performance bottleneck", "start": 523.919, "duration": 4.801}, {"text": "and code is it portable yes or no we'll", "start": 525.959, "duration": 4.141}, {"text": "learn a little bit more about that in", "start": 528.72, "duration": 3.119}, {"text": "principle", "start": 530.1, "duration": 4.2}, {"text": "um also GPU code is portable if it's", "start": 531.839, "duration": 5.821}, {"text": "been written with open ACC or opencl", "start": 534.3, "duration": 6.18}, {"text": "um not necessarily with Cura although", "start": 537.66, "duration": 5.52}, {"text": "you know Nvidia is a very strong company", "start": 540.48, "duration": 4.68}, {"text": "so it's probably not going to be going", "start": 543.18, "duration": 5.9}, {"text": "to go away anytime soon either", "start": 545.16, "duration": 3.92}, {"text": "uh just as a historical perspective", "start": 549.6, "duration": 4.32}, {"text": "um", "start": 552.54, "duration": 3.54}, {"text": "if you look at the top", "start": 553.92, "duration": 4.74}, {"text": "left so that's ASCII white uh", "start": 556.08, "duration": 4.379}, {"text": "supercomputed it was the number one on", "start": 558.66, "duration": 4.38}, {"text": "the top 500 list in 2001 so almost two", "start": 560.459, "duration": 3.961}, {"text": "decades ago in Lawrence Livermore", "start": 563.04, "duration": 3.84}, {"text": "National Lab had a peak a performance of", "start": 564.42, "duration": 4.44}, {"text": "12 teraflop per second", "start": 566.88, "duration": 4.56}, {"text": "and cost 110 million US dollar back in", "start": 568.86, "duration": 5.039}, {"text": "2001 so really really expensive so what", "start": 571.44, "duration": 6.12}, {"text": "we have at sdsc has 2.8 petaflops per", "start": 573.899, "duration": 5.161}, {"text": "second aggregate performance so that's", "start": 577.56, "duration": 3.6}, {"text": "the performance that we get out out of", "start": 579.06, "duration": 4.44}, {"text": "all our CPU cores and all the gpus that", "start": 581.16, "duration": 5.22}, {"text": "are installed we have 36 nodes that have", "start": 583.5, "duration": 5.76}, {"text": "each two Nvidia k80 gpus", "start": 586.38, "duration": 5.519}, {"text": "and we have 36 nodes that have each four", "start": 589.26, "duration": 6.12}, {"text": "Nvidia p100 gpus", "start": 591.899, "duration": 6.0}, {"text": "um so each of those nodes has a", "start": 595.38, "duration": 5.639}, {"text": "performance of 5.5 teraflops double", "start": 597.899, "duration": 5.281}, {"text": "Precision or 16.4 teraflop single", "start": 601.019, "duration": 3.301}, {"text": "Precision", "start": 603.18, "duration": 2.88}, {"text": "um so you can see that a single node at", "start": 604.32, "duration": 4.62}, {"text": "least um of the k80 notes already has a", "start": 606.06, "duration": 4.08}, {"text": "higher performance in single Precision", "start": 608.94, "duration": 3.48}, {"text": "than the entire ASCII White Machine", "start": 610.14, "duration": 6.18}, {"text": "and the p100 so four p100s outperform", "start": 612.42, "duration": 5.76}, {"text": "the entire ASCII White Machine already", "start": 616.32, "duration": 4.62}, {"text": "on the double precision and in this case", "start": 618.18, "duration": 4.32}, {"text": "the cost for the entire machine is 25", "start": 620.94, "duration": 3.72}, {"text": "million approximately where 14 million", "start": 622.5, "duration": 3.42}, {"text": "went into hardware and the rest is", "start": 624.66, "duration": 3.54}, {"text": "Operation costs now if you want to build", "start": 625.92, "duration": 3.84}, {"text": "your own supercomputer effectively", "start": 628.2, "duration": 3.3}, {"text": "something it all performs at least in", "start": 629.76, "duration": 3.42}, {"text": "single Precision", "start": 631.5, "duration": 3.54}, {"text": "um a supercomputer from two decades ago", "start": 633.18, "duration": 4.26}, {"text": "you can go just online and you buy four", "start": 635.04, "duration": 5.88}, {"text": "Nvidia RTX 2080 gpus that's uh these are", "start": 637.44, "duration": 4.68}, {"text": "some of the latest", "start": 640.92, "duration": 2.64}, {"text": "um", "start": 642.12, "duration": 4.44}, {"text": "um gpus that are available and you get a", "start": 643.56, "duration": 5.82}, {"text": "performance of 1.3 teraflops single", "start": 646.56, "duration": 4.74}, {"text": "Precision so that's 10 perfect percent", "start": 649.38, "duration": 4.019}, {"text": "of the entire supercomputer ASCII white", "start": 651.3, "duration": 5.039}, {"text": "back then or you can outperform that", "start": 653.399, "duration": 5.821}, {"text": "significantly with 40 teraflops in uh", "start": 656.339, "duration": 4.68}, {"text": "single precision and that costs you", "start": 659.22, "duration": 3.179}, {"text": "approximately five thousand dollars so", "start": 661.019, "duration": 2.581}, {"text": "you have a lot of computing power", "start": 662.399, "duration": 4.261}, {"text": "available by using gpus", "start": 663.6, "duration": 3.66}, {"text": "um", "start": 666.66, "duration": 3.6}, {"text": "okay", "start": 667.26, "duration": 3.0}, {"text": "so there's a question", "start": 672.12, "duration": 3.24}, {"text": "um is quad Precision calculations", "start": 673.92, "duration": 3.96}, {"text": "possible on gpus if not now anytime in", "start": 675.36, "duration": 4.979}, {"text": "the near future so drug Precision is not", "start": 677.88, "duration": 4.199}, {"text": "supported in Hardware so what you would", "start": 680.339, "duration": 3.361}, {"text": "have to do is you would have to", "start": 682.079, "duration": 3.961}, {"text": "implement that in software and make sure", "start": 683.7, "duration": 4.44}, {"text": "that um", "start": 686.04, "duration": 5.82}, {"text": "it executes efficiently on on the gpus", "start": 688.14, "duration": 6.98}, {"text": "so that's that's", "start": 691.86, "duration": 3.26}, {"text": "all", "start": 695.579, "duration": 2.481}, {"text": "nowadays literally virtually from any", "start": 699.74, "duration": 3.88}, {"text": "field you can go from chemistry Life", "start": 702.0, "duration": 3.839}, {"text": "Sciences bioinformatics so uh the", "start": 703.62, "duration": 4.26}, {"text": "astrophysics or standard science is of", "start": 705.839, "duration": 4.381}, {"text": "course also then Finance Medical Imaging", "start": 707.88, "duration": 4.019}, {"text": "natural language processing social", "start": 710.22, "duration": 4.02}, {"text": "sciences weather and climate codes", "start": 711.899, "duration": 4.201}, {"text": "fluid dynamics and of course machine", "start": 714.24, "duration": 4.08}, {"text": "learning lots of machine learning lately", "start": 716.1, "duration": 4.799}, {"text": "um if you go to that link up there", "start": 718.32, "duration": 5.459}, {"text": "Nvidia has a very nice compilation of", "start": 720.899, "duration": 5.101}, {"text": "software that they support on their", "start": 723.779, "duration": 3.481}, {"text": "Hardware that they have supported", "start": 726.0, "duration": 4.5}, {"text": "developing and with performance examples", "start": 727.26, "duration": 4.74}, {"text": "um so in general it's actually a very", "start": 730.5, "duration": 2.94}, {"text": "nice idea to look around on their web", "start": 732.0, "duration": 2.88}, {"text": "page because they have a lot of useful", "start": 733.44, "duration": 3.66}, {"text": "information out there that's not only on", "start": 734.88, "duration": 3.54}, {"text": "marketing", "start": 737.1, "duration": 3.239}, {"text": "um you have to take everything with a", "start": 738.42, "duration": 3.06}, {"text": "little bit of grain of salt of course", "start": 740.339, "duration": 2.521}, {"text": "there is some marketing involved but", "start": 741.48, "duration": 2.58}, {"text": "they have a lot of background and", "start": 742.86, "duration": 3.659}, {"text": "programming examples and so on I'll tell", "start": 744.06, "duration": 5.6}, {"text": "more about that in a minute", "start": 746.519, "duration": 3.141}, {"text": "um yeah machine learning so why why is", "start": 749.94, "duration": 4.96}, {"text": "machine learning um so popular in gpus", "start": 752.04, "duration": 3.239}, {"text": "[Music]", "start": 754.9, "duration": 1.28}, {"text": "um", "start": 755.279, "duration": 4.021}, {"text": "or as such a big bus on gpus or gpus are", "start": 756.18, "duration": 4.62}, {"text": "basically is driving a lot of the sales", "start": 759.3, "duration": 3.36}, {"text": "of gpus and so what you do in machine", "start": 760.8, "duration": 4.2}, {"text": "learning so if you're not familiar with", "start": 762.66, "duration": 3.299}, {"text": "that", "start": 765.0, "duration": 3.18}, {"text": "is effectively You're Building uh", "start": 765.959, "duration": 3.781}, {"text": "predictive models based on reference", "start": 768.18, "duration": 4.68}, {"text": "data so you have like training data that", "start": 769.74, "duration": 4.44}, {"text": "you train in a machine learning", "start": 772.86, "duration": 3.06}, {"text": "algorithm to build a model", "start": 774.18, "duration": 3.719}, {"text": "that then can be used with input data to", "start": 775.92, "duration": 4.14}, {"text": "make a prediction and so we can check", "start": 777.899, "duration": 3.721}, {"text": "whether the predictions are accurate and", "start": 780.06, "duration": 3.0}, {"text": "then improve your training model so", "start": 781.62, "duration": 3.54}, {"text": "that's basically very simplified the", "start": 783.06, "duration": 3.66}, {"text": "workflow that you're typically typically", "start": 785.16, "duration": 4.26}, {"text": "take with machine learning this is not a", "start": 786.72, "duration": 4.08}, {"text": "machine learning webinar so I'm not", "start": 789.42, "duration": 3.0}, {"text": "going to talk much about it", "start": 790.8, "duration": 4.44}, {"text": "but there's it's a big field there are", "start": 792.42, "duration": 4.14}, {"text": "many different methods and algorithms", "start": 795.24, "duration": 4.02}, {"text": "and what gpus really shine at are deep", "start": 796.56, "duration": 4.98}, {"text": "learning workloads and deep learning", "start": 799.26, "duration": 4.019}, {"text": "basically are neural networks with many", "start": 801.54, "duration": 4.2}, {"text": "hidden layers as exemplified here on the", "start": 803.279, "duration": 3.901}, {"text": "bottom right", "start": 805.74, "duration": 3.539}, {"text": "um so you have an input layer", "start": 807.18, "duration": 4.2}, {"text": "and then many of these hidden layers and", "start": 809.279, "duration": 4.261}, {"text": "then an output layer and effectively", "start": 811.38, "duration": 4.98}, {"text": "what that is is tensor operations which", "start": 813.54, "duration": 4.38}, {"text": "means a lot of Matrix multiplications", "start": 816.36, "duration": 3.719}, {"text": "and gpus are very efficient at these and", "start": 817.92, "duration": 4.32}, {"text": "if you go back and thinking about where", "start": 820.079, "duration": 4.38}, {"text": "gpus come from they were very good", "start": 822.24, "duration": 5.52}, {"text": "already like 10 15 20 years ago in doing", "start": 824.459, "duration": 5.641}, {"text": "4x4 Matrix algebra because that's used", "start": 827.76, "duration": 4.56}, {"text": "in 3D graphics for rotations and", "start": 830.1, "duration": 4.859}, {"text": "translations and and scaling of objects", "start": 832.32, "duration": 6.06}, {"text": "to compute um the the value of pixels on", "start": 834.959, "duration": 4.261}, {"text": "the screen", "start": 838.38, "duration": 2.639}, {"text": "and then in addition to that there's", "start": 839.22, "duration": 4.38}, {"text": "half Precision arithmetic Hardware that", "start": 841.019, "duration": 4.141}, {"text": "can be used for many machine learning", "start": 843.6, "duration": 3.66}, {"text": "applications at least at the inference", "start": 845.16, "duration": 5.94}, {"text": "step so at the prediction step um and if", "start": 847.26, "duration": 5.04}, {"text": "you go out and look at the machine", "start": 851.1, "duration": 2.94}, {"text": "learning Frameworks that are available", "start": 852.3, "duration": 4.56}, {"text": "for instance like Pi torch or tensorflow", "start": 854.04, "duration": 4.739}, {"text": "um these all provide GPU support so you", "start": 856.86, "duration": 3.419}, {"text": "don't need to know much about gpus you", "start": 858.779, "duration": 3.12}, {"text": "basically need to tell that framework", "start": 860.279, "duration": 3.661}, {"text": "just well I want you to execute on the", "start": 861.899, "duration": 3.781}, {"text": "gpus and they will do that for you so", "start": 863.94, "duration": 4.5}, {"text": "you don't really have to um", "start": 865.68, "duration": 5.94}, {"text": "program and know a lot about gpus in", "start": 868.44, "duration": 5.42}, {"text": "order to use those", "start": 871.62, "duration": 6.12}, {"text": "a few benchmarks examples right so just", "start": 873.86, "duration": 5.5}, {"text": "just a short comic strip right so you", "start": 877.74, "duration": 4.2}, {"text": "know that that Albert comics and the", "start": 879.36, "duration": 4.979}, {"text": "question is manager asks do you have the", "start": 881.94, "duration": 4.98}, {"text": "Benchmark results and those do you want", "start": 884.339, "duration": 4.201}, {"text": "the 10 minute explanation of why these", "start": 886.92, "duration": 3.479}, {"text": "data are useless or a simple here you go", "start": 888.54, "duration": 4.979}, {"text": "it says I'm in sales well here you go so", "start": 890.399, "duration": 4.921}, {"text": "you have to be careful with benchmarks", "start": 893.519, "duration": 3.721}, {"text": "because often people show benchmarks", "start": 895.32, "duration": 3.319}, {"text": "that um", "start": 897.24, "duration": 4.159}, {"text": "show a certain performance increase", "start": 898.639, "duration": 6.82}, {"text": "of 100x of a GPU over some single", "start": 901.399, "duration": 5.221}, {"text": "threaded core", "start": 905.459, "duration": 3.601}, {"text": "that is short against single threads or", "start": 906.62, "duration": 4.659}, {"text": "they show it against a non-optimized", "start": 909.06, "duration": 3.42}, {"text": "code and that's of course not a fair", "start": 911.279, "duration": 3.24}, {"text": "comparison so if you go back to some of", "start": 912.48, "duration": 3.539}, {"text": "the earlier slides that I showed what is", "start": 914.519, "duration": 3.12}, {"text": "the performance differential", "start": 916.019, "duration": 3.901}, {"text": "on paper in memory bandwidth and", "start": 917.639, "duration": 4.32}, {"text": "floating Point operations for GPU then", "start": 919.92, "duration": 3.719}, {"text": "you can approximately estimate what is", "start": 921.959, "duration": 3.06}, {"text": "the maximum speed up that you could get", "start": 923.639, "duration": 3.481}, {"text": "and that realistically you should be", "start": 925.019, "duration": 4.62}, {"text": "somewhere around 10 at most", "start": 927.12, "duration": 7.079}, {"text": "in comparison to say a a a dual core a", "start": 929.639, "duration": 7.081}, {"text": "dual dual CPU socket", "start": 934.199, "duration": 4.561}, {"text": "um and there are many Benchmark examples", "start": 936.72, "duration": 4.08}, {"text": "um I'll just show you two from my", "start": 938.76, "duration": 4.079}, {"text": "background because I'm a Quantum chemist", "start": 940.8, "duration": 4.5}, {"text": "and work in computational chemistry so", "start": 942.839, "duration": 3.961}, {"text": "that's actually the main research that", "start": 945.3, "duration": 3.24}, {"text": "I'm doing is a computational chemistry", "start": 946.8, "duration": 4.02}, {"text": "in molecular Dynamics", "start": 948.54, "duration": 4.82}, {"text": "um but here's an example of a", "start": 950.82, "duration": 4.92}, {"text": "quantum chemistry code that basically is", "start": 953.36, "duration": 3.82}, {"text": "able to compute molecular properties", "start": 955.74, "duration": 3.539}, {"text": "from quantum mechanics and a comparison", "start": 957.18, "duration": 4.38}, {"text": "of for different types of molecules you", "start": 959.279, "duration": 3.601}, {"text": "can see that you can get a significant", "start": 961.56, "duration": 4.139}, {"text": "speed up again here versus a reference", "start": 962.88, "duration": 5.36}, {"text": "code that is a different code", "start": 965.699, "duration": 5.101}, {"text": "that doesn't necessarily Excel on single", "start": 968.24, "duration": 4.3}, {"text": "core performance but the comparisons", "start": 970.8, "duration": 5.279}, {"text": "against a single core in any case", "start": 972.54, "duration": 4.38}, {"text": "um", "start": 976.079, "duration": 3.06}, {"text": "it's a very fast quantum chemistry code", "start": 976.92, "duration": 4.859}, {"text": "also molecular Dynamics is very", "start": 979.139, "duration": 4.021}, {"text": "successful that we have a very", "start": 981.779, "duration": 3.661}, {"text": "successful implementation for GPU so", "start": 983.16, "duration": 4.14}, {"text": "what we do for instance is atomistic", "start": 985.44, "duration": 3.78}, {"text": "simulations of condensed phase", "start": 987.3, "duration": 4.62}, {"text": "biomolecular systems and what we need to", "start": 989.22, "duration": 4.799}, {"text": "do is we can need to compute the forces", "start": 991.92, "duration": 4.74}, {"text": "of all the particles that interact many", "start": 994.019, "duration": 4.801}, {"text": "many millions of times to propagate the", "start": 996.66, "duration": 4.02}, {"text": "Dynamics of the system", "start": 998.82, "duration": 5.16}, {"text": "and this is an example of the Amber", "start": 1000.68, "duration": 5.219}, {"text": "molecular Dynamics software package and", "start": 1003.98, "duration": 3.359}, {"text": "one of the latest benchmarks that you", "start": 1005.899, "duration": 2.461}, {"text": "can see over", "start": 1007.339, "duration": 4.44}, {"text": "versus the um CPU implementation that we", "start": 1008.36, "duration": 5.94}, {"text": "have so you can see the performance", "start": 1011.779, "duration": 5.941}, {"text": "of a 36 this year's for instance a 36", "start": 1014.3, "duration": 5.399}, {"text": "core CPU implementation gets a", "start": 1017.72, "duration": 5.28}, {"text": "performance of 2.35 that's the amount of", "start": 1019.699, "duration": 6.12}, {"text": "simulation that we can get within 24", "start": 1023.0, "duration": 7.319}, {"text": "hours in a day and already on gaming", "start": 1025.819, "duration": 7.201}, {"text": "gpus you get a factor of 10 more with", "start": 1030.319, "duration": 4.62}, {"text": "the implementation that we have so if", "start": 1033.02, "duration": 4.02}, {"text": "the code is implemented properly you can", "start": 1034.939, "duration": 4.441}, {"text": "get a significant speed of performance", "start": 1037.04, "duration": 5.46}, {"text": "Improvement and that said you know this", "start": 1039.38, "duration": 5.1}, {"text": "was a lot of work so we'll get back to", "start": 1042.5, "duration": 3.24}, {"text": "that so it's not something that happens", "start": 1044.48, "duration": 4.319}, {"text": "automatically necessary", "start": 1045.74, "duration": 5.16}, {"text": "what's the catch right again another", "start": 1048.799, "duration": 4.201}, {"text": "Dilbert comic so our servers are using", "start": 1050.9, "duration": 4.019}, {"text": "too much electricity we need to use gpus", "start": 1053.0, "duration": 4.38}, {"text": "right we've seen that we get much more", "start": 1054.919, "duration": 4.981}, {"text": "performance with much less power", "start": 1057.38, "duration": 4.2}, {"text": "and manager says I did my part by", "start": 1059.9, "duration": 3.24}, {"text": "reading about gpus in a trade Journal", "start": 1061.58, "duration": 3.839}, {"text": "now you do the software part and", "start": 1063.14, "duration": 4.02}, {"text": "after Valley comes back you can ask why", "start": 1065.419, "duration": 3.181}, {"text": "is your part checking so long right so", "start": 1067.16, "duration": 4.2}, {"text": "it's not necessarily easy but I'll show", "start": 1068.6, "duration": 4.68}, {"text": "you what is involved in review", "start": 1071.36, "duration": 4.74}, {"text": "programming and then we can", "start": 1073.28, "duration": 4.44}, {"text": "can decide for yourself with your own", "start": 1076.1, "duration": 3.72}, {"text": "projects whether you want to move", "start": 1077.72, "duration": 4.74}, {"text": "forward with gpus or not", "start": 1079.82, "duration": 6.239}, {"text": "so just as a very broad overview a GPU", "start": 1082.46, "duration": 5.88}, {"text": "versus CPU architecture if you take the", "start": 1086.059, "duration": 6.0}, {"text": "real estate of a integrated ship surface", "start": 1088.34, "duration": 7.26}, {"text": "area in a CPU there are a few processing", "start": 1092.059, "duration": 5.0}, {"text": "cores right", "start": 1095.6, "duration": 5.22}, {"text": "arithmetic logical units aalus", "start": 1097.059, "duration": 7.12}, {"text": "but with sophisticated Hardware", "start": 1100.82, "duration": 6.18}, {"text": "um what's implemented is multi-level", "start": 1104.179, "duration": 4.86}, {"text": "caching pre-fetching Branch prediction", "start": 1107.0, "duration": 4.86}, {"text": "so the CPU does a lot in order to speed", "start": 1109.039, "duration": 6.321}, {"text": "up the code that you have written", "start": 1111.86, "duration": 6.66}, {"text": "the GPU on the other hand has thousands", "start": 1115.36, "duration": 4.96}, {"text": "of more simplistic compute cores in a", "start": 1118.52, "duration": 3.84}, {"text": "sense that are packaged actually into a", "start": 1120.32, "duration": 3.66}, {"text": "few multi-processors so they are", "start": 1122.36, "duration": 4.1}, {"text": "physically arranged into multiprocessors", "start": 1123.98, "duration": 4.62}, {"text": "we'll see a little bit more about it", "start": 1126.46, "duration": 3.219}, {"text": "later", "start": 1128.6, "duration": 3.3}, {"text": "and these operate at least partially in", "start": 1129.679, "duration": 4.321}, {"text": "lockstep right so they're not in if they", "start": 1131.9, "duration": 4.019}, {"text": "do not fully execute independently and", "start": 1134.0, "duration": 3.86}, {"text": "so you have to program that way", "start": 1135.919, "duration": 4.321}, {"text": "but you can have back to rest loads and", "start": 1137.86, "duration": 4.42}, {"text": "stores to memory um and you need to", "start": 1140.24, "duration": 4.86}, {"text": "manage the memory hierarchy", "start": 1142.28, "duration": 3.66}, {"text": "um", "start": 1145.1, "duration": 3.42}, {"text": "this has not changed much so here you", "start": 1145.94, "duration": 4.26}, {"text": "see a little bit of more details that's", "start": 1148.52, "duration": 3.48}, {"text": "actually from approximately 10 years ago", "start": 1150.2, "duration": 4.8}, {"text": "an Nvidia GPU architecture server with", "start": 1152.0, "duration": 5.34}, {"text": "an early data center GPU um the basic", "start": 1155.0, "duration": 4.02}, {"text": "Arctic architecture is still still the", "start": 1157.34, "duration": 4.14}, {"text": "same and what you see here is s m is a", "start": 1159.02, "duration": 5.279}, {"text": "so-called multi-processor which contains", "start": 1161.48, "duration": 5.04}, {"text": "these processing cores that operate in", "start": 1164.299, "duration": 4.921}, {"text": "lockstep it contains um also a double", "start": 1166.52, "duration": 4.8}, {"text": "Precision compute core so often the", "start": 1169.22, "duration": 4.14}, {"text": "performance ratio of single Precision to", "start": 1171.32, "duration": 4.08}, {"text": "double Precision is not two to one like", "start": 1173.36, "duration": 3.66}, {"text": "on a CPU", "start": 1175.4, "duration": 4.68}, {"text": "in particular for gaming gpus then there", "start": 1177.02, "duration": 4.98}, {"text": "are special function units for um", "start": 1180.08, "duration": 2.9}, {"text": "special", "start": 1182.0, "duration": 3.419}, {"text": "math operations that you might want to", "start": 1182.98, "duration": 4.72}, {"text": "to do um so not all the operations", "start": 1185.419, "duration": 4.26}, {"text": "execute with the same performance of", "start": 1187.7, "duration": 2.9}, {"text": "course", "start": 1189.679, "duration": 3.181}, {"text": "there's an instruction cache and then", "start": 1190.6, "duration": 4.48}, {"text": "there's shared memory data cache that's", "start": 1192.86, "duration": 4.74}, {"text": "shared across the processing units on", "start": 1195.08, "duration": 4.56}, {"text": "these individual multi-processors but", "start": 1197.6, "duration": 4.26}, {"text": "not across multi-processors", "start": 1199.64, "duration": 6.3}, {"text": "and these streaming multi-processors can", "start": 1201.86, "duration": 5.58}, {"text": "handle many more threats than processing", "start": 1205.94, "duration": 3.42}, {"text": "cores so what you typically do is you", "start": 1207.44, "duration": 4.02}, {"text": "launch many many many many threads so", "start": 1209.36, "duration": 4.74}, {"text": "like it's written up here you can launch", "start": 1211.46, "duration": 5.459}, {"text": "in that old Hardware configuration 128", "start": 1214.1, "duration": 4.319}, {"text": "threads per processor and if you would", "start": 1216.919, "duration": 4.921}, {"text": "do that you would be having 30 720", "start": 1218.419, "duration": 5.461}, {"text": "threads running concurrently on that", "start": 1221.84, "duration": 3.42}, {"text": "Hardware doesn't mean they execute", "start": 1223.88, "duration": 3.48}, {"text": "exactly at the same time but they will", "start": 1225.26, "duration": 5.419}, {"text": "be running at the same time", "start": 1227.36, "duration": 3.319}, {"text": "um", "start": 1231.2, "duration": 2.64}, {"text": "then you know there are certain Hardware", "start": 1231.98, "duration": 3.78}, {"text": "complexities because Hardware", "start": 1233.84, "duration": 3.42}, {"text": "characteristics have been changing", "start": 1235.76, "duration": 3.9}, {"text": "across GPU models and generations um you", "start": 1237.26, "duration": 4.08}, {"text": "still will write your code in such a way", "start": 1239.66, "duration": 3.24}, {"text": "that it is portable and does not depend", "start": 1241.34, "duration": 3.6}, {"text": "on the hardware but you need to keep in", "start": 1242.9, "duration": 3.659}, {"text": "mind that the single Precision to double", "start": 1244.94, "duration": 3.06}, {"text": "Precision floating Point performance", "start": 1246.559, "duration": 4.86}, {"text": "ratio J has been changing along the way", "start": 1248.0, "duration": 5.28}, {"text": "um memory bandwidth is different for", "start": 1251.419, "duration": 5.041}, {"text": "different gpus GPU types um the number", "start": 1253.28, "duration": 4.92}, {"text": "of compute cores and multi-processors", "start": 1256.46, "duration": 3.839}, {"text": "has been changing the number of threads", "start": 1258.2, "duration": 4.14}, {"text": "that the hardware can execute", "start": 1260.299, "duration": 4.201}, {"text": "um the number of registers on a", "start": 1262.34, "duration": 4.8}, {"text": "multiprocessor and the cache sizes and", "start": 1264.5, "duration": 4.62}, {"text": "also the available GPU memory on the", "start": 1267.14, "duration": 5.1}, {"text": "device because the the GPU memory is", "start": 1269.12, "duration": 5.28}, {"text": "fixed and you know you can can't just", "start": 1272.24, "duration": 3.72}, {"text": "add more memory and so if you have", "start": 1274.4, "duration": 3.48}, {"text": "applications that need a lot of ram they", "start": 1275.96, "duration": 4.86}, {"text": "might actually not fit onto the GPU", "start": 1277.88, "duration": 4.32}, {"text": "um the memory hierarchy needs to be", "start": 1280.82, "duration": 3.18}, {"text": "explicitly managed as different types of", "start": 1282.2, "duration": 3.9}, {"text": "of memory", "start": 1284.0, "duration": 4.679}, {"text": "you have to know that there is CPU of", "start": 1286.1, "duration": 4.38}, {"text": "course your CPU memory then on the GPU", "start": 1288.679, "duration": 3.12}, {"text": "you have different types of memory", "start": 1290.48, "duration": 3.72}, {"text": "that's Global memory shared memory", "start": 1291.799, "duration": 3.901}, {"text": "that's sitting on the multi-processors", "start": 1294.2, "duration": 3.06}, {"text": "there's texture memory and constant", "start": 1295.7, "duration": 3.66}, {"text": "memory and there's something called", "start": 1297.26, "duration": 5.82}, {"text": "unified memory in Cuda at least um that", "start": 1299.36, "duration": 5.52}, {"text": "allows you to allocate memory and then", "start": 1303.08, "duration": 4.92}, {"text": "the queue the toolkit the runtime takes", "start": 1304.88, "duration": 5.1}, {"text": "care of automatically shuffling the data", "start": 1308.0, "duration": 4.14}, {"text": "for them back but you still have to know", "start": 1309.98, "duration": 3.78}, {"text": "that", "start": 1312.14, "duration": 3.06}, {"text": "um you know that shuffling happens", "start": 1313.76, "duration": 3.24}, {"text": "because the memory here hierarchy still", "start": 1315.2, "duration": 4.2}, {"text": "exists and it will impact the", "start": 1317.0, "duration": 4.14}, {"text": "performance of your code so typically", "start": 1319.4, "duration": 3.3}, {"text": "that's a good way to get started but", "start": 1321.14, "duration": 3.06}, {"text": "down the road you should explicitly", "start": 1322.7, "duration": 3.12}, {"text": "manage the memory yourself in order to", "start": 1324.2, "duration": 3.54}, {"text": "get performance then there are of course", "start": 1325.82, "duration": 3.239}, {"text": "different Hardware vendors I've been", "start": 1327.74, "duration": 3.299}, {"text": "talking a lot of out in video because we", "start": 1329.059, "duration": 5.541}, {"text": "do have Nvidia gpus at sdsc and um", "start": 1331.039, "duration": 7.561}, {"text": "almost all um data centers who are", "start": 1334.6, "duration": 7.3}, {"text": "running a GPU workloads are working with", "start": 1338.6, "duration": 6.42}, {"text": "Nvidia gpus but there's also ND gpus of", "start": 1341.9, "duration": 5.159}, {"text": "course and they work differently", "start": 1345.02, "duration": 4.2}, {"text": "um in some in some ways", "start": 1347.059, "duration": 4.201}, {"text": "there here's a summary of different GPU", "start": 1349.22, "duration": 3.42}, {"text": "models", "start": 1351.26, "duration": 1.98}, {"text": "um", "start": 1352.64, "duration": 2.58}, {"text": "the so-called compute capability", "start": 1353.24, "duration": 4.5}, {"text": "determines the features that are", "start": 1355.22, "duration": 4.26}, {"text": "available on Nvidia gpus for instance", "start": 1357.74, "duration": 3.96}, {"text": "double Precision support was introduced", "start": 1359.48, "duration": 4.38}, {"text": "in version 1.3 but that's a long time", "start": 1361.7, "duration": 4.92}, {"text": "ago so all current cards support double", "start": 1363.86, "duration": 5.22}, {"text": "precision and that's here just an", "start": 1366.62, "duration": 5.039}, {"text": "overview that", "start": 1369.08, "duration": 3.86}, {"text": "um", "start": 1371.659, "duration": 3.841}, {"text": "you can pull information also from", "start": 1372.94, "duration": 4.8}, {"text": "Wikipedia or from", "start": 1375.5, "duration": 6.5}, {"text": "websites and videos websites", "start": 1377.74, "duration": 4.26}, {"text": "what does that all mean for your program", "start": 1382.28, "duration": 2.94}, {"text": "so you", "start": 1384.14, "duration": 3.0}, {"text": "should never write a code with any", "start": 1385.22, "duration": 3.42}, {"text": "Assumption of how many threads it will", "start": 1387.14, "duration": 5.039}, {"text": "use so but similar to a CPU code where", "start": 1388.64, "duration": 5.7}, {"text": "you will you will check how many CPU", "start": 1392.179, "duration": 3.721}, {"text": "cores you have available and then for", "start": 1394.34, "duration": 4.5}, {"text": "instance an openmp code uh you know you", "start": 1395.9, "duration": 4.68}, {"text": "execute it with a number of threads that", "start": 1398.84, "duration": 3.42}, {"text": "is equivalent to the number of available", "start": 1400.58, "duration": 5.099}, {"text": "CPU cores so you will use Cuda calls", "start": 1402.26, "duration": 4.74}, {"text": "Cuda functions to vary the hardware", "start": 1405.679, "duration": 3.721}, {"text": "configuration at runtime and then launch", "start": 1407.0, "duration": 4.14}, {"text": "many many more threads and processing", "start": 1409.4, "duration": 4.139}, {"text": "cores but based on the type of Hardware", "start": 1411.14, "duration": 4.26}, {"text": "that you have", "start": 1413.539, "duration": 4.741}, {"text": "and what you also can do in order to", "start": 1415.4, "duration": 5.159}, {"text": "improve performance is avoid using", "start": 1418.28, "duration": 4.2}, {"text": "double Precision when you don't", "start": 1420.559, "duration": 3.661}, {"text": "specifically need it so you have to be", "start": 1422.48, "duration": 3.66}, {"text": "careful because many algorithms and", "start": 1424.22, "duration": 4.14}, {"text": "particular scientific Computing are very", "start": 1426.14, "duration": 3.96}, {"text": "sensitive to numerical Precision so you", "start": 1428.36, "duration": 3.78}, {"text": "can't just switch double PlayStation for", "start": 1430.1, "duration": 3.84}, {"text": "single Precision but if you're careful", "start": 1432.14, "duration": 3.12}, {"text": "perhaps you can do some of your", "start": 1433.94, "duration": 3.239}, {"text": "computations in single precision and", "start": 1435.26, "duration": 3.96}, {"text": "then for instance summations in double", "start": 1437.179, "duration": 4.141}, {"text": "precision and that's for instance what", "start": 1439.22, "duration": 4.74}, {"text": "we're doing in a simplified explanation", "start": 1441.32, "duration": 4.38}, {"text": "now um also in the Amber molecular", "start": 1443.96, "duration": 4.52}, {"text": "Dynamics code", "start": 1445.7, "duration": 2.78}, {"text": "so if you want to program gpus", "start": 1448.64, "duration": 4.62}, {"text": "um", "start": 1452.659, "duration": 2.281}, {"text": "there are different ways to do that", "start": 1453.26, "duration": 5.52}, {"text": "there's opencl that stays it stands for", "start": 1454.94, "duration": 6.239}, {"text": "open compute language that's a industry", "start": 1458.78, "duration": 3.899}, {"text": "standard and it works both for NVIDIA", "start": 1461.179, "duration": 4.98}, {"text": "and AMD gpus also for other devices", "start": 1462.679, "duration": 5.941}, {"text": "um I'm not going to talk about that", "start": 1466.159, "duration": 6.0}, {"text": "there's Cuda which is proprietary from", "start": 1468.62, "duration": 6.14}, {"text": "Nvidia but it's freely available and", "start": 1472.159, "duration": 5.161}, {"text": "it's not going to go away anytime soon", "start": 1474.76, "duration": 4.779}, {"text": "and this the de facto standard for high", "start": 1477.32, "duration": 4.44}, {"text": "performance code on Nvidia gpus of", "start": 1479.539, "duration": 4.201}, {"text": "course it's going to run only on Nvidia", "start": 1481.76, "duration": 4.62}, {"text": "GQ so that's the downside", "start": 1483.74, "duration": 4.98}, {"text": "um then there's open ACC", "start": 1486.38, "duration": 5.159}, {"text": "these are accelerator directives that do", "start": 1488.72, "duration": 5.0}, {"text": "work both for NVIDIA and AMD however", "start": 1491.539, "duration": 4.26}, {"text": "I'll talk a little bit more about that", "start": 1493.72, "duration": 5.02}, {"text": "later on the implementations are not", "start": 1495.799, "duration": 4.081}, {"text": "necessarily", "start": 1498.74, "duration": 4.679}, {"text": "um very very efficient for both Hardware", "start": 1499.88, "duration": 6.84}, {"text": "types or render types but it works with", "start": 1503.419, "duration": 6.24}, {"text": "a C C plus plus and Fortran so you have", "start": 1506.72, "duration": 4.199}, {"text": "you would have a portable code that's", "start": 1509.659, "duration": 4.441}, {"text": "similar to openmp now openmp also has", "start": 1510.919, "duration": 5.101}, {"text": "introduced accelerator and vectorization", "start": 1514.1, "duration": 4.68}, {"text": "directives that works well with Intel", "start": 1516.02, "duration": 5.46}, {"text": "Xeon Phi and you know ibx 500 12 Vector", "start": 1518.78, "duration": 4.44}, {"text": "extensions on CPUs but it's not made", "start": 1521.48, "duration": 4.199}, {"text": "sure at the moment for gpus I would say", "start": 1523.22, "duration": 3.959}, {"text": "so it's not something", "start": 1525.679, "duration": 3.661}, {"text": "um that's recommended at the moment if", "start": 1527.179, "duration": 5.701}, {"text": "you want to develop a production code to", "start": 1529.34, "duration": 5.4}, {"text": "run on gpus", "start": 1532.88, "duration": 4.62}, {"text": "uh here's an overview of what I would", "start": 1534.74, "duration": 5.76}, {"text": "call the Nvidia GPU Computing universe", "start": 1537.5, "duration": 6.539}, {"text": "so this overview basically has", "start": 1540.5, "duration": 5.46}, {"text": "um it's just taken from the qdc", "start": 1544.039, "duration": 3.541}, {"text": "programming guide so that's also a very", "start": 1545.96, "duration": 3.54}, {"text": "good resource to look at if you want to", "start": 1547.58, "duration": 3.9}, {"text": "learn more about a GPU programming you", "start": 1549.5, "duration": 3.779}, {"text": "can just read the programming guide", "start": 1551.48, "duration": 3.78}, {"text": "that's online", "start": 1553.279, "duration": 4.741}, {"text": "um so what Nvidia provides is uh", "start": 1555.26, "duration": 4.26}, {"text": "libraries", "start": 1558.02, "duration": 5.46}, {"text": "um like qdnn for deep neural networks", "start": 1559.52, "duration": 6.84}, {"text": "um qfft for fast Fourier transforms Q", "start": 1563.48, "duration": 4.88}, {"text": "Plus that's a Cuda plus implementation", "start": 1566.36, "duration": 5.16}, {"text": "and so on so there's many um libraries", "start": 1568.36, "duration": 5.319}, {"text": "then there's programming languages", "start": 1571.52, "duration": 4.98}, {"text": "so you can use", "start": 1573.679, "duration": 3.781}, {"text": "um", "start": 1576.5, "duration": 3.9}, {"text": "you can you can program gpus in cuder", "start": 1577.46, "duration": 5.819}, {"text": "from CC plus plus and Fortran there are", "start": 1580.4, "duration": 5.22}, {"text": "also ways to use gpus from java and", "start": 1583.279, "duration": 4.28}, {"text": "python", "start": 1585.62, "duration": 4.58}, {"text": "or you can use a directive based", "start": 1587.559, "duration": 4.961}, {"text": "programming for instance like open ACC", "start": 1590.2, "duration": 4.78}, {"text": "and then here's a an overview of all the", "start": 1592.52, "duration": 3.84}, {"text": "different types of", "start": 1594.98, "duration": 3.96}, {"text": "gpus that Nvidia are currently is", "start": 1596.36, "duration": 4.439}, {"text": "supporting um so in principle if you", "start": 1598.94, "duration": 4.739}, {"text": "write a GPU code also in Cuda or with", "start": 1600.799, "duration": 5.12}, {"text": "any of the other um", "start": 1603.679, "duration": 4.86}, {"text": "methods the directive based programming", "start": 1605.919, "duration": 4.601}, {"text": "open ACC is going to run on embedded", "start": 1608.539, "duration": 4.02}, {"text": "devices consumer desktops laptops", "start": 1610.52, "duration": 3.96}, {"text": "professional workstations and data", "start": 1612.559, "duration": 3.841}, {"text": "center gpus", "start": 1614.48, "duration": 3.78}, {"text": "so in that sense the code is of course", "start": 1616.4, "duration": 3.96}, {"text": "portable", "start": 1618.26, "duration": 2.76}, {"text": "um", "start": 1620.36, "duration": 2.64}, {"text": "what you have to know about is the Cuda", "start": 1621.02, "duration": 2.88}, {"text": "toolkit", "start": 1623.0, "duration": 3.059}, {"text": "you can obtain that from free for free", "start": 1623.9, "duration": 5.82}, {"text": "from Nvidia at this link here", "start": 1626.059, "duration": 6.061}, {"text": "it contains the Cuda compiler so in the", "start": 1629.72, "duration": 5.459}, {"text": "nvcc it also contains development tools", "start": 1632.12, "duration": 6.08}, {"text": "so there's a debugger there's profilers", "start": 1635.179, "duration": 6.301}, {"text": "in particular navp is a visual profiler", "start": 1638.2, "duration": 5.859}, {"text": "is very nice and easy to use and gives", "start": 1641.48, "duration": 4.02}, {"text": "you an overview of what's happening", "start": 1644.059, "duration": 4.141}, {"text": "during code execution", "start": 1645.5, "duration": 4.98}, {"text": "um there's also an IDE for eclipse and", "start": 1648.2, "duration": 4.8}, {"text": "visual studio and a range of different", "start": 1650.48, "duration": 5.699}, {"text": "libraries that I just mentioned on the", "start": 1653.0, "duration": 5.82}, {"text": "slide before and very importantly there", "start": 1656.179, "duration": 4.201}, {"text": "are also cuter code samples that will", "start": 1658.82, "duration": 3.66}, {"text": "have a quick look at later um so these", "start": 1660.38, "duration": 3.899}, {"text": "are very nice to actually just you know", "start": 1662.48, "duration": 3.72}, {"text": "get started a little bit and look at how", "start": 1664.279, "duration": 6.741}, {"text": "Cuda programs can be written", "start": 1666.2, "duration": 4.82}, {"text": "um", "start": 1671.539, "duration": 3.721}, {"text": "so let's look at how to programs you can", "start": 1672.799, "duration": 4.081}, {"text": "use um so there's basically three ways", "start": 1675.26, "duration": 3.6}, {"text": "to use gpus so you have an application", "start": 1676.88, "duration": 4.14}, {"text": "and either you can use libraries that's", "start": 1678.86, "duration": 4.199}, {"text": "basically what we call a drop in", "start": 1681.02, "duration": 3.72}, {"text": "acceleration right so you have a", "start": 1683.059, "duration": 4.74}, {"text": "function call to a standard library and", "start": 1684.74, "duration": 4.86}, {"text": "there's a Cuda implementation so you can", "start": 1687.799, "duration": 4.26}, {"text": "effectively just switch that in with a", "start": 1689.6, "duration": 4.14}, {"text": "little bit of work around", "start": 1692.059, "duration": 4.561}, {"text": "um in order to initialize the gpus for", "start": 1693.74, "duration": 4.38}, {"text": "instance", "start": 1696.62, "duration": 3.48}, {"text": "um then the next", "start": 1698.12, "duration": 4.5}, {"text": "intermediate say difficult part is so if", "start": 1700.1, "duration": 3.6}, {"text": "you have part of your code that's", "start": 1702.62, "duration": 2.4}, {"text": "compute intensive and you need to", "start": 1703.7, "duration": 4.2}, {"text": "accelerate or want to run on a GPU and", "start": 1705.02, "duration": 4.62}, {"text": "it's not just a library function call", "start": 1707.9, "duration": 3.96}, {"text": "you can use directives and add those to", "start": 1709.64, "duration": 4.44}, {"text": "your code um we will look at that at the", "start": 1711.86, "duration": 5.46}, {"text": "end of this webinar webinar and then", "start": 1714.08, "duration": 4.86}, {"text": "there are programming languages that", "start": 1717.32, "duration": 2.76}, {"text": "give you of course the maximum", "start": 1718.94, "duration": 4.02}, {"text": "flexibility to explicitly program using", "start": 1720.08, "duration": 4.92}, {"text": "gpus", "start": 1722.96, "duration": 4.62}, {"text": "so GPU accelerated libraries of course", "start": 1725.0, "duration": 4.5}, {"text": "they're easy to use so you don't need to", "start": 1727.58, "duration": 4.44}, {"text": "know much about GPU programming there's", "start": 1729.5, "duration": 4.5}, {"text": "minimal code changes required and of", "start": 1732.02, "duration": 2.88}, {"text": "course these are high quality", "start": 1734.0, "duration": 3.059}, {"text": "implementations or functions that are", "start": 1734.9, "duration": 3.18}, {"text": "encountered in a broad range of", "start": 1737.059, "duration": 3.24}, {"text": "applications and these have been tuned", "start": 1738.08, "duration": 4.92}, {"text": "by experts software Engineers that have", "start": 1740.299, "duration": 4.74}, {"text": "been put a lot of effort into that so", "start": 1743.0, "duration": 3.539}, {"text": "there's no way you can beat that usually", "start": 1745.039, "duration": 4.081}, {"text": "so if you use them if you can for", "start": 1746.539, "duration": 3.781}, {"text": "instance don't write your own matrix", "start": 1749.12, "duration": 2.88}, {"text": "multiplication or for a fast Fourier", "start": 1750.32, "duration": 4.14}, {"text": "transform functions unless you want to", "start": 1752.0, "duration": 6.44}, {"text": "learn more about how to program and", "start": 1754.46, "duration": 3.98}, {"text": "um and that that could be a simple", "start": 1758.48, "duration": 3.299}, {"text": "example for instance the program matrix", "start": 1759.74, "duration": 3.419}, {"text": "multiplication and I have an example", "start": 1761.779, "duration": 3.481}, {"text": "later that I'm going to show you if I", "start": 1763.159, "duration": 4.62}, {"text": "don't use those in your own code", "start": 1765.26, "duration": 4.62}, {"text": "um so if you go to nvidia's website here", "start": 1767.779, "duration": 3.721}, {"text": "so there's an overview of all the", "start": 1769.88, "duration": 3.06}, {"text": "libraries so there's deep learning", "start": 1771.5, "duration": 3.779}, {"text": "libraries there's linear algebra and", "start": 1772.94, "duration": 4.56}, {"text": "math libraries available there's", "start": 1775.279, "duration": 4.921}, {"text": "um image and video processing libraries", "start": 1777.5, "duration": 4.919}, {"text": "um other libraries for other parallel", "start": 1780.2, "duration": 4.74}, {"text": "algorithms like photograph computations", "start": 1782.419, "duration": 4.62}, {"text": "for instance and a lot of other", "start": 1784.94, "duration": 5.04}, {"text": "libraries available by other partners", "start": 1787.039, "duration": 5.52}, {"text": "that have been contributing to the GPU", "start": 1789.98, "duration": 5.579}, {"text": "universe", "start": 1792.559, "duration": 3.0}, {"text": "so here's an example", "start": 1797.659, "duration": 5.701}, {"text": "so how to use libraries there's", "start": 1800.08, "duration": 5.02}, {"text": "basically three steps right so imagine", "start": 1803.36, "duration": 3.539}, {"text": "you have a function call here step one", "start": 1805.1, "duration": 4.92}, {"text": "that is a function called sax P that's a", "start": 1806.899, "duration": 5.581}, {"text": "single Precision a times X Plus y That's", "start": 1810.02, "duration": 5.7}, {"text": "a so-called blast function", "start": 1812.48, "duration": 7.02}, {"text": "linear algebra function and you just", "start": 1815.72, "duration": 5.339}, {"text": "would replace that with a function", "start": 1819.5, "duration": 4.1}, {"text": "called to cube last XP okay", "start": 1821.059, "duration": 5.041}, {"text": "you also need to manage what is called", "start": 1823.6, "duration": 4.9}, {"text": "Data locality so basically", "start": 1826.1, "duration": 4.62}, {"text": "um we'll look at this later with Cuda", "start": 1828.5, "duration": 6.84}, {"text": "you can allocate and copy data from the", "start": 1830.72, "duration": 6.9}, {"text": "CPU to the GPU memory with Cuda malloc", "start": 1835.34, "duration": 4.14}, {"text": "and Cuda mem copy that corresponds to", "start": 1837.62, "duration": 4.02}, {"text": "see malloc and CMM copy if you're", "start": 1839.48, "duration": 3.48}, {"text": "familiar with c", "start": 1841.64, "duration": 3.24}, {"text": "or unfortunately would be basically an", "start": 1842.96, "duration": 4.079}, {"text": "allocate and uh", "start": 1844.88, "duration": 5.1}, {"text": "um just copying data right from one", "start": 1847.039, "duration": 5.52}, {"text": "array to another array now if you use", "start": 1849.98, "duration": 4.14}, {"text": "Cube plus there are certain functions", "start": 1852.559, "duration": 3.301}, {"text": "that you have to use for or can use for", "start": 1854.12, "duration": 3.419}, {"text": "instance Q Plus set vector and Q Plus", "start": 1855.86, "duration": 5.64}, {"text": "get vector and uh that that's basically", "start": 1857.539, "duration": 5.461}, {"text": "it and then you rebuild and Link the", "start": 1861.5, "duration": 3.96}, {"text": "queue to accelerated Library so you have", "start": 1863.0, "duration": 4.62}, {"text": "to use the nvcc compiler", "start": 1865.46, "duration": 4.02}, {"text": "and Link it against the cube loss", "start": 1867.62, "duration": 3.659}, {"text": "library and that's basically what's", "start": 1869.48, "duration": 3.419}, {"text": "involved in using GPU accelerated", "start": 1871.279, "duration": 3.721}, {"text": "libraries usually", "start": 1872.899, "duration": 4.921}, {"text": "um so here's an example for that as a", "start": 1875.0, "duration": 5.76}, {"text": "xpy function", "start": 1877.82, "duration": 5.28}, {"text": "um so you we we're doing a call to this", "start": 1880.76, "duration": 3.899}, {"text": "XP function", "start": 1883.1, "duration": 3.959}, {"text": "where now I have", "start": 1884.659, "duration": 6.24}, {"text": "an element and a so this is a factor of", "start": 1887.059, "duration": 5.641}, {"text": "2 that's going to multiply", "start": 1890.899, "duration": 3.921}, {"text": "um", "start": 1892.7, "duration": 2.12}, {"text": "um X which is DX and Y which is d y and", "start": 1894.919, "duration": 6.721}, {"text": "the stride is one for this function um", "start": 1898.1, "duration": 5.819}, {"text": "so it's called DX and d y now here", "start": 1901.64, "duration": 3.72}, {"text": "that's typically", "start": 1903.919, "duration": 3.24}, {"text": "um Cuda program is used in order to", "start": 1905.36, "duration": 3.48}, {"text": "distinguish", "start": 1907.159, "duration": 3.421}, {"text": "um variables that are", "start": 1908.84, "duration": 4.8}, {"text": "allocated in memory on the CPU and on a", "start": 1910.58, "duration": 5.219}, {"text": "GPU and then D for device so that would", "start": 1913.64, "duration": 5.759}, {"text": "be data that is available on the GPU um", "start": 1915.799, "duration": 6.421}, {"text": "so what I would do here is I call the Q", "start": 1919.399, "duration": 5.16}, {"text": "Plus XB function I have to pass it a", "start": 1922.22, "duration": 3.72}, {"text": "so-called handle", "start": 1924.559, "duration": 3.6}, {"text": "that I create by initializing the", "start": 1925.94, "duration": 4.04}, {"text": "library so that some memory management", "start": 1928.159, "duration": 5.161}, {"text": "for a cube loss and then I have to shut", "start": 1929.98, "duration": 6.22}, {"text": "down Cube Plus at the end", "start": 1933.32, "duration": 6.3}, {"text": "I um allocate the device vectors so", "start": 1936.2, "duration": 6.26}, {"text": "these are these memory", "start": 1939.62, "duration": 5.76}, {"text": "arrays here DX and d y", "start": 1942.46, "duration": 5.02}, {"text": "on the GPU so that's the Q term unlock", "start": 1945.38, "duration": 4.26}, {"text": "function and then I use the cube loss", "start": 1947.48, "duration": 4.799}, {"text": "set Vector function to copy", "start": 1949.64, "duration": 6.539}, {"text": "the the vector X and the vector y to the", "start": 1952.279, "duration": 6.12}, {"text": "corresponding array that's sitting on", "start": 1956.179, "duration": 3.961}, {"text": "the GPU so I'll transfer the data to the", "start": 1958.399, "duration": 2.76}, {"text": "GPU", "start": 1960.14, "duration": 2.58}, {"text": "and at the end I read it back from the", "start": 1961.159, "duration": 3.781}, {"text": "GPU and that's basically how usually", "start": 1962.72, "duration": 4.86}, {"text": "um Cuda codes work so you have data on", "start": 1964.94, "duration": 4.859}, {"text": "the CPU you transfer it to the GPU then", "start": 1967.58, "duration": 4.079}, {"text": "you execute your code on the GPU and", "start": 1969.799, "duration": 5.181}, {"text": "then you transfer the results back", "start": 1971.659, "duration": 3.321}, {"text": "now", "start": 1977.72, "duration": 3.54}, {"text": "that's all I want to tell about", "start": 1979.94, "duration": 3.0}, {"text": "libraries because", "start": 1981.26, "duration": 3.18}, {"text": "um you know if you want to use some of", "start": 1982.94, "duration": 3.239}, {"text": "the other libraries you just look into", "start": 1984.44, "duration": 4.339}, {"text": "this manuals and see how they they they", "start": 1986.179, "duration": 5.901}, {"text": "can be used", "start": 1988.779, "duration": 3.301}, {"text": "um but now let's talk about Cuda so uh", "start": 1992.659, "duration": 5.461}, {"text": "Cuda is available", "start": 1996.2, "duration": 4.62}, {"text": "again here from this link here", "start": 1998.12, "duration": 5.76}, {"text": "and we have Cuda C which is a solution", "start": 2000.82, "duration": 5.64}, {"text": "basically to run C code seamlessly on", "start": 2003.88, "duration": 5.22}, {"text": "gpus it's available only for NVIDIA gpus", "start": 2006.46, "duration": 4.8}, {"text": "but it is the de facto standard for high", "start": 2009.1, "duration": 4.439}, {"text": "performance code and Nvidia gpus", "start": 2011.26, "duration": 5.1}, {"text": "um it's Nvidia proprietary so it will", "start": 2013.539, "duration": 6.961}, {"text": "not run on AMD gpus or Intel gpus and", "start": 2016.36, "duration": 6.84}, {"text": "better gpus", "start": 2020.5, "duration": 4.08}, {"text": "um and", "start": 2023.2, "duration": 3.719}, {"text": "q2c basically has a handful of", "start": 2024.58, "duration": 3.36}, {"text": "extensions", "start": 2026.919, "duration": 3.721}, {"text": "that you use with regular C code so if", "start": 2027.94, "duration": 4.38}, {"text": "you're familiar with C it's not an issue", "start": 2030.64, "duration": 4.98}, {"text": "to to to write Cuda code but it still", "start": 2032.32, "duration": 5.04}, {"text": "requires major rewriting of the code", "start": 2035.62, "duration": 3.72}, {"text": "because you have to think about", "start": 2037.36, "duration": 4.02}, {"text": "how you want all the different threads", "start": 2039.34, "duration": 5.52}, {"text": "to work on the data um that you you need", "start": 2041.38, "duration": 5.58}, {"text": "to work on", "start": 2044.86, "duration": 4.08}, {"text": "the Cuda toolkit is free already talked", "start": 2046.96, "duration": 3.6}, {"text": "about that and it comes with a compiler", "start": 2048.94, "duration": 3.84}, {"text": "the number libraries debugging and", "start": 2050.56, "duration": 3.72}, {"text": "profiling tools and there's also", "start": 2052.78, "duration": 3.18}, {"text": "something called q24trans so if you're a", "start": 2054.28, "duration": 3.599}, {"text": "Fortran programmer", "start": 2055.96, "duration": 3.899}, {"text": "there are Cuda extensions in fortnite", "start": 2057.879, "duration": 4.381}, {"text": "that have been developed by the Portland", "start": 2059.859, "duration": 5.28}, {"text": "group PGI", "start": 2062.26, "duration": 4.44}, {"text": "um they are available in the PGI Fortran", "start": 2065.139, "duration": 4.561}, {"text": "compiler and PGI is now part of Nvidia", "start": 2066.7, "duration": 5.159}, {"text": "so again so", "start": 2069.7, "duration": 4.62}, {"text": "you know extensions that used to work", "start": 2071.859, "duration": 5.221}, {"text": "for IMD and Nvidia gpus work only for", "start": 2074.32, "duration": 5.839}, {"text": "NVIDIA gpus now", "start": 2077.08, "duration": 3.079}, {"text": "if you want to get started these are two", "start": 2080.74, "duration": 5.46}, {"text": "books that are actually good to to have", "start": 2083.44, "duration": 4.8}, {"text": "a look at Cuda by example and", "start": 2086.2, "duration": 3.179}, {"text": "programming massively parallel", "start": 2088.24, "duration": 2.34}, {"text": "processors", "start": 2089.379, "duration": 4.561}, {"text": "and of course you can look at the Cuda", "start": 2090.58, "duration": 4.86}, {"text": "programming guide", "start": 2093.94, "duration": 5.04}, {"text": "which is um quite helpful", "start": 2095.44, "duration": 5.52}, {"text": "so", "start": 2098.98, "duration": 3.72}, {"text": "how", "start": 2100.96, "duration": 5.34}, {"text": "does a GPU program work typically so", "start": 2102.7, "duration": 5.34}, {"text": "people talk about heterogeneous", "start": 2106.3, "duration": 4.02}, {"text": "Computing what that means is you know", "start": 2108.04, "duration": 3.84}, {"text": "it's not homogeneous you just don't have", "start": 2110.32, "duration": 4.32}, {"text": "just a CPU but you have a CPU and a GPU", "start": 2111.88, "duration": 5.06}, {"text": "and the CPU is often referred to as host", "start": 2114.64, "duration": 5.88}, {"text": "and the gpus device and so you would", "start": 2116.94, "duration": 6.22}, {"text": "have for instance a c program that", "start": 2120.52, "duration": 5.46}, {"text": "executes serially on the host and at", "start": 2123.16, "duration": 5.82}, {"text": "some point you hit a region of your code", "start": 2125.98, "duration": 6.24}, {"text": "that then is executed on the device here", "start": 2128.98, "duration": 6.48}, {"text": "simplify exemplified by many many", "start": 2132.22, "duration": 5.1}, {"text": "threads here that are grouped into", "start": 2135.46, "duration": 4.1}, {"text": "so-called blocks on a grid", "start": 2137.32, "duration": 4.44}, {"text": "by calling something that's a kernel", "start": 2139.56, "duration": 3.64}, {"text": "that's a function that executes on the", "start": 2141.76, "duration": 3.359}, {"text": "GPU and when that is done you can go", "start": 2143.2, "duration": 3.54}, {"text": "back to your host you have another part", "start": 2145.119, "duration": 3.901}, {"text": "of your code to execute serially does", "start": 2146.74, "duration": 4.92}, {"text": "for instance some IO operations and then", "start": 2149.02, "duration": 4.5}, {"text": "you have another section of the code", "start": 2151.66, "duration": 6.0}, {"text": "that executes in parallel on the GPU", "start": 2153.52, "duration": 7.5}, {"text": "so that's the general workflow of GPU", "start": 2157.66, "duration": 5.4}, {"text": "program", "start": 2161.02, "duration": 3.96}, {"text": "um now if you think about what I said is", "start": 2163.06, "duration": 3.96}, {"text": "the host and a device so in the host you", "start": 2164.98, "duration": 3.9}, {"text": "have a CPU", "start": 2167.02, "duration": 6.36}, {"text": "um and your GPU CPU memory that you", "start": 2168.88, "duration": 6.3}, {"text": "would normally program in SEO or Fortran", "start": 2173.38, "duration": 3.84}, {"text": "program or well any any other", "start": 2175.18, "duration": 4.32}, {"text": "programming language for that matter and", "start": 2177.22, "duration": 4.32}, {"text": "then typically connected by the PCI bus", "start": 2179.5, "duration": 4.82}, {"text": "you have the device which is the GPU", "start": 2181.54, "duration": 6.96}, {"text": "that contains all these multi-processors", "start": 2184.32, "duration": 6.519}, {"text": "Each of which has many many compute", "start": 2188.5, "duration": 3.3}, {"text": "cores", "start": 2190.839, "duration": 4.201}, {"text": "some cash that's sitting between the RAM", "start": 2191.8, "duration": 4.98}, {"text": "and the gpus and the ram that you can", "start": 2195.04, "duration": 3.539}, {"text": "access on the device so you what you", "start": 2196.78, "duration": 3.96}, {"text": "would do is to copy input data from the", "start": 2198.579, "duration": 4.861}, {"text": "CPU memory to the GPU memory", "start": 2200.74, "duration": 4.619}, {"text": "then you load your GPU program and", "start": 2203.44, "duration": 3.44}, {"text": "execute", "start": 2205.359, "duration": 3.841}, {"text": "executed caching the data on the chip", "start": 2206.88, "duration": 4.6}, {"text": "for performance ideally", "start": 2209.2, "duration": 4.62}, {"text": "and we'll get to that in a little bit", "start": 2211.48, "duration": 4.139}, {"text": "and when you're done you copy your", "start": 2213.82, "duration": 4.38}, {"text": "results back from GPU memory to CPU", "start": 2215.619, "duration": 4.861}, {"text": "memory and what you have to keep in mind", "start": 2218.2, "duration": 5.22}, {"text": "that this data transfer between CPU and", "start": 2220.48, "duration": 5.099}, {"text": "GPU is uh", "start": 2223.42, "duration": 5.46}, {"text": "um slow uh comparatively slow because of", "start": 2225.579, "duration": 4.861}, {"text": "the PCI Express bus", "start": 2228.88, "duration": 2.66}, {"text": "um", "start": 2230.44, "duration": 4.86}, {"text": "limited memory bandwidth and that can be", "start": 2231.54, "duration": 6.16}, {"text": "a significant performance at bottleneck", "start": 2235.3, "duration": 3.96}, {"text": "um there is something that's called", "start": 2237.7, "duration": 3.48}, {"text": "unified memory and you don't I mentioned", "start": 2239.26, "duration": 3.359}, {"text": "before you don't have to explicitly", "start": 2241.18, "duration": 4.8}, {"text": "manage the memory", "start": 2242.619, "duration": 5.821}, {"text": "um so so what we looked at", "start": 2245.98, "duration": 4.8}, {"text": "um just on the slides before is you know", "start": 2248.44, "duration": 4.32}, {"text": "you have basically your your system", "start": 2250.78, "duration": 4.5}, {"text": "memory the CPU memory and the GPU memory", "start": 2252.76, "duration": 5.099}, {"text": "and you manage transfer between CPU and", "start": 2255.28, "duration": 4.319}, {"text": "GPU memory and if you use so-called", "start": 2257.859, "duration": 3.361}, {"text": "unified memory", "start": 2259.599, "duration": 3.061}, {"text": "um you don't have to worry about that", "start": 2261.22, "duration": 2.879}, {"text": "that's basically a pool of managed", "start": 2262.66, "duration": 2.939}, {"text": "memory that is shared between the host", "start": 2264.099, "duration": 3.5}, {"text": "and the device and the Q2 runtimes", "start": 2265.599, "duration": 4.201}, {"text": "automatically shuffling data for them", "start": 2267.599, "duration": 3.76}, {"text": "back whenever it needs to execute code", "start": 2269.8, "duration": 4.44}, {"text": "on the GPU or transfer it back to the", "start": 2271.359, "duration": 6.361}, {"text": "CPU to do some operations on the CPU", "start": 2274.24, "duration": 5.339}, {"text": "now the memory copies of course still", "start": 2277.72, "duration": 4.26}, {"text": "happen under the hood and", "start": 2279.579, "duration": 6.121}, {"text": "um so you have to it's a good idea to", "start": 2281.98, "duration": 5.76}, {"text": "know that this still happens and in", "start": 2285.7, "duration": 4.74}, {"text": "order to optimize the performance you", "start": 2287.74, "duration": 4.44}, {"text": "might have to explicitly manage the", "start": 2290.44, "duration": 4.22}, {"text": "memory yourself", "start": 2292.18, "duration": 2.48}, {"text": "I'll talk about some cuter Basics now", "start": 2295.359, "duration": 3.421}, {"text": "um", "start": 2298.18, "duration": 3.06}, {"text": "this is only going to be a few slides so", "start": 2298.78, "duration": 3.54}, {"text": "don't worry if you don't understand", "start": 2301.24, "duration": 3.72}, {"text": "everything it's impossible to um you", "start": 2302.32, "duration": 4.98}, {"text": "know put everything into a short short", "start": 2304.96, "duration": 3.84}, {"text": "webinar", "start": 2307.3, "duration": 3.12}, {"text": "um but I hope it's giving you a little", "start": 2308.8, "duration": 3.9}, {"text": "bit of a background and a little bit of", "start": 2310.42, "duration": 4.919}, {"text": "maybe a start to start digging a little", "start": 2312.7, "duration": 5.46}, {"text": "bit deeper so um common common terms", "start": 2315.339, "duration": 5.221}, {"text": "that we hear about is a kernel and in", "start": 2318.16, "duration": 5.4}, {"text": "Cuda kernel is code so typically a", "start": 2320.56, "duration": 4.08}, {"text": "function", "start": 2323.56, "duration": 4.799}, {"text": "that can be executed on the GPU and the", "start": 2324.64, "duration": 6.12}, {"text": "kernel code is operates in lockstep on", "start": 2328.359, "duration": 5.161}, {"text": "the multi-processors of the GPU and so", "start": 2330.76, "duration": 5.46}, {"text": "these are executed in and operate in", "start": 2333.52, "duration": 4.5}, {"text": "parallel and so-called warps that", "start": 2336.22, "duration": 4.08}, {"text": "currently consist of 32 threats so the", "start": 2338.02, "duration": 3.839}, {"text": "size of our work is not guaranteed but", "start": 2340.3, "duration": 4.68}, {"text": "it has been 32 um throughout the history", "start": 2341.859, "duration": 6.661}, {"text": "of GPU so far and video gpus", "start": 2344.98, "duration": 6.96}, {"text": "a thread is an execution unit of a", "start": 2348.52, "duration": 5.819}, {"text": "kernel that has a given index attached", "start": 2351.94, "duration": 5.46}, {"text": "to it to identify each thread and each", "start": 2354.339, "duration": 5.341}, {"text": "thread uses this index or can use these", "start": 2357.4, "duration": 4.439}, {"text": "indices to access a subset of data to", "start": 2359.68, "duration": 4.32}, {"text": "operate on", "start": 2361.839, "duration": 4.74}, {"text": "um threads are grouped into blocks uh", "start": 2364.0, "duration": 4.2}, {"text": "blocks are guaranteed to execute on the", "start": 2366.579, "duration": 3.301}, {"text": "same multi-processor which means that", "start": 2368.2, "duration": 3.419}, {"text": "threats within a block can synchronize", "start": 2369.88, "duration": 4.26}, {"text": "and they can share data via a shared", "start": 2371.619, "duration": 4.22}, {"text": "memory that's sitting on that", "start": 2374.14, "duration": 4.68}, {"text": "multiprocessor and then the so-called", "start": 2375.839, "duration": 5.681}, {"text": "grid is basically the total number of", "start": 2378.82, "duration": 5.039}, {"text": "blocks that are executing on the GPU at", "start": 2381.52, "duration": 5.04}, {"text": "the same time and the total number of", "start": 2383.859, "duration": 4.141}, {"text": "running threads of course is the number", "start": 2386.56, "duration": 3.96}, {"text": "of threads per blocks times the number", "start": 2388.0, "duration": 4.4}, {"text": "of blocks that are", "start": 2390.52, "duration": 4.559}, {"text": "launched on the GPU to execute them", "start": 2392.4, "duration": 4.62}, {"text": "parallel at the same time", "start": 2395.079, "duration": 4.921}, {"text": "so we see that exemplified here on the", "start": 2397.02, "duration": 4.059}, {"text": "right", "start": 2400.0, "duration": 3.3}, {"text": "we have", "start": 2401.079, "duration": 4.5}, {"text": "threads blocks grids and warps so the", "start": 2403.3, "duration": 4.68}, {"text": "grid is the entirety of all our threads", "start": 2405.579, "duration": 4.5}, {"text": "that are running on the GPU they map", "start": 2407.98, "duration": 4.08}, {"text": "onto a single GPU so one grid is", "start": 2410.079, "duration": 4.861}, {"text": "executing on the GPU and it consists of", "start": 2412.06, "duration": 5.039}, {"text": "multiple blocks the blocks mapped to the", "start": 2414.94, "duration": 3.6}, {"text": "multi-processors", "start": 2417.099, "duration": 4.081}, {"text": "and blocks are never split across", "start": 2418.54, "duration": 4.799}, {"text": "multiprocessors", "start": 2421.18, "duration": 2.76}, {"text": "um", "start": 2423.339, "duration": 2.821}, {"text": "but they can execute simultaneously on a", "start": 2423.94, "duration": 3.6}, {"text": "multi-processor so you can have many", "start": 2426.16, "duration": 3.0}, {"text": "blocks running at the same time on a", "start": 2427.54, "duration": 3.24}, {"text": "multi-processors they don't necessarily", "start": 2429.16, "duration": 3.72}, {"text": "execute at the same time and then the", "start": 2430.78, "duration": 5.1}, {"text": "threads are executed on the GPU cores", "start": 2432.88, "duration": 6.84}, {"text": "and in groups of 32 executing in Block", "start": 2435.88, "duration": 6.12}, {"text": "steps so what that means is you", "start": 2439.72, "duration": 3.84}, {"text": "launch", "start": 2442.0, "duration": 4.32}, {"text": "um a block of threads you will always", "start": 2443.56, "duration": 4.92}, {"text": "want to have a multiple of 32 so you", "start": 2446.32, "duration": 3.84}, {"text": "don't have any threads that are sitting", "start": 2448.48, "duration": 5.28}, {"text": "idle and doing no Ops", "start": 2450.16, "duration": 5.699}, {"text": "there are built-in variables in order to", "start": 2453.76, "duration": 4.38}, {"text": "control your code", "start": 2455.859, "duration": 2.941}, {"text": "um", "start": 2458.14, "duration": 2.88}, {"text": "and you can use those to compute the ID", "start": 2458.8, "duration": 3.72}, {"text": "of each individual thread that is", "start": 2461.02, "duration": 3.24}, {"text": "executing in a grid block so you have", "start": 2462.52, "duration": 4.14}, {"text": "block indices that give you the", "start": 2464.26, "duration": 4.079}, {"text": "dimension of the grid", "start": 2466.66, "duration": 4.98}, {"text": "griddim dot X Y and Z and then block ID", "start": 2468.339, "duration": 7.26}, {"text": "so for the ID of the block and so these", "start": 2471.64, "duration": 5.939}, {"text": "variables return the grid Dimension and", "start": 2475.599, "duration": 4.621}, {"text": "the block ID in the X Y and Z axis", "start": 2477.579, "duration": 5.28}, {"text": "so you can have multi-dimensional blocks", "start": 2480.22, "duration": 5.76}, {"text": "that can make it easier to map onto", "start": 2482.859, "duration": 3.961}, {"text": "certain", "start": 2485.98, "duration": 2.46}, {"text": "um problems that you have for instance", "start": 2486.82, "duration": 3.48}, {"text": "if you are operating on a vector you", "start": 2488.44, "duration": 3.24}, {"text": "might only want to use a single", "start": 2490.3, "duration": 3.6}, {"text": "Dimension if you operate on a matrix you", "start": 2491.68, "duration": 4.32}, {"text": "might want to use the two Dimensions to", "start": 2493.9, "duration": 4.8}, {"text": "um make indexing easier", "start": 2496.0, "duration": 4.26}, {"text": "and then you have individual thread", "start": 2498.7, "duration": 5.1}, {"text": "thread indices that give you the size of", "start": 2500.26, "duration": 6.359}, {"text": "the blocks so the block dim dot X Y and", "start": 2503.8, "duration": 5.7}, {"text": "Z and the thread ID within that given", "start": 2506.619, "duration": 3.72}, {"text": "block", "start": 2509.5, "duration": 2.18}, {"text": "and", "start": 2510.339, "duration": 3.78}, {"text": "for instance an example on the right", "start": 2511.68, "duration": 4.72}, {"text": "here we would have a grid that consists", "start": 2514.119, "duration": 5.761}, {"text": "of three by two blocks right so we have", "start": 2516.4, "duration": 5.699}, {"text": "six blocks running and each block", "start": 2519.88, "duration": 5.88}, {"text": "consists of four by three threads so", "start": 2522.099, "duration": 5.52}, {"text": "that means we would have 12 threads per", "start": 2525.76, "duration": 2.64}, {"text": "block", "start": 2527.619, "duration": 3.781}, {"text": "times six blocks a total of 72 threads", "start": 2528.4, "duration": 5.16}, {"text": "running in this example", "start": 2531.4, "duration": 4.38}, {"text": "then if you write a function that has", "start": 2533.56, "duration": 4.5}, {"text": "should execute on the GPU you use the", "start": 2535.78, "duration": 4.079}, {"text": "underscore underscore Global underscore", "start": 2538.06, "duration": 3.24}, {"text": "underscore keyword", "start": 2539.859, "duration": 3.841}, {"text": "and that designates a function that", "start": 2541.3, "duration": 5.58}, {"text": "executes on the GPU it must return void", "start": 2543.7, "duration": 5.52}, {"text": "but it is callable from the host code", "start": 2546.88, "duration": 4.08}, {"text": "you also can write functions that are", "start": 2549.22, "duration": 4.56}, {"text": "callable from the from the GPU", "start": 2550.96, "duration": 4.86}, {"text": "for instance here is a vector addition", "start": 2553.78, "duration": 4.559}, {"text": "kernel that takes", "start": 2555.82, "duration": 5.82}, {"text": "um three pointers in a b and c and what", "start": 2558.339, "duration": 4.74}, {"text": "we do", "start": 2561.64, "duration": 4.26}, {"text": "is we have c as a result that it's the", "start": 2563.079, "duration": 4.621}, {"text": "sum of a plus b", "start": 2565.9, "duration": 3.84}, {"text": "and the total size we're passing in the", "start": 2567.7, "duration": 3.96}, {"text": "size of the of the array and so what we", "start": 2569.74, "duration": 5.22}, {"text": "do is we compute a thread ID", "start": 2571.66, "duration": 7.38}, {"text": "based on the thread ID that of the", "start": 2574.96, "duration": 6.659}, {"text": "current and the executing kernel so as", "start": 2579.04, "duration": 5.88}, {"text": "you know so each thread knows its thread", "start": 2581.619, "duration": 5.041}, {"text": "ID and the block Dimension and the block", "start": 2584.92, "duration": 3.419}, {"text": "ID", "start": 2586.66, "duration": 4.32}, {"text": "and here we compute an offset based on", "start": 2588.339, "duration": 4.321}, {"text": "the total number of threads in a blocks", "start": 2590.98, "duration": 3.119}, {"text": "which is the block Dimension times the", "start": 2592.66, "duration": 2.64}, {"text": "block ID", "start": 2594.099, "duration": 3.24}, {"text": "and this is the stride that we want to", "start": 2595.3, "duration": 4.14}, {"text": "use to", "start": 2597.339, "duration": 3.721}, {"text": "um walk through the", "start": 2599.44, "duration": 2.76}, {"text": "um", "start": 2601.06, "duration": 4.5}, {"text": "the arrays and so basically what we do", "start": 2602.2, "duration": 5.159}, {"text": "as long as you know our thread ID is", "start": 2605.56, "duration": 3.96}, {"text": "less than the total size size of the", "start": 2607.359, "duration": 4.98}, {"text": "vector we do that addition right so in", "start": 2609.52, "duration": 4.86}, {"text": "each of the threads that is launched at", "start": 2612.339, "duration": 4.141}, {"text": "the same time there's an addition on a", "start": 2614.38, "duration": 4.38}, {"text": "different element based on the thread ID", "start": 2616.48, "duration": 4.32}, {"text": "and that's how you write a parallel code", "start": 2618.76, "duration": 3.96}, {"text": "on gpus", "start": 2620.8, "duration": 5.039}, {"text": "um the Cuda API also handles the device", "start": 2622.72, "duration": 5.46}, {"text": "memory I talked before about Cuda malloc", "start": 2625.839, "duration": 4.141}, {"text": "q23 and Cuda mem copy which is an", "start": 2628.18, "duration": 4.74}, {"text": "equivalent of the memory allocation copy", "start": 2629.98, "duration": 4.44}, {"text": "and uh", "start": 2632.92, "duration": 5.88}, {"text": "functions of c and so Tudor mem copy is", "start": 2634.42, "duration": 5.699}, {"text": "used to transfer the data between the", "start": 2638.8, "duration": 3.12}, {"text": "CPU and the GPU memory", "start": 2640.119, "duration": 3.96}, {"text": "and then finally once you've written a", "start": 2641.92, "duration": 3.3}, {"text": "kernel", "start": 2644.079, "duration": 3.54}, {"text": "um you also have to launch the kernel", "start": 2645.22, "duration": 4.56}, {"text": "and you have to tell the GPU how many", "start": 2647.619, "duration": 4.261}, {"text": "threads you want to run and that's used", "start": 2649.78, "duration": 3.96}, {"text": "with the kernel launch specification so", "start": 2651.88, "duration": 3.979}, {"text": "you use the triple angle bracket here", "start": 2653.74, "duration": 4.379}, {"text": "when you're calling the vector addition", "start": 2655.859, "duration": 4.661}, {"text": "kernel here with uh this triple angle", "start": 2658.119, "duration": 5.401}, {"text": "bracket notation and you give it the", "start": 2660.52, "duration": 5.819}, {"text": "number of blocks right and you give it", "start": 2663.52, "duration": 4.5}, {"text": "the number of threads in a so-called", "start": 2666.339, "duration": 3.121}, {"text": "there's a dim three data type basically", "start": 2668.02, "duration": 3.36}, {"text": "that just contains three integers that", "start": 2669.46, "duration": 5.399}, {"text": "default to one so if you don't specify", "start": 2671.38, "duration": 7.739}, {"text": "py and bz only the X um", "start": 2674.859, "duration": 7.141}, {"text": "uh would have would would explicitly get", "start": 2679.119, "duration": 4.621}, {"text": "a number that's different from one and", "start": 2682.0, "duration": 3.06}, {"text": "the same for the number of threads so", "start": 2683.74, "duration": 3.42}, {"text": "here you would launch", "start": 2685.06, "duration": 5.16}, {"text": "uh grid of certain number of blocks and", "start": 2687.16, "duration": 5.459}, {"text": "here you define the number of block size", "start": 2690.22, "duration": 4.68}, {"text": "and um so that determines the number of", "start": 2692.619, "duration": 3.541}, {"text": "threads that are running at the same", "start": 2694.9, "duration": 3.439}, {"text": "time", "start": 2696.16, "duration": 2.179}, {"text": "um so here's something about the memory", "start": 2698.8, "duration": 4.26}, {"text": "hierarchy", "start": 2700.9, "duration": 3.179}, {"text": "um", "start": 2703.06, "duration": 5.279}, {"text": "I will try to go fast because we're", "start": 2704.079, "duration": 5.76}, {"text": "already running a little bit late and I", "start": 2708.339, "duration": 3.961}, {"text": "wanted to show you also how to access", "start": 2709.839, "duration": 3.841}, {"text": "comet", "start": 2712.3, "duration": 4.74}, {"text": "in Comet GPU nodes but this is exactly", "start": 2713.68, "duration": 6.12}, {"text": "what I've been talking about before we", "start": 2717.04, "duration": 4.319}, {"text": "have memory here that's sitting on the", "start": 2719.8, "duration": 3.18}, {"text": "host you have to transfer data to the", "start": 2721.359, "duration": 4.321}, {"text": "GPU on the GPU you have something that's", "start": 2722.98, "duration": 4.56}, {"text": "called Global memory", "start": 2725.68, "duration": 4.02}, {"text": "that's visible to all threads and access", "start": 2727.54, "duration": 4.44}, {"text": "to that memory is low so what you want", "start": 2729.7, "duration": 4.44}, {"text": "to use is so-called shared memory but", "start": 2731.98, "duration": 4.8}, {"text": "that's shared only within blocks that", "start": 2734.14, "duration": 4.74}, {"text": "are guaranteed to execute on a given", "start": 2736.78, "duration": 4.319}, {"text": "multiprocessor because that's specific", "start": 2738.88, "duration": 5.04}, {"text": "memory for each of the multi-processors", "start": 2741.099, "duration": 3.421}, {"text": "um", "start": 2743.92, "duration": 2.46}, {"text": "so that shared memory is visible to all", "start": 2744.52, "duration": 3.68}, {"text": "threads in a block", "start": 2746.38, "duration": 5.16}, {"text": "only within that block but it's fast and", "start": 2748.2, "duration": 5.02}, {"text": "then you have registers that's basically", "start": 2751.54, "duration": 4.26}, {"text": "a per thread memory and you have", "start": 2753.22, "duration": 4.08}, {"text": "something that's called local memory", "start": 2755.8, "duration": 3.36}, {"text": "that's a little bit confusing it's", "start": 2757.3, "duration": 5.46}, {"text": "actually not on that schematic here", "start": 2759.16, "duration": 5.4}, {"text": "um that's actually if you run out of", "start": 2762.76, "duration": 4.02}, {"text": "registers", "start": 2764.56, "duration": 4.92}, {"text": "um the data that your kernel that is", "start": 2766.78, "duration": 5.1}, {"text": "executing requires to execute is going", "start": 2769.48, "duration": 4.32}, {"text": "to store it into local memory which is", "start": 2771.88, "duration": 3.66}, {"text": "thread", "start": 2773.8, "duration": 4.019}, {"text": "um private but resides in global memory", "start": 2775.54, "duration": 4.44}, {"text": "so it's going to slow down your code so", "start": 2777.819, "duration": 3.54}, {"text": "what you want to do is you want to write", "start": 2779.98, "duration": 4.32}, {"text": "code that doesn't spill out of registers", "start": 2781.359, "duration": 4.74}, {"text": "there's something called constant memory", "start": 2784.3, "duration": 3.48}, {"text": "that can be very useful it's visible to", "start": 2786.099, "duration": 3.541}, {"text": "all threads", "start": 2787.78, "duration": 5.42}, {"text": "it's read only from the GPU so you can", "start": 2789.64, "duration": 7.38}, {"text": "copy data there from this CPU", "start": 2793.2, "duration": 5.139}, {"text": "um it's off chip so it's not on the", "start": 2797.02, "duration": 4.38}, {"text": "multi-processors but it is cached and", "start": 2798.339, "duration": 5.341}, {"text": "importantly broadcasts to all threads in", "start": 2801.4, "duration": 4.74}, {"text": "a so-called half work so that can", "start": 2803.68, "duration": 3.3}, {"text": "increase", "start": 2806.14, "duration": 1.56}, {"text": "um", "start": 2806.98, "duration": 2.639}, {"text": "significantly increased memory bandwidth", "start": 2807.7, "duration": 4.74}, {"text": "if you have data that is used", "start": 2809.619, "duration": 5.401}, {"text": "um by many different threads at the same", "start": 2812.44, "duration": 4.52}, {"text": "time", "start": 2815.02, "duration": 5.7}, {"text": "so given that memory hierarchy so the", "start": 2816.96, "duration": 6.22}, {"text": "general tutor programming strategy is to", "start": 2820.72, "duration": 4.5}, {"text": "avoid data transfers between CPU and GPU", "start": 2823.18, "duration": 3.659}, {"text": "as far as possible because these are", "start": 2825.22, "duration": 3.24}, {"text": "slow due to low PCI Express Bus", "start": 2826.839, "duration": 3.0}, {"text": "bandwidth", "start": 2828.46, "duration": 4.68}, {"text": "minimize access to global memory so we", "start": 2829.839, "duration": 5.041}, {"text": "would hide in our Kernel's memory access", "start": 2833.14, "duration": 4.52}, {"text": "latency by launching many threads and", "start": 2834.88, "duration": 5.52}, {"text": "take advantage of the fast shared memory", "start": 2837.66, "duration": 5.62}, {"text": "that's sitting on the multi-processors", "start": 2840.4, "duration": 5.52}, {"text": "by tiling data so we would partition", "start": 2843.28, "duration": 5.16}, {"text": "data into subsets that fit into the", "start": 2845.92, "duration": 5.04}, {"text": "shared memory then handle each data", "start": 2848.44, "duration": 5.639}, {"text": "subset within a given thread block load", "start": 2850.96, "duration": 5.399}, {"text": "the subset of data from Global to Shared", "start": 2854.079, "duration": 5.28}, {"text": "memory using multiple threads so you can", "start": 2856.359, "duration": 4.681}, {"text": "exploit parallelism in the memory access", "start": 2859.359, "duration": 4.201}, {"text": "and then perform computations only on", "start": 2861.04, "duration": 4.5}, {"text": "the data subset and shared memory and", "start": 2863.56, "duration": 3.66}, {"text": "each thread in a thread block can access", "start": 2865.54, "duration": 4.02}, {"text": "that data model times of course", "start": 2867.22, "duration": 4.08}, {"text": "um and then once you're done you copy", "start": 2869.56, "duration": 3.24}, {"text": "the results back from the shared memory", "start": 2871.3, "duration": 4.14}, {"text": "to the global memory and at the very end", "start": 2872.8, "duration": 4.019}, {"text": "of your program of course back to the", "start": 2875.44, "duration": 3.06}, {"text": "CPU", "start": 2876.819, "duration": 2.581}, {"text": "um", "start": 2878.5, "duration": 2.7}, {"text": "there's an example that I have here", "start": 2879.4, "duration": 3.8}, {"text": "about matrix multiplication", "start": 2881.2, "duration": 5.639}, {"text": "I think we are going to leave the slides", "start": 2883.2, "duration": 8.26}, {"text": "on the available at our website we will", "start": 2886.839, "duration": 7.141}, {"text": "post them on a training page so I won't", "start": 2891.46, "duration": 4.98}, {"text": "go in detail through that", "start": 2893.98, "duration": 4.74}, {"text": "um it might it's going to take too long", "start": 2896.44, "duration": 4.8}, {"text": "to explain probably but you know what", "start": 2898.72, "duration": 4.02}, {"text": "you do is you allocate your memory on", "start": 2901.24, "duration": 2.64}, {"text": "the CPU", "start": 2902.74, "duration": 3.0}, {"text": "on the device", "start": 2903.88, "duration": 4.5}, {"text": "you read in data from somewhere", "start": 2905.74, "duration": 4.56}, {"text": "then you copy the", "start": 2908.38, "duration": 4.439}, {"text": "data that you have so the Matrix A and B", "start": 2910.3, "duration": 5.16}, {"text": "from the device to from the host to the", "start": 2912.819, "duration": 5.28}, {"text": "device was appear to GPU you set your", "start": 2915.46, "duration": 4.32}, {"text": "execution parameters", "start": 2918.099, "duration": 3.421}, {"text": "um meaning the number of threads and the", "start": 2919.78, "duration": 3.48}, {"text": "number of blocks in the grid", "start": 2921.52, "duration": 3.54}, {"text": "and then you call your kernel in this", "start": 2923.26, "duration": 4.559}, {"text": "case a matrix multiplication kernel now", "start": 2925.06, "duration": 6.24}, {"text": "on the output data here that's sitting", "start": 2927.819, "duration": 5.941}, {"text": "on the device so the resulting Matrix C", "start": 2931.3, "duration": 6.42}, {"text": "that's a product of a times B and then", "start": 2933.76, "duration": 5.52}, {"text": "we when you've done you copy that", "start": 2937.72, "duration": 3.48}, {"text": "results Matrix back from the device to", "start": 2939.28, "duration": 4.079}, {"text": "the host and you free your memory and", "start": 2941.2, "duration": 5.28}, {"text": "you're done and the way you want to do", "start": 2943.359, "duration": 4.861}, {"text": "this is exemplified here so this is", "start": 2946.48, "duration": 3.72}, {"text": "actually taken from the q2c programming", "start": 2948.22, "duration": 4.02}, {"text": "examples if you look at that you would", "start": 2950.2, "duration": 4.2}, {"text": "um see also a little bit of an", "start": 2952.24, "duration": 3.72}, {"text": "explanation about that but what you want", "start": 2954.4, "duration": 3.179}, {"text": "to do is exactly what I was explaining", "start": 2955.96, "duration": 3.659}, {"text": "so you have a matrix multiplayer", "start": 2957.579, "duration": 5.401}, {"text": "multiplication of a times B and the", "start": 2959.619, "duration": 5.7}, {"text": "results ultimate Matrix is C and that's", "start": 2962.98, "duration": 4.08}, {"text": "basically a lot of dot products so", "start": 2965.319, "duration": 6.741}, {"text": "Vector dot products of rows by columns", "start": 2967.06, "duration": 8.94}, {"text": "Matrix A and B and what you would do is", "start": 2972.06, "duration": 5.44}, {"text": "each block", "start": 2976.0, "duration": 4.74}, {"text": "would copy a subset of the data into the", "start": 2977.5, "duration": 6.24}, {"text": "shared memory operate on that subset so", "start": 2980.74, "duration": 4.619}, {"text": "that you don't have to access the data", "start": 2983.74, "duration": 3.54}, {"text": "repeatedly from Global memory but you", "start": 2985.359, "duration": 3.301}, {"text": "have it basically cached in the fast", "start": 2987.28, "duration": 3.0}, {"text": "shared memory that you have explicitly", "start": 2988.66, "duration": 3.3}, {"text": "managed", "start": 2990.28, "duration": 4.92}, {"text": "um to compute here the resulting matrix", "start": 2991.96, "duration": 4.1}, {"text": "product", "start": 2995.2, "duration": 3.18}, {"text": "Matrix elements of the product Matrix", "start": 2996.06, "duration": 4.6}, {"text": "and then you move to the next block", "start": 2998.38, "duration": 4.5}, {"text": "and that's exemplified on the next two", "start": 3000.66, "duration": 4.5}, {"text": "slides but I'll just skip over this", "start": 3002.88, "duration": 4.739}, {"text": "you can have a look at that yourself", "start": 3005.16, "duration": 5.34}, {"text": "um it's just I'm showing on how to do", "start": 3007.619, "duration": 4.2}, {"text": "that if you would write your own matrix", "start": 3010.5, "duration": 3.119}, {"text": "multiplication", "start": 3011.819, "duration": 2.581}, {"text": "um", "start": 3013.619, "duration": 3.661}, {"text": "with optimization to use shared memory", "start": 3014.4, "duration": 5.699}, {"text": "to minimize Global memory access latency", "start": 3017.28, "duration": 5.94}, {"text": "and bandwidth problems um but of course", "start": 3020.099, "duration": 6.541}, {"text": "you wouldn't do that right so we would", "start": 3023.22, "duration": 5.58}, {"text": "in this example have have used a few", "start": 3026.64, "duration": 4.26}, {"text": "Cuda features and that's very typical", "start": 3028.8, "duration": 4.319}, {"text": "example of how you would write Cuda code", "start": 3030.9, "duration": 4.26}, {"text": "but in reality for Matrix Matrix", "start": 3033.119, "duration": 3.48}, {"text": "multiplication of course we would just", "start": 3035.16, "duration": 3.179}, {"text": "use the Cuda implementation of glass", "start": 3036.599, "duration": 5.881}, {"text": "which is highly optimized for gpus", "start": 3038.339, "duration": 6.48}, {"text": "so I think that was the last slide about", "start": 3042.48, "duration": 5.879}, {"text": "Cuda and let me very quickly go over", "start": 3044.819, "duration": 6.361}, {"text": "directive based programming", "start": 3048.359, "duration": 6.661}, {"text": "um just to show you some Basics and then", "start": 3051.18, "duration": 6.6}, {"text": "have a quick look on how we can access", "start": 3055.02, "duration": 4.92}, {"text": "gpus on Comet and how we can compile", "start": 3057.78, "duration": 5.039}, {"text": "code on comment and execute um GPU code", "start": 3059.94, "duration": 6.179}, {"text": "on gpus and Comet good question yeah so", "start": 3062.819, "duration": 6.921}, {"text": "we have a question that uh", "start": 3066.119, "duration": 3.621}, {"text": "is there any latex library for gpus", "start": 3070.079, "duration": 4.441}, {"text": "um", "start": 3073.319, "duration": 5.76}, {"text": "there is as far as I remember so there's", "start": 3074.52, "duration": 8.039}, {"text": "different options and I personally don't", "start": 3079.079, "duration": 5.881}, {"text": "have much experience with a with a", "start": 3082.559, "duration": 4.321}, {"text": "these libraries there's some libraries", "start": 3084.96, "duration": 3.8}, {"text": "that's called q and LA", "start": 3086.88, "duration": 4.8}, {"text": "that is partially free I don't know if", "start": 3088.76, "duration": 4.72}, {"text": "it's entirely free now so you'd have to", "start": 3091.68, "duration": 4.5}, {"text": "have a look at the Nvidia websites there", "start": 3093.48, "duration": 4.379}, {"text": "is an effort that comes out of checked", "start": 3096.18, "duration": 3.98}, {"text": "on garage group that's called magma", "start": 3097.859, "duration": 4.74}, {"text": "that actually works across different", "start": 3100.16, "duration": 5.32}, {"text": "types of accelerators", "start": 3102.599, "duration": 7.141}, {"text": "um and certainly it it supports gpus and", "start": 3105.48, "duration": 5.76}, {"text": "multiple gpus", "start": 3109.74, "duration": 4.56}, {"text": "um and Maxima is an implementation that", "start": 3111.24, "duration": 6.18}, {"text": "as far as I know contains um", "start": 3114.3, "duration": 6.84}, {"text": "it is stable and contains all functions", "start": 3117.42, "duration": 5.699}, {"text": "that are available or many of the", "start": 3121.14, "duration": 3.479}, {"text": "functions that are available in lifehack", "start": 3123.119, "duration": 3.361}, {"text": "so you would have to look about", "start": 3124.619, "duration": 3.601}, {"text": "specifically the functions that you need", "start": 3126.48, "duration": 7.339}, {"text": "but um I think there is a large large", "start": 3128.22, "duration": 8.82}, {"text": "code base available to you for use on", "start": 3133.819, "duration": 5.861}, {"text": "gpus for linear algebra that would", "start": 3137.04, "duration": 8.12}, {"text": "replace La pack I paste it once okay", "start": 3139.68, "duration": 5.48}, {"text": "so directive based programming open ACC", "start": 3145.819, "duration": 7.0}, {"text": "is designed to make porting to gpus easy", "start": 3150.02, "duration": 5.26}, {"text": "quick and portable it's an openmp like", "start": 3152.819, "duration": 5.161}, {"text": "compiler directive language", "start": 3155.28, "duration": 4.02}, {"text": "um so if the compiler does not", "start": 3157.98, "duration": 2.94}, {"text": "understand the directors it will ignore", "start": 3159.3, "duration": 4.5}, {"text": "them it's just comments in the code and", "start": 3160.92, "duration": 5.04}, {"text": "so the same code the ideas can be used", "start": 3163.8, "duration": 4.14}, {"text": "to work with or without accelerators in", "start": 3165.96, "duration": 3.48}, {"text": "practice that's not entirely true but", "start": 3167.94, "duration": 3.6}, {"text": "because you often have to think of what", "start": 3169.44, "duration": 3.54}, {"text": "it might have to restructure your code", "start": 3171.54, "duration": 3.9}, {"text": "but in principle that is true and it", "start": 3172.98, "duration": 4.26}, {"text": "works for fortune and C is fully", "start": 3175.44, "duration": 4.139}, {"text": "supported by the PGI compilers that you", "start": 3177.24, "duration": 5.76}, {"text": "can get for free from Nvidia", "start": 3179.579, "duration": 5.881}, {"text": "create compilers and craze and there's a", "start": 3183.0, "duration": 5.16}, {"text": "partial support by glue compilers and", "start": 3185.46, "duration": 4.379}, {"text": "also some less commonly used and", "start": 3188.16, "duration": 3.179}, {"text": "experimental compilers so that's", "start": 3189.839, "duration": 3.601}, {"text": "something that's moving forward but in a", "start": 3191.339, "duration": 3.901}, {"text": "few years from now it might be that also", "start": 3193.44, "duration": 4.44}, {"text": "I that you know open ACC or", "start": 3195.24, "duration": 4.619}, {"text": "functionality of openlcc is basically", "start": 3197.88, "duration": 4.56}, {"text": "merged into openmp there are efforts", "start": 3199.859, "duration": 6.301}, {"text": "along the along that um Direction but", "start": 3202.44, "duration": 5.34}, {"text": "it's not made sure at this moment so we", "start": 3206.16, "duration": 3.12}, {"text": "won't discuss this", "start": 3207.78, "duration": 4.079}, {"text": "so the PGI Community Edition", "start": 3209.28, "duration": 4.559}, {"text": "um is free", "start": 3211.859, "duration": 5.22}, {"text": "um it contains the PGI compilers Fortran", "start": 3213.839, "duration": 5.581}, {"text": "C C plus plus compilers and has open ACC", "start": 3217.079, "duration": 4.98}, {"text": "support it also contains the PG Prof", "start": 3219.42, "duration": 4.98}, {"text": "performance profiler and a handful of", "start": 3222.059, "duration": 4.56}, {"text": "GPU enabled libraries and some open ACC", "start": 3224.4, "duration": 4.679}, {"text": "code samples", "start": 3226.619, "duration": 4.801}, {"text": "so how to use it so", "start": 3229.079, "duration": 4.681}, {"text": "um I talked earlier about Saxby function", "start": 3231.42, "duration": 4.08}, {"text": "right so you have an example here of sax", "start": 3233.76, "duration": 5.099}, {"text": "p and that's an implementation of a", "start": 3235.5, "duration": 6.66}, {"text": "times X Plus Y and just a for Loop right", "start": 3238.859, "duration": 4.2}, {"text": "um", "start": 3242.16, "duration": 3.0}, {"text": "and all you do is you have insert a", "start": 3243.059, "duration": 5.401}, {"text": "paragraph here ACC kernels and that", "start": 3245.16, "duration": 5.34}, {"text": "should automatically generate the", "start": 3248.46, "duration": 5.399}, {"text": "corresponding GPU code and memory", "start": 3250.5, "duration": 5.4}, {"text": "management and so on X executed on the", "start": 3253.859, "duration": 3.541}, {"text": "GPU so in principle is really simple", "start": 3255.9, "duration": 4.919}, {"text": "right and Fortune looks very similar", "start": 3257.4, "duration": 4.32}, {"text": "um", "start": 3260.819, "duration": 4.441}, {"text": "so you have a directive syntax where you", "start": 3261.72, "duration": 6.06}, {"text": "have the the here unfortunately the", "start": 3265.26, "duration": 5.339}, {"text": "dollar ACC directive or a pragma ACC and", "start": 3267.78, "duration": 5.18}, {"text": "C then the directive and some clauses", "start": 3270.599, "duration": 4.5}, {"text": "often followed by structured code block", "start": 3272.96, "duration": 4.119}, {"text": "and for instance if you look at the", "start": 3275.099, "duration": 4.081}, {"text": "kernels Constructor will construct that", "start": 3277.079, "duration": 4.321}, {"text": "will generate kernels to be executed on", "start": 3279.18, "duration": 6.54}, {"text": "the GPU and then there are clauses if", "start": 3281.4, "duration": 5.82}, {"text": "certain conditions you can have if", "start": 3285.72, "duration": 4.5}, {"text": "Clauses or asynchronous execution and", "start": 3287.22, "duration": 5.28}, {"text": "then there are data Clauses as well", "start": 3290.22, "duration": 5.099}, {"text": "what are data Clauses they are actually", "start": 3292.5, "duration": 5.22}, {"text": "used to manage memory allocation and", "start": 3295.319, "duration": 3.481}, {"text": "that's what we've been talking about", "start": 3297.72, "duration": 2.76}, {"text": "before with Cuda you know you have to", "start": 3298.8, "duration": 3.36}, {"text": "know where your memory is living and", "start": 3300.48, "duration": 3.18}, {"text": "what is happening so there are different", "start": 3302.16, "duration": 3.439}, {"text": "types of data Clauses that you can use", "start": 3303.66, "duration": 5.28}, {"text": "and I'll show you an example", "start": 3305.599, "duration": 6.22}, {"text": "that is an example that comes um I think", "start": 3308.94, "duration": 5.46}, {"text": "from nvidia's websites as well", "start": 3311.819, "duration": 4.921}, {"text": "foreign copy iterations that are used to", "start": 3314.4, "duration": 6.179}, {"text": "iteratively converge to a final value by", "start": 3316.74, "duration": 5.4}, {"text": "Computing new values at each point from", "start": 3320.579, "duration": 3.421}, {"text": "the average of neighboring points if you", "start": 3322.14, "duration": 3.54}, {"text": "have data on a grid for instance here so", "start": 3324.0, "duration": 4.92}, {"text": "aij would be at time step or step K plus", "start": 3325.68, "duration": 7.26}, {"text": "1 computed as the average of the data", "start": 3328.92, "duration": 6.3}, {"text": "from the resulting um", "start": 3332.94, "duration": 4.44}, {"text": "points", "start": 3335.22, "duration": 4.98}, {"text": "on a grid right so you can use that for", "start": 3337.38, "duration": 4.5}, {"text": "instance to solve the plus equation in", "start": 3340.2, "duration": 4.8}, {"text": "2D and here's the code", "start": 3341.88, "duration": 5.34}, {"text": "where you iterate until you converged", "start": 3345.0, "duration": 3.9}, {"text": "right so there's a while loop so code", "start": 3347.22, "duration": 2.82}, {"text": "and C here", "start": 3348.9, "duration": 1.8}, {"text": "um", "start": 3350.04, "duration": 3.48}, {"text": "and then you go across all the Matrix", "start": 3350.7, "duration": 6.419}, {"text": "elements J and y i of your 2D Grid in", "start": 3353.52, "duration": 5.16}, {"text": "this case and then you compute your", "start": 3357.119, "duration": 4.801}, {"text": "resulting new values as the average of", "start": 3358.68, "duration": 6.72}, {"text": "the neighboring values I plus 1 I minus", "start": 3361.92, "duration": 6.6}, {"text": "one J minus one J plus one right from", "start": 3365.4, "duration": 4.74}, {"text": "the previous iteration", "start": 3368.52, "duration": 4.319}, {"text": "and then you compute the error as the", "start": 3370.14, "duration": 5.1}, {"text": "difference of the Matrix elements from", "start": 3372.839, "duration": 5.701}, {"text": "before and and with respect to the", "start": 3375.24, "duration": 5.46}, {"text": "previous iteration and you will stop and", "start": 3378.54, "duration": 4.14}, {"text": "if your errors below a certain threshold", "start": 3380.7, "duration": 2.82}, {"text": "right", "start": 3382.68, "duration": 2.939}, {"text": "here in this while loop", "start": 3383.52, "duration": 4.079}, {"text": "and at the end you need to swap the", "start": 3385.619, "duration": 4.44}, {"text": "input and output arrays actually and see", "start": 3387.599, "duration": 4.141}, {"text": "also unfortunately you could do that", "start": 3390.059, "duration": 3.54}, {"text": "better just with pointers but I mean for", "start": 3391.74, "duration": 5.52}, {"text": "just the purpose of um this example you", "start": 3393.599, "duration": 4.441}, {"text": "would", "start": 3397.26, "duration": 3.78}, {"text": "um now copy the new results into the old", "start": 3398.04, "duration": 4.26}, {"text": "results and then go to the next", "start": 3401.04, "duration": 4.799}, {"text": "iteration of your Loop now okay you", "start": 3402.3, "duration": 5.819}, {"text": "could parallelize this on the GPU just", "start": 3405.839, "duration": 5.101}, {"text": "by inserting track my ACC kernels", "start": 3408.119, "duration": 5.94}, {"text": "and hope that it will execute on the GPU", "start": 3410.94, "duration": 4.86}, {"text": "so the compiler will generate code that", "start": 3414.059, "duration": 3.661}, {"text": "executes and this part of the code on", "start": 3415.8, "duration": 3.42}, {"text": "the GPU and then this part of the code", "start": 3417.72, "duration": 2.879}, {"text": "on the GPU", "start": 3419.22, "duration": 4.56}, {"text": "and um so oops this is actually the", "start": 3420.599, "duration": 4.081}, {"text": "wrong", "start": 3423.78, "duration": 3.059}, {"text": "command tier pgf 90 is for Fortran code", "start": 3424.68, "duration": 4.26}, {"text": "but it's the same for pgcc is the C", "start": 3426.839, "duration": 4.98}, {"text": "compiler I would say that you want to", "start": 3428.94, "duration": 4.32}, {"text": "accelerate the code", "start": 3431.819, "duration": 4.381}, {"text": "um and M in for equals Excel will tell", "start": 3433.26, "duration": 4.559}, {"text": "you will give you some information about", "start": 3436.2, "duration": 3.96}, {"text": "what the code is doing so it's going to", "start": 3437.819, "duration": 3.901}, {"text": "tell you you know it's generating copy", "start": 3440.16, "duration": 3.62}, {"text": "out and copy in so that's data transfers", "start": 3441.72, "duration": 5.04}, {"text": "it will generate a kernel and has some", "start": 3443.78, "duration": 5.319}, {"text": "information about the kernel", "start": 3446.76, "duration": 5.819}, {"text": "and if you run the code so here's an", "start": 3449.099, "duration": 6.121}, {"text": "example executed on", "start": 3452.579, "duration": 5.941}, {"text": "um sdsc comment", "start": 3455.22, "duration": 5.04}, {"text": "um with the same code executing on a", "start": 3458.52, "duration": 4.319}, {"text": "single thread or opening P parallel so", "start": 3460.26, "duration": 4.2}, {"text": "it doesn't scale very well but you can", "start": 3462.839, "duration": 3.121}, {"text": "see you get a speed of approximately", "start": 3464.46, "duration": 4.08}, {"text": "three running on six cores and if you", "start": 3465.96, "duration": 5.099}, {"text": "use open ACC", "start": 3468.54, "duration": 3.48}, {"text": "um", "start": 3471.059, "duration": 2.701}, {"text": "exactly the way that I've thrown you in", "start": 3472.02, "duration": 3.96}, {"text": "the slide before it is terribly slow", "start": 3473.76, "duration": 4.559}, {"text": "right so you get a Slowdown and not a", "start": 3475.98, "duration": 4.56}, {"text": "speed up so something has gone wrong", "start": 3478.319, "duration": 4.921}, {"text": "now if we profile the code", "start": 3480.54, "duration": 4.74}, {"text": "and it can be done by setting like", "start": 3483.24, "duration": 3.359}, {"text": "environment", "start": 3485.28, "duration": 5.64}, {"text": "arrival here PGI ACC time equals one and", "start": 3486.599, "duration": 6.0}, {"text": "execute the code again we will see some", "start": 3490.92, "duration": 3.12}, {"text": "information about what the code is doing", "start": 3492.599, "duration": 2.701}, {"text": "and", "start": 3494.04, "duration": 4.559}, {"text": "um there is here something device time", "start": 3495.3, "duration": 5.88}, {"text": "data copy and transfers right and data", "start": 3498.599, "duration": 4.201}, {"text": "copy out transfer so we have like here", "start": 3501.18, "duration": 4.26}, {"text": "22.5 seconds just data transfers right", "start": 3502.8, "duration": 5.4}, {"text": "and then the compute is just 1.5 seconds", "start": 3505.44, "duration": 4.86}, {"text": "down here right so you see the device", "start": 3508.2, "duration": 3.6}, {"text": "time here for the kernel that's been", "start": 3510.3, "duration": 2.46}, {"text": "launched", "start": 3511.8, "duration": 3.9}, {"text": "and what we have done is we spent all", "start": 3512.76, "duration": 4.26}, {"text": "the time with data transfers between", "start": 3515.7, "duration": 2.94}, {"text": "host and device effectively and that's", "start": 3517.02, "duration": 3.779}, {"text": "where we need to use those data Clauses", "start": 3518.64, "duration": 4.74}, {"text": "right because what happens is these", "start": 3520.799, "duration": 5.04}, {"text": "excessive data transfers in our code we", "start": 3523.38, "duration": 5.459}, {"text": "told uh AC we told open access key to", "start": 3525.839, "duration": 4.561}, {"text": "generate these kernels and it did that", "start": 3528.839, "duration": 3.78}, {"text": "and it realized well I need the data on", "start": 3530.4, "duration": 5.58}, {"text": "the on the GPU in order to do these", "start": 3532.619, "duration": 5.281}, {"text": "operations on the GPU and at each", "start": 3535.98, "duration": 4.68}, {"text": "iteration of the loop it copies the data", "start": 3537.9, "duration": 4.919}, {"text": "from the CPU to the GPU and then back", "start": 3540.66, "duration": 4.26}, {"text": "from the GPU to the CQ and that is of", "start": 3542.819, "duration": 3.601}, {"text": "course not necessary", "start": 3544.92, "duration": 6.06}, {"text": "so what we do is we we add data Clauses", "start": 3546.42, "duration": 6.24}, {"text": "here and we're going to tell the code", "start": 3550.98, "duration": 7.2}, {"text": "that well copy data the the so a that is", "start": 3552.66, "duration": 8.58}, {"text": "the Matrix a through the GPU at the", "start": 3558.18, "duration": 4.619}, {"text": "beginning of the loop and it also", "start": 3561.24, "duration": 4.52}, {"text": "implicitly means back out at the end", "start": 3562.799, "duration": 5.82}, {"text": "create a new means allocate a new on the", "start": 3565.76, "duration": 5.44}, {"text": "accelerator and just um", "start": 3568.619, "duration": 4.921}, {"text": "do no copies for a new because that is", "start": 3571.2, "duration": 5.04}, {"text": "used only on the accelerator now and if", "start": 3573.54, "duration": 4.92}, {"text": "we do that now you see that now we get", "start": 3576.24, "duration": 4.26}, {"text": "the time down to five seconds right so", "start": 3578.46, "duration": 3.72}, {"text": "that's significantly faster that says", "start": 3580.5, "duration": 4.799}, {"text": "speed up or 4.8 versus the six open MP", "start": 3582.18, "duration": 4.32}, {"text": "threads so that's actually pretty", "start": 3585.299, "duration": 3.721}, {"text": "reasonable and I would say you know", "start": 3586.5, "duration": 4.02}, {"text": "um given the little work that was", "start": 3589.02, "duration": 3.299}, {"text": "involved is actually", "start": 3590.52, "duration": 2.339}, {"text": "um", "start": 3592.319, "duration": 5.54}, {"text": "um very nice um result", "start": 3592.859, "duration": 5.0}, {"text": "um there are more details about open ACC", "start": 3598.26, "duration": 5.7}, {"text": "um because we are running late I'm not", "start": 3601.319, "duration": 5.401}, {"text": "going to to talk much about that you can", "start": 3603.96, "duration": 4.02}, {"text": "have a look at that but there are", "start": 3606.72, "duration": 2.339}, {"text": "certain things that you have to be", "start": 3607.98, "duration": 2.639}, {"text": "careful with your code", "start": 3609.059, "duration": 3.0}, {"text": "um there are many pitfalls that you know", "start": 3610.619, "duration": 2.161}, {"text": "you", "start": 3612.059, "duration": 3.721}, {"text": "need to be aware of that while the code", "start": 3612.78, "duration": 4.819}, {"text": "is portable you know you must", "start": 3615.78, "duration": 4.5}, {"text": "may have to restructure your code for", "start": 3617.599, "duration": 4.121}, {"text": "instance functions", "start": 3620.28, "duration": 3.42}, {"text": "with an accelerated regions must be", "start": 3621.72, "duration": 5.52}, {"text": "inlineable and so on and so forth", "start": 3623.7, "duration": 4.379}, {"text": "um", "start": 3627.24, "duration": 2.76}, {"text": "what's a good idea is to use the time", "start": 3628.079, "duration": 3.48}, {"text": "option to learn where time is being", "start": 3630.0, "duration": 4.559}, {"text": "spent in your code and well you have to", "start": 3631.559, "duration": 4.921}, {"text": "eliminate pointer arithmetic otherwise", "start": 3634.559, "duration": 5.101}, {"text": "the code cannot be cannot automatically", "start": 3636.48, "duration": 5.639}, {"text": "generate kernels because it doesn't know", "start": 3639.66, "duration": 4.439}, {"text": "what you're doing in memory it's no", "start": 3642.119, "duration": 3.781}, {"text": "guarantee it doesn't have guarantees", "start": 3644.099, "duration": 2.641}, {"text": "about", "start": 3645.9, "duration": 4.08}, {"text": "um memory being immutable", "start": 3646.74, "duration": 5.579}, {"text": "um you should inline function calls", "start": 3649.98, "duration": 3.9}, {"text": "use contiguous memory for", "start": 3652.319, "duration": 3.24}, {"text": "multi-dimensional arrays that's the same", "start": 3653.88, "duration": 5.4}, {"text": "for Cuda and for um open ICC code", "start": 3655.559, "duration": 5.701}, {"text": "and then use data regions to avoid", "start": 3659.28, "duration": 4.019}, {"text": "excessive memory transfers and what you", "start": 3661.26, "duration": 3.299}, {"text": "can also do is use a conditional", "start": 3663.299, "duration": 3.121}, {"text": "computation with our underscore open ACC", "start": 3664.559, "duration": 3.601}, {"text": "macro depending whether you want to", "start": 3666.42, "duration": 4.74}, {"text": "generate open ACC code or not", "start": 3668.16, "duration": 5.04}, {"text": "so I know that I'm at the end of my time", "start": 3671.16, "duration": 6.54}, {"text": "but I will now if you I hope that", "start": 3673.2, "duration": 6.48}, {"text": "um I apologize for people who have to", "start": 3677.7, "duration": 3.419}, {"text": "leave", "start": 3679.68, "duration": 2.659}, {"text": "um", "start": 3681.119, "duration": 4.621}, {"text": "but it will be another maybe 10 minutes", "start": 3682.339, "duration": 5.861}, {"text": "approximately so if you if you have some", "start": 3685.74, "duration": 4.26}, {"text": "minutes some time to spare I would like", "start": 3688.2, "duration": 4.02}, {"text": "to tell you on how to access the GPU", "start": 3690.0, "duration": 5.22}, {"text": "notes um if you have to run I apologize", "start": 3692.22, "duration": 3.599}, {"text": "um", "start": 3695.22, "duration": 4.619}, {"text": "so just a quick overview we have 36", "start": 3695.819, "duration": 7.681}, {"text": "older GPU nodes that have k80 gpus a k80", "start": 3699.839, "duration": 7.201}, {"text": "GPU has internally two gpus so really", "start": 3703.5, "duration": 6.359}, {"text": "these are compute nodes with 4 gpus the", "start": 3707.04, "duration": 5.72}, {"text": "nodes are 2 times 12 core until Xeon", "start": 3709.859, "duration": 6.841}, {"text": "nodes that have these gpus attached and", "start": 3712.76, "duration": 6.46}, {"text": "um there's 12 gigabyte of ram per GPU", "start": 3716.7, "duration": 5.7}, {"text": "then we have a newer p100 GPU notes also", "start": 3719.22, "duration": 6.96}, {"text": "36 nodes um with four p100 gpus on each", "start": 3722.4, "duration": 5.76}, {"text": "node um", "start": 3726.18, "duration": 3.84}, {"text": "the configuration is very similar but", "start": 3728.16, "duration": 4.38}, {"text": "it's a more modern also more modern GPU", "start": 3730.02, "duration": 4.62}, {"text": "Broadwell CPU here with two times 14", "start": 3732.54, "duration": 5.039}, {"text": "cores the p100 gpus have a little bit of", "start": 3734.64, "duration": 5.1}, {"text": "more memory they have 16 gigabyte of ram", "start": 3737.579, "duration": 3.661}, {"text": "per GPU and they're of course more", "start": 3739.74, "duration": 3.9}, {"text": "efficient so also when you use those you", "start": 3741.24, "duration": 4.319}, {"text": "get charged more as used because I think", "start": 3743.64, "duration": 3.78}, {"text": "it's a factor of 1.5 because the", "start": 3745.559, "duration": 3.361}, {"text": "performance is higher actually typically", "start": 3747.42, "duration": 3.12}, {"text": "you get a factor of two out of many", "start": 3748.92, "duration": 3.48}, {"text": "codes", "start": 3750.54, "duration": 3.779}, {"text": "um so it could be um beneficial to", "start": 3752.4, "duration": 3.54}, {"text": "actually use art also if you want to", "start": 3754.319, "duration": 3.78}, {"text": "know more there is a user guide of", "start": 3755.94, "duration": 3.84}, {"text": "course with all the information on the", "start": 3758.099, "duration": 4.141}, {"text": "sdsc website", "start": 3759.78, "duration": 4.62}, {"text": "um it's very simple if you know how to", "start": 3762.24, "duration": 4.619}, {"text": "log into Comet you just log into Comet", "start": 3764.4, "duration": 5.34}, {"text": "and if you type Q step minus Q you see", "start": 3766.859, "duration": 5.46}, {"text": "the available queues there are two cues", "start": 3769.74, "duration": 6.599}, {"text": "for gpus one is called GPU that's a", "start": 3772.319, "duration": 6.0}, {"text": "queue where you would get", "start": 3776.339, "duration": 4.201}, {"text": "um entire GPU node with all the four", "start": 3778.319, "duration": 4.321}, {"text": "gpus and there's a GPU shared queue", "start": 3780.54, "duration": 4.019}, {"text": "where you can request individual gpus", "start": 3782.64, "duration": 3.84}, {"text": "you can say I want to use only a single", "start": 3784.559, "duration": 3.56}, {"text": "GPU or two gpus", "start": 3786.48, "duration": 4.02}, {"text": "many codes will not run in parallel", "start": 3788.119, "duration": 4.841}, {"text": "across multiple gpus so you often will", "start": 3790.5, "duration": 5.64}, {"text": "just request a single GPU and do some", "start": 3792.96, "duration": 4.74}, {"text": "sort of like Ensemble simulations for", "start": 3796.14, "duration": 3.06}, {"text": "instance that we do molecular dynamics", "start": 3797.7, "duration": 3.0}, {"text": "that we run many molecular Dynamic", "start": 3799.2, "duration": 3.3}, {"text": "simulations independently on individual", "start": 3800.7, "duration": 4.02}, {"text": "gpus", "start": 3802.5, "duration": 4.799}, {"text": "um so the GPU nodes can be accessed by", "start": 3804.72, "duration": 4.379}, {"text": "either the GPU or the GPU shared", "start": 3807.299, "duration": 3.961}, {"text": "partitions so if you have a drop", "start": 3809.099, "duration": 4.681}, {"text": "submission script you would have S batch", "start": 3811.26, "duration": 7.5}, {"text": "command minus P for partition GPU or GPU", "start": 3813.78, "duration": 7.559}, {"text": "shared in addition to the partition name", "start": 3818.76, "duration": 5.039}, {"text": "which is required", "start": 3821.339, "duration": 4.5}, {"text": "um you can also specify the type of GPU", "start": 3823.799, "duration": 4.081}, {"text": "that is optional", "start": 3825.839, "duration": 3.26}, {"text": "um", "start": 3827.88, "duration": 4.919}, {"text": "and uh the type of GPU and you can also", "start": 3829.099, "duration": 6.041}, {"text": "individual gpus are scheduled as a", "start": 3832.799, "duration": 3.421}, {"text": "resource", "start": 3835.14, "duration": 5.04}, {"text": "um so you can have this Dash so minus", "start": 3836.22, "duration": 5.339}, {"text": "minus G res", "start": 3840.18, "duration": 3.84}, {"text": "equals GPU and then you can specify the", "start": 3841.559, "duration": 4.8}, {"text": "type and the numbers", "start": 3844.02, "duration": 4.02}, {"text": "so the gpus will be allocated in the", "start": 3846.359, "duration": 3.48}, {"text": "first available for a scheduled basis", "start": 3848.04, "duration": 3.779}, {"text": "unless you specified with the type", "start": 3849.839, "duration": 3.661}, {"text": "option so if you don't say which type", "start": 3851.819, "duration": 3.721}, {"text": "you're using for instance here G rest", "start": 3853.5, "duration": 4.799}, {"text": "equals GPU colon 4 you will get the", "start": 3855.54, "duration": 4.62}, {"text": "first available GPU node and that can be", "start": 3858.299, "duration": 4.32}, {"text": "either k80 or p100 note if you want to", "start": 3860.16, "duration": 4.56}, {"text": "make sure that you get a k80 note you", "start": 3862.619, "duration": 5.761}, {"text": "have to tell the S patch command or if", "start": 3864.72, "duration": 5.639}, {"text": "you want to have a p100 model in this", "start": 3868.38, "duration": 3.479}, {"text": "case you know you would request four GPS", "start": 3870.359, "duration": 5.0}, {"text": "this example here", "start": 3871.859, "duration": 3.5}, {"text": "um yeah so this is the example when I", "start": 3876.059, "duration": 6.141}, {"text": "get the GPU partition and the p100 uh", "start": 3878.04, "duration": 8.039}, {"text": "4p100 gpus yeah so posted a link to the", "start": 3882.2, "duration": 6.84}, {"text": "introduction to learning jobs", "start": 3886.079, "duration": 5.821}, {"text": "yeah so there have been GPU examples", "start": 3889.04, "duration": 4.539}, {"text": "before also in some of the previous", "start": 3891.9, "duration": 3.36}, {"text": "webinars", "start": 3893.579, "duration": 4.26}, {"text": "on how to run drops and comment", "start": 3895.26, "duration": 5.16}, {"text": "um if one thing is that you should also", "start": 3897.839, "duration": 4.681}, {"text": "know and it's all documented but I'll", "start": 3900.42, "duration": 4.439}, {"text": "repeat it here you should always set if", "start": 3902.52, "duration": 6.24}, {"text": "you if you use a shared GPU um node you", "start": 3904.859, "duration": 5.881}, {"text": "should set the tasks per node so the", "start": 3908.76, "duration": 3.72}, {"text": "number of CPUs that you get promote", "start": 3910.74, "duration": 4.8}, {"text": "equal to six times the number of gpus if", "start": 3912.48, "duration": 5.639}, {"text": "it is a k80 node or seven times the", "start": 3915.54, "duration": 4.74}, {"text": "number of gpus if it is a p100 note", "start": 3918.119, "duration": 4.921}, {"text": "because for each GPU there are six", "start": 3920.28, "duration": 6.12}, {"text": "respectively seven CPU cores right so", "start": 3923.04, "duration": 5.759}, {"text": "here's an example on a GPU shared queue", "start": 3926.4, "duration": 5.88}, {"text": "you would choose 14 tasks when you", "start": 3928.799, "duration": 4.441}, {"text": "request", "start": 3932.28, "duration": 4.5}, {"text": "um two p100 gpus there are also example", "start": 3933.24, "duration": 5.339}, {"text": "drop submission scripts and share apps", "start": 3936.78, "duration": 4.799}, {"text": "examples GPU and the charging is", "start": 3938.579, "duration": 6.301}, {"text": "basically one Su for each k80 GPU hour", "start": 3941.579, "duration": 9.301}, {"text": "and 1.5 is used for each p100 GPU hours", "start": 3944.88, "duration": 6.78}, {"text": "um", "start": 3950.88, "duration": 3.12}, {"text": "if you want to use Cuda you have to load", "start": 3951.66, "duration": 4.26}, {"text": "the Cuda module so you just say module", "start": 3954.0, "duration": 3.72}, {"text": "log tutor that's it and then the", "start": 3955.92, "duration": 3.3}, {"text": "compiler will be available so if you", "start": 3957.72, "duration": 3.839}, {"text": "type nvcc minus minus version you will", "start": 3959.22, "duration": 5.339}, {"text": "get information about the Cuda compiler", "start": 3961.559, "duration": 4.5}, {"text": "um there are different versions", "start": 3964.559, "duration": 3.54}, {"text": "available so by default we have queue to", "start": 3966.059, "duration": 4.26}, {"text": "7.0 but there's also newer versions if", "start": 3968.099, "duration": 4.98}, {"text": "you require that for your code", "start": 3970.319, "duration": 6.0}, {"text": "the PGI module loads the pgic and", "start": 3973.079, "duration": 5.22}, {"text": "Fortran compilers so for instance", "start": 3976.319, "duration": 3.961}, {"text": "modular PGI and then here you could", "start": 3978.299, "duration": 4.621}, {"text": "check the pgcc that version is going to", "start": 3980.28, "duration": 4.62}, {"text": "tell you it should spit out the version", "start": 3982.92, "duration": 4.02}, {"text": "of the PGI compiler that we have", "start": 3984.9, "duration": 3.84}, {"text": "installed", "start": 3986.94, "duration": 3.119}, {"text": "um", "start": 3988.74, "duration": 3.78}, {"text": "here's an example how you could get an", "start": 3990.059, "duration": 4.921}, {"text": "interactive access to GPU nodes that's", "start": 3992.52, "duration": 4.44}, {"text": "basically using exactly the S bash", "start": 3994.98, "duration": 4.099}, {"text": "commands that I have", "start": 3996.96, "duration": 4.8}, {"text": "just been going over before this is for", "start": 3999.079, "duration": 4.841}, {"text": "interactive access", "start": 4001.76, "duration": 3.72}, {"text": "um very often it's hard to get", "start": 4003.92, "duration": 3.3}, {"text": "interactive access on the GPU nodes", "start": 4005.48, "duration": 3.599}, {"text": "Because unless you have a specific", "start": 4007.22, "duration": 3.899}, {"text": "reservation that you have requested", "start": 4009.079, "duration": 3.78}, {"text": "because", "start": 4011.119, "duration": 2.401}, {"text": "um", "start": 4012.859, "duration": 2.401}, {"text": "there are just too many people running", "start": 4013.52, "duration": 3.9}, {"text": "drops on their gpus and we don't have a", "start": 4015.26, "duration": 4.079}, {"text": "development cues for for gpus because", "start": 4017.42, "duration": 3.8}, {"text": "there's so much in demand", "start": 4019.339, "duration": 4.2}, {"text": "you can use the Nvidia system management", "start": 4021.22, "duration": 4.24}, {"text": "and interface to get information about", "start": 4023.539, "duration": 4.861}, {"text": "the GPU so if you type Nvidia SMR you", "start": 4025.46, "duration": 4.56}, {"text": "will get information about the gpus that", "start": 4028.4, "duration": 3.36}, {"text": "are installed here for instance and", "start": 4030.02, "duration": 4.14}, {"text": "output to tell CC's um you know you have", "start": 4031.76, "duration": 5.339}, {"text": "Tesla p100 gpus available so there", "start": 4034.16, "duration": 5.28}, {"text": "should be four showing up", "start": 4037.099, "duration": 4.381}, {"text": "um other jobs may already be running on", "start": 4039.44, "duration": 4.08}, {"text": "the shared GPU nodes so here's an", "start": 4041.48, "duration": 4.8}, {"text": "example where you know Nvidia SMI gives", "start": 4043.52, "duration": 4.62}, {"text": "an output and says the GPU one two and", "start": 4046.28, "duration": 3.839}, {"text": "three are running these jobs here which", "start": 4048.14, "duration": 3.6}, {"text": "in this case is a GPU accelerated", "start": 4050.119, "duration": 3.18}, {"text": "version of a molecular Dynamics program", "start": 4051.74, "duration": 4.44}, {"text": "called pnn pmem need", "start": 4053.299, "duration": 4.861}, {"text": "that notes of the sharep gpus they are", "start": 4056.18, "duration": 4.02}, {"text": "configured in such a way that the queue", "start": 4058.16, "duration": 4.26}, {"text": "the runtime only uses the requested", "start": 4060.2, "duration": 4.379}, {"text": "number of gpus so you don't have to", "start": 4062.42, "duration": 4.379}, {"text": "worry about doing that so there's an", "start": 4064.579, "duration": 3.78}, {"text": "environment variable that actually gets", "start": 4066.799, "duration": 3.361}, {"text": "set it's called cuda visible devices and", "start": 4068.359, "duration": 4.5}, {"text": "that should be set to the GPU that is", "start": 4070.16, "duration": 4.5}, {"text": "assigned to you and you should not", "start": 4072.859, "duration": 4.021}, {"text": "change that but you can actually check", "start": 4074.66, "duration": 3.72}, {"text": "it", "start": 4076.88, "duration": 5.64}, {"text": "um in terms of Cuda programming examples", "start": 4078.38, "duration": 5.939}, {"text": "um the Cuda toolkit as I was mentioning", "start": 4082.52, "duration": 4.26}, {"text": "earlier comes with Cuda code samples so", "start": 4084.319, "duration": 4.381}, {"text": "you can install these right so you just", "start": 4086.78, "duration": 3.9}, {"text": "execute this command Cuda install", "start": 4088.7, "duration": 6.659}, {"text": "samples in this case minus 7.0.sh", "start": 4090.68, "duration": 6.72}, {"text": "and the directory where you want to have", "start": 4095.359, "duration": 4.5}, {"text": "that and then it copies the code samples", "start": 4097.4, "duration": 3.66}, {"text": "into this directory in your home", "start": 4099.859, "duration": 2.701}, {"text": "directory for instance and then you can", "start": 4101.06, "duration": 3.6}, {"text": "explore these right so if I would move", "start": 4102.56, "duration": 4.4}, {"text": "them into this directory CD and video", "start": 4104.66, "duration": 6.3}, {"text": "q.7.0 samples and do an LS I will see", "start": 4106.96, "duration": 5.739}, {"text": "like different subdirectories about like", "start": 4110.96, "duration": 3.6}, {"text": "simple examples there are some utilities", "start": 4112.699, "duration": 3.901}, {"text": "there are examples for graphics Imaging", "start": 4114.56, "duration": 5.279}, {"text": "and so on and advanced examples and", "start": 4116.6, "duration": 4.86}, {"text": "there's a make file so you can compile", "start": 4119.839, "duration": 3.0}, {"text": "all of these", "start": 4121.46, "duration": 5.7}, {"text": "just by typing make and then execute", "start": 4122.839, "duration": 6.48}, {"text": "those so the calculation takes a while", "start": 4127.16, "duration": 4.98}, {"text": "and the executables will be in a", "start": 4129.319, "duration": 6.92}, {"text": "subdirectory pin x8664 Linux release", "start": 4132.14, "duration": 6.3}, {"text": "for instance there's a tool that's", "start": 4136.239, "duration": 4.301}, {"text": "called device query", "start": 4138.44, "duration": 3.6}, {"text": "um if I go back on the previous slide", "start": 4140.54, "duration": 4.139}, {"text": "it's in this directory called uh one", "start": 4142.04, "duration": 4.56}, {"text": "underscore utilities I believe so you", "start": 4144.679, "duration": 3.841}, {"text": "could also go directly there and compile", "start": 4146.6, "duration": 2.759}, {"text": "that", "start": 4148.52, "duration": 1.56}, {"text": "um", "start": 4149.359, "duration": 4.081}, {"text": "and then if you execute this code and", "start": 4150.08, "duration": 4.739}, {"text": "you can should have a look at it because", "start": 4153.44, "duration": 3.06}, {"text": "that's actually", "start": 4154.819, "duration": 4.5}, {"text": "um Cuda code that interrogates the GPU", "start": 4156.5, "duration": 6.12}, {"text": "about features of the GPU", "start": 4159.319, "duration": 5.341}, {"text": "um that in a way that you can also use", "start": 4162.62, "duration": 4.199}, {"text": "in your own program so if you execute", "start": 4164.66, "duration": 4.92}, {"text": "this device query executable it checks", "start": 4166.819, "duration": 5.4}, {"text": "actually what capabilities the gpus have", "start": 4169.58, "duration": 4.619}, {"text": "that you currently have available and in", "start": 4172.219, "duration": 3.721}, {"text": "this case example here the output that", "start": 4174.199, "duration": 3.0}, {"text": "you see", "start": 4175.94, "duration": 3.779}, {"text": "means you know it found one Cuda capable", "start": 4177.199, "duration": 4.08}, {"text": "device which means I was on a shared", "start": 4179.719, "duration": 3.241}, {"text": "queue with a single and requesting a", "start": 4181.279, "duration": 5.701}, {"text": "single GPU and it found a Tesla k80 and", "start": 4182.96, "duration": 5.64}, {"text": "it's talking about the driver version", "start": 4186.98, "duration": 4.56}, {"text": "the runtime version the Cuda capability", "start": 4188.6, "duration": 4.86}, {"text": "the memory that's available the number", "start": 4191.54, "duration": 4.8}, {"text": "of GPU Cuda cores and so on and so forth", "start": 4193.46, "duration": 5.04}, {"text": "so all sorts of information", "start": 4196.34, "duration": 4.8}, {"text": "that you could use for your um", "start": 4198.5, "duration": 4.92}, {"text": "that might be useful for program", "start": 4201.14, "duration": 4.14}, {"text": "execution and setting up on how to", "start": 4203.42, "duration": 4.279}, {"text": "execute and launch kernels", "start": 4205.28, "duration": 5.6}, {"text": "configuration at launch time", "start": 4207.699, "duration": 5.621}, {"text": "there are many other examples so for", "start": 4210.88, "duration": 3.76}, {"text": "instance the matrix multiplication", "start": 4213.32, "duration": 2.899}, {"text": "example you can have a look at that", "start": 4214.64, "duration": 5.039}, {"text": "that's in the also a simple example zero", "start": 4216.219, "duration": 5.561}, {"text": "underscore simple of the Cuda toolkit", "start": 4219.679, "duration": 4.321}, {"text": "examples", "start": 4221.78, "duration": 5.7}, {"text": "um and there is a basically hand-coded", "start": 4224.0, "duration": 6.179}, {"text": "optimized shared memory", "start": 4227.48, "duration": 4.62}, {"text": "um matrix multiplication example you can", "start": 4230.179, "duration": 3.841}, {"text": "have a look at that to learn how how", "start": 4232.1, "duration": 4.619}, {"text": "that is programmed in addition to the", "start": 4234.02, "duration": 4.139}, {"text": "very simplified version that I have here", "start": 4236.719, "duration": 3.48}, {"text": "on the slides and you get a performance", "start": 4238.159, "duration": 4.321}, {"text": "here for instance that's running on a", "start": 4240.199, "duration": 5.46}, {"text": "Tesla k80 of 230 gigaflops that's much", "start": 4242.48, "duration": 5.759}, {"text": "less than the Peak Performance right", "start": 4245.659, "duration": 4.861}, {"text": "so then if you use Cube loss there's an", "start": 4248.239, "duration": 4.261}, {"text": "example on how to use Q Plus", "start": 4250.52, "duration": 4.44}, {"text": "now if you look here I'm executing the", "start": 4252.5, "duration": 4.38}, {"text": "same", "start": 4254.96, "duration": 5.1}, {"text": "it's a I think yeah it's it's actually a", "start": 4256.88, "duration": 4.74}, {"text": "slightly different Matrix size so it's", "start": 4260.06, "duration": 5.34}, {"text": "not 100 fair comparison but um", "start": 4261.62, "duration": 5.22}, {"text": "uh if you look at that you get a", "start": 4265.4, "duration": 3.42}, {"text": "performance of nine almost a teraflop", "start": 4266.84, "duration": 3.18}, {"text": "right so", "start": 4268.82, "duration": 4.859}, {"text": "yeah use the libraries if you can and um", "start": 4270.02, "duration": 5.88}, {"text": "that's the end so I could show you", "start": 4273.679, "duration": 4.801}, {"text": "interactively um how to access the GPU", "start": 4275.9, "duration": 4.98}, {"text": "nodes and look at the Cuda toolkit and", "start": 4278.48, "duration": 5.58}, {"text": "compile sample open ACC code exactly", "start": 4280.88, "duration": 6.12}, {"text": "that LaPlace example so I have the code", "start": 4284.06, "duration": 6.84}, {"text": "available we can post that on the", "start": 4287.0, "duration": 6.78}, {"text": "um I I believe on the webinar the", "start": 4290.9, "duration": 5.58}, {"text": "webinar Pages where the intro is but it", "start": 4293.78, "duration": 4.62}, {"text": "all takes the challenge yeah so so it", "start": 4296.48, "duration": 3.84}, {"text": "will take some time but um we'll have", "start": 4298.4, "duration": 4.56}, {"text": "that information available so I'll stick", "start": 4300.32, "duration": 3.899}, {"text": "around for a while if you have any", "start": 4302.96, "duration": 3.0}, {"text": "questions I apologize for taking longer", "start": 4304.219, "duration": 7.161}, {"text": "than originally planned but um I hope", "start": 4305.96, "duration": 5.42}, {"text": "so if anybody wants to hang around I'll", "start": 4312.159, "duration": 3.401}, {"text": "stick around for a few minutes", "start": 4314.6, "duration": 2.7}, {"text": "interactively if you have any uh", "start": 4315.56, "duration": 2.58}, {"text": "questions", "start": 4317.3, "duration": 3.5}, {"text": "thank you", "start": 4318.14, "duration": 2.66}, {"text": "aquarium we'll just see who hangs around", "start": 4322.42, "duration": 4.92}, {"text": "hi Andreas this is Dimitri", "start": 4331.219, "duration": 5.581}, {"text": "yes hi uh I have a question", "start": 4333.92, "duration": 5.46}, {"text": "is it common that uh how common is it", "start": 4336.8, "duration": 6.12}, {"text": "that uh people use gpus", "start": 4339.38, "duration": 6.48}, {"text": "one piece of code on gpus while at the", "start": 4342.92, "duration": 5.34}, {"text": "same time running something in CPU kind", "start": 4345.86, "duration": 3.74}, {"text": "of overlapping", "start": 4348.26, "duration": 4.5}, {"text": "this is actually a very common scenario", "start": 4349.6, "duration": 6.639}, {"text": "um it requires a lot of load balancing", "start": 4352.76, "duration": 5.82}, {"text": "and um but also for instance you know", "start": 4356.239, "duration": 4.381}, {"text": "the molecular Dynamics course there are", "start": 4358.58, "duration": 4.32}, {"text": "different approaches", "start": 4360.62, "duration": 3.599}, {"text": "um you know that's something I can speak", "start": 4362.9, "duration": 5.18}, {"text": "of from my own experience the Amber code", "start": 4364.219, "duration": 7.141}, {"text": "executes entirely on the GPU right so", "start": 4368.08, "duration": 4.9}, {"text": "everything has been Rewritten for high", "start": 4371.36, "duration": 3.42}, {"text": "performance on the GPU and the CPU", "start": 4372.98, "duration": 3.96}, {"text": "literally only does IO right so you can", "start": 4374.78, "duration": 5.58}, {"text": "attach a GPU to a very cheap CPU and the", "start": 4376.94, "duration": 5.1}, {"text": "performance doesn't change", "start": 4380.36, "duration": 5.359}, {"text": "other codes like uh namdi or", "start": 4382.04, "duration": 6.06}, {"text": "Romex you know they have highly", "start": 4385.719, "duration": 5.081}, {"text": "optimized kernels for CPU and for GPU", "start": 4388.1, "duration": 5.34}, {"text": "and they do part of the operations on", "start": 4390.8, "duration": 6.84}, {"text": "the GPU and so once a kernel is launched", "start": 4393.44, "duration": 7.259}, {"text": "um you know the CPU doesn't have to wait", "start": 4397.64, "duration": 5.4}, {"text": "until the kernel finishes right it can", "start": 4400.699, "duration": 4.741}, {"text": "do work at the same time openmp threads", "start": 4403.04, "duration": 5.159}, {"text": "for that you can use openmp threads so", "start": 4405.44, "duration": 4.739}, {"text": "you can use a threaded openmp a code", "start": 4408.199, "duration": 4.081}, {"text": "that runs at the same time as as the GPU", "start": 4410.179, "duration": 4.201}, {"text": "code you just have to be careful you", "start": 4412.28, "duration": 3.54}, {"text": "know there are commands for synchronized", "start": 4414.38, "duration": 3.72}, {"text": "synchronization so that um you know you", "start": 4415.82, "duration": 3.899}, {"text": "don't have any race conditions in your", "start": 4418.1, "duration": 4.32}, {"text": "course and using this such overlap can", "start": 4419.719, "duration": 4.701}, {"text": "be efficient or", "start": 4422.42, "duration": 4.739}, {"text": "it can be very efficient yeah so you can", "start": 4424.42, "duration": 4.96}, {"text": "actually hide um you know memory", "start": 4427.159, "duration": 5.161}, {"text": "transfer latencies with that", "start": 4429.38, "duration": 5.819}, {"text": "etc etc so it can be very efficient but", "start": 4432.32, "duration": 4.68}, {"text": "the one thing you have to know is that", "start": 4435.199, "duration": 3.781}, {"text": "what you have to balance is of course in", "start": 4437.0, "duration": 3.42}, {"text": "your Hardware configuration the", "start": 4438.98, "duration": 2.699}, {"text": "performance of the GPU and the", "start": 4440.42, "duration": 3.9}, {"text": "performance of the CPU right so", "start": 4441.679, "duration": 5.221}, {"text": "if you execute part of your code in two", "start": 4444.32, "duration": 4.5}, {"text": "different types of Hardware you and you", "start": 4446.9, "duration": 3.779}, {"text": "want to use those to the full extent you", "start": 4448.82, "duration": 3.12}, {"text": "have to make sure that you have a", "start": 4450.679, "duration": 4.441}, {"text": "balanced balanced hardware setup", "start": 4451.94, "duration": 6.44}, {"text": "okay thank you", "start": 4455.12, "duration": 3.26}, {"text": "we'll just wait a couple minutes then", "start": 4477.739, "duration": 4.021}, {"text": "yeah", "start": 4480.62, "duration": 3.36}, {"text": "whenever you can set up and get started", "start": 4481.76, "duration": 4.32}, {"text": "for that this is great because then we", "start": 4483.98, "duration": 5.42}, {"text": "can take that part of the video", "start": 4486.08, "duration": 3.32}, {"text": "foreign", "start": 4552.92, "duration": 3.0}, {"text": "people have some interest I think in a", "start": 4562.659, "duration": 7.121}, {"text": "quick demo um yes", "start": 4565.76, "duration": 6.18}, {"text": "so", "start": 4569.78, "duration": 7.22}, {"text": "let's have a quick look here this is", "start": 4571.94, "duration": 5.06}, {"text": "uh yes I'm live", "start": 4577.48, "duration": 5.62}, {"text": "um let me see if I can", "start": 4579.8, "duration": 6.96}, {"text": "can people see this but no yes okay so", "start": 4583.1, "duration": 6.119}, {"text": "what do I have open here", "start": 4586.76, "duration": 4.22}, {"text": "hold up", "start": 4589.219, "duration": 4.98}, {"text": "I uh my shell is hidden under this how", "start": 4590.98, "duration": 5.92}, {"text": "do I get move this oh just move the", "start": 4594.199, "duration": 4.921}, {"text": "cursor down and it'll go away in just a", "start": 4596.9, "duration": 4.14}, {"text": "second it should no it's been hanging", "start": 4599.12, "duration": 5.46}, {"text": "around there all the time oh it has uh", "start": 4601.04, "duration": 7.56}, {"text": "how can I move this oh okay sorry", "start": 4604.58, "duration": 5.639}, {"text": "all right", "start": 4608.6, "duration": 2.7}, {"text": "yeah", "start": 4610.219, "duration": 3.721}, {"text": "so let's make this a little bit bigger", "start": 4611.3, "duration": 4.859}, {"text": "okay here's an example where I have", "start": 4613.94, "duration": 3.779}, {"text": "logged into", "start": 4616.159, "duration": 6.301}, {"text": "Comet already and I have access the GPU", "start": 4617.719, "duration": 6.841}, {"text": "node and let me just show you so I", "start": 4622.46, "duration": 4.86}, {"text": "executed the script here let me show you", "start": 4624.56, "duration": 4.98}, {"text": "what's in that script so that really is", "start": 4627.32, "duration": 4.26}, {"text": "just an s-ron command", "start": 4629.54, "duration": 4.86}, {"text": "where I had a reservation in place for", "start": 4631.58, "duration": 3.78}, {"text": "this", "start": 4634.4, "duration": 1.56}, {"text": "um", "start": 4635.36, "duration": 2.879}, {"text": "webinar the reservation under this name", "start": 4635.96, "duration": 3.719}, {"text": "so that's something that you typically", "start": 4638.239, "duration": 3.48}, {"text": "don't have", "start": 4639.679, "duration": 5.461}, {"text": "um so I requested a GPU partition so", "start": 4641.719, "duration": 6.781}, {"text": "a node on the GPU partition a single", "start": 4645.14, "duration": 6.96}, {"text": "node with a 28 tasks per node so the", "start": 4648.5, "duration": 9.92}, {"text": "entire node and I requested a GPU um", "start": 4652.1, "duration": 6.32}, {"text": "p100 GPU and I requested four of them", "start": 4658.52, "duration": 6.9}, {"text": "right so in this case time two hours I", "start": 4662.78, "duration": 5.28}, {"text": "guess I did that at", "start": 4665.42, "duration": 5.759}, {"text": "um let's see at 11 around 11 so there", "start": 4668.06, "duration": 4.98}, {"text": "should be enough time", "start": 4671.179, "duration": 3.54}, {"text": "available", "start": 4673.04, "duration": 3.72}, {"text": "actually also if I'm looking at the job", "start": 4674.719, "duration": 3.061}, {"text": "but still running it's been running", "start": 4676.76, "duration": 2.7}, {"text": "there in the background for one hour and", "start": 4677.78, "duration": 4.7}, {"text": "21 minutes", "start": 4679.46, "duration": 3.02}, {"text": "um okay", "start": 4682.52, "duration": 4.139}, {"text": "so let's", "start": 4684.14, "duration": 6.2}, {"text": "question about what what how you can see", "start": 4686.659, "duration": 3.681}, {"text": "here so let's let's just keep going", "start": 4693.5, "duration": 4.679}, {"text": "quickly here um the modules that I have", "start": 4695.6, "duration": 3.84}, {"text": "loaded these are just the default", "start": 4698.179, "duration": 2.641}, {"text": "modules loaded", "start": 4699.44, "duration": 3.66}, {"text": "um when I log into Comet", "start": 4700.82, "duration": 4.62}, {"text": "um if I want to use Cura", "start": 4703.1, "duration": 5.099}, {"text": "um I have to say module load Cuda of", "start": 4705.44, "duration": 4.14}, {"text": "course I can", "start": 4708.199, "duration": 3.181}, {"text": "look at which other modules that are", "start": 4709.58, "duration": 4.639}, {"text": "available and", "start": 4711.38, "duration": 2.839}, {"text": "so here are the Kudo modules so the", "start": 4714.32, "duration": 6.24}, {"text": "default is 7.0 or if you want other", "start": 4717.44, "duration": 4.739}, {"text": "versions you can load the specific", "start": 4720.56, "duration": 3.599}, {"text": "versions and let's just", "start": 4722.179, "duration": 7.201}, {"text": "uh load the default version module.cuda", "start": 4724.159, "duration": 6.901}, {"text": "it's available now", "start": 4729.38, "duration": 2.52}, {"text": "um", "start": 4731.06, "duration": 4.679}, {"text": "and if I type Nvidia SMI", "start": 4731.9, "duration": 6.12}, {"text": "that's exactly what you see here are the", "start": 4735.739, "duration": 5.581}, {"text": "four Tesla p100 gpus", "start": 4738.02, "duration": 5.42}, {"text": "um", "start": 4741.32, "duration": 2.12}, {"text": "visible devices actually set to zero one", "start": 4743.739, "duration": 6.221}, {"text": "two and three so all the four gpus are", "start": 4747.739, "duration": 3.301}, {"text": "available to me if I would have", "start": 4749.96, "duration": 4.08}, {"text": "requested a shared compute node um with", "start": 4751.04, "duration": 6.24}, {"text": "only single GPU then you know only one", "start": 4754.04, "duration": 5.76}, {"text": "of the devices would be visible to me um", "start": 4757.28, "duration": 4.98}, {"text": "to the codes that execute", "start": 4759.8, "duration": 5.16}, {"text": "on the gpus um", "start": 4762.26, "duration": 5.3}, {"text": "so", "start": 4764.96, "duration": 2.6}, {"text": "let's have a look what we have here I", "start": 4767.96, "duration": 4.199}, {"text": "have the open ACC examples here", "start": 4769.82, "duration": 4.5}, {"text": "um the", "start": 4772.159, "duration": 4.941}, {"text": "install", "start": 4774.32, "duration": 2.78}, {"text": "of course when you want to show", "start": 4781.82, "duration": 4.56}, {"text": "something it doesn't work", "start": 4783.8, "duration": 5.58}, {"text": "I think there should be a command for", "start": 4786.38, "duration": 5.96}, {"text": "installing the computer", "start": 4789.38, "duration": 2.96}, {"text": "almost here", "start": 4795.02, "duration": 3.62}, {"text": "computer home shows you where the Cuda", "start": 4798.739, "duration": 4.92}, {"text": "installation is", "start": 4802.159, "duration": 3.241}, {"text": "um", "start": 4803.659, "duration": 4.5}, {"text": "have a look there", "start": 4805.4, "duration": 5.16}, {"text": "um there is a pin directory there should", "start": 4808.159, "duration": 4.621}, {"text": "be oh it's called cuda install so it's", "start": 4810.56, "duration": 3.36}, {"text": "okay", "start": 4812.78, "duration": 3.0}, {"text": "so the command was not installed to the", "start": 4813.92, "duration": 4.14}, {"text": "stumbles is Cuda install samples so if I", "start": 4815.78, "duration": 4.74}, {"text": "execute the Cuda", "start": 4818.06, "duration": 5.52}, {"text": "there's install sample 7.0", "start": 4820.52, "duration": 4.56}, {"text": "um", "start": 4823.58, "duration": 4.44}, {"text": "the current directory is going to copy", "start": 4825.08, "duration": 4.5}, {"text": "the samples for instance to the current", "start": 4828.02, "duration": 4.1}, {"text": "directory", "start": 4829.58, "duration": 2.54}, {"text": "now I have the Nvidia Cuda samples here", "start": 4836.179, "duration": 3.421}, {"text": "and that's what I was showing you", "start": 4838.28, "duration": 2.52}, {"text": "earlier", "start": 4839.6, "duration": 3.0}, {"text": "and if I go into this directory there", "start": 4840.8, "duration": 4.98}, {"text": "are different examples here", "start": 4842.6, "duration": 5.28}, {"text": "you can compile those just by tapping", "start": 4845.78, "duration": 4.379}, {"text": "make here or you can also compile", "start": 4847.88, "duration": 4.56}, {"text": "individual examples um that's what we're", "start": 4850.159, "duration": 4.441}, {"text": "going to do now for instance", "start": 4852.44, "duration": 3.84}, {"text": "I went into this directory one", "start": 4854.6, "duration": 4.139}, {"text": "underscore utilities and it has", "start": 4856.28, "duration": 4.02}, {"text": "different things like a bandwidth test", "start": 4858.739, "duration": 3.601}, {"text": "and a device query for instance so let's", "start": 4860.3, "duration": 4.14}, {"text": "have a look at this", "start": 4862.34, "duration": 5.22}, {"text": "device queries as a make file and", "start": 4864.44, "duration": 6.84}, {"text": "there's the C plus plus source code", "start": 4867.56, "duration": 5.52}, {"text": "we have a quick look at the C plus", "start": 4871.28, "duration": 4.08}, {"text": "source code so there's a lot of like", "start": 4873.08, "duration": 5.099}, {"text": "boilerplate typically um code in there", "start": 4875.36, "duration": 5.0}, {"text": "by", "start": 4878.179, "duration": 6.56}, {"text": "Nvidia to catch errors and so on but", "start": 4880.36, "duration": 6.879}, {"text": "here it has a function that a student", "start": 4884.739, "duration": 4.661}, {"text": "get attribute and", "start": 4887.239, "duration": 5.221}, {"text": "so let's see what it's doing", "start": 4889.4, "duration": 5.12}, {"text": "um", "start": 4892.46, "duration": 2.06}, {"text": "it basically costs a q device get", "start": 4896.48, "duration": 5.58}, {"text": "attribute function here um", "start": 4899.06, "duration": 5.159}, {"text": "and here's the main program", "start": 4902.06, "duration": 3.96}, {"text": "so it's going to print some information", "start": 4904.219, "duration": 4.52}, {"text": "about the um", "start": 4906.02, "duration": 7.02}, {"text": "the device query is running and", "start": 4908.739, "duration": 6.641}, {"text": "about the gpus that are going that are", "start": 4913.04, "duration": 4.199}, {"text": "in the system um so here you know", "start": 4915.38, "duration": 3.54}, {"text": "there's a Cuda device this is part of", "start": 4917.239, "duration": 3.42}, {"text": "the Huda language the queue the device", "start": 4918.92, "duration": 3.96}, {"text": "prop uh data type", "start": 4920.659, "duration": 3.841}, {"text": "and you can just call the variable", "start": 4922.88, "duration": 4.319}, {"text": "device prop and then it causes accurate", "start": 4924.5, "duration": 4.44}, {"text": "device properties function and then it's", "start": 4927.199, "duration": 3.54}, {"text": "gonna", "start": 4928.94, "duration": 5.279}, {"text": "um write information about", "start": 4930.739, "duration": 5.521}, {"text": "the driver's version the runtime version", "start": 4934.219, "duration": 4.141}, {"text": "and all sorts of other properties that", "start": 4936.26, "duration": 6.32}, {"text": "you know have been queried by this um", "start": 4938.36, "duration": 6.66}, {"text": "get device properties function for", "start": 4942.58, "duration": 5.139}, {"text": "instance right so you see here device", "start": 4945.02, "duration": 5.159}, {"text": "prop dot so it says data type that", "start": 4947.719, "duration": 3.96}, {"text": "contains for instance the total Global", "start": 4950.179, "duration": 3.321}, {"text": "memory right and", "start": 4951.679, "duration": 5.161}, {"text": "multi-processor contents on so", "start": 4953.5, "duration": 4.78}, {"text": "um", "start": 4956.84, "duration": 4.26}, {"text": "should be able to compile this", "start": 4958.28, "duration": 4.919}, {"text": "let's compile the code", "start": 4961.1, "duration": 5.16}, {"text": "there's an executable here now and if I", "start": 4963.199, "duration": 4.381}, {"text": "call it", "start": 4966.26, "duration": 3.0}, {"text": "I will see a lot of information because", "start": 4967.58, "duration": 3.48}, {"text": "I have four gpus in the system actually", "start": 4969.26, "duration": 3.72}, {"text": "so it just gets information for all the", "start": 4971.06, "duration": 6.0}, {"text": "gpus and not surprisingly on on my p100", "start": 4972.98, "duration": 6.179}, {"text": "notes so it knows that device 0 is a", "start": 4977.06, "duration": 3.619}, {"text": "Tesla p100", "start": 4979.159, "duration": 3.06}, {"text": "and", "start": 4980.679, "duration": 2.881}, {"text": "um", "start": 4982.219, "duration": 4.141}, {"text": "the driver and the runtime version so", "start": 4983.56, "duration": 5.74}, {"text": "runtime 7.0 that's what I have loaded", "start": 4986.36, "duration": 5.64}, {"text": "um this is the capability version of the", "start": 4989.3, "duration": 6.24}, {"text": "p100 GPU it has 16 gigabyte of ram it", "start": 4992.0, "duration": 5.46}, {"text": "has so let's see what information we", "start": 4995.54, "duration": 5.58}, {"text": "have here we have a total of 7168 Cuda", "start": 4997.46, "duration": 5.58}, {"text": "cores right so you already know if you", "start": 5001.12, "duration": 4.619}, {"text": "launched Cuda code you want to launch", "start": 5003.04, "duration": 4.98}, {"text": "many more threads than there are cores", "start": 5005.739, "duration": 4.44}, {"text": "so you will you know launch like tens of", "start": 5008.02, "duration": 3.42}, {"text": "thousands hundreds of thousands of", "start": 5010.179, "duration": 2.761}, {"text": "threads at the same time which also", "start": 5011.44, "duration": 2.88}, {"text": "means that the problems you're operating", "start": 5012.94, "duration": 4.2}, {"text": "on should be sufficiently launched right", "start": 5014.32, "duration": 4.44}, {"text": "so if you have a just a small matrix", "start": 5017.14, "duration": 4.019}, {"text": "multiplication it's not going to be very", "start": 5018.76, "duration": 4.8}, {"text": "efficient on the GPU um so ideally", "start": 5021.159, "duration": 4.02}, {"text": "you're working for instance on larger", "start": 5023.56, "duration": 4.52}, {"text": "metrics for the applications", "start": 5025.179, "duration": 2.901}, {"text": "um the base clock frequency and so on", "start": 5028.12, "duration": 6.86}, {"text": "um there are two cache sizes and", "start": 5031.96, "duration": 5.88}, {"text": "there's some other information that is", "start": 5034.98, "duration": 5.08}, {"text": "um important you know the shared memory", "start": 5037.84, "duration": 4.14}, {"text": "per block for instance that you can use", "start": 5040.06, "duration": 4.2}, {"text": "at the registers are available available", "start": 5041.98, "duration": 4.5}, {"text": "to each block", "start": 5044.26, "duration": 2.939}, {"text": "um", "start": 5046.48, "duration": 2.28}, {"text": "the maximum threats that you can launch", "start": 5047.199, "duration": 3.781}, {"text": "per multi-processor somewhere then", "start": 5048.76, "duration": 3.54}, {"text": "should be the number of multiple", "start": 5050.98, "duration": 5.179}, {"text": "processors I don't see it currently um", "start": 5052.3, "duration": 3.859}, {"text": "um but basically you get all the", "start": 5057.34, "duration": 3.359}, {"text": "information about or much of the", "start": 5058.84, "duration": 3.6}, {"text": "information about your GPU and then", "start": 5060.699, "duration": 4.02}, {"text": "there's four of them device one two", "start": 5062.44, "duration": 3.96}, {"text": "three and four and then there's some", "start": 5064.719, "duration": 2.52}, {"text": "information", "start": 5066.4, "duration": 2.339}, {"text": "um whether you can have like um", "start": 5067.239, "duration": 4.801}, {"text": "peer-to-peer access between the gpus and", "start": 5068.739, "duration": 5.94}, {"text": "which gpus right so between GPU zero and", "start": 5072.04, "duration": 4.199}, {"text": "one you have peer-to-peer access that", "start": 5074.679, "duration": 2.941}, {"text": "means you know you can", "start": 5076.239, "duration": 3.901}, {"text": "transfer data quickly across the gpus", "start": 5077.62, "duration": 4.5}, {"text": "and", "start": 5080.14, "duration": 4.559}, {"text": "between two and three and one and zero", "start": 5082.12, "duration": 5.28}, {"text": "both and back but so that has to do but", "start": 5084.699, "duration": 3.781}, {"text": "not between", "start": 5087.4, "duration": 3.18}, {"text": "um for instance GPU two and zero because", "start": 5088.48, "duration": 3.78}, {"text": "they are sitting on different", "start": 5090.58, "duration": 3.3}, {"text": "um", "start": 5092.26, "duration": 4.74}, {"text": "uh they are connected to the PCI Express", "start": 5093.88, "duration": 5.4}, {"text": "bus that basically", "start": 5097.0, "duration": 3.48}, {"text": "um", "start": 5099.28, "duration": 4.379}, {"text": "runs of the two different CPU cores um", "start": 5100.48, "duration": 5.64}, {"text": "so that would be not possible but", "start": 5103.659, "duration": 3.781}, {"text": "something that's called key to peer", "start": 5106.12, "duration": 3.24}, {"text": "access", "start": 5107.44, "duration": 4.2}, {"text": "so that that's about device query for", "start": 5109.36, "duration": 3.24}, {"text": "instance and then there are other", "start": 5111.64, "duration": 3.539}, {"text": "examples like um", "start": 5112.6, "duration": 4.92}, {"text": "the simple examples here about", "start": 5115.179, "duration": 4.261}, {"text": "The Matrix multiplications", "start": 5117.52, "duration": 6.24}, {"text": "um we could just look into their Matrix", "start": 5119.44, "duration": 7.259}, {"text": "Rule and it's the same same thing you", "start": 5123.76, "duration": 5.399}, {"text": "can compile the code and execute it and", "start": 5126.699, "duration": 3.781}, {"text": "then you can have a look at the code on", "start": 5129.159, "duration": 4.56}, {"text": "as an example on how the code um is", "start": 5130.48, "duration": 4.38}, {"text": "written", "start": 5133.719, "duration": 2.601}, {"text": "so", "start": 5134.86, "duration": 5.78}, {"text": "this already takes a little bit longer", "start": 5136.32, "duration": 4.32}, {"text": "also the make files written in such a", "start": 5142.239, "duration": 4.861}, {"text": "case such a way that it generates code", "start": 5144.94, "duration": 3.96}, {"text": "for different compute architectures we", "start": 5147.1, "duration": 3.119}, {"text": "wouldn't have to do that because we have", "start": 5148.9, "duration": 3.12}, {"text": "a p100 here but the make file is written", "start": 5150.219, "duration": 4.141}, {"text": "such that it generates", "start": 5152.02, "duration": 4.74}, {"text": "um executable code that runs on the", "start": 5154.36, "duration": 3.96}, {"text": "different Hardware architecture so in", "start": 5156.76, "duration": 3.66}, {"text": "our case we had compute capability six", "start": 5158.32, "duration": 3.78}, {"text": "so", "start": 5160.42, "duration": 3.239}, {"text": "um it's actually no explicit code", "start": 5162.1, "duration": 3.539}, {"text": "generated for compute capability six", "start": 5163.659, "duration": 4.381}, {"text": "zero the lower one five two so that's", "start": 5165.639, "duration": 4.08}, {"text": "probably the one called path that's", "start": 5168.04, "duration": 3.84}, {"text": "going to execute and once the code", "start": 5169.719, "duration": 5.341}, {"text": "executes on the 100 GPU", "start": 5171.88, "duration": 4.56}, {"text": "um there's this matrix multiplication", "start": 5175.06, "duration": 3.9}, {"text": "example and", "start": 5176.44, "duration": 4.68}, {"text": "there we go so", "start": 5178.96, "duration": 4.44}, {"text": "um we're getting here a performance of", "start": 5181.12, "duration": 6.9}, {"text": "1.7 teraflops I think we had 300", "start": 5183.4, "duration": 8.52}, {"text": "teraflops on the on the k80 for the same", "start": 5188.02, "duration": 5.76}, {"text": "example right so this is significantly", "start": 5191.92, "duration": 4.68}, {"text": "faster the p100 GPU already with this", "start": 5193.78, "duration": 5.82}, {"text": "simple matrix multiplication example", "start": 5196.6, "duration": 7.02}, {"text": "um and if we go to the cube loss example", "start": 5199.6, "duration": 5.4}, {"text": "and", "start": 5203.62, "duration": 3.18}, {"text": "we compile that", "start": 5205.0, "duration": 3.78}, {"text": "compiles faster because you know it", "start": 5206.8, "duration": 3.899}, {"text": "doesn't have to optimize and compile and", "start": 5208.78, "duration": 3.06}, {"text": "generate code for the matrix", "start": 5210.699, "duration": 2.821}, {"text": "multiplication itself because it just", "start": 5211.84, "duration": 3.24}, {"text": "blinks in effectively a call to the", "start": 5213.52, "duration": 3.56}, {"text": "kubelights library", "start": 5215.08, "duration": 5.159}, {"text": "and we execute that", "start": 5217.08, "duration": 6.119}, {"text": "let's see", "start": 5220.239, "duration": 2.96}, {"text": "it should work I haven't tested these", "start": 5226.06, "duration": 2.88}, {"text": "things so", "start": 5227.62, "duration": 2.82}, {"text": "I should know what's happening now", "start": 5228.94, "duration": 3.299}, {"text": "that's what what always happens if you", "start": 5230.44, "duration": 5.12}, {"text": "want to show something it doesn't work", "start": 5232.239, "duration": 3.321}, {"text": "I'm not 100 sure what's happening here", "start": 5236.62, "duration": 4.68}, {"text": "now um", "start": 5238.719, "duration": 4.381}, {"text": "um", "start": 5241.3, "duration": 3.6}, {"text": "should it should work so I'm not I'm not", "start": 5243.1, "duration": 3.42}, {"text": "able to tell you why it's not working in", "start": 5244.9, "duration": 3.06}, {"text": "principle it should work I have tested", "start": 5246.52, "duration": 4.98}, {"text": "this earlier k80 gpus it does work", "start": 5247.96, "duration": 5.219}, {"text": "um", "start": 5251.5, "duration": 4.139}, {"text": "so unfortunately not able to to tell you", "start": 5253.179, "duration": 4.56}, {"text": "no um but I so I highly recommend", "start": 5255.639, "duration": 4.801}, {"text": "looking at these code samples", "start": 5257.739, "duration": 5.281}, {"text": "and", "start": 5260.44, "duration": 3.48}, {"text": "um", "start": 5263.02, "duration": 3.179}, {"text": "then I have this open ACC samples that", "start": 5263.92, "duration": 4.56}, {"text": "are exactly this LaPlace to D um the", "start": 5266.199, "duration": 3.96}, {"text": "same way that you would do the module", "start": 5268.48, "duration": 3.659}, {"text": "load PGI", "start": 5270.159, "duration": 3.601}, {"text": "and", "start": 5272.139, "duration": 4.621}, {"text": "this Jacobi code is exactly the one that", "start": 5273.76, "duration": 5.1}, {"text": "I was showing previously on the slides", "start": 5276.76, "duration": 4.68}, {"text": "and I have pre-compiled here", "start": 5278.86, "duration": 3.299}, {"text": "um", "start": 5281.44, "duration": 4.32}, {"text": "an opening P version and a version that", "start": 5282.159, "duration": 7.621}, {"text": "is optimized for the um GPU so for", "start": 5285.76, "duration": 5.7}, {"text": "instance if I do export", "start": 5289.78, "duration": 3.24}, {"text": "um OMP num", "start": 5291.46, "duration": 4.199}, {"text": "threads so that's for openmp equals six", "start": 5293.02, "duration": 5.42}, {"text": "and I run", "start": 5295.659, "duration": 2.781}, {"text": "the open key version of the code", "start": 5298.48, "duration": 4.34}, {"text": "um", "start": 5300.76, "duration": 2.06}, {"text": "it's limited to 1000 iterations", "start": 5303.639, "duration": 3.841}, {"text": "um", "start": 5306.34, "duration": 3.42}, {"text": "we'll see how long it takes it takes I", "start": 5307.48, "duration": 4.02}, {"text": "don't know something like 20", "start": 5309.76, "duration": 4.2}, {"text": "15 20 seconds", "start": 5311.5, "duration": 6.06}, {"text": "running on six threads it's a little bit", "start": 5313.96, "duration": 5.219}, {"text": "faster than what I have on my slides", "start": 5317.56, "duration": 3.0}, {"text": "because what I have on the slides I did", "start": 5319.179, "duration": 5.821}, {"text": "on the k80 notes that have older CPUs so", "start": 5320.56, "duration": 6.78}, {"text": "here it took 17 seconds I think of the", "start": 5325.0, "duration": 4.98}, {"text": "other CPUs the Hasbro CPUs takes around", "start": 5327.34, "duration": 7.5}, {"text": "20 seconds and if I execute the GPU code", "start": 5329.98, "duration": 7.739}, {"text": "or significantly faster it takes it's 10", "start": 5334.84, "duration": 6.0}, {"text": "times faster right so the p100s are", "start": 5337.719, "duration": 4.741}, {"text": "actually very efficient so on much", "start": 5340.84, "duration": 5.16}, {"text": "faster for this example than the um k80", "start": 5342.46, "duration": 4.8}, {"text": "gpus", "start": 5346.0, "duration": 3.9}, {"text": "so that's more or less what I wanted to", "start": 5347.26, "duration": 3.419}, {"text": "show", "start": 5349.9, "duration": 1.38}, {"text": "um", "start": 5350.679, "duration": 1.861}, {"text": "I don't know if there are additional", "start": 5351.28, "duration": 3.62}, {"text": "questions", "start": 5352.54, "duration": 2.36}, {"text": "I think there was a question about", "start": 5356.199, "duration": 7.381}, {"text": "open ACC libraries and how to", "start": 5358.78, "duration": 6.54}, {"text": "information", "start": 5363.58, "duration": 4.44}, {"text": "um so I highly recommend to go to the", "start": 5365.32, "duration": 4.5}, {"text": "website", "start": 5368.02, "duration": 6.84}, {"text": "um the documentation of both openacc.org", "start": 5369.82, "duration": 5.04}, {"text": "um there's a lot of information and then", "start": 5375.219, "duration": 7.261}, {"text": "Nvidia has for the um PGI compiler also", "start": 5377.199, "duration": 6.421}, {"text": "information", "start": 5382.48, "duration": 1.86}, {"text": "um", "start": 5383.62, "duration": 3.48}, {"text": "so let me see on my slides for instance", "start": 5384.34, "duration": 4.76}, {"text": "um", "start": 5387.1, "duration": 2.0}, {"text": "if I go back", "start": 5389.32, "duration": 5.72}, {"text": "to open ACC", "start": 5392.02, "duration": 3.02}, {"text": "ra", "start": 5395.5, "duration": 3.139}, {"text": "[Music]", "start": 5397.46, "duration": 3.8}, {"text": "openacc.org and here the open ACC", "start": 5398.639, "duration": 5.261}, {"text": "toolkit", "start": 5401.26, "duration": 4.14}, {"text": "um", "start": 5403.9, "duration": 5.279}, {"text": "there is information about", "start": 5405.4, "duration": 4.799}, {"text": "um", "start": 5409.179, "duration": 3.781}, {"text": "additional resources and you can find", "start": 5410.199, "duration": 4.321}, {"text": "information so I don't know details", "start": 5412.96, "duration": 4.98}, {"text": "about the libraries for open ACC", "start": 5414.52, "duration": 6.06}, {"text": "um but all the information is here", "start": 5417.94, "duration": 4.86}, {"text": "online and", "start": 5420.58, "duration": 6.0}, {"text": "also on openacc.org is a quite a good", "start": 5422.8, "duration": 6.26}, {"text": "resource", "start": 5426.58, "duration": 2.48}, {"text": "hi Andreas yes do you know if uh do you", "start": 5431.639, "duration": 5.141}, {"text": "know worry", "start": 5435.58, "duration": 3.72}, {"text": "what is a good uh place to find examples", "start": 5436.78, "duration": 8.64}, {"text": "of MPI programs uh using gpus", "start": 5439.3, "duration": 8.12}, {"text": "um", "start": 5445.42, "duration": 2.0}, {"text": "that that I actually don't know for", "start": 5449.92, "duration": 5.58}, {"text": "example so most of the examples are", "start": 5453.159, "duration": 4.741}, {"text": "simple I mean in the sense that you know", "start": 5455.5, "duration": 6.78}, {"text": "it's focused on Cuda but I mean there I", "start": 5457.9, "duration": 5.7}, {"text": "mean a lot of MPI programs of course", "start": 5462.28, "duration": 3.6}, {"text": "with Cuda and you can use MPI with Cuda", "start": 5463.6, "duration": 3.48}, {"text": "and", "start": 5465.88, "duration": 3.9}, {"text": "um so if you just Google for it um there", "start": 5467.08, "duration": 4.32}, {"text": "must be information on nvidia's website", "start": 5469.78, "duration": 3.72}, {"text": "so Jeff saying all also also said so if", "start": 5471.4, "duration": 5.4}, {"text": "you Google actually MPI", "start": 5473.5, "duration": 4.4}, {"text": "um", "start": 5476.8, "duration": 4.439}, {"text": "Fielder and GPU um", "start": 5477.9, "duration": 5.44}, {"text": "yeah we have examples so we have MPI", "start": 5481.239, "duration": 4.321}, {"text": "solutions for gpus and there are also", "start": 5483.34, "duration": 4.62}, {"text": "examples on common uh comment", "start": 5485.56, "duration": 4.8}, {"text": "documentations", "start": 5487.96, "duration": 4.62}, {"text": "historical so", "start": 5490.36, "duration": 5.46}, {"text": "it's it's a very common setup to use MPI", "start": 5492.58, "duration": 7.8}, {"text": "programs with Cuda and use multiple gpus", "start": 5495.82, "duration": 7.5}, {"text": "there's also Cuda aware NPR and he had", "start": 5500.38, "duration": 6.259}, {"text": "their own color yeah", "start": 5503.32, "duration": 3.319}, {"text": "okay", "start": 5511.719, "duration": 2.361}, {"text": "one more questions okay so if there are", "start": 5514.48, "duration": 4.32}, {"text": "no more questions then I would like to", "start": 5517.3, "duration": 3.359}, {"text": "thank all the participants", "start": 5518.8, "duration": 4.32}, {"text": "um for listening and", "start": 5520.659, "duration": 3.54}, {"text": "um", "start": 5523.12, "duration": 3.36}, {"text": "yeah so the slides", "start": 5524.199, "duration": 4.081}, {"text": "um and I think the recording will be", "start": 5526.48, "duration": 4.32}, {"text": "made available online later on the", "start": 5528.28, "duration": 6.0}, {"text": "sdsc's webinar website and", "start": 5530.8, "duration": 4.32}, {"text": "um", "start": 5534.28, "duration": 3.359}, {"text": "so thank you very much and I hope I you", "start": 5535.12, "duration": 4.559}, {"text": "will have fun with GPU Computing and", "start": 5537.639, "duration": 4.141}, {"text": "programming and make good use of our", "start": 5539.679, "duration": 6.361}, {"text": "resources here at sdsc thank you", "start": 5541.78, "duration": 6.439}, {"text": "foreign", "start": 5546.04, "duration": 2.179}]