[{"text": "all", "start": 0.44, "duration": 5.64}, {"text": "right just minimize", "start": 1.879, "duration": 4.201}, {"text": "this okay so um as as I mentioned in the", "start": 6.16, "duration": 6.12}, {"text": "introduction we've started taking a", "start": 10.36, "duration": 3.359}, {"text": "different approach to the summer", "start": 12.28, "duration": 3.12}, {"text": "Institute we know that there are a lot", "start": 13.719, "duration": 4.48}, {"text": "of you here who um who who are", "start": 15.4, "duration": 5.6}, {"text": "essentially users of our Advan of our", "start": 18.199, "duration": 4.721}, {"text": "Advanced cyber infrastructure rather", "start": 21.0, "duration": 3.84}, {"text": "than programmers but I think this is a", "start": 22.92, "duration": 4.04}, {"text": "topic that everybody can really benefit", "start": 24.84, "duration": 3.32}, {"text": "from you doing a little bit of", "start": 26.96, "duration": 3.96}, {"text": "introduction to some of the key parallel", "start": 28.16, "duration": 4.599}, {"text": "Computing some of the key parallel", "start": 30.92, "duration": 4.88}, {"text": "Computing Concepts um I have a few", "start": 32.759, "duration": 6.041}, {"text": "examples in my GitHub repo um you don't", "start": 35.8, "duration": 5.0}, {"text": "need to remember this the content's also", "start": 38.8, "duration": 4.84}, {"text": "been copied into the summer Institute", "start": 40.8, "duration": 5.52}, {"text": "repo so as I mentioned I'm the Director", "start": 43.64, "duration": 4.28}, {"text": "of Education and Training here at the", "start": 46.32, "duration": 3.52}, {"text": "supercomputer Center physicist by", "start": 47.92, "duration": 5.319}, {"text": "training um but more of a um", "start": 49.84, "duration": 6.12}, {"text": "computational science", "start": 53.239, "duration": 5.601}, {"text": "generalist yeah so for our um agenda for", "start": 55.96, "duration": 4.239}, {"text": "this session we're going to start off", "start": 58.84, "duration": 3.559}, {"text": "with an introduction we're then going to", "start": 60.199, "duration": 5.521}, {"text": "go into processes and threads and MPI", "start": 62.399, "duration": 5.921}, {"text": "and openmp we're going to talk about", "start": 65.72, "duration": 5.8}, {"text": "hybrid applications that involve a", "start": 68.32, "duration": 5.0}, {"text": "combination of processes and threads", "start": 71.52, "duration": 3.44}, {"text": "this is becoming a really important", "start": 73.32, "duration": 3.839}, {"text": "topic now that our nodes are getting", "start": 74.96, "duration": 5.28}, {"text": "fatter and fatter in that sense I mean", "start": 77.159, "duration": 4.92}, {"text": "more Hardware with a node more memory", "start": 80.24, "duration": 3.08}, {"text": "more compute", "start": 82.079, "duration": 3.481}, {"text": "cores we're going to talk about om", "start": 83.32, "duration": 6.96}, {"text": "doll's law um if you learn just one of", "start": 85.56, "duration": 7.44}, {"text": "the fundamentals of parallel Computing", "start": 90.28, "duration": 4.36}, {"text": "I'm going to say that omd's law is", "start": 93.0, "duration": 3.799}, {"text": "probably the most important we're going", "start": 94.64, "duration": 3.72}, {"text": "to talk about other limits on the", "start": 96.799, "duration": 3.801}, {"text": "scalability of", "start": 98.36, "duration": 5.039}, {"text": "applications um we'll then cover running", "start": 100.6, "duration": 4.36}, {"text": "parallel applications and doing a", "start": 103.399, "duration": 3.921}, {"text": "scaling study and then finally where to", "start": 104.96, "duration": 7.92}, {"text": "go next to get um to get some more", "start": 107.32, "duration": 5.56}, {"text": "information okay so I realized I um I", "start": 114.68, "duration": 7.359}, {"text": "edit this for a for for a different", "start": 119.96, "duration": 4.159}, {"text": "presentation so ignore all of the", "start": 122.039, "duration": 5.0}, {"text": "references to machine learning this is", "start": 124.119, "duration": 5.241}, {"text": "actually much more General um so the", "start": 127.039, "duration": 4.081}, {"text": "reason that that most of you are here is", "start": 129.36, "duration": 3.08}, {"text": "that you have applications that have", "start": 131.12, "duration": 3.28}, {"text": "grown to the point where you can no", "start": 132.44, "duration": 4.159}, {"text": "longer spec run them on your local", "start": 134.4, "duration": 4.44}, {"text": "resources you know our laptops or", "start": 136.599, "duration": 4.121}, {"text": "desktops they're so powerful that you", "start": 138.84, "duration": 3.96}, {"text": "can really do a lot with them but", "start": 140.72, "duration": 3.92}, {"text": "eventually you may out grow them and you", "start": 142.8, "duration": 3.76}, {"text": "had need to start using using parallel", "start": 144.64, "duration": 3.76}, {"text": "computers and that's what we do here our", "start": 146.56, "duration": 4.8}, {"text": "big big systems you know", "start": 148.4, "duration": 7.16}, {"text": "for oh for for the last 25 years have", "start": 151.36, "duration": 7.0}, {"text": "essentially been constructed from from", "start": 155.56, "duration": 4.16}, {"text": "multiple compute nodes that are", "start": 158.36, "duration": 3.879}, {"text": "networked together to to work as a to", "start": 159.72, "duration": 4.32}, {"text": "work as a supercomputer we realize we", "start": 162.239, "duration": 4.64}, {"text": "could only squeeze so much power out of", "start": 164.04, "duration": 5.36}, {"text": "a out of a particular", "start": 166.879, "duration": 5.36}, {"text": "CPU even if you don't go beyond running", "start": 169.4, "duration": 4.88}, {"text": "on a single compute node you're probably", "start": 172.239, "duration": 3.92}, {"text": "going to be using multiple cores on that", "start": 174.28, "duration": 5.48}, {"text": "node so um like I said I had edited this", "start": 176.159, "duration": 5.36}, {"text": "presentation and then I forgot to edit", "start": 179.76, "duration": 3.72}, {"text": "it back", "start": 181.519, "duration": 6.28}, {"text": "um but you know the um the concepts I", "start": 183.48, "duration": 5.96}, {"text": "talk about a little bit in the context", "start": 187.799, "duration": 3.281}, {"text": "of machine learning but it's going to", "start": 189.44, "duration": 3.879}, {"text": "apply to any parallel Computing", "start": 191.08, "duration": 4.68}, {"text": "Computing", "start": 193.319, "duration": 2.441}, {"text": "workload so this session is intended", "start": 196.28, "duration": 8.12}, {"text": "for sorry for for anyone who currently", "start": 201.4, "duration": 4.88}, {"text": "runs or is planning to run or is even", "start": 204.4, "duration": 3.52}, {"text": "thinking about running parallel um", "start": 206.28, "duration": 3.679}, {"text": "applications and parallel computers so", "start": 207.92, "duration": 3.879}, {"text": "this audience I'm going to assume it's", "start": 209.959, "duration": 3.92}, {"text": "pretty much everybody who's", "start": 211.799, "duration": 5.041}, {"text": "here um if you write if you write", "start": 213.879, "duration": 4.881}, {"text": "proposals for computer time on your", "start": 216.84, "duration": 4.039}, {"text": "campus cluster on nationally allocated", "start": 218.76, "duration": 4.559}, {"text": "systems like expans or other", "start": 220.879, "duration": 5.0}, {"text": "resources um if you purchase time on", "start": 223.319, "duration": 4.84}, {"text": "compute resources and want to maximize", "start": 225.879, "duration": 4.881}, {"text": "return on investment again this could be", "start": 228.159, "duration": 5.881}, {"text": "a time on ass system at an HPC Center or", "start": 230.76, "duration": 5.52}, {"text": "it could be could be in the", "start": 234.04, "duration": 5.08}, {"text": "cloud if you have any role in purchasing", "start": 236.28, "duration": 6.0}, {"text": "Hardware for your lab or if you are", "start": 239.12, "duration": 5.28}, {"text": "simply curious about parallel Computing", "start": 242.28, "duration": 4.44}, {"text": "which again I think is going to spend", "start": 244.4, "duration": 5.559}, {"text": "everybody who who's in this", "start": 246.72, "duration": 3.239}, {"text": "session so the motivation for um for for", "start": 251.319, "duration": 6.801}, {"text": "for kicking things off with these with", "start": 255.4, "duration": 5.72}, {"text": "with these parallel Computing Concepts", "start": 258.12, "duration": 5.72}, {"text": "is we find that much of the training in", "start": 261.12, "duration": 4.84}, {"text": "parallel Computing is targeted people", "start": 263.84, "duration": 3.639}, {"text": "who write their own parallel", "start": 265.96, "duration": 3.799}, {"text": "applications and it really focuses on", "start": 267.479, "duration": 3.72}, {"text": "programmer topics that you're going to", "start": 269.759, "duration": 4.201}, {"text": "see later in this week MPI and openmp", "start": 271.199, "duration": 5.401}, {"text": "and Cuda and profile and performance", "start": 273.96, "duration": 5.32}, {"text": "tuning so as a consequence those of you", "start": 276.6, "duration": 4.48}, {"text": "who are end users who are not really", "start": 279.28, "duration": 4.28}, {"text": "developers you rarely or never get a", "start": 281.08, "duration": 5.8}, {"text": "proper introduction to to parallel", "start": 283.56, "duration": 6.24}, {"text": "Computing so even if you don't write", "start": 286.88, "duration": 5.879}, {"text": "code yet like many of you using somebody", "start": 289.8, "duration": 5.04}, {"text": "else's somebody else's application it's", "start": 292.759, "duration": 4.44}, {"text": "still important to understand some of", "start": 294.84, "duration": 4.12}, {"text": "these basic principles of parallel", "start": 297.199, "duration": 4.121}, {"text": "computing so that you can make the use", "start": 298.96, "duration": 4.04}, {"text": "the most effective use of valuable", "start": 301.32, "duration": 3.719}, {"text": "Advanced cyber infrastructure so we're", "start": 303.0, "duration": 4.32}, {"text": "talking about um improving your own", "start": 305.039, "duration": 4.641}, {"text": "research productivity but also making", "start": 307.32, "duration": 5.36}, {"text": "sure that we as a center get the most um", "start": 309.68, "duration": 5.519}, {"text": "we put the mo the most science done per", "start": 312.68, "duration": 5.0}, {"text": "per per", "start": 315.199, "duration": 2.481}, {"text": "cycle so there are a few myths of", "start": 318.12, "duration": 4.28}, {"text": "parallel Computing again for this", "start": 320.4, "duration": 4.68}, {"text": "audience I hope that most of you um are", "start": 322.4, "duration": 4.6}, {"text": "already aware that these are myths but", "start": 325.08, "duration": 4.2}, {"text": "I'd encourage you to to spread the word", "start": 327.0, "duration": 4.24}, {"text": "um to to your colleagues to to your", "start": 329.28, "duration": 4.479}, {"text": "labmates um others you work with at your", "start": 331.24, "duration": 3.36}, {"text": "home", "start": 333.759, "duration": 3.321}, {"text": "institutions so for a very very long", "start": 334.6, "duration": 4.76}, {"text": "time there was this myth that parallel", "start": 337.08, "duration": 5.16}, {"text": "Computing was for you know for areas", "start": 339.36, "duration": 5.24}, {"text": "like astrophysics and engineering and", "start": 342.24, "duration": 4.28}, {"text": "climate modeling and and others who are", "start": 344.6, "duration": 3.439}, {"text": "working in these traditionally math", "start": 346.52, "duration": 5.28}, {"text": "intensive Fields pretty much you know", "start": 348.039, "duration": 5.6}, {"text": "Fields where you're solving systems of", "start": 351.8, "duration": 4.28}, {"text": "partial part partial differential", "start": 353.639, "duration": 6.201}, {"text": "equations um so this might have been par", "start": 356.08, "duration": 6.08}, {"text": "true decades ago when when I first came", "start": 359.84, "duration": 6.359}, {"text": "to the center back in um 1997 our user", "start": 362.16, "duration": 8.439}, {"text": "base was relatively narrow but nowadays", "start": 366.199, "duration": 6.081}, {"text": "um pretty much every field of research", "start": 370.599, "duration": 3.801}, {"text": "makes use of parallel Computing and", "start": 372.28, "duration": 3.96}, {"text": "we're finding more and more uptake from", "start": 374.4, "duration": 4.799}, {"text": "the social sciences the life sciences in", "start": 376.24, "duration": 6.799}, {"text": "fact um feels like like molecular bi", "start": 379.199, "duration": 7.4}, {"text": "biology and plant animal genomics and", "start": 383.039, "duration": 6.761}, {"text": "biomedical research th those those", "start": 386.599, "duration": 5.32}, {"text": "researchers actually use more than half", "start": 389.8, "duration": 4.32}, {"text": "of the time on our on our supercomputers", "start": 391.919, "duration": 4.241}, {"text": "we see um some from the Arts and the", "start": 394.12, "duration": 5.0}, {"text": "humanities but of course the traditional", "start": 396.16, "duration": 6.2}, {"text": "users of HPC physics and chemistry and", "start": 399.12, "duration": 4.919}, {"text": "engineering and Material Science they're", "start": 402.36, "duration": 3.8}, {"text": "still around but you know main main", "start": 404.039, "duration": 4.0}, {"text": "takeaway is that parallel Computing is", "start": 406.16, "duration": 3.92}, {"text": "pretty much for everybody now who has um", "start": 408.039, "duration": 4.201}, {"text": "large computational", "start": 410.08, "duration": 7.48}, {"text": "needs um another myth that we see", "start": 412.24, "duration": 7.88}, {"text": "and how I put it um", "start": 417.56, "duration": 4.079}, {"text": "you know what would it would affect but", "start": 420.12, "duration": 4.44}, {"text": "both your own productivity and the", "start": 421.639, "duration": 5.881}, {"text": "effective use of our of our HPC systems", "start": 424.56, "duration": 5.039}, {"text": "is that throwing more Hardware at a", "start": 427.52, "duration": 3.799}, {"text": "problem is going to automatically reduce", "start": 429.599, "duration": 4.0}, {"text": "the time to solution um it's it's", "start": 431.319, "duration": 4.361}, {"text": "surprising the number of new users who", "start": 433.599, "duration": 4.561}, {"text": "will get on a system like expans or", "start": 435.68, "duration": 4.56}, {"text": "Stampede or Bridges 2 or these other", "start": 438.16, "duration": 4.12}, {"text": "national resources and they have a", "start": 440.24, "duration": 4.28}, {"text": "Serial application but they'll run it on", "start": 442.28, "duration": 6.199}, {"text": "multiple cores and what happens is the", "start": 444.52, "duration": 5.32}, {"text": "same application", "start": 448.479, "duration": 2.961}, {"text": "is going to be running", "start": 449.84, "duration": 4.0}, {"text": "independently often with the same input", "start": 451.44, "duration": 4.64}, {"text": "data on each of those cores so parallel", "start": 453.84, "duration": 3.44}, {"text": "Computing is only going to help if you", "start": 456.08, "duration": 2.679}, {"text": "have an application that's already been", "start": 457.28, "duration": 3.08}, {"text": "written to take advantage of that", "start": 458.759, "duration": 3.921}, {"text": "parallel hard hardware and even if you", "start": 460.36, "duration": 4.239}, {"text": "do have a parallel code and this is", "start": 462.68, "duration": 3.079}, {"text": "something we're going to talk about", "start": 464.599, "duration": 4.44}, {"text": "quite a bit a little bit later on is", "start": 465.759, "duration": 4.801}, {"text": "that there's going to be an inherent", "start": 469.039, "duration": 3.401}, {"text": "limit on the scalability and there are", "start": 470.56, "duration": 4.72}, {"text": "Mo multiple factors that affect how much", "start": 472.44, "duration": 4.8}, {"text": "Hardware you'll be able to use to reduce", "start": 475.28, "duration": 3.039}, {"text": "your time", "start": 477.24, "duration": 3.44}, {"text": "dissolution", "start": 478.319, "duration": 4.401}, {"text": "um the one caveat I want to mention and", "start": 480.68, "duration": 4.199}, {"text": "I kind of alluded to this to this", "start": 482.72, "duration": 5.68}, {"text": "already is what we call High throughput", "start": 484.879, "duration": 5.6}, {"text": "Computing where you can use parallel", "start": 488.4, "duration": 4.72}, {"text": "Computing to run many single core or", "start": 490.479, "duration": 5.44}, {"text": "single GPU instances of an application", "start": 493.12, "duration": 5.519}, {"text": "to achieve near-perfect scaling in fact", "start": 495.919, "duration": 5.481}, {"text": "if you have", "start": 498.639, "duration": 6.721}, {"text": "a if if you have that kind of a workload", "start": 501.4, "duration": 5.519}, {"text": "where each of the each of the", "start": 505.36, "duration": 4.36}, {"text": "computations could be done independently", "start": 506.919, "duration": 4.84}, {"text": "we strongly encourage you to use this", "start": 509.72, "duration": 4.48}, {"text": "High throughput Computing model because", "start": 511.759, "duration": 4.321}, {"text": "there's", "start": 514.2, "duration": 4.319}, {"text": "um yeah how would put that that will", "start": 516.08, "duration": 4.8}, {"text": "essentially give you what 100%", "start": 518.519, "duration": 4.52}, {"text": "scalability um most of our workloads", "start": 520.88, "duration": 5.68}, {"text": "though are not are not high throughput", "start": 523.039, "duration": 3.521}, {"text": "Computing and then finally the the third", "start": 527.08, "duration": 4.199}, {"text": "the third myth we find for parallel", "start": 529.88, "duration": 3.079}, {"text": "Computing is that you need to be a", "start": 531.279, "duration": 3.68}, {"text": "programmer or software developer to make", "start": 532.959, "duration": 5.521}, {"text": "use of parallel Computing and as many of", "start": 534.959, "duration": 5.12}, {"text": "you know especially those of you who are", "start": 538.48, "duration": 3.12}, {"text": "just attending the first two days of", "start": 540.079, "duration": 3.88}, {"text": "this week most users of parallel", "start": 541.6, "duration": 4.52}, {"text": "computers are not programmers instead", "start": 543.959, "duration": 4.56}, {"text": "they use mature thirdparty software", "start": 546.12, "duration": 4.24}, {"text": "that's been developed elsewhere and made", "start": 548.519, "duration": 3.961}, {"text": "available to the community and to me", "start": 550.36, "duration": 5.479}, {"text": "this is one of the um you know what one", "start": 552.48, "duration": 5.12}, {"text": "one of the best Trends in scientific", "start": 555.839, "duration": 3.281}, {"text": "Computing that I've seen in the last", "start": 557.6, "duration": 4.4}, {"text": "couple of decades where back in the 80s", "start": 559.12, "duration": 4.959}, {"text": "or the 90s pretty much everybody was on", "start": 562.0, "duration": 3.959}, {"text": "their own there there were there were a", "start": 564.079, "duration": 4.801}, {"text": "handful of mature applications out there", "start": 565.959, "duration": 5.12}, {"text": "to velop by groups that you know really", "start": 568.88, "duration": 6.16}, {"text": "specialize in in in creating these um", "start": 571.079, "duration": 6.721}, {"text": "these applications but nowadays there", "start": 575.04, "duration": 5.76}, {"text": "are that there's so much so much mature", "start": 577.8, "duration": 5.4}, {"text": "software that you don't need to write", "start": 580.8, "duration": 4.24}, {"text": "your Chemistry or your molecular", "start": 583.2, "duration": 6.52}, {"text": "Dynamics um or or your um cl cl climate", "start": 585.04, "duration": 7.28}, {"text": "modu application these are really really", "start": 589.72, "duration": 4.76}, {"text": "in many cases really big applications", "start": 592.32, "duration": 4.6}, {"text": "that required large teams of developers", "start": 594.48, "duration": 5.359}, {"text": "so again there's all those", "start": 596.92, "duration": 5.64}, {"text": "um all the software resources available", "start": 599.839, "duration": 5.081}, {"text": "to you and you don't need to be a", "start": 602.56, "duration": 5.8}, {"text": "programmer or developer to use", "start": 604.92, "duration": 3.44}, {"text": "them so so to kind of summarize many of", "start": 608.56, "duration": 5.36}, {"text": "you are going to be using somebody", "start": 612.8, "duration": 3.52}, {"text": "else's code some I'm sh examples here on", "start": 613.92, "duration": 4.52}, {"text": "the left of what weather modeling using", "start": 616.32, "duration": 5.28}, {"text": "Warf molecular Dynamics using molecular", "start": 618.44, "duration": 6.88}, {"text": "dynamics of biological systems yet using", "start": 621.6, "duration": 6.679}, {"text": "Amber and gromax and um namdi and other", "start": 625.32, "duration": 6.04}, {"text": "applications um on the bottom here the", "start": 628.279, "duration": 5.641}, {"text": "construction of phog gentic trees um I", "start": 631.36, "duration": 4.76}, {"text": "would say this is one of the big success", "start": 633.92, "duration": 5.039}, {"text": "stories in the um in the last decade at", "start": 636.12, "duration": 5.159}, {"text": "the supercomputer Center we have a", "start": 638.959, "duration": 3.68}, {"text": "science", "start": 641.279, "duration": 5.201}, {"text": "Gateway that that allows users to upload", "start": 642.639, "duration": 5.801}, {"text": "upload genomic data and construct these", "start": 646.48, "duration": 4.56}, {"text": "phenetic trees so we've made parallel", "start": 648.44, "duration": 4.48}, {"text": "Computing available to an entire", "start": 651.04, "duration": 4.0}, {"text": "community of ecologists and", "start": 652.92, "duration": 5.12}, {"text": "environmental scientists", "start": 655.04, "duration": 6.359}, {"text": "who otherwise would would need to um", "start": 658.04, "duration": 4.76}, {"text": "well I'm going to say not develop their", "start": 661.399, "duration": 4.12}, {"text": "own applications but would have to um", "start": 662.8, "duration": 5.92}, {"text": "really figure out how how to run these", "start": 665.519, "duration": 4.961}, {"text": "applications on their own and again", "start": 668.72, "duration": 3.359}, {"text": "supported by that community of", "start": 670.48, "duration": 5.359}, {"text": "developers in phog gentics um and also", "start": 672.079, "duration": 6.2}, {"text": "example here of electronic electronic", "start": 675.839, "duration": 4.44}, {"text": "structure calculations using something", "start": 678.279, "duration": 4.24}, {"text": "called", "start": 680.279, "duration": 2.24}, {"text": "cp2k okay um a little bit about parallel", "start": 683.36, "duration": 4.279}, {"text": "computers I think you got an", "start": 686.0, "duration": 3.959}, {"text": "introduction to this on Wednesday so I'm", "start": 687.639, "duration": 4.0}, {"text": "going to go through this prettyy quickly", "start": 689.959, "duration": 3.241}, {"text": "but modern clusters and parallel", "start": 691.639, "duration": 3.561}, {"text": "computers really comp they're they", "start": 693.2, "duration": 4.36}, {"text": "they're comprised of multiple compute", "start": 695.2, "duration": 5.0}, {"text": "nodes that are networked together using", "start": 697.56, "duration": 5.519}, {"text": "a very fast interconnect it could", "start": 700.2, "duration": 6.52}, {"text": "be something like 100 gigb ethernet so", "start": 703.079, "duration": 4.801}, {"text": "you're probably all familiar with", "start": 706.72, "duration": 3.119}, {"text": "ethernet this kind of takes it up to the", "start": 707.88, "duration": 3.72}, {"text": "next level in terms in terms of", "start": 709.839, "duration": 3.841}, {"text": "bandwidth most of the machines that", "start": 711.6, "duration": 4.4}, {"text": "we've had in recent years used something", "start": 713.68, "duration": 4.24}, {"text": "called infiniband you don't really need", "start": 716.0, "duration": 3.519}, {"text": "to worry about the details except that", "start": 717.92, "duration": 5.719}, {"text": "infin band gives you depending on the", "start": 719.519, "duration": 6.241}, {"text": "generation of infin band that you're", "start": 723.639, "duration": 6.161}, {"text": "using um equal or better bandwidth to", "start": 725.76, "duration": 6.04}, {"text": "the to the latest versions of of", "start": 729.8, "duration": 4.88}, {"text": "ethernet but also very very low", "start": 731.8, "duration": 5.12}, {"text": "latency and I'm going to say this is", "start": 734.68, "duration": 4.48}, {"text": "what really makes a parallel computer a", "start": 736.92, "duration": 3.8}, {"text": "parallel computer that interconnect", "start": 739.16, "duration": 4.039}, {"text": "between the nodes each of those", "start": 740.72, "duration": 5.32}, {"text": "CPUs sorry each of those CPU nodes", "start": 743.199, "duration": 5.76}, {"text": "typically contains two to two multi-or", "start": 746.04, "duration": 4.84}, {"text": "processors in our case with expans you", "start": 748.959, "duration": 6.12}, {"text": "have 2 64 2 64 core processors for a", "start": 750.88, "duration": 7.72}, {"text": "total of 128 cores and the GPU nodes", "start": 755.079, "duration": 6.721}, {"text": "normally contain four gpus so if we want", "start": 758.6, "duration": 5.239}, {"text": "to effectively use this Hardware we need", "start": 761.8, "duration": 3.96}, {"text": "to have applications been paralyzed that", "start": 763.839, "duration": 4.36}, {"text": "they can run across the multiple cores", "start": 765.76, "duration": 6.68}, {"text": "or the gpus within a node or across", "start": 768.199, "duration": 4.241}, {"text": "nodes okay so moving on to the next", "start": 772.6, "duration": 4.52}, {"text": "topic we're going to discuss processes", "start": 775.12, "duration": 8.399}, {"text": "threads MPI and open MP make a note", "start": 777.12, "duration": 6.399}, {"text": "here okay so you might have you might", "start": 787.16, "duration": 4.64}, {"text": "have heard these terms um in the past", "start": 789.279, "duration": 5.081}, {"text": "processes and threads you could think of", "start": 791.8, "duration": 5.96}, {"text": "both of these as ways to implement um to", "start": 794.36, "duration": 6.2}, {"text": "to implement par", "start": 797.76, "duration": 5.04}, {"text": "paralyzation threads and process are", "start": 800.56, "duration": 5.6}, {"text": "both independent sequences of execution", "start": 802.8, "duration": 4.719}, {"text": "but there's an important difference", "start": 806.16, "duration": 3.32}, {"text": "between the two of them you could think", "start": 807.519, "duration": 5.241}, {"text": "of a process as being an instance of a", "start": 809.48, "duration": 6.88}, {"text": "program um so these the these processes", "start": 812.76, "duration": 5.28}, {"text": "each of them will have their own pool of", "start": 816.36, "duration": 3.52}, {"text": "memory they're going to have what they", "start": 818.04, "duration": 4.919}, {"text": "call prepr preserve State they can um", "start": 819.88, "duration": 5.04}, {"text": "open and open and close files they have", "start": 822.959, "duration": 4.161}, {"text": "access to to file descriptors so again", "start": 824.92, "duration": 3.88}, {"text": "just think of a process as an instance", "start": 827.12, "duration": 5.719}, {"text": "of program when we get to when we get to", "start": 828.8, "duration": 7.24}, {"text": "MPI um when we get to talking about", "start": 832.839, "duration": 7.601}, {"text": "MPI keep in mind that a application", "start": 836.04, "duration": 6.719}, {"text": "that's been paralyzed using MPI is", "start": 840.44, "duration": 5.319}, {"text": "really just just running multiple", "start": 842.759, "duration": 5.2}, {"text": "instances of that program each one of", "start": 845.759, "duration": 5.32}, {"text": "process and then moving data between", "start": 847.959, "duration": 6.12}, {"text": "those processes a thread is something a", "start": 851.079, "duration": 5.641}, {"text": "little more lightwe that executes inside", "start": 854.079, "duration": 6.241}, {"text": "of a process so", "start": 856.72, "duration": 6.64}, {"text": "every every process is going to have at", "start": 860.32, "duration": 5.24}, {"text": "least one thread so you kind of get one", "start": 863.36, "duration": 4.32}, {"text": "thread by default and you can have", "start": 865.56, "duration": 4.519}, {"text": "multiple threads within a process and", "start": 867.68, "duration": 6.0}, {"text": "they can all access shared memory so", "start": 870.079, "duration": 5.88}, {"text": "there are some great online resources", "start": 873.68, "duration": 4.399}, {"text": "describing the difference between be", "start": 875.959, "duration": 4.68}, {"text": "between threads and processes um only", "start": 878.079, "duration": 4.161}, {"text": "problem is that they tend to be geared", "start": 880.639, "duration": 3.681}, {"text": "more toward computer scientists rather", "start": 882.24, "duration": 4.279}, {"text": "than computational scientists like", "start": 884.32, "duration": 3.8}, {"text": "yourself but I'm going to say that the", "start": 886.519, "duration": 3.361}, {"text": "following two resources do a pretty good", "start": 888.12, "duration": 4.959}, {"text": "job of addressing the topic so I found a", "start": 889.88, "duration": 6.56}, {"text": "really good thread on stack Overflow but", "start": 893.079, "duration": 4.641}, {"text": "as you might expect if you're familiar", "start": 896.44, "duration": 2.959}, {"text": "with stack Overflow it's it's a little", "start": 897.72, "duration": 4.119}, {"text": "deeper a little nerdier um but there's", "start": 899.399, "duration": 3.8}, {"text": "also", "start": 901.839, "duration": 5.24}, {"text": "a um another great description that I", "start": 903.199, "duration": 6.801}, {"text": "found a little a little more informal um", "start": 907.079, "duration": 5.44}, {"text": "at not sure how you pronounce it educ", "start": 910.0, "duration": 5.199}, {"text": "cba.com talks about process versus", "start": 912.519, "duration": 4.32}, {"text": "thread so I'm going to say maybe take a", "start": 915.199, "duration": 4.32}, {"text": "look at the um at that second link first", "start": 916.839, "duration": 4.481}, {"text": "if you want to dig deeper and then if", "start": 919.519, "duration": 4.521}, {"text": "you want to go deeper dig deeper so um", "start": 921.32, "duration": 5.639}, {"text": "you can go to stack", "start": 924.04, "duration": 2.919}, {"text": "overflow", "start": 927.6, "duration": 3.0}, {"text": "okay a few a few more distinctions", "start": 932.0, "duration": 4.759}, {"text": "between process and threads processes", "start": 934.44, "duration": 4.399}, {"text": "they are heavyweight they incur a little", "start": 936.759, "duration": 2.921}, {"text": "more", "start": 938.839, "duration": 3.961}, {"text": "overhead but they're also more flexible", "start": 939.68, "duration": 5.2}, {"text": "than well more flexible than threads", "start": 942.8, "duration": 3.92}, {"text": "multiple processes they can be run", "start": 944.88, "duration": 4.879}, {"text": "within a compute node or you can have", "start": 946.72, "duration": 5.359}, {"text": "processes across multiple compute nodes", "start": 949.759, "duration": 3.64}, {"text": "taking advantage what we call", "start": 952.079, "duration": 3.88}, {"text": "distributed memory and then explicitly", "start": 953.399, "duration": 5.761}, {"text": "communicating data over the network on", "start": 955.959, "duration": 6.32}, {"text": "be between those using something like", "start": 959.16, "duration": 6.359}, {"text": "MPI threads incur less overhead they're", "start": 962.279, "duration": 5.601}, {"text": "much lighter weight threaded codes can", "start": 965.519, "duration": 5.361}, {"text": "use less memory since the threads within", "start": 967.88, "duration": 5.84}, {"text": "a process have access to that same data", "start": 970.88, "duration": 6.759}, {"text": "structure if you um stick around for for", "start": 973.72, "duration": 6.16}, {"text": "the for the entire week you'll get a", "start": 977.639, "duration": 4.601}, {"text": "you'll see a session um that covers", "start": 979.88, "duration": 4.68}, {"text": "openmp and MPI and talks a little bit", "start": 982.24, "duration": 3.64}, {"text": "more about threads and", "start": 984.56, "duration": 3.6}, {"text": "processes but threads are also less", "start": 985.88, "duration": 3.199}, {"text": "flexible", "start": 988.16, "duration": 3.159}, {"text": "multiple threads associated with process", "start": 989.079, "duration": 4.801}, {"text": "can only be run within a compute node So", "start": 991.319, "duration": 5.041}, {"text": "within um within a node where they have", "start": 993.88, "duration": 6.199}, {"text": "access to that to that shared", "start": 996.36, "duration": 3.719}, {"text": "memory okay so we talked about process", "start": 1000.68, "duration": 5.159}, {"text": "and threads we're going to get to the", "start": 1003.0, "duration": 5.36}, {"text": "um I think the key point now and that is", "start": 1005.839, "duration": 4.401}, {"text": "what why do I care about processing", "start": 1008.36, "duration": 4.399}, {"text": "threads especially if I am not", "start": 1010.24, "duration": 5.68}, {"text": "developing developing my own software so", "start": 1012.759, "duration": 4.64}, {"text": "the type of paralyzation is going to", "start": 1015.92, "duration": 3.68}, {"text": "determine how and where you're going to", "start": 1017.399, "duration": 5.721}, {"text": "run your code so for distributed memory", "start": 1019.6, "duration": 5.599}, {"text": "applications this is the multiple", "start": 1023.12, "duration": 4.079}, {"text": "processes or multiple instances of a", "start": 1025.199, "duration": 4.961}, {"text": "program this can be run on one or more", "start": 1027.199, "duration": 5.521}, {"text": "nodes shared memory or threaded", "start": 1030.16, "duration": 4.36}, {"text": "applications should only be run on a", "start": 1032.72, "duration": 4.239}, {"text": "single node if you try to run on one", "start": 1034.52, "duration": 4.279}, {"text": "more on more than one node you are going", "start": 1036.959, "duration": 4.12}, {"text": "to get absolutely no additional", "start": 1038.799, "duration": 5.321}, {"text": "benefits um hybrid applications so this", "start": 1041.079, "duration": 4.521}, {"text": "is a combination of threads and", "start": 1044.12, "duration": 3.719}, {"text": "processes can be run on one or more", "start": 1045.6, "duration": 4.319}, {"text": "nodes but you should consider the", "start": 1047.839, "duration": 4.401}, {"text": "balance between threads and processes in", "start": 1049.919, "duration": 3.921}, {"text": "order to get the best", "start": 1052.24, "duration": 4.319}, {"text": "performance and in all cases you may", "start": 1053.84, "duration": 4.719}, {"text": "need to consider how processes and", "start": 1056.559, "duration": 4.641}, {"text": "threads are mapped and bound to", "start": 1058.559, "duration": 5.881}, {"text": "cores so in addition just being aware of", "start": 1061.2, "duration": 5.8}, {"text": "threads and processes will help you to", "start": 1064.44, "duration": 4.479}, {"text": "understand how your code is utilized in", "start": 1067.0, "duration": 4.2}, {"text": "the hardware and sometimes identify", "start": 1068.919, "duration": 5.161}, {"text": "identify common", "start": 1071.2, "duration": 2.88}, {"text": "problems so I said upfront that this is", "start": 1075.36, "duration": 6.36}, {"text": "not a session for programmers this is a", "start": 1078.799, "duration": 7.401}, {"text": "topic for for for end users um but I do", "start": 1081.72, "duration": 5.92}, {"text": "want to talk just a little bit about", "start": 1086.2, "duration": 4.32}, {"text": "message passing interface or MPI and O", "start": 1087.64, "duration": 4.6}, {"text": "openmp you're you're going to you're", "start": 1090.52, "duration": 3.48}, {"text": "going to hear these discussed you don't", "start": 1092.24, "duration": 3.439}, {"text": "need to program in them if you're not", "start": 1094.0, "duration": 3.24}, {"text": "developing applications but you should", "start": 1095.679, "duration": 4.401}, {"text": "at least be aware of what they are so", "start": 1097.24, "duration": 7.12}, {"text": "MPI is the what we call the defective", "start": 1100.08, "duration": 8.36}, {"text": "standard for paralyzing C C++ and for", "start": 1104.36, "duration": 6.24}, {"text": "Trend codes to run on distri on", "start": 1108.44, "duration": 5.04}, {"text": "distributed memory multiple compute node", "start": 1110.6, "duration": 5.52}, {"text": "systems so this surprises me sometime it", "start": 1113.48, "duration": 5.84}, {"text": "is not an official standard um but it", "start": 1116.12, "duration": 6.48}, {"text": "has become the the defao standard", "start": 1119.32, "duration": 5.32}, {"text": "meaning that pretty much everybody uses", "start": 1122.6, "duration": 5.4}, {"text": "it so it's um back back in the days when", "start": 1124.64, "duration": 6.8}, {"text": "MPI was first developed there were a few", "start": 1128.0, "duration": 5.4}, {"text": "um a few competing standards remember", "start": 1131.44, "duration": 3.32}, {"text": "there's something called", "start": 1133.4, "duration": 4.48}, {"text": "pvm uh I forget what else U which I", "start": 1134.76, "duration": 5.12}, {"text": "don't believe any of those are are", "start": 1137.88, "duration": 4.679}, {"text": "supported anymore so M MPI was kind of", "start": 1139.88, "duration": 4.56}, {"text": "the kind of the Survivor which you'll", "start": 1142.559, "duration": 5.521}, {"text": "see used very very", "start": 1144.44, "duration": 6.479}, {"text": "extensively so there are multiple", "start": 1148.08, "duration": 6.64}, {"text": "open-source implementations of of MPI so", "start": 1150.919, "duration": 6.561}, {"text": "you may hear terms like open MPI or", "start": 1154.72, "duration": 6.4}, {"text": "mvapich or mvapich 2 or Mich and then", "start": 1157.48, "duration": 7.48}, {"text": "vendors like Intel can also", "start": 1161.12, "duration": 7.88}, {"text": "sorry um Okay can also um support their", "start": 1164.96, "duration": 6.839}, {"text": "own versions of MPI what's important", "start": 1169.0, "duration": 4.72}, {"text": "though is that these are just", "start": 1171.799, "duration": 5.24}, {"text": "implementations of the MPI standard to", "start": 1173.72, "duration": 4.959}, {"text": "within maybe some of the some of the", "start": 1177.039, "duration": 4.161}, {"text": "Cutting Edge features and the newer", "start": 1178.679, "duration": 7.521}, {"text": "releases of MPI um where where say all", "start": 1181.2, "duration": 7.24}, {"text": "of the very very newest newest", "start": 1186.2, "duration": 4.04}, {"text": "developments haven't been implemented", "start": 1188.44, "duration": 3.84}, {"text": "everywhere other than that though open", "start": 1190.24, "duration": 4.919}, {"text": "MPI mvapich M pitch they all give you", "start": 1192.28, "duration": 5.44}, {"text": "the same interface to to MPI so you do", "start": 1195.159, "duration": 5.281}, {"text": "not need to rewrite your applications in", "start": 1197.72, "duration": 5.079}, {"text": "order to use one or the other now what", "start": 1200.44, "duration": 4.76}, {"text": "we do find though is that certain", "start": 1202.799, "duration": 4.961}, {"text": "applications will run will run faster", "start": 1205.2, "duration": 5.08}, {"text": "which or get better scaling using a", "start": 1207.76, "duration": 4.84}, {"text": "particular", "start": 1210.28, "duration": 4.759}, {"text": "implementation MPI applications they can", "start": 1212.6, "duration": 4.68}, {"text": "be run within a single shared memory", "start": 1215.039, "duration": 4.88}, {"text": "node so all widely used MPI", "start": 1217.28, "duration": 4.92}, {"text": "implementations they are optimized to", "start": 1219.919, "duration": 5.521}, {"text": "take advantage of the faster inode", "start": 1222.2, "duration": 5.08}, {"text": "Communications mentioned already that", "start": 1225.44, "duration": 3.479}, {"text": "that MPI is", "start": 1227.28, "duration": 4.16}, {"text": "portable it can be used it can be used", "start": 1228.919, "duration": 4.88}, {"text": "pretty much anywhere um and then", "start": 1231.44, "duration": 5.119}, {"text": "although MPI is often synonymous with", "start": 1233.799, "duration": 4.161}, {"text": "distributed memory", "start": 1236.559, "duration": 3.681}, {"text": "paralyzation there are other options out", "start": 1237.96, "duration": 5.599}, {"text": "there there's a system called charm", "start": 1240.24, "duration": 7.319}, {"text": "Plus+ which is um really really gaining", "start": 1243.559, "duration": 8.801}, {"text": "a lot of interest it is the", "start": 1247.559, "duration": 8.521}, {"text": "um it it is used what is um in namd what", "start": 1252.36, "duration": 5.96}, {"text": "one of what one of the um what most", "start": 1256.08, "duration": 4.0}, {"text": "widely used molecular Dynamic", "start": 1258.32, "duration": 3.64}, {"text": "applications there's something called", "start": 1260.08, "duration": 5.76}, {"text": "UPC um X10 so even though you think of", "start": 1261.96, "duration": 6.16}, {"text": "MPI is being synonymous with distributed", "start": 1265.84, "duration": 4.64}, {"text": "memory paralyzation there there are", "start": 1268.12, "duration": 4.12}, {"text": "there are other options out there and", "start": 1270.48, "duration": 3.24}, {"text": "some which give you a little bit more", "start": 1272.24, "duration": 4.439}, {"text": "flexibility than", "start": 1273.72, "duration": 2.959}, {"text": "MPI okay so although we've been", "start": 1279.36, "duration": 4.799}, {"text": "discussing MPI the same principles are", "start": 1281.48, "duration": 4.4}, {"text": "going to apply when paralyzing deep", "start": 1284.159, "duration": 3.321}, {"text": "learning applications again this is a", "start": 1285.88, "duration": 3.44}, {"text": "slide that was left over from our", "start": 1287.48, "duration": 4.079}, {"text": "machine learning Institute but there's", "start": 1289.32, "duration": 4.2}, {"text": "something called horovod This is a", "start": 1291.559, "duration": 3.521}, {"text": "distributed deep learning training", "start": 1293.52, "duration": 4.039}, {"text": "framework for tense flow Caris P torch", "start": 1295.08, "duration": 5.56}, {"text": "Apache mxnet um there's also nickel", "start": 1297.559, "duration": 4.961}, {"text": "which is the Nvidia Collective", "start": 1300.64, "duration": 4.12}, {"text": "communication library and this is used", "start": 1302.52, "duration": 5.279}, {"text": "for from multi-gpu and multi M multi-", "start": 1304.76, "duration": 5.24}, {"text": "node", "start": 1307.799, "duration": 2.201}, {"text": "Communications so going to show you a", "start": 1311.52, "duration": 6.56}, {"text": "small um a small fragment of a of an m", "start": 1315.159, "duration": 6.241}, {"text": "MPI application again if you are not a", "start": 1318.08, "duration": 5.4}, {"text": "programmer or an application developer", "start": 1321.4, "duration": 4.6}, {"text": "you do not need to know MPI but I just", "start": 1323.48, "duration": 4.76}, {"text": "want you to be aware that it exists and", "start": 1326.0, "duration": 4.6}, {"text": "let's get a peek of what it looks like", "start": 1328.24, "duration": 4.2}, {"text": "so when you started programming um", "start": 1330.6, "duration": 6.12}, {"text": "whether it was C C++ Java um python you", "start": 1332.44, "duration": 7.92}, {"text": "probably wrote um some variation of a", "start": 1336.72, "duration": 4.68}, {"text": "Hello", "start": 1340.36, "duration": 3.919}, {"text": "World um in this case that this is a", "start": 1341.4, "duration": 5.8}, {"text": "snippet of C code in our main program we", "start": 1344.279, "duration": 6.241}, {"text": "have one state print hello world now if", "start": 1347.2, "duration": 6.04}, {"text": "we wanted to paralyze this so that each", "start": 1350.52, "duration": 5.68}, {"text": "process wres", "start": 1353.24, "duration": 6.88}, {"text": "um right hello world it would now look", "start": 1356.2, "duration": 7.24}, {"text": "like this where all of the new content", "start": 1360.12, "duration": 6.84}, {"text": "needed to paralyze um is shown in in", "start": 1363.44, "duration": 5.76}, {"text": "purple and bold so in this case we had", "start": 1366.96, "duration": 5.52}, {"text": "to include a header to get access to the", "start": 1369.2, "duration": 6.079}, {"text": "um to to the MPI function prototypes and", "start": 1372.48, "duration": 4.88}, {"text": "constants we needed to initialize", "start": 1375.279, "duration": 3.241}, {"text": "environment", "start": 1377.36, "duration": 3.48}, {"text": "get the number of processors get the", "start": 1378.52, "duration": 5.279}, {"text": "rank of the process so when we have an", "start": 1380.84, "duration": 5.24}, {"text": "MPI application when we start off the", "start": 1383.799, "duration": 5.36}, {"text": "only thing that distinguishes the the", "start": 1386.08, "duration": 4.92}, {"text": "individual instances of the program or", "start": 1389.159, "duration": 4.12}, {"text": "the processes is what they call the rank", "start": 1391.0, "duration": 5.039}, {"text": "number zero through through n where or", "start": 1393.279, "duration": 4.76}, {"text": "sorry n minus one where n is the total", "start": 1396.039, "duration": 4.0}, {"text": "number of processes we can get the name", "start": 1398.039, "duration": 4.161}, {"text": "of the processor and then finally down", "start": 1400.039, "duration": 6.561}, {"text": "here we have each um each process saying", "start": 1402.2, "duration": 8.52}, {"text": "hello world and telling us um what what", "start": 1406.6, "duration": 5.88}, {"text": "what their node name is and the rank out", "start": 1410.72, "duration": 3.839}, {"text": "of how many processes and then finally", "start": 1412.48, "duration": 4.439}, {"text": "we need to finalize the MPI environment", "start": 1414.559, "duration": 4.641}, {"text": "so you can see that an MPI application", "start": 1416.919, "duration": 5.521}, {"text": "can become um a little more complicated", "start": 1419.2, "duration": 5.68}, {"text": "a little bit harder to read um this case", "start": 1422.44, "duration": 4.16}, {"text": "is a little extreme since we had to do", "start": 1424.88, "duration": 6.36}, {"text": "all of the parallel um what put the yeah", "start": 1426.6, "duration": 7.679}, {"text": "all all of the MPI overhead of declar", "start": 1431.24, "duration": 5.64}, {"text": "and processes Rank and so on in a real", "start": 1434.279, "duration": 4.52}, {"text": "application you're going to find that", "start": 1436.88, "duration": 3.76}, {"text": "the ratio of", "start": 1438.799, "duration": 7.401}, {"text": "code of a original seral code to the um", "start": 1440.64, "duration": 7.6}, {"text": "additional code added for for", "start": 1446.2, "duration": 4.4}, {"text": "paralyzation is is fairly", "start": 1448.24, "duration": 4.4}, {"text": "small okay", "start": 1450.6, "duration": 4.76}, {"text": "openmp um is another application Pro", "start": 1452.64, "duration": 4.84}, {"text": "programming interface or API and this is", "start": 1455.36, "duration": 5.36}, {"text": "for shared memory within a node and", "start": 1457.48, "duration": 6.199}, {"text": "currently open MP is available for C C++", "start": 1460.72, "duration": 5.4}, {"text": "and for Trend it provides a collection", "start": 1463.679, "duration": 4.561}, {"text": "of compiler directives Library routines", "start": 1466.12, "duration": 3.439}, {"text": "and environment", "start": 1468.24, "duration": 4.64}, {"text": "variables unlike MPI which which is the", "start": 1469.559, "duration": 6.24}, {"text": "library openmp is supported by the", "start": 1472.88, "duration": 7.039}, {"text": "compiler and to date I have not seen a", "start": 1475.799, "duration": 5.721}, {"text": "um seen a major", "start": 1479.919, "duration": 5.521}, {"text": "compiler including IBM Intel GCC that's", "start": 1481.52, "duration": 7.159}, {"text": "the new piler collection PGI the AMD", "start": 1485.44, "duration": 5.119}, {"text": "optimizing compilers that does not", "start": 1488.679, "duration": 5.36}, {"text": "support openmp again MPI is sorry open", "start": 1490.559, "duration": 6.281}, {"text": "MP is portable so once we have a code", "start": 1494.039, "duration": 5.601}, {"text": "this par been paralyzed with openmp we", "start": 1496.84, "duration": 5.0}, {"text": "can run it on on any", "start": 1499.64, "duration": 6.919}, {"text": "system and like MPI um openmp is often", "start": 1501.84, "duration": 6.52}, {"text": "taken to be synonymous with shared", "start": 1506.559, "duration": 4.24}, {"text": "memory paralyzation but there are other", "start": 1508.36, "duration": 6.039}, {"text": "application um other options there's a", "start": 1510.799, "duration": 5.6}, {"text": "language out there called Silk which is", "start": 1514.399, "duration": 4.64}, {"text": "essentially C with a few minor", "start": 1516.399, "duration": 5.321}, {"text": "extensions posic threads and specialized", "start": 1519.039, "duration": 4.921}, {"text": "libraries for python R and other", "start": 1521.72, "duration": 4.0}, {"text": "programming", "start": 1523.96, "duration": 4.4}, {"text": "languages to show you what an open", "start": 1525.72, "duration": 5.6}, {"text": "P example would look like I'm going to", "start": 1528.36, "duration": 5.799}, {"text": "say the same thing I did for did for MPI", "start": 1531.32, "duration": 4.239}, {"text": "if you are not a programmer or an", "start": 1534.159, "duration": 3.76}, {"text": "application developer you do not need to", "start": 1535.559, "duration": 5.48}, {"text": "know openmp but you do need to know that", "start": 1537.919, "duration": 4.721}, {"text": "it exists this is going to be", "start": 1541.039, "duration": 3.0}, {"text": "particularly important if you are", "start": 1542.64, "duration": 3.159}, {"text": "building your executable let's say you", "start": 1544.039, "duration": 3.76}, {"text": "get somebody else's code and you need to", "start": 1545.799, "duration": 3.961}, {"text": "compile it there are certain flags that", "start": 1547.799, "duration": 4.281}, {"text": "you'll need to use depending on which", "start": 1549.76, "duration": 5.68}, {"text": "compiler it might be- f openmp - Q", "start": 1552.08, "duration": 8.76}, {"text": "openmp or- MP and this is all um this is", "start": 1555.44, "duration": 7.599}, {"text": "all recorded in the in the expense user", "start": 1560.84, "duration": 4.28}, {"text": "guide and in fact you'll probably see", "start": 1563.039, "duration": 4.52}, {"text": "see this described in the user guides", "start": 1565.12, "duration": 4.72}, {"text": "for all of the major through computer", "start": 1567.559, "duration": 5.281}, {"text": "centers so here's a simple case again if", "start": 1569.84, "duration": 5.76}, {"text": "you're not a programmer you can um you", "start": 1572.84, "duration": 5.52}, {"text": "know kind of ignore what we're doing but", "start": 1575.6, "duration": 3.959}, {"text": "if you have a little bit of program", "start": 1578.36, "duration": 3.72}, {"text": "experience see here we have a main", "start": 1579.559, "duration": 5.161}, {"text": "program we're going to initialize three", "start": 1582.08, "duration": 7.4}, {"text": "arrays um a b and c we're going to um", "start": 1584.72, "duration": 6.959}, {"text": "you know assign them assign them some", "start": 1589.48, "duration": 4.439}, {"text": "values and then we're going", "start": 1591.679, "duration": 5.36}, {"text": "to um add add those arrays at element", "start": 1593.919, "duration": 5.64}, {"text": "element by", "start": 1597.039, "duration": 2.52}, {"text": "element so this is what the code would", "start": 1599.76, "duration": 6.24}, {"text": "look like if we paraliz it using open MP", "start": 1602.32, "duration": 6.12}, {"text": "we need to we start off by by adding a", "start": 1606.0, "duration": 6.76}, {"text": "header so that we have um so we have", "start": 1608.44, "duration": 6.92}, {"text": "sorry have access to to the constants", "start": 1612.76, "duration": 6.12}, {"text": "and runtime Library um we defined a", "start": 1615.36, "duration": 7.6}, {"text": "chunk size here which um which", "start": 1618.88, "duration": 6.76}, {"text": "sets how large the unit of work is", "start": 1622.96, "duration": 3.92}, {"text": "that's going to be assigned to each of", "start": 1625.64, "duration": 3.759}, {"text": "the threads and then the construct that", "start": 1626.88, "duration": 5.56}, {"text": "you're going to see most often in a code", "start": 1629.399, "duration": 5.361}, {"text": "paralyzed using open MP is one of these", "start": 1632.44, "duration": 5.04}, {"text": "pragmas where we say which VAR which", "start": 1634.76, "duration": 5.799}, {"text": "variables or arrays are shared um we're", "start": 1637.48, "duration": 5.52}, {"text": "doing a static a static scheduling we're", "start": 1640.559, "duration": 4.761}, {"text": "taking chunks work and handing them to", "start": 1643.0, "duration": 5.08}, {"text": "each thread um and once they're done", "start": 1645.32, "duration": 3.839}, {"text": "they can go back and get their next", "start": 1648.08, "duration": 3.4}, {"text": "chunk so what we've done is we've taken", "start": 1649.159, "duration": 4.24}, {"text": "this Loop and we paralyzed it so that", "start": 1651.48, "duration": 4.52}, {"text": "those iterations could be spread across", "start": 1653.399, "duration": 4.961}, {"text": "across multiple cores like I said you", "start": 1656.0, "duration": 3.76}, {"text": "don't if you're not a programmer you", "start": 1658.36, "duration": 3.679}, {"text": "don't need to know openmp but you do", "start": 1659.76, "duration": 5.48}, {"text": "need to aware need to be aware it exists", "start": 1662.039, "duration": 5.161}, {"text": "and for your parallel application you", "start": 1665.24, "duration": 4.76}, {"text": "want to know if it's been paralyzed to", "start": 1667.2, "duration": 5.12}, {"text": "use processes to use threads or to use", "start": 1670.0, "duration": 5.2}, {"text": "the combination of", "start": 1672.32, "duration": 2.88}, {"text": "both so the big picture you know for for", "start": 1676.0, "duration": 6.32}, {"text": "MPI and openmp on the left hand side we", "start": 1679.0, "duration": 6.72}, {"text": "show openmp where multiple processes are", "start": 1682.32, "duration": 6.719}, {"text": "managed using using MPI and then MPI is", "start": 1685.72, "duration": 6.16}, {"text": "implemented in libraries such as Mich or", "start": 1689.039, "duration": 5.561}, {"text": "mvapich or mvapich 2 and open MPI and", "start": 1691.88, "duration": 5.88}, {"text": "vendor implementations threads M", "start": 1694.6, "duration": 5.0}, {"text": "multiple threads are managed using", "start": 1697.76, "duration": 4.519}, {"text": "openmp and then openmp is implemented by", "start": 1699.6, "duration": 5.799}, {"text": "the compilers and that could be the GCC", "start": 1702.279, "duration": 6.561}, {"text": "Intel aocc PGI", "start": 1705.399, "duration": 6.041}, {"text": "others and with that I realized I've", "start": 1708.84, "duration": 4.559}, {"text": "been talking for for quite a while are", "start": 1711.44, "duration": 3.959}, {"text": "there any are there any questions before", "start": 1713.399, "duration": 4.361}, {"text": "I move", "start": 1715.399, "duration": 2.361}, {"text": "on no questions at this time", "start": 1719.399, "duration": 7.481}, {"text": "Bob okay great thanks Cindy so if you do", "start": 1723.159, "duration": 5.721}, {"text": "have any questions um you know feel free", "start": 1726.88, "duration": 3.88}, {"text": "to chime in as we go along we have folks", "start": 1728.88, "duration": 4.12}, {"text": "on on our side who we're going to be who", "start": 1730.76, "duration": 4.639}, {"text": "we going to be mon monitoring the", "start": 1733.0, "duration": 4.44}, {"text": "slack all right so I'm going to talk", "start": 1735.399, "duration": 6.12}, {"text": "talk next about hybrid", "start": 1737.44, "duration": 6.44}, {"text": "applications so many modern parallel", "start": 1741.519, "duration": 4.121}, {"text": "applications are built using a hybrid", "start": 1743.88, "duration": 3.72}, {"text": "approach to take advantage of both", "start": 1745.64, "duration": 4.039}, {"text": "distributed and shared memory and", "start": 1747.6, "duration": 4.16}, {"text": "typically this is going to involve some", "start": 1749.679, "duration": 4.801}, {"text": "combination of MPI and openmp although", "start": 1751.76, "duration": 4.24}, {"text": "other combinations are possible we", "start": 1754.48, "duration": 3.079}, {"text": "talked about", "start": 1756.0, "duration": 4.72}, {"text": "other um other approaches to to", "start": 1757.559, "duration": 5.72}, {"text": "parallelism beside MPI and open MP but", "start": 1760.72, "duration": 4.24}, {"text": "generally your hybrid application is", "start": 1763.279, "duration": 4.441}, {"text": "going to be this this mix of MPI an open", "start": 1764.96, "duration": 6.52}, {"text": "and P the reason we do this is over the", "start": 1767.72, "duration": 6.959}, {"text": "years these nodes have become say fatter", "start": 1771.48, "duration": 6.84}, {"text": "and fatter we now have 128 cores in each", "start": 1774.679, "duration": 5.201}, {"text": "in each of our compute", "start": 1778.32, "duration": 4.92}, {"text": "nodes and we find", "start": 1779.88, "duration": 6.72}, {"text": "that um that say building just straight", "start": 1783.24, "duration": 6.08}, {"text": "MPI applications are a little more", "start": 1786.6, "duration": 4.48}, {"text": "efficient when you were when you were", "start": 1789.32, "duration": 4.839}, {"text": "working within a node so hybrid codes", "start": 1791.08, "duration": 5.079}, {"text": "have this advantage over purely shared", "start": 1794.159, "duration": 3.721}, {"text": "or purely distributed memory", "start": 1796.159, "duration": 3.681}, {"text": "applications so a shared memory", "start": 1797.88, "duration": 4.84}, {"text": "application can have limited scal can", "start": 1799.84, "duration": 5.52}, {"text": "have limited scalability within a", "start": 1802.72, "duration": 5.799}, {"text": "node but it cannot be run at all across", "start": 1805.36, "duration": 5.08}, {"text": "multiple nodes so if it's a shared", "start": 1808.519, "duration": 5.081}, {"text": "memory app say paralyzes you are stuck", "start": 1810.44, "duration": 5.4}, {"text": "running within that node dist", "start": 1813.6, "duration": 3.84}, {"text": "distributed memory applications you", "start": 1815.84, "duration": 2.88}, {"text": "might think this is going to be a little", "start": 1817.44, "duration": 4.239}, {"text": "more General because I could run say an", "start": 1818.72, "duration": 5.679}, {"text": "um MPI application within a node or", "start": 1821.679, "duration": 5.281}, {"text": "across nodes but these distributed", "start": 1824.399, "duration": 4.681}, {"text": "memory apps they may have higher memory", "start": 1826.96, "duration": 4.28}, {"text": "requirements they may have they may have", "start": 1829.08, "duration": 4.56}, {"text": "more overhead when running within node", "start": 1831.24, "duration": 4.36}, {"text": "so we if we're really conern with", "start": 1833.64, "duration": 5.24}, {"text": "performance we may find that's just more", "start": 1835.6, "duration": 6.079}, {"text": "efficient to to run using a combination", "start": 1838.88, "duration": 5.399}, {"text": "of these two two approaches distributed", "start": 1841.679, "duration": 5.761}, {"text": "and Shar shared memory", "start": 1844.279, "duration": 3.161}, {"text": "paralyzation so we're we're going to", "start": 1847.88, "duration": 4.44}, {"text": "strip things down talk about a very very", "start": 1849.84, "duration": 5.719}, {"text": "simplified parallel computer um in this", "start": 1852.32, "duration": 6.319}, {"text": "case our par par computer consists of", "start": 1855.559, "duration": 5.801}, {"text": "just two compute nodes each with 16", "start": 1858.639, "duration": 4.601}, {"text": "cores and they're joined they're they're", "start": 1861.36, "duration": 4.919}, {"text": "joined by a network so on on the left we", "start": 1863.24, "duration": 5.559}, {"text": "have we we show one of the noes think of", "start": 1866.279, "duration": 6.76}, {"text": "that large um large box shaded gray as", "start": 1868.799, "duration": 6.161}, {"text": "as being a node and there's little blue", "start": 1873.039, "duration": 4.561}, {"text": "boxes as being the compute cores within", "start": 1874.96, "duration": 5.199}, {"text": "a node and these are two identical nodes", "start": 1877.6, "duration": 6.799}, {"text": "again connected by that um but by that", "start": 1880.159, "duration": 4.24}, {"text": "interconnect so if we have a message p", "start": 1884.6, "duration": 4.28}, {"text": "an", "start": 1887.36, "duration": 4.08}, {"text": "application", "start": 1888.88, "duration": 6.519}, {"text": "um we would and let's say it's just p p", "start": 1891.44, "duration": 5.68}, {"text": "purely message passing say paralyze", "start": 1895.399, "duration": 3.64}, {"text": "using using", "start": 1897.12, "duration": 7.0}, {"text": "MPI um we typically um we could execute", "start": 1899.039, "duration": 7.52}, {"text": "one process per core and I'm going to", "start": 1904.12, "duration": 4.64}, {"text": "say this is the this is the old style", "start": 1906.559, "duration": 3.96}, {"text": "where we're running just a straight MPI", "start": 1908.76, "duration": 4.12}, {"text": "application in this case we're running", "start": 1910.519, "duration": 6.361}, {"text": "32 processes across 16 cores The Orange", "start": 1912.88, "duration": 6.48}, {"text": "Box indicate indicates a process so we", "start": 1916.88, "duration": 5.08}, {"text": "have one process associated with each of", "start": 1919.36, "duration": 4.799}, {"text": "those", "start": 1921.96, "duration": 2.199}, {"text": "cores if we have a straight threaded", "start": 1924.639, "duration": 6.241}, {"text": "application again paralyzed using open", "start": 1927.639, "duration": 5.841}, {"text": "MP um we're going to be restricted to", "start": 1930.88, "duration": 5.08}, {"text": "running within a single node and this", "start": 1933.48, "duration": 4.039}, {"text": "this example shows a threaded", "start": 1935.96, "duration": 4.679}, {"text": "application single process running 16", "start": 1937.519, "duration": 6.88}, {"text": "threads on 16 cores so here the the the", "start": 1940.639, "duration": 6.841}, {"text": "large Orange Box again is is one process", "start": 1944.399, "duration": 6.041}, {"text": "and it's spanning all of those cores now", "start": 1947.48, "duration": 5.84}, {"text": "I have a caveat at the bottom of the", "start": 1950.44, "duration": 6.079}, {"text": "slide technically any pro programming", "start": 1953.32, "duration": 6.319}, {"text": "model can be mapped to any hardware but", "start": 1956.519, "duration": 5.441}, {"text": "in practice threaded applications are", "start": 1959.639, "duration": 4.081}, {"text": "always run within a single node for", "start": 1961.96, "duration": 3.839}, {"text": "single node for best performance about", "start": 1963.72, "duration": 4.48}, {"text": "eight years ago we had a system where", "start": 1965.799, "duration": 4.681}, {"text": "we're um working with some experimental", "start": 1968.2, "duration": 6.24}, {"text": "software that would allow you to run um", "start": 1970.48, "duration": 6.919}, {"text": "to run threaded applications across", "start": 1974.44, "duration": 5.64}, {"text": "multiple nodes across distributed memory", "start": 1977.399, "duration": 5.081}, {"text": "so again although it could be done it", "start": 1980.08, "duration": 4.36}, {"text": "was terribly terribly inefficient and we", "start": 1982.48, "duration": 4.0}, {"text": "really didn't find any use cases where", "start": 1984.44, "duration": 3.92}, {"text": "where it was where it was", "start": 1986.48, "duration": 4.36}, {"text": "valuable now hybrid applications and", "start": 1988.36, "duration": 3.559}, {"text": "this is what you're going to be seeing", "start": 1990.84, "duration": 3.76}, {"text": "more and more often is a combination of", "start": 1991.919, "duration": 5.24}, {"text": "threads and processes so here's an", "start": 1994.6, "duration": 6.12}, {"text": "example going back to our two node", "start": 1997.159, "duration": 6.681}, {"text": "system each with the 16 cores where we", "start": 2000.72, "duration": 5.839}, {"text": "where we're running four processes two", "start": 2003.84, "duration": 5.0}, {"text": "process on each node and then eight", "start": 2006.559, "duration": 4.521}, {"text": "threads within the process so on the", "start": 2008.84, "duration": 4.88}, {"text": "Node and left on node and left we have", "start": 2011.08, "duration": 4.599}, {"text": "there two processes each running on", "start": 2013.72, "duration": 4.72}, {"text": "eight cores and then when we need to do", "start": 2015.679, "duration": 4.36}, {"text": "the message pass and across nodes we", "start": 2018.44, "duration": 4.359}, {"text": "will go over this", "start": 2020.039, "duration": 6.52}, {"text": "network um a typical scenario is to run", "start": 2022.799, "duration": 5.36}, {"text": "one process per", "start": 2026.559, "duration": 4.881}, {"text": "node using threads within that node and", "start": 2028.159, "duration": 5.561}, {"text": "then message passing across nodes but", "start": 2031.44, "duration": 3.599}, {"text": "we're seeing though that this is", "start": 2033.72, "duration": 3.439}, {"text": "becoming a little less common as the", "start": 2035.039, "duration": 3.841}, {"text": "number of cores on on a on a node", "start": 2037.159, "duration": 3.841}, {"text": "continues to", "start": 2038.88, "duration": 4.48}, {"text": "grow and this is where things get a", "start": 2041.0, "duration": 5.399}, {"text": "little tricky when we have a hybrid", "start": 2043.36, "duration": 6.759}, {"text": "application we can and we and we want to", "start": 2046.399, "duration": 6.24}, {"text": "run across multiple nodes we could do", "start": 2050.119, "duration": 4.641}, {"text": "everything from the situation here on", "start": 2052.639, "duration": 5.28}, {"text": "the left where we have one process per", "start": 2054.76, "duration": 6.919}, {"text": "node and then however many cores we have", "start": 2057.919, "duration": 6.48}, {"text": "within that node that number of threads", "start": 2061.679, "duration": 4.16}, {"text": "all the way over to the Other Extreme", "start": 2064.399, "duration": 2.96}, {"text": "which would be like running a straight", "start": 2065.839, "duration": 3.721}, {"text": "MPI application where we have one", "start": 2067.359, "duration": 5.24}, {"text": "process per core but in reality The", "start": 2069.56, "duration": 5.64}, {"text": "Sweet Spot is going to be somewhere in", "start": 2072.599, "duration": 5.52}, {"text": "between so in the case of our 16 core", "start": 2075.2, "duration": 5.719}, {"text": "nodes anywhere from one process per node", "start": 2078.119, "duration": 5.361}, {"text": "with 16 threads for process down to", "start": 2080.919, "duration": 5.081}, {"text": "using Straight MPI or somewhere in", "start": 2083.48, "duration": 5.0}, {"text": "between say like this middle case where", "start": 2086.0, "duration": 5.839}, {"text": "it's four processes per node times four", "start": 2088.48, "duration": 7.28}, {"text": "cores per four cores per process main", "start": 2091.839, "duration": 6.721}, {"text": "thing is that the you want the number of", "start": 2095.76, "duration": 5.4}, {"text": "processes times the number of cores per", "start": 2098.56, "duration": 5.44}, {"text": "process on each node to to add up sorry", "start": 2101.16, "duration": 7.0}, {"text": "to the product to to equal the number of", "start": 2104.0, "duration": 4.16}, {"text": "cores all right um before we go on any", "start": 2108.24, "duration": 5.64}, {"text": "questions about hybrid", "start": 2110.88, "duration": 3.0}, {"text": "applications all right well I will", "start": 2116.599, "duration": 4.561}, {"text": "continue then so we're going to shift", "start": 2118.52, "duration": 6.2}, {"text": "gears a little bit talk about um TR talk", "start": 2121.16, "duration": 5.12}, {"text": "about the theory going behind going", "start": 2124.72, "duration": 4.0}, {"text": "going behind parallel", "start": 2126.28, "duration": 5.16}, {"text": "Computing so I'm sure many of you have", "start": 2128.72, "duration": 6.24}, {"text": "already heard omd's law that this is the", "start": 2131.44, "duration": 5.639}, {"text": "fundamental law that describes the", "start": 2134.96, "duration": 4.04}, {"text": "absolute limit on the speed up that your", "start": 2137.079, "duration": 4.721}, {"text": "code can get as a function of the", "start": 2139.0, "duration": 4.119}, {"text": "proportion of the code that can be", "start": 2141.8, "duration": 4.319}, {"text": "paralyzed and the number of processes if", "start": 2143.119, "duration": 5.321}, {"text": "you know just one law of parallel", "start": 2146.119, "duration": 4.24}, {"text": "Computing this is the one to know this", "start": 2148.44, "duration": 4.919}, {"text": "is the most fundamental law of parallel", "start": 2150.359, "duration": 6.121}, {"text": "Computing so let's let P be the fraction", "start": 2153.359, "duration": 5.72}, {"text": "of the code that can be paralyzed so", "start": 2156.48, "duration": 5.92}, {"text": "work that can be um you know that that", "start": 2159.079, "duration": 5.321}, {"text": "could that could be divided up shared", "start": 2162.4, "duration": 4.04}, {"text": "shared across shared across multiple", "start": 2164.4, "duration": 5.199}, {"text": "processes or threads and let s be the", "start": 2166.44, "duration": 5.04}, {"text": "fraction of the code that must be run", "start": 2169.599, "duration": 6.24}, {"text": "sequentially and of course the um S Plus", "start": 2171.48, "duration": 7.0}, {"text": "p is going to have to equal to one and N", "start": 2175.839, "duration": 5.081}, {"text": "be the N be the number of processors so", "start": 2178.48, "duration": 4.4}, {"text": "again this is this is very theoretical", "start": 2180.92, "duration": 4.8}, {"text": "very high level we're not getting into", "start": 2182.88, "duration": 5.64}, {"text": "um you know whe whether or not a piece", "start": 2185.72, "duration": 5.24}, {"text": "of code could be paralyzed but rather", "start": 2188.52, "duration": 3.4}, {"text": "just", "start": 2190.96, "duration": 4.96}, {"text": "the um you know what what", "start": 2191.92, "duration": 7.24}, {"text": "what can can be run in", "start": 2195.92, "duration": 6.84}, {"text": "parallel so the speed up as a function", "start": 2199.16, "duration": 5.4}, {"text": "of the number of processes is going to", "start": 2202.76, "duration": 7.04}, {"text": "be 1 over 1 minus P plus p over", "start": 2204.56, "duration": 5.24}, {"text": "n and as the number processors goes to", "start": 2212.599, "duration": 5.72}, {"text": "Infinity the theoretical speed up is", "start": 2216.04, "duration": 4.24}, {"text": "going to depend only on the proportion", "start": 2218.319, "duration": 4.081}, {"text": "of the parallel content so let's look at", "start": 2220.28, "duration": 4.36}, {"text": "om's law again we'll let N Go to", "start": 2222.4, "duration": 4.439}, {"text": "infinity and we could see that this term", "start": 2224.64, "duration": 5.479}, {"text": "here the the second the second half of", "start": 2226.839, "duration": 6.201}, {"text": "the of the denominator P Over N that's", "start": 2230.119, "duration": 5.361}, {"text": "going to vanish um we we take that", "start": 2233.04, "duration": 4.24}, {"text": "parallel content we keep throwing more", "start": 2235.48, "duration": 4.599}, {"text": "and more Hardware at it until um until", "start": 2237.28, "duration": 4.64}, {"text": "it takes the negligible amount of time", "start": 2240.079, "duration": 3.28}, {"text": "and then we're going to be left with", "start": 2241.92, "duration": 4.08}, {"text": "that serial content so the serial", "start": 2243.359, "duration": 5.841}, {"text": "content could could be IO it could be a", "start": 2246.0, "duration": 5.88}, {"text": "bottleneck where we need to collect data", "start": 2249.2, "duration": 5.28}, {"text": "from from across all process and threads", "start": 2251.88, "duration": 4.56}, {"text": "and do some sort of paralyzation", "start": 2254.48, "duration": 3.879}, {"text": "regardless it's that content that we", "start": 2256.44, "duration": 4.96}, {"text": "cannot take and divide and subdivide and", "start": 2258.359, "duration": 5.24}, {"text": "spread across spread across multiple", "start": 2261.4, "duration": 4.6}, {"text": "processes so when you look at lll's law", "start": 2263.599, "duration": 4.601}, {"text": "you say that that doesn't look too bad", "start": 2266.0, "duration": 3.64}, {"text": "you know let's say I just have a little", "start": 2268.2, "duration": 3.68}, {"text": "bit of Serial content I should be able", "start": 2269.64, "duration": 4.04}, {"text": "to get a really good speed up but we're", "start": 2271.88, "duration": 3.8}, {"text": "going to see in the next slide that it", "start": 2273.68, "duration": 5.2}, {"text": "only takes a a little bit seral content", "start": 2275.68, "duration": 5.88}, {"text": "to really um to really limit your", "start": 2278.88, "duration": 7.36}, {"text": "ability to to scale out on a parallel", "start": 2281.56, "duration": 4.68}, {"text": "machine so I'm showing a plot here this", "start": 2288.44, "duration": 5.84}, {"text": "is I have code for this um in in the", "start": 2290.88, "duration": 6.32}, {"text": "GitHub repo again both in the URL that I", "start": 2294.28, "duration": 6.319}, {"text": "had on the opening slide and in", "start": 2297.2, "duration": 7.32}, {"text": "the um and in GitHub repo for the summer", "start": 2300.599, "duration": 6.52}, {"text": "Institute so what we're showing here", "start": 2304.52, "duration": 4.4}, {"text": "is the speed up that you get for your", "start": 2307.119, "duration": 4.601}, {"text": "code as a function of the number of", "start": 2308.92, "duration": 5.8}, {"text": "cores and we did this for different", "start": 2311.72, "duration": 5.72}, {"text": "different degrees of parallel content so", "start": 2314.72, "duration": 6.08}, {"text": "let's say that um half of the work could", "start": 2317.44, "duration": 5.919}, {"text": "be paralyzed the maximum speed up that", "start": 2320.8, "duration": 4.72}, {"text": "we're going to get regardless of how", "start": 2323.359, "duration": 5.281}, {"text": "many cors we run on is going to be two", "start": 2325.52, "duration": 6.2}, {"text": "and if we have 75% parallel content", "start": 2328.64, "duration": 6.959}, {"text": "maximum speed up is going to be four 90%", "start": 2331.72, "duration": 7.24}, {"text": "max speed up of of 10 95% max speed up", "start": 2335.599, "duration": 4.641}, {"text": "of", "start": 2338.96, "duration": 4.32}, {"text": "20 this a good time to add that you", "start": 2340.24, "duration": 5.119}, {"text": "can't look at your", "start": 2343.28, "duration": 7.16}, {"text": "code and say you know no outright how", "start": 2345.359, "duration": 7.121}, {"text": "much parallel content how much parallel", "start": 2350.44, "duration": 3.879}, {"text": "content is going to have you can kind of", "start": 2352.48, "duration": 3.879}, {"text": "estimate this by do by doing", "start": 2354.319, "duration": 4.201}, {"text": "benchmarking um by benchmarking at", "start": 2356.359, "duration": 3.681}, {"text": "different different core", "start": 2358.52, "duration": 4.799}, {"text": "counts um but you know a little a little", "start": 2360.04, "duration": 5.16}, {"text": "a little hard to a little hard to", "start": 2363.319, "duration": 3.881}, {"text": "exactly pin down but the big takeaway", "start": 2365.2, "duration": 4.52}, {"text": "here is that it doesn't take much serial", "start": 2367.2, "duration": 4.56}, {"text": "content to to Li to limit your", "start": 2369.72, "duration": 5.24}, {"text": "scalability to severely limit your", "start": 2371.76, "duration": 5.48}, {"text": "scalability so I added one more data", "start": 2374.96, "duration": 5.119}, {"text": "point on this next slide and that is", "start": 2377.24, "duration": 5.839}, {"text": "having 99% parallel", "start": 2380.079, "duration": 6.641}, {"text": "content and even with 99% of the content", "start": 2383.079, "duration": 6.04}, {"text": "being able to be parallelized the", "start": 2386.72, "duration": 3.92}, {"text": "maximum speed up we're going to be able", "start": 2389.119, "duration": 5.841}, {"text": "get is a factor of 100 so um this is the", "start": 2390.64, "duration": 6.04}, {"text": "same plot that we had on the previous", "start": 2394.96, "duration": 4.08}, {"text": "slide except with the with with one more", "start": 2396.68, "duration": 6.48}, {"text": "curve and note that you need to be aware", "start": 2399.04, "duration": 6.559}, {"text": "of not just the maximum speed up but the", "start": 2403.16, "duration": 3.64}, {"text": "speed up that you're getting at lower", "start": 2405.599, "duration": 3.841}, {"text": "core counts so here we have this this", "start": 2406.8, "duration": 5.799}, {"text": "code I think that's um the upper curve", "start": 2409.44, "duration": 5.6}, {"text": "in in the teal color even with a code", "start": 2412.599, "duration": 3.48}, {"text": "with", "start": 2415.04, "duration": 4.88}, {"text": "99% um paralyzable content even there we", "start": 2416.079, "duration": 7.081}, {"text": "would make terrible use of a full 128", "start": 2419.92, "duration": 6.56}, {"text": "core node so in this case we're running", "start": 2423.16, "duration": 6.52}, {"text": "on 128 cores we would only get a speed", "start": 2426.48, "duration": 6.2}, {"text": "up of a little under a little under 60x", "start": 2429.68, "duration": 4.919}, {"text": "so that's that's actually a terrible", "start": 2432.68, "duration": 4.04}, {"text": "terrible use of that node we should be", "start": 2434.599, "duration": 4.161}, {"text": "running the application further down in", "start": 2436.72, "duration": 7.16}, {"text": "the curve using maybe 64 or even 32", "start": 2438.76, "duration": 5.12}, {"text": "cores okay any questions before I move", "start": 2444.8, "duration": 4.279}, {"text": "on okay if not just have a few more", "start": 2449.96, "duration": 4.56}, {"text": "topics before we wrap", "start": 2452.96, "duration": 4.32}, {"text": "up and we're going to be now discussing", "start": 2454.52, "duration": 4.559}, {"text": "other limits and", "start": 2457.28, "duration": 6.48}, {"text": "scalability so om's law that this sets a", "start": 2459.079, "duration": 7.161}, {"text": "theoretical upper limit on speed up but", "start": 2463.76, "duration": 3.839}, {"text": "there are other factors that are going", "start": 2466.24, "duration": 3.44}, {"text": "to affect your scalability there's going", "start": 2467.599, "duration": 4.201}, {"text": "to be Communications overhead how long", "start": 2469.68, "duration": 4.04}, {"text": "does it take to move data between", "start": 2471.8, "duration": 4.4}, {"text": "between processes across that Network", "start": 2473.72, "duration": 6.32}, {"text": "between nodes um there's going to be um", "start": 2476.2, "duration": 6.28}, {"text": "let limit set by the set by the problem", "start": 2480.04, "duration": 5.0}, {"text": "size you know if if our problem's not", "start": 2482.48, "duration": 5.52}, {"text": "large enough we just don't have enough", "start": 2485.04, "duration": 5.36}, {"text": "units of work to spread it across say", "start": 2488.0, "duration": 4.48}, {"text": "thousands or tens of thousands of of", "start": 2490.4, "duration": 4.199}, {"text": "processes you know a good a good example", "start": 2492.48, "duration": 3.0}, {"text": "I", "start": 2494.599, "duration": 4.321}, {"text": "see um online in introduction to", "start": 2495.48, "duration": 6.04}, {"text": "parallel Computing is say painting a", "start": 2498.92, "duration": 7.159}, {"text": "fence where let's say we have a um 40", "start": 2501.52, "duration": 6.88}, {"text": "foot length length of fence we get one", "start": 2506.079, "duration": 4.601}, {"text": "person painting you know they could", "start": 2508.4, "duration": 4.6}, {"text": "let's say they could accomplish it in", "start": 2510.68, "duration": 5.0}, {"text": "accomplish a job in in 10 hours we get", "start": 2513.0, "duration": 4.48}, {"text": "two people people on there they can kind", "start": 2515.68, "duration": 3.52}, {"text": "of work independently in each half of", "start": 2517.48, "duration": 4.599}, {"text": "their fence they could you know cut cut", "start": 2519.2, "duration": 5.2}, {"text": "cut the runtime in half but eventually", "start": 2522.079, "duration": 3.841}, {"text": "we get to the point where you can get", "start": 2524.4, "duration": 3.12}, {"text": "you can only get so many people on the", "start": 2525.92, "duration": 3.84}, {"text": "fence maybe you could get a 2X speed up", "start": 2527.52, "duration": 4.24}, {"text": "with two painters a 4X speed up with", "start": 2529.76, "duration": 4.72}, {"text": "four painters but on a 40 foot wide", "start": 2531.76, "duration": 5.12}, {"text": "section of fence you can't put a 100", "start": 2534.48, "duration": 4.08}, {"text": "people or a thousand people you just", "start": 2536.88, "duration": 4.16}, {"text": "don't have a large enough problem to", "start": 2538.56, "duration": 4.48}, {"text": "divide up the work that finally um", "start": 2541.04, "duration": 4.12}, {"text": "there's also the issue of uneven load", "start": 2543.04, "duration": 4.36}, {"text": "balancing and then so real life", "start": 2545.16, "duration": 4.679}, {"text": "applications that involve Communications", "start": 2547.4, "duration": 4.199}, {"text": "synchronization you know where threads", "start": 2549.839, "duration": 3.681}, {"text": "or processes must complete their work", "start": 2551.599, "duration": 4.561}, {"text": "before everybody can proceed irregular", "start": 2553.52, "duration": 4.24}, {"text": "problems non-cartesian", "start": 2556.16, "duration": 4.159}, {"text": "grids all of these factors mean that", "start": 2557.76, "duration": 4.799}, {"text": "your speed up can be much less than", "start": 2560.319, "duration": 4.601}, {"text": "what's predicted by omd's", "start": 2562.559, "duration": 5.321}, {"text": "law so in terms of load balancing", "start": 2564.92, "duration": 4.84}, {"text": "paralyzing a code requires dividing the", "start": 2567.88, "duration": 5.439}, {"text": "computational work that can be executed", "start": 2569.76, "duration": 5.799}, {"text": "independently but if the work cannot be", "start": 2573.319, "duration": 5.04}, {"text": "distributed evenly then processors will", "start": 2575.559, "duration": 4.641}, {"text": "sit idle waiting for the longest chunk", "start": 2578.359, "duration": 4.921}, {"text": "to finish so here I'm showing a case", "start": 2580.2, "duration": 6.32}, {"text": "with um running an application on on", "start": 2583.28, "duration": 5.76}, {"text": "four CPUs same thing would apply if", "start": 2586.52, "duration": 4.319}, {"text": "we're running on gpus or if we're", "start": 2589.04, "duration": 3.44}, {"text": "running on more more than", "start": 2590.839, "duration": 5.201}, {"text": "four um the blue bar the blue chunks", "start": 2592.48, "duration": 7.119}, {"text": "indicate when that CPU is busy and the", "start": 2596.04, "duration": 5.92}, {"text": "red and white hash chunks are when that", "start": 2599.599, "duration": 4.641}, {"text": "process is Idle so let's say we start", "start": 2601.96, "duration": 4.44}, {"text": "off we have chunks of work that we", "start": 2604.24, "duration": 5.16}, {"text": "assigned to CPUs 012 and", "start": 2606.4, "duration": 7.56}, {"text": "3 CPU 2 has that largest chunk of work", "start": 2609.4, "duration": 6.56}, {"text": "oh and we also can assume that there are", "start": 2613.96, "duration": 4.28}, {"text": "synchronization points where all of the", "start": 2615.96, "duration": 5.2}, {"text": "tasks must complete their processing", "start": 2618.24, "duration": 4.96}, {"text": "let's say because they need to exchange", "start": 2621.16, "duration": 4.76}, {"text": "newly computed results or threads need", "start": 2623.2, "duration": 5.0}, {"text": "to finish updating a shared array we're", "start": 2625.92, "duration": 3.199}, {"text": "going to assume that there are these", "start": 2628.2, "duration": 3.0}, {"text": "synchronization points that nobody can", "start": 2629.119, "duration": 4.921}, {"text": "go past until all of the processes or", "start": 2631.2, "duration": 5.04}, {"text": "threads have finished so in this case", "start": 2634.04, "duration": 5.039}, {"text": "cpu2 got the largest lar largest chunk", "start": 2636.24, "duration": 4.8}, {"text": "of work and", "start": 2639.079, "duration": 4.24}, {"text": "it's it's gonna sit and it's going to be", "start": 2641.04, "duration": 5.64}, {"text": "waiting until CPU 01 and three finish", "start": 2643.319, "duration": 7.721}, {"text": "their work in fact if we", "start": 2646.68, "duration": 6.72}, {"text": "um you know if we if we could say say", "start": 2651.04, "duration": 4.64}, {"text": "run a movie of this we'd see that CPU 2", "start": 2653.4, "duration": 5.6}, {"text": "finishes first followed by CPU zero so", "start": 2655.68, "duration": 4.84}, {"text": "now they're both sitting idle waiting", "start": 2659.0, "duration": 3.88}, {"text": "for one and three then three finishes", "start": 2660.52, "duration": 3.96}, {"text": "and then finally one now we can", "start": 2662.88, "duration": 3.199}, {"text": "synchronize now we go on to the next", "start": 2664.48, "duration": 4.04}, {"text": "next step we distribute we distribute", "start": 2666.079, "duration": 5.04}, {"text": "the work again again it's uneven and we", "start": 2668.52, "duration": 5.319}, {"text": "may find that that processes or threads", "start": 2671.119, "duration": 6.321}, {"text": "have to wait so this is going to um that", "start": 2673.839, "duration": 5.881}, {"text": "this is going to affect your", "start": 2677.44, "duration": 4.28}, {"text": "scalability in fact I had worked in", "start": 2679.72, "duration": 4.28}, {"text": "application about 10 years ago that took", "start": 2681.72, "duration": 4.72}, {"text": "this to an extreme where the chunks of", "start": 2684.0, "duration": 5.28}, {"text": "work varied anywhere", "start": 2686.44, "duration": 6.44}, {"text": "from from well under a second to", "start": 2689.28, "duration": 7.44}, {"text": "multiple minutes so we had to do some um", "start": 2692.88, "duration": 6.64}, {"text": "um oh say some clever implementation and", "start": 2696.72, "duration": 6.8}, {"text": "paralyzation to take that work and and", "start": 2699.52, "duration": 6.36}, {"text": "um achieve achieve a better a better", "start": 2703.52, "duration": 3.44}, {"text": "load", "start": 2705.88, "duration": 3.32}, {"text": "balancing there's also Communications", "start": 2706.96, "duration": 4.56}, {"text": "overhead so this is going to apply", "start": 2709.2, "duration": 3.84}, {"text": "especially to those of you who are", "start": 2711.52, "duration": 3.48}, {"text": "solving systems of partial differential", "start": 2713.04, "duration": 5.44}, {"text": "equations so if you're doing say cfd or", "start": 2715.0, "duration": 5.68}, {"text": "Magneto hydrodynamics or climate or", "start": 2718.48, "duration": 5.04}, {"text": "weather or any of these other types of", "start": 2720.68, "duration": 5.52}, {"text": "simulations that involve solving pte on", "start": 2723.52, "duration": 5.16}, {"text": "a grid this is going to be a very common", "start": 2726.2, "duration": 4.76}, {"text": "situation so let's say that we're and", "start": 2728.68, "duration": 4.48}, {"text": "this is unrealistically small problem", "start": 2730.96, "duration": 4.2}, {"text": "but let's say that we're working on a", "start": 2733.16, "duration": 5.12}, {"text": "16x 16 grid and we can divide this into", "start": 2735.16, "duration": 5.439}, {"text": "four 8 by8 chunks and we're going to", "start": 2738.28, "duration": 5.4}, {"text": "distribute them across across processes", "start": 2740.599, "duration": 6.401}, {"text": "so process 0 one two and three each of", "start": 2743.68, "duration": 5.12}, {"text": "them is going to get a chunk of that", "start": 2747.0, "duration": 5.28}, {"text": "problem say in 8 by8 chunk out of that", "start": 2748.8, "duration": 6.08}, {"text": "out of that 16 by 16", "start": 2752.28, "duration": 5.279}, {"text": "grid", "start": 2754.88, "duration": 4.12}, {"text": "and when we're when we're solving these", "start": 2757.559, "duration": 5.081}, {"text": "systems of pdes typically we need to we", "start": 2759.0, "duration": 5.64}, {"text": "we need to get access to", "start": 2762.64, "duration": 4.959}, {"text": "values um say for temperatures pressure", "start": 2764.64, "duration": 5.64}, {"text": "salinity um you know chemical", "start": 2767.599, "duration": 4.72}, {"text": "composition and all of the all of the", "start": 2770.28, "duration": 4.76}, {"text": "neighboring cells so for cells that are", "start": 2772.319, "duration": 5.361}, {"text": "within the interior of each chunk", "start": 2775.04, "duration": 4.079}, {"text": "calculations can be done within a", "start": 2777.68, "duration": 4.439}, {"text": "process so I've highlighted one red cell", "start": 2779.119, "duration": 4.561}, {"text": "in in each of these um each of these", "start": 2782.119, "duration": 4.081}, {"text": "four chunks and you can see that the", "start": 2783.68, "duration": 4.24}, {"text": "that the neighbors that you need are all", "start": 2786.2, "duration": 4.159}, {"text": "contained within within their", "start": 2787.92, "duration": 4.919}, {"text": "chunk", "start": 2790.359, "duration": 5.72}, {"text": "but for cells at the", "start": 2792.839, "duration": 6.401}, {"text": "boundaries they are going to need data", "start": 2796.079, "duration": 4.561}, {"text": "belonging to but belonging to neighbor", "start": 2799.24, "duration": 3.76}, {"text": "and processes so to accommodate this the", "start": 2800.64, "duration": 5.04}, {"text": "code is going to be written to put a Hal", "start": 2803.0, "duration": 4.559}, {"text": "of what they call ghost cells around the", "start": 2805.68, "duration": 3.76}, {"text": "chunk and now we need to communicate", "start": 2807.559, "duration": 4.601}, {"text": "data between processes and here the data", "start": 2809.44, "duration": 4.56}, {"text": "movement is going to depend on both the", "start": 2812.16, "duration": 4.0}, {"text": "latency and the bandwidth the network", "start": 2814.0, "duration": 3.359}, {"text": "and this is going to introduce", "start": 2816.16, "duration": 2.84}, {"text": "Communications", "start": 2817.359, "duration": 5.121}, {"text": "overheads so before before we um before", "start": 2819.0, "duration": 5.319}, {"text": "we update the values of this cells", "start": 2822.48, "duration": 4.68}, {"text": "around the around the edge of each chunk", "start": 2824.319, "duration": 4.76}, {"text": "we're going to need to exchange columns", "start": 2827.16, "duration": 3.88}, {"text": "or rows of", "start": 2829.079, "duration": 4.961}, {"text": "data we show shown", "start": 2831.04, "duration": 6.24}, {"text": "here ohop", "start": 2834.04, "duration": 3.24}, {"text": "sorry yeah so we're going to need to to", "start": 2838.04, "duration": 6.079}, {"text": "um exchange entire columns rows of data", "start": 2841.44, "duration": 6.8}, {"text": "so the dark let's let's look at the um", "start": 2844.119, "duration": 5.161}, {"text": "top", "start": 2848.24, "duration": 4.079}, {"text": "row this this column column cells in", "start": 2849.28, "duration": 6.4}, {"text": "dark blue is going to have to be passed", "start": 2852.319, "duration": 6.641}, {"text": "communicated to the um to to to the", "start": 2855.68, "duration": 5.399}, {"text": "neighboring process and it's going to", "start": 2858.96, "duration": 4.32}, {"text": "fill a column of the ghost cells", "start": 2861.079, "duration": 4.081}, {"text": "similarly with the rows and and the", "start": 2863.28, "duration": 3.799}, {"text": "other processes again don't need to get", "start": 2865.16, "duration": 3.679}, {"text": "into the details but you just need to be", "start": 2867.079, "duration": 3.361}, {"text": "aware that there is going to be this", "start": 2868.839, "duration": 3.76}, {"text": "Communications", "start": 2870.44, "duration": 4.32}, {"text": "overhead so by now you're probably", "start": 2872.599, "duration": 4.401}, {"text": "thinking you know how how does anybody", "start": 2874.76, "duration": 4.079}, {"text": "ever really use a use a parallel", "start": 2877.0, "duration": 3.76}, {"text": "computer effectively you know we're", "start": 2878.839, "duration": 3.441}, {"text": "we're all we already have that hard", "start": 2880.76, "duration": 4.88}, {"text": "limit imply imposed by OMD doll's law", "start": 2882.28, "duration": 5.0}, {"text": "and again that's just a theoretical", "start": 2885.64, "duration": 4.04}, {"text": "upper limit that doesn't say anything", "start": 2887.28, "duration": 5.279}, {"text": "about um doesn't take in account load", "start": 2889.68, "duration": 7.32}, {"text": "balancing or um or or Communications", "start": 2892.559, "duration": 6.961}, {"text": "overhead or any of the other factors", "start": 2897.0, "duration": 4.28}, {"text": "that that come into play with the", "start": 2899.52, "duration": 3.52}, {"text": "paralyzation of your algorithm so how", "start": 2901.28, "duration": 5.36}, {"text": "does anybody ever use all the cores on", "start": 2903.04, "duration": 6.36}, {"text": "on a single modern computer compute Noe", "start": 2906.64, "duration": 5.04}, {"text": "let alone the full power of a large", "start": 2909.4, "duration": 4.56}, {"text": "supercomputer so the reality first of", "start": 2911.68, "duration": 4.48}, {"text": "all is that most parallel applications", "start": 2913.96, "duration": 4.48}, {"text": "do not scale to thousands or even", "start": 2916.16, "duration": 4.199}, {"text": "hundreds of cores you're often going to", "start": 2918.44, "duration": 4.48}, {"text": "be running within a node or on on a", "start": 2920.359, "duration": 4.72}, {"text": "small number of nodes but there are", "start": 2922.92, "duration": 4.76}, {"text": "still applications that do that do scale", "start": 2925.079, "duration": 5.081}, {"text": "to these thousands cores so they have a", "start": 2927.68, "duration": 5.879}, {"text": "couple of strategies to um to handle", "start": 2930.16, "duration": 6.0}, {"text": "this first of all they'll off often grow", "start": 2933.559, "duration": 4.481}, {"text": "the problem size with the number of", "start": 2936.16, "duration": 5.88}, {"text": "cores or nodes so rather than trying to", "start": 2938.04, "duration": 6.36}, {"text": "um solve a fix size problem using more", "start": 2942.04, "duration": 4.2}, {"text": "and more cores they'll they'll go out", "start": 2944.4, "duration": 4.04}, {"text": "and they'll solve a larger problem so", "start": 2946.24, "duration": 3.76}, {"text": "going back to the fence", "start": 2948.44, "duration": 4.399}, {"text": "analogy um you might", "start": 2950.0, "duration": 5.48}, {"text": "not know how I would put it you you you", "start": 2952.839, "duration": 4.841}, {"text": "wouldn't you wouldn't hire a thousand a", "start": 2955.48, "duration": 4.4}, {"text": "thousand people to paint a 40 foot", "start": 2957.68, "duration": 4.399}, {"text": "section of fence but you might hire a", "start": 2959.88, "duration": 6.88}, {"text": "thousand people to um to paint a a mile", "start": 2962.079, "duration": 7.321}, {"text": "long stretch of fence so you start", "start": 2966.76, "duration": 4.76}, {"text": "working on larger problems you'll see", "start": 2969.4, "duration": 3.76}, {"text": "this a lot in climate and weather", "start": 2971.52, "duration": 3.92}, {"text": "simulations instead of using say a", "start": 2973.16, "duration": 4.199}, {"text": "particular grid size they'll use a much", "start": 2975.44, "duration": 3.879}, {"text": "much finer grid now they have a larger", "start": 2977.359, "duration": 4.2}, {"text": "problem that could map onto a onto a", "start": 2979.319, "duration": 4.641}, {"text": "larger supercomputer you can overlap", "start": 2981.559, "duration": 4.721}, {"text": "Communications with computation you can", "start": 2983.96, "duration": 3.599}, {"text": "so for example you could have", "start": 2986.28, "duration": 3.72}, {"text": "communications going on at the same time", "start": 2987.559, "duration": 4.921}, {"text": "that you're also doing doing useful work", "start": 2990.0, "duration": 4.559}, {"text": "you could do Dynamic load balancing to", "start": 2992.48, "duration": 4.96}, {"text": "assign work to cars as they become idle", "start": 2994.559, "duration": 5.481}, {"text": "and you can also increase the ratio of", "start": 2997.44, "duration": 5.639}, {"text": "the computation and Communications so if", "start": 3000.04, "duration": 5.68}, {"text": "we were doing", "start": 3003.079, "duration": 6.121}, {"text": "um you know if we're if we have a cfd", "start": 3005.72, "duration": 5.44}, {"text": "application you know that those", "start": 3009.2, "duration": 3.76}, {"text": "computational flow dynamics that they", "start": 3011.16, "duration": 3.36}, {"text": "scale well but eventually they'll run", "start": 3012.96, "duration": 4.24}, {"text": "out of steam because you're doing you", "start": 3014.52, "duration": 4.319}, {"text": "know relatively large amount of", "start": 3017.2, "duration": 3.639}, {"text": "communications compared to the amount of", "start": 3018.839, "duration": 3.801}, {"text": "work within those cells but if we do", "start": 3020.839, "duration": 4.161}, {"text": "something like reactive flow where we", "start": 3022.64, "duration": 3.84}, {"text": "actually need to solve the chemistry", "start": 3025.0, "duration": 3.24}, {"text": "within each of their cells we've now", "start": 3026.48, "duration": 3.72}, {"text": "increased the ratio of that computation", "start": 3028.24, "duration": 4.64}, {"text": "to the to the", "start": 3030.2, "duration": 2.68}, {"text": "communications okay so have one more", "start": 3032.92, "duration": 5.879}, {"text": "topic be before we wrap up and this is", "start": 3036.52, "duration": 3.799}, {"text": "going to be on on running parallel", "start": 3038.799, "duration": 4.52}, {"text": "applications and doing scaling", "start": 3040.319, "duration": 5.361}, {"text": "studies so when we run a parallel", "start": 3043.319, "duration": 4.601}, {"text": "application um so far we' cover the", "start": 3045.68, "duration": 4.28}, {"text": "basics of parallel Computing we talked", "start": 3047.92, "duration": 4.12}, {"text": "about hardware and threads and processes", "start": 3049.96, "duration": 4.56}, {"text": "and hybrid applications and how we", "start": 3052.04, "duration": 4.88}, {"text": "Implement paralyzation say using MPI and", "start": 3054.52, "duration": 4.4}, {"text": "openmp and om doll's law and other", "start": 3056.92, "duration": 3.879}, {"text": "factors that affect", "start": 3058.92, "duration": 4.24}, {"text": "scalability so all this Theory and", "start": 3060.799, "duration": 4.56}, {"text": "background is great but how do we know", "start": 3063.16, "duration": 4.6}, {"text": "how many CPUs or gpus to use when we're", "start": 3065.359, "duration": 4.321}, {"text": "running our parallel application say", "start": 3067.76, "duration": 4.4}, {"text": "let's go back to Aldo's law you know", "start": 3069.68, "duration": 4.6}, {"text": "it's nice that we have this um", "start": 3072.16, "duration": 4.88}, {"text": "fundamental law that tells us what speed", "start": 3074.28, "duration": 4.92}, {"text": "up we get as a function of the parallel", "start": 3077.04, "duration": 5.559}, {"text": "or seral content but it's very hard to", "start": 3079.2, "duration": 5.639}, {"text": "look at your application and figure out", "start": 3082.599, "duration": 5.0}, {"text": "what the values are going to be for", "start": 3084.839, "duration": 5.801}, {"text": "pns so the only way to definitively", "start": 3087.599, "duration": 6.48}, {"text": "answer this question how many cores CPUs", "start": 3090.64, "duration": 5.679}, {"text": "GPU should be running on is to perform", "start": 3094.079, "duration": 4.881}, {"text": "what we call a scaling study and we need", "start": 3096.319, "duration": 5.48}, {"text": "to do this with a representative problem", "start": 3098.96, "duration": 4.599}, {"text": "that's run on different number of", "start": 3101.799, "duration": 3.481}, {"text": "processors and when we say", "start": 3103.559, "duration": 4.121}, {"text": "representative we mean one with the same", "start": 3105.28, "duration": 4.519}, {"text": "size say grid dimensions number of", "start": 3107.68, "duration": 5.28}, {"text": "particles images genomes and complexity", "start": 3109.799, "duration": 4.841}, {"text": "you know the level of theory for", "start": 3112.96, "duration": 3.839}, {"text": "computational chemistry calculation the", "start": 3114.64, "duration": 4.36}, {"text": "type of analysis the physics that we", "start": 3116.799, "duration": 5.04}, {"text": "include um the approximations that we", "start": 3119.0, "duration": 5.799}, {"text": "make so this represented problem has to", "start": 3121.839, "duration": 4.76}, {"text": "look like the research problems that you", "start": 3124.799, "duration": 3.921}, {"text": "want to", "start": 3126.599, "duration": 6.041}, {"text": "solve so we see this a lot and and", "start": 3128.72, "duration": 6.24}, {"text": "applications for computer time where we", "start": 3132.64, "duration": 5.88}, {"text": "will get a scaling curve showing showing", "start": 3134.96, "duration": 6.08}, {"text": "runtime as a function of the number of", "start": 3138.52, "duration": 4.36}, {"text": "cores um what we're doing here is", "start": 3141.04, "duration": 3.44}, {"text": "something called strong scaling we have", "start": 3142.88, "duration": 4.84}, {"text": "a fixed problem that we run on", "start": 3144.48, "duration": 5.44}, {"text": "increasingly large number of cores", "start": 3147.72, "duration": 6.24}, {"text": "Believe It or Not these are um the the", "start": 3149.92, "duration": 6.08}, {"text": "these codes have very very different", "start": 3153.96, "duration": 4.52}, {"text": "scaling Behavior but when I present it", "start": 3156.0, "duration": 5.28}, {"text": "like this when I show it on linear scale", "start": 3158.48, "duration": 5.56}, {"text": "on linear axes time versus cores they", "start": 3161.28, "duration": 5.079}, {"text": "look", "start": 3164.04, "duration": 2.319}, {"text": "indistinguishable but if we do this the", "start": 3166.559, "duration": 5.481}, {"text": "right way and we and we plot the results", "start": 3168.88, "duration": 6.64}, {"text": "on log axis this gives us a lot more", "start": 3172.04, "duration": 6.96}, {"text": "sight so we have um log axis for time", "start": 3175.52, "duration": 7.0}, {"text": "log AIS for the for for the number of", "start": 3179.0, "duration": 7.0}, {"text": "cores and we can see that on the", "start": 3182.52, "duration": 6.52}, {"text": "left that this is a code that code that", "start": 3186.0, "duration": 5.44}, {"text": "has perfect scalability I'm showing here", "start": 3189.04, "duration": 4.36}, {"text": "in red that the runtime is a function", "start": 3191.44, "duration": 4.919}, {"text": "number of cores get a straight line I've", "start": 3193.4, "duration": 6.64}, {"text": "included that this black bar um as as a", "start": 3196.359, "duration": 5.521}, {"text": "guide to to show you what what linear", "start": 3200.04, "duration": 4.4}, {"text": "scaling would be so it's entirely", "start": 3201.88, "duration": 5.0}, {"text": "possible that this red line is going to", "start": 3204.44, "duration": 4.359}, {"text": "be linear but it would have a different", "start": 3206.88, "duration": 5.04}, {"text": "slope um so it would be shallower than", "start": 3208.799, "duration": 5.52}, {"text": "than than linear", "start": 3211.92, "duration": 4.439}, {"text": "scaling another thing that you might", "start": 3214.319, "duration": 6.04}, {"text": "might find find useful is if you include", "start": 3216.359, "duration": 6.601}, {"text": "the in include the parallel efficiency", "start": 3220.359, "duration": 5.081}, {"text": "on the on the right Axis so parallel", "start": 3222.96, "duration": 5.24}, {"text": "efficiency is just a measure of your", "start": 3225.44, "duration": 5.2}, {"text": "speed up relative to the number of cores", "start": 3228.2, "duration": 7.119}, {"text": "for example if I um if I run on 64 cores", "start": 3230.64, "duration": 8.12}, {"text": "and it's 64 times faster that gives us a", "start": 3235.319, "duration": 6.48}, {"text": "parallel efficiency of 100% now on the", "start": 3238.76, "duration": 7.44}, {"text": "right and this is indeed the same data", "start": 3241.799, "duration": 6.04}, {"text": "that we had that we had in the previous", "start": 3246.2, "duration": 3.56}, {"text": "slide where where the left and right", "start": 3247.839, "duration": 4.601}, {"text": "plots are indistinguishable on the right", "start": 3249.76, "duration": 6.16}, {"text": "now we see that we start off with with", "start": 3252.44, "duration": 6.32}, {"text": "PR with pretty good parallel efficiency", "start": 3255.92, "duration": 7.28}, {"text": "um if we plot the um yeah we're we're", "start": 3258.76, "duration": 6.599}, {"text": "getting good scaling we're we're pretty", "start": 3263.2, "duration": 5.04}, {"text": "close to to linear at at small core", "start": 3265.359, "duration": 5.641}, {"text": "counts um parallel efficiency is up here", "start": 3268.24, "duration": 7.319}, {"text": "at close to 90% and eventually it drops", "start": 3271.0, "duration": 8.0}, {"text": "off so where should I be on the scaling", "start": 3275.559, "duration": 6.121}, {"text": "curve so if your work is not", "start": 3279.0, "duration": 4.559}, {"text": "particularly sensitive to the time to", "start": 3281.68, "duration": 4.6}, {"text": "complete a single one consider using CPU", "start": 3283.559, "duration": 4.961}, {"text": "or GPU counts that are close or very", "start": 3286.28, "duration": 4.92}, {"text": "close to 100% efficiency even if that", "start": 3288.52, "duration": 5.4}, {"text": "means running multiple instances on a", "start": 3291.2, "duration": 5.2}, {"text": "single core and this is going to make", "start": 3293.92, "duration": 5.0}, {"text": "make make a lot of sense if you have", "start": 3296.4, "duration": 4.52}, {"text": "parameter sweep workloads say where if", "start": 3298.92, "duration": 3.56}, {"text": "you're a computational chemist or a", "start": 3300.92, "duration": 3.679}, {"text": "molecular molecular biologist and you", "start": 3302.48, "duration": 4.2}, {"text": "need to do the same kind of calculation", "start": 3304.599, "duration": 5.24}, {"text": "many times I'm going to say run out here", "start": 3306.68, "duration": 5.36}, {"text": "on these small core counts try to keep", "start": 3309.839, "duration": 6.881}, {"text": "your parallel efficiency over um o over", "start": 3312.04, "duration": 4.68}, {"text": "80% now sometimes it's okay to go a", "start": 3317.839, "duration": 4.561}, {"text": "little bit further out on the scaling", "start": 3320.92, "duration": 4.399}, {"text": "curve so if the time to solution for", "start": 3322.4, "duration": 5.48}, {"text": "that particular task is important or if", "start": 3325.319, "duration": 4.52}, {"text": "it would just take an unreasonably long", "start": 3327.88, "duration": 4.64}, {"text": "time at lower core counts in that case", "start": 3329.839, "duration": 4.601}, {"text": "you may even may want to to run at a", "start": 3332.52, "duration": 3.839}, {"text": "little bit lower parallel efficiency but", "start": 3334.44, "duration": 4.04}, {"text": "you should have a good justification for", "start": 3336.359, "duration": 4.281}, {"text": "this um you would definitely want to do", "start": 3338.48, "duration": 3.76}, {"text": "this if your code does not have", "start": 3340.64, "duration": 3.8}, {"text": "checkpoint restart capabilities that", "start": 3342.24, "duration": 3.799}, {"text": "means that you can write out the state", "start": 3344.44, "duration": 4.28}, {"text": "of the system this is very common for", "start": 3346.039, "duration": 4.28}, {"text": "climate and weather codes and", "start": 3348.72, "duration": 4.16}, {"text": "computational flow Dynamics where you", "start": 3350.319, "duration": 4.921}, {"text": "can after a particular time step you", "start": 3352.88, "duration": 4.159}, {"text": "write out the data so that you could", "start": 3355.24, "duration": 4.52}, {"text": "then read that in later and restart but", "start": 3357.039, "duration": 4.481}, {"text": "if you don't have those checkpoint", "start": 3359.76, "duration": 3.839}, {"text": "checkpoint restart capabilities and the", "start": 3361.52, "duration": 4.2}, {"text": "run time would exceed the Q or the", "start": 3363.599, "duration": 3.921}, {"text": "partition limits then you'll have no", "start": 3365.72, "duration": 3.639}, {"text": "choice except to run at a little bit", "start": 3367.52, "duration": 4.4}, {"text": "higher core", "start": 3369.359, "duration": 2.561}, {"text": "counts sometimes it's okay to run at", "start": 3372.0, "duration": 5.52}, {"text": "even poorer parallel", "start": 3375.72, "duration": 5.16}, {"text": "efficiency and that's if the um if time", "start": 3377.52, "duration": 6.4}, {"text": "to solution is absolutely critical so", "start": 3380.88, "duration": 4.959}, {"text": "let's say that you're doing calculations", "start": 3383.92, "duration": 3.6}, {"text": "that need to be run on a regular", "start": 3385.839, "duration": 4.921}, {"text": "schedule say data is collected every day", "start": 3387.52, "duration": 5.12}, {"text": "during the day and we need to process", "start": 3390.76, "duration": 4.92}, {"text": "this overnight um a good example is the", "start": 3392.64, "duration": 5.56}, {"text": "use of our previous supercomputer Comet", "start": 3395.68, "duration": 4.2}, {"text": "which we Now operate for the scripts", "start": 3398.2, "duration": 4.0}, {"text": "institution of oceanography they use", "start": 3399.88, "duration": 4.08}, {"text": "that during the rainy season to do", "start": 3402.2, "duration": 4.8}, {"text": "atmospheric River calculation oh sorry", "start": 3403.96, "duration": 5.72}, {"text": "simulations of atmospheric Rivers based", "start": 3407.0, "duration": 5.039}, {"text": "on recent satellite data and they use", "start": 3409.68, "duration": 4.28}, {"text": "that to make a prediction of rainfall", "start": 3412.039, "duration": 4.32}, {"text": "throughout the West so that's case where", "start": 3413.96, "duration": 4.599}, {"text": "you know it's really critical that we", "start": 3416.359, "duration": 4.401}, {"text": "process that data each day and they may", "start": 3418.559, "duration": 4.8}, {"text": "run at a lower parallel efficiency", "start": 3420.76, "duration": 4.0}, {"text": "because it's just important that they", "start": 3423.359, "duration": 4.121}, {"text": "get it done or in the extreme", "start": 3424.76, "duration": 5.279}, {"text": "case if we need you know really really", "start": 3427.48, "duration": 4.359}, {"text": "quick turnaround we don't use our", "start": 3430.039, "duration": 5.0}, {"text": "systems for for these kind of essential", "start": 3431.839, "duration": 6.081}, {"text": "operational tasks but say tornado or", "start": 3435.039, "duration": 6.08}, {"text": "tsunami predictions in that case it is", "start": 3437.92, "duration": 6.0}, {"text": "so important that we get that solution", "start": 3441.119, "duration": 4.48}, {"text": "as quickly as possible possible that we", "start": 3443.92, "duration": 3.36}, {"text": "should just keep using more and more", "start": 3445.599, "duration": 4.76}, {"text": "cores as long as the runtime doesn't", "start": 3447.28, "duration": 7.88}, {"text": "increase as we as we um use more", "start": 3450.359, "duration": 4.801}, {"text": "cores um some other considerations I'll", "start": 3455.48, "duration": 4.639}, {"text": "just kind of run over this very quickly", "start": 3457.88, "duration": 4.6}, {"text": "sometimes you'll need to use more cores", "start": 3460.119, "duration": 4.401}, {"text": "in order to in order to get in order to", "start": 3462.48, "duration": 6.639}, {"text": "get more memory um know on on expense we", "start": 3464.52, "duration": 9.279}, {"text": "have 2 gabt of memory per core but let's", "start": 3469.119, "duration": 7.281}, {"text": "say that you just have a very large", "start": 3473.799, "duration": 3.641}, {"text": "memory", "start": 3476.4, "duration": 3.159}, {"text": "footprint you would already be charged", "start": 3477.44, "duration": 4.44}, {"text": "for those cores so you might so you", "start": 3479.559, "duration": 5.121}, {"text": "might as well use them um you know it's", "start": 3481.88, "duration": 3.8}, {"text": "you're you're going to be getting a", "start": 3484.68, "duration": 2.679}, {"text": "little bit poor scaling but you could", "start": 3485.68, "duration": 4.84}, {"text": "justify using more cores to get more", "start": 3487.359, "duration": 5.72}, {"text": "memory so with that I'm going to I'm", "start": 3490.52, "duration": 3.92}, {"text": "going to wrap things up and hopefully", "start": 3493.079, "duration": 3.52}, {"text": "take a few questions you know we've", "start": 3494.44, "duration": 3.639}, {"text": "really just scratched the surface of", "start": 3496.599, "duration": 4.76}, {"text": "parallel Computing um but I think you", "start": 3498.079, "duration": 5.641}, {"text": "you got access to to the to to the key", "start": 3501.359, "duration": 4.68}, {"text": "points there are many other training", "start": 3503.72, "duration": 5.48}, {"text": "resources though if you go to um x.org", "start": 3506.039, "duration": 7.52}, {"text": "and this is going to be um no long no", "start": 3509.2, "duration": 6.52}, {"text": "longer active soon but access the follow", "start": 3513.559, "duration": 4.04}, {"text": "on program to exceed will have their own", "start": 3515.72, "duration": 5.56}, {"text": "training Pages eventually um we have a", "start": 3517.599, "duration": 7.121}, {"text": "lot of materials here on the stsc", "start": 3521.28, "duration": 6.36}, {"text": "website um if you are using the", "start": 3524.72, "duration": 5.599}, {"text": "nationally allocated systems expans and", "start": 3527.64, "duration": 5.56}, {"text": "bridges to and Stampede and others um", "start": 3530.319, "duration": 5.321}, {"text": "each of those will have um what will", "start": 3533.2, "duration": 4.44}, {"text": "have their own user guides and they're", "start": 3535.64, "duration": 3.719}, {"text": "also being tracked through the exed", "start": 3537.64, "duration": 4.679}, {"text": "portal with resource monitor again exced", "start": 3539.359, "duration": 5.041}, {"text": "is going away going away very soon but", "start": 3542.319, "duration": 5.601}, {"text": "access will be picking up these to these", "start": 3544.4, "duration": 6.28}, {"text": "duties so in conclusion parallel", "start": 3547.92, "duration": 5.56}, {"text": "computers for everyone who has a problem", "start": 3550.68, "duration": 5.32}, {"text": "that they um that they need to solve", "start": 3553.48, "duration": 4.599}, {"text": "that doesn't fit on their local", "start": 3556.0, "duration": 4.48}, {"text": "resources you generally don't need to be", "start": 3558.079, "duration": 4.401}, {"text": "a programmer but you do need to know the", "start": 3560.48, "duration": 3.879}, {"text": "fundamentals in order to effectively use", "start": 3562.48, "duration": 4.68}, {"text": "the parallel computers we talked about", "start": 3564.359, "duration": 4.841}, {"text": "processes and threads just remember that", "start": 3567.16, "duration": 4.639}, {"text": "a process is an instance of a program a", "start": 3569.2, "duration": 4.399}, {"text": "thread is run within a process and", "start": 3571.799, "duration": 4.841}, {"text": "access shared data and MPI and openmp", "start": 3573.599, "duration": 5.76}, {"text": "are the but most commonly used", "start": 3576.64, "duration": 5.32}, {"text": "approaches to paralyzing codes talked", "start": 3579.359, "duration": 4.44}, {"text": "about omd's law it gives you that upper", "start": 3581.96, "duration": 3.639}, {"text": "limit and scalability but there are", "start": 3583.799, "duration": 4.161}, {"text": "other factors that infect scalability", "start": 3585.599, "duration": 3.641}, {"text": "such as load and balance and", "start": 3587.96, "duration": 3.96}, {"text": "Communications overhead and finally know", "start": 3589.24, "duration": 4.559}, {"text": "how to display your scaling data and", "start": 3591.92, "duration": 3.399}, {"text": "choose core count", "start": 3593.799, "duration": 3.8}, {"text": "when we see applications for for", "start": 3595.319, "duration": 6.081}, {"text": "computer time um this is", "start": 3597.599, "duration": 6.24}, {"text": "often this is often the weakest part of", "start": 3601.4, "duration": 4.24}, {"text": "The Proposal", "start": 3603.839, "duration": 4.96}, {"text": "so you know to display to display your", "start": 3605.64, "duration": 6.08}, {"text": "your scaling data you use using log axes", "start": 3608.799, "duration": 4.921}, {"text": "provide tables that we know what what", "start": 3611.72, "duration": 4.079}, {"text": "your efficiency is at each core count", "start": 3613.72, "duration": 4.639}, {"text": "and give us a strong justification for", "start": 3615.799, "duration": 4.56}, {"text": "why you were choosing to run each of the", "start": 3618.359, "duration": 5.24}, {"text": "calculations on the that number of cor", "start": 3620.359, "duration": 7.2}, {"text": "stated and that is it um and just f one", "start": 3623.599, "duration": 6.24}, {"text": "final reminder that I have code that was", "start": 3627.559, "duration": 5.04}, {"text": "used to generate these figures um in my", "start": 3629.839, "duration": 7.96}, {"text": "in my GitHub repo and in the um in the", "start": 3632.599, "duration": 8.281}, {"text": "Institute um the summer Institute repo", "start": 3637.799, "duration": 5.04}, {"text": "and in fact you could you you could use", "start": 3640.88, "duration": 3.28}, {"text": "my use my", "start": 3642.839, "duration": 4.641}, {"text": "notebooks to um to generate the the", "start": 3644.16, "duration": 5.8}, {"text": "scaling curves for for for your own data", "start": 3647.48, "duration": 3.879}, {"text": "and with that I'm going to conclude and", "start": 3649.96, "duration": 2.839}, {"text": "we have I think just a couple minutes", "start": 3651.359, "duration": 2.161}, {"text": "for", "start": 3652.799, "duration": 3.721}, {"text": "questions", "start": 3653.52, "duration": 3.0}]