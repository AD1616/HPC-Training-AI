[{"text": "hi everybody and I got some stuff on my", "start": 2.399, "duration": 5.161}, {"text": "screen", "start": 5.64, "duration": 4.98}, {"text": "okay so today I'm gonna do uh it's kind", "start": 7.56, "duration": 4.44}, {"text": "of a broad talk I know I have a broad", "start": 10.62, "duration": 4.08}, {"text": "audience I think so I'm going to", "start": 12.0, "duration": 5.16}, {"text": "um do sort of a broad introduction and", "start": 14.7, "duration": 4.32}, {"text": "try to get into some details and then", "start": 17.16, "duration": 4.26}, {"text": "try to get into you know an example of a", "start": 19.02, "duration": 4.86}, {"text": "deep learning with convolution networks", "start": 21.42, "duration": 4.38}, {"text": "and then talk about how we run this on", "start": 23.88, "duration": 4.739}, {"text": "HPC and it's going to be a lot of stuff", "start": 25.8, "duration": 4.559}, {"text": "I got packed in here it's usually a", "start": 28.619, "duration": 3.78}, {"text": "longer talk we we usually do these", "start": 30.359, "duration": 3.781}, {"text": "workshops with um", "start": 32.399, "duration": 3.621}, {"text": "what I'm going to do today is usually", "start": 34.14, "duration": 5.4}, {"text": "includes tutorials and and maybe some", "start": 36.02, "duration": 4.6}, {"text": "demos", "start": 39.54, "duration": 3.3}, {"text": "that we that we run for longer so this", "start": 40.62, "duration": 5.7}, {"text": "will be um some some parts I hope uh", "start": 42.84, "duration": 5.76}, {"text": "I'll go try to go through quickly and if", "start": 46.32, "duration": 5.3}, {"text": "it's too quickly let me know", "start": 48.6, "duration": 3.02}, {"text": "all right so just my quick outline I'm", "start": 54.3, "duration": 5.279}, {"text": "going to do an overview talk about", "start": 58.14, "duration": 3.239}, {"text": "neural networks or you know originally", "start": 59.579, "duration": 3.061}, {"text": "they've been called multi-layer", "start": 61.379, "duration": 2.64}, {"text": "perceptrons", "start": 62.64, "duration": 3.659}, {"text": "uh convolution neural networks and talk", "start": 64.019, "duration": 3.541}, {"text": "a little bit about how they work when", "start": 66.299, "duration": 3.901}, {"text": "they're scaling I won't probably won't", "start": 67.56, "duration": 4.62}, {"text": "do the demo but I will show what a demo", "start": 70.2, "duration": 4.26}, {"text": "looks like for a convolution network", "start": 72.18, "duration": 3.84}, {"text": "classification problem and then talk", "start": 74.46, "duration": 2.4}, {"text": "about", "start": 76.02, "duration": 2.4}, {"text": "you know the Practical consideration", "start": 76.86, "duration": 2.88}, {"text": "some of the guidelines when we run this", "start": 78.42, "duration": 3.3}, {"text": "what do we do for hyper parameter", "start": 79.74, "duration": 3.54}, {"text": "searching setting up workflows and so", "start": 81.72, "duration": 3.86}, {"text": "forth", "start": 83.28, "duration": 2.3}, {"text": "all right I always like to start my", "start": 86.04, "duration": 3.0}, {"text": "neural network explanation with logistic", "start": 87.36, "duration": 3.119}, {"text": "regression I think that's because a lot", "start": 89.04, "duration": 3.0}, {"text": "of people maybe come from different", "start": 90.479, "duration": 3.481}, {"text": "fields where they've been they've been", "start": 92.04, "duration": 3.539}, {"text": "uh you know taught some linear", "start": 93.96, "duration": 3.479}, {"text": "regression models and maybe some linear", "start": 95.579, "duration": 5.641}, {"text": "uh um logistic regression", "start": 97.439, "duration": 5.941}, {"text": "so I always like to start with well this", "start": 101.22, "duration": 3.719}, {"text": "is what the logistic function looks like", "start": 103.38, "duration": 3.3}, {"text": "and", "start": 104.939, "duration": 3.781}, {"text": "um here it is for a couple just you know", "start": 106.68, "duration": 3.36}, {"text": "there's a couple parameters when you're", "start": 108.72, "duration": 2.64}, {"text": "when we're talking about a function of", "start": 110.04, "duration": 3.96}, {"text": "just uh y equals f of x", "start": 111.36, "duration": 4.38}, {"text": "and these two parameters can shift this", "start": 114.0, "duration": 2.939}, {"text": "function around so it can look very", "start": 115.74, "duration": 3.059}, {"text": "different you know the if you if you", "start": 116.939, "duration": 3.0}, {"text": "increase the weight it can be looking", "start": 118.799, "duration": 2.761}, {"text": "more like a step function and that could", "start": 119.939, "duration": 3.901}, {"text": "be shifted around the x-axis", "start": 121.56, "duration": 3.78}, {"text": "and with different combinations of", "start": 123.84, "duration": 2.7}, {"text": "parameters you can get all kinds of", "start": 125.34, "duration": 4.199}, {"text": "different looking functions", "start": 126.54, "duration": 5.46}, {"text": "and if we start combining these so here", "start": 129.539, "duration": 6.84}, {"text": "I have one input and two units", "start": 132.0, "duration": 6.599}, {"text": "and each unit is a function and they", "start": 136.379, "duration": 3.981}, {"text": "might have some different", "start": 138.599, "duration": 3.72}, {"text": "parameters they have different shapes", "start": 140.36, "duration": 3.7}, {"text": "and if we add them together", "start": 142.319, "duration": 3.06}, {"text": "we're going to get something more", "start": 144.06, "duration": 3.12}, {"text": "interesting so if we do the in this case", "start": 145.379, "duration": 3.661}, {"text": "we add them together and these two", "start": 147.18, "duration": 3.48}, {"text": "original functions are shifted over a", "start": 149.04, "duration": 3.18}, {"text": "little bit and so now we get a bump", "start": 150.66, "duration": 3.9}, {"text": "function it Peaks over it Peaks at a", "start": 152.22, "duration": 4.14}, {"text": "certain value of X1", "start": 154.56, "duration": 3.36}, {"text": "and we can play this game and you can", "start": 156.36, "duration": 3.84}, {"text": "build all kinds of uh you know", "start": 157.92, "duration": 4.08}, {"text": "complicated functions", "start": 160.2, "duration": 3.36}, {"text": "so just by playing around with those", "start": 162.0, "duration": 3.36}, {"text": "parameters", "start": 163.56, "duration": 4.08}, {"text": "and what does it what this points to is", "start": 165.36, "duration": 3.75}, {"text": "the fact that", "start": 167.64, "duration": 1.56}, {"text": "[Music]", "start": 169.11, "duration": 1.95}, {"text": "um", "start": 169.2, "duration": 3.24}, {"text": "you know multi-layer networks can", "start": 171.06, "duration": 4.319}, {"text": "represent any any logical or real valued", "start": 172.44, "duration": 4.079}, {"text": "function so in that sense they're", "start": 175.379, "duration": 2.58}, {"text": "unbiased they have the potential to", "start": 176.519, "duration": 3.121}, {"text": "overfit their potential to fit any", "start": 177.959, "duration": 3.481}, {"text": "anything", "start": 179.64, "duration": 2.879}, {"text": "that doesn't mean they're going to", "start": 181.44, "duration": 4.019}, {"text": "always easily learn but the", "start": 182.519, "duration": 6.921}, {"text": "representational power is there", "start": 185.459, "duration": 3.981}, {"text": "and so once we you know are comfortable", "start": 191.28, "duration": 3.84}, {"text": "with the logistic function we can to", "start": 193.68, "duration": 3.54}, {"text": "draw this out as a little graph here", "start": 195.12, "duration": 3.96}, {"text": "I've drawn it out with what an input", "start": 197.22, "duration": 3.42}, {"text": "line there's a weight", "start": 199.08, "duration": 4.439}, {"text": "and a bias unit and these names come out", "start": 200.64, "duration": 4.44}, {"text": "of the fact that you know they're sort", "start": 203.519, "duration": 3.841}, {"text": "of biologically inspired", "start": 205.08, "duration": 4.019}, {"text": "um so they're you know they're basically", "start": 207.36, "duration": 4.26}, {"text": "same as linear regression parameters", "start": 209.099, "duration": 6.72}, {"text": "and when we draw the node the unit and", "start": 211.62, "duration": 5.94}, {"text": "we talk about a trans it's making a", "start": 215.819, "duration": 3.181}, {"text": "transformation from an input to output", "start": 217.56, "duration": 4.679}, {"text": "and we call it the activation function", "start": 219.0, "duration": 5.519}, {"text": "and we often don't draw the fact that", "start": 222.239, "duration": 4.08}, {"text": "when the lines converge it's usually a", "start": 224.519, "duration": 3.841}, {"text": "summation so we're taking a summation", "start": 226.319, "duration": 3.901}, {"text": "producing a", "start": 228.36, "duration": 3.299}, {"text": "um you know incoming sum for the", "start": 230.22, "duration": 3.96}, {"text": "activation function and of course we", "start": 231.659, "duration": 5.881}, {"text": "could write this out as dot products", "start": 234.18, "duration": 6.66}, {"text": "and when we have an extra unit that's", "start": 237.54, "duration": 5.52}, {"text": "for output we you know we'll call that", "start": 240.84, "duration": 4.08}, {"text": "an output unit and then the intermediate", "start": 243.06, "duration": 5.239}, {"text": "units we'll call Hidden units", "start": 244.92, "duration": 3.379}, {"text": "and if we're smart about how we index", "start": 248.7, "duration": 3.599}, {"text": "our parameters in this case our weight", "start": 250.68, "duration": 3.66}, {"text": "parameters we can all set this up as", "start": 252.299, "duration": 5.461}, {"text": "just a matrix operation so this is um", "start": 254.34, "duration": 5.639}, {"text": "typically people use an indexing scheme", "start": 257.76, "duration": 5.999}, {"text": "of to and from and uh you know you have", "start": 259.979, "duration": 6.66}, {"text": "a weight Matrix and you multiply it by", "start": 263.759, "duration": 5.041}, {"text": "your input", "start": 266.639, "duration": 4.201}, {"text": "so more generally we can add hidden", "start": 268.8, "duration": 3.6}, {"text": "layer we can have many inputs we could", "start": 270.84, "duration": 3.72}, {"text": "have many outputs a multi-layer", "start": 272.4, "duration": 5.1}, {"text": "perceptron we typically mean one input", "start": 274.56, "duration": 4.8}, {"text": "one hidden layer one output this was the", "start": 277.5, "duration": 4.38}, {"text": "basic neural network that um", "start": 279.36, "duration": 5.22}, {"text": "people started with years ago", "start": 281.88, "duration": 5.039}, {"text": "and we have an input and there's many", "start": 284.58, "duration": 4.92}, {"text": "you know if it's coming from a data", "start": 286.919, "duration": 5.821}, {"text": "Matrix we'll have some you know I one", "start": 289.5, "duration": 6.12}, {"text": "equals one to n observations and maybe a", "start": 292.74, "duration": 6.14}, {"text": "p Vector for every input vector", "start": 295.62, "duration": 5.46}, {"text": "and we might have some number of output", "start": 298.88, "duration": 3.7}, {"text": "units it doesn't have to be one to one", "start": 301.08, "duration": 3.179}, {"text": "it could be many to one it could be many", "start": 302.58, "duration": 3.78}, {"text": "to many we'll have some number of hidden", "start": 304.259, "duration": 4.5}, {"text": "units and each line represents a weight", "start": 306.36, "duration": 4.559}, {"text": "parameter", "start": 308.759, "duration": 3.901}, {"text": "and so we can make this even more", "start": 310.919, "duration": 3.121}, {"text": "General and talk about the fact that we", "start": 312.66, "duration": 3.539}, {"text": "have many layers uh you know the", "start": 314.04, "duration": 3.9}, {"text": "operations we're doing here the first", "start": 316.199, "duration": 3.661}, {"text": "we're going to take some input", "start": 317.94, "duration": 3.0}, {"text": "and we're going to do a forward", "start": 319.86, "duration": 2.399}, {"text": "propagation that means we're going to", "start": 320.94, "duration": 3.3}, {"text": "apply the input to the input layer layer", "start": 322.259, "duration": 4.681}, {"text": "one we're going to calculate activations", "start": 324.24, "duration": 5.22}, {"text": "for Layer Two calculate the outputs for", "start": 326.94, "duration": 4.62}, {"text": "Layer Two and so forth and so on so", "start": 329.46, "duration": 4.5}, {"text": "we're doing a forward propagation and", "start": 331.56, "duration": 4.8}, {"text": "when we get to our output we're going to", "start": 333.96, "duration": 4.26}, {"text": "have some label it's going to be a", "start": 336.36, "duration": 3.3}, {"text": "supervised learning problem typically", "start": 338.22, "duration": 3.9}, {"text": "and we'll have some error information so", "start": 339.66, "duration": 3.96}, {"text": "we can back propagate that information", "start": 342.12, "duration": 3.54}, {"text": "the way we back propagate is using", "start": 343.62, "duration": 5.16}, {"text": "derivatives and the chain rule", "start": 345.66, "duration": 4.86}, {"text": "and I'm not going to you know go in", "start": 348.78, "duration": 2.88}, {"text": "through the details of the chain rule", "start": 350.52, "duration": 3.0}, {"text": "but it's essentially saying the chain", "start": 351.66, "duration": 3.78}, {"text": "you know the the error the change in", "start": 353.52, "duration": 4.2}, {"text": "error is a function of your weight", "start": 355.44, "duration": 6.06}, {"text": "um depends on everything in between", "start": 357.72, "duration": 6.3}, {"text": "and as you go farther back though", "start": 361.5, "duration": 4.68}, {"text": "the information gets diluted and the", "start": 364.02, "duration": 3.899}, {"text": "error grade it starts Vanishing we call", "start": 366.18, "duration": 3.359}, {"text": "this the vanishing gradient problem so", "start": 367.919, "duration": 2.941}, {"text": "this was a problem", "start": 369.539, "duration": 5.581}, {"text": "for a while for many years maybe and um", "start": 370.86, "duration": 7.74}, {"text": "and after you know after some some trial", "start": 375.12, "duration": 5.1}, {"text": "and errors people people figured out", "start": 378.6, "duration": 2.879}, {"text": "that oh there's there's a certain kind", "start": 380.22, "duration": 2.819}, {"text": "of activation function instead of using", "start": 381.479, "duration": 3.06}, {"text": "logistic we could use this other", "start": 383.039, "duration": 3.181}, {"text": "activation function", "start": 384.539, "duration": 4.921}, {"text": "called the rectified linear unit and it", "start": 386.22, "duration": 6.3}, {"text": "has you know a large linear it's linear", "start": 389.46, "duration": 3.66}, {"text": "um", "start": 392.52, "duration": 2.82}, {"text": "for for a big part of its for half its", "start": 393.12, "duration": 3.6}, {"text": "domain so", "start": 395.34, "duration": 3.66}, {"text": "it actually helps solve that it's a", "start": 396.72, "duration": 3.479}, {"text": "gradient problem because the gradients", "start": 399.0, "duration": 3.06}, {"text": "don't there's no saturation", "start": 400.199, "duration": 4.881}, {"text": "um when it's active", "start": 402.06, "duration": 3.02}, {"text": "so that was one thing that helped build", "start": 405.6, "duration": 4.98}, {"text": "networks with deeper more layers", "start": 407.699, "duration": 5.041}, {"text": "so the algorithm once we you know", "start": 410.58, "duration": 3.839}, {"text": "comfortable with forward propagation and", "start": 412.74, "duration": 3.54}, {"text": "backward propagation is doing", "start": 414.419, "duration": 3.961}, {"text": "and if we if we're comfortable with what", "start": 416.28, "duration": 3.419}, {"text": "machine learning algorithms look like", "start": 418.38, "duration": 2.759}, {"text": "it's easy to write down this algorithm", "start": 419.699, "duration": 2.94}, {"text": "it's you know we initialize our weight", "start": 421.139, "duration": 4.441}, {"text": "to some small value we have a loop we're", "start": 422.639, "duration": 4.441}, {"text": "doing forward propagation backward", "start": 425.58, "duration": 2.88}, {"text": "propagation", "start": 427.08, "duration": 3.42}, {"text": "and then we update weights", "start": 428.46, "duration": 4.5}, {"text": "uh and typically this is like a version", "start": 430.5, "duration": 4.38}, {"text": "of stochastic gradient descent", "start": 432.96, "duration": 4.2}, {"text": "and what's interesting we do this for a", "start": 434.88, "duration": 4.379}, {"text": "batch of inputs we don't do this for one", "start": 437.16, "duration": 4.86}, {"text": "input at a time uh we'll do this for a", "start": 439.259, "duration": 4.56}, {"text": "set of inputs and maybe it's all the", "start": 442.02, "duration": 3.959}, {"text": "inputs at once but typically you might", "start": 443.819, "duration": 3.66}, {"text": "have a memory issue if you have a lot of", "start": 445.979, "duration": 3.241}, {"text": "data so you do it for a batch some", "start": 447.479, "duration": 4.261}, {"text": "subset of all your data", "start": 449.22, "duration": 3.9}, {"text": "and there's some learning rate and", "start": 451.74, "duration": 2.82}, {"text": "there's some optimizers and you would", "start": 453.12, "duration": 3.299}, {"text": "you know everything everything kind of", "start": 454.56, "duration": 3.72}, {"text": "works towards like a optimization", "start": 456.419, "duration": 3.06}, {"text": "problems", "start": 458.28, "duration": 3.3}, {"text": "and you stop typically when our", "start": 459.479, "duration": 4.021}, {"text": "validation error reaches a minimum or", "start": 461.58, "duration": 3.239}, {"text": "maybe we just have a maximum number of", "start": 463.5, "duration": 4.94}, {"text": "epics or bail or both", "start": 464.819, "duration": 3.621}, {"text": "okay so the neural networks um the main", "start": 469.86, "duration": 6.08}, {"text": "things we have to choose", "start": 472.86, "duration": 3.08}, {"text": "is setting up an architecture that's", "start": 476.039, "duration": 3.6}, {"text": "typically your numbered hidden units", "start": 477.96, "duration": 3.0}, {"text": "your number of layers as we get into", "start": 479.639, "duration": 3.421}, {"text": "more dealing with unstructured data", "start": 480.96, "duration": 4.5}, {"text": "we're going to have special layers", "start": 483.06, "duration": 4.46}, {"text": "uh Optimizer and their learning rate", "start": 485.46, "duration": 5.1}, {"text": "your loss function is probably going to", "start": 487.52, "duration": 5.079}, {"text": "depend on your task as you're if you're", "start": 490.56, "duration": 4.74}, {"text": "building you know doing classification", "start": 492.599, "duration": 4.981}, {"text": "or doing regression", "start": 495.3, "duration": 3.66}, {"text": "um you know you're either going to use", "start": 497.58, "duration": 2.82}, {"text": "some sort error maybe or some kind of", "start": 498.96, "duration": 4.38}, {"text": "entropy if you have a particular other", "start": 500.4, "duration": 4.62}, {"text": "task you can build custom loss functions", "start": 503.34, "duration": 4.74}, {"text": "which are combinations of things", "start": 505.02, "duration": 5.64}, {"text": "and in general the more hidden layers", "start": 508.08, "duration": 4.079}, {"text": "the more hidden units that's more that's", "start": 510.66, "duration": 3.179}, {"text": "those are our resources that's that's", "start": 512.159, "duration": 3.421}, {"text": "what creates the complexity of the", "start": 513.839, "duration": 4.44}, {"text": "neural network and that also means you", "start": 515.58, "duration": 6.259}, {"text": "have more potential for overfitting", "start": 518.279, "duration": 3.56}, {"text": "oops", "start": 523.08, "duration": 2.54}, {"text": "okay", "start": 528.48, "duration": 4.62}, {"text": "so in summary our our neural networks", "start": 530.76, "duration": 4.68}, {"text": "are general in general they're flexible", "start": 533.1, "duration": 3.96}, {"text": "they're powerful Learners hidden layers", "start": 535.44, "duration": 2.82}, {"text": "are doing some kind of you know", "start": 537.06, "duration": 4.38}, {"text": "non-linear transformation of the input", "start": 538.26, "duration": 5.4}, {"text": "but now the the bad side is that it", "start": 541.44, "duration": 4.26}, {"text": "takes lots of parameters sometimes it's", "start": 543.66, "duration": 3.6}, {"text": "hard to interpret and some typically", "start": 545.7, "duration": 2.759}, {"text": "compared to other machine learning", "start": 547.26, "duration": 4.019}, {"text": "models you need more data", "start": 548.459, "duration": 4.981}, {"text": "but if you have your data you can do a", "start": 551.279, "duration": 4.56}, {"text": "lot of useful things so a URL Network", "start": 553.44, "duration": 4.38}, {"text": "can discover visual features using a", "start": 555.839, "duration": 3.841}, {"text": "convolution so and and we'll talk about", "start": 557.82, "duration": 3.66}, {"text": "the convolution layer next", "start": 559.68, "duration": 3.18}, {"text": "so imagine we're doing an image", "start": 561.48, "duration": 3.78}, {"text": "classification of digits so here is the", "start": 562.86, "duration": 5.22}, {"text": "hello world problem of deep learning", "start": 565.26, "duration": 5.639}, {"text": "we have uh imagine we we have this", "start": 568.08, "duration": 4.319}, {"text": "database called the administ database", "start": 570.899, "duration": 3.361}, {"text": "and there's a bunch of you know", "start": 572.399, "duration": 3.421}, {"text": "handwritten digits it comes from postal", "start": 574.26, "duration": 3.24}, {"text": "codes it's been around a while and", "start": 575.82, "duration": 4.019}, {"text": "people have been experimenting this with", "start": 577.5, "duration": 4.62}, {"text": "this for a long time and the issue is", "start": 579.839, "duration": 5.541}, {"text": "how do you classify digits", "start": 582.12, "duration": 3.26}, {"text": "so", "start": 585.48, "duration": 3.78}, {"text": "image image problems typically they they", "start": 587.16, "duration": 3.54}, {"text": "used to start with something like this", "start": 589.26, "duration": 3.96}, {"text": "well let's say we you know so I've got a", "start": 590.7, "duration": 4.02}, {"text": "picture of this the number seven here", "start": 593.22, "duration": 3.179}, {"text": "where I've I've scaled everything to", "start": 594.72, "duration": 4.5}, {"text": "between 0 and 10. and I'm going to look", "start": 596.399, "duration": 4.081}, {"text": "at little patches so I'm going to look", "start": 599.22, "duration": 4.38}, {"text": "at a three by three patch patch of the", "start": 600.48, "duration": 5.76}, {"text": "image", "start": 603.6, "duration": 4.02}, {"text": "um and we're looking at near the tip of", "start": 606.24, "duration": 2.7}, {"text": "the seven so I'm just kind of zoomed in", "start": 607.62, "duration": 2.04}, {"text": "here", "start": 608.94, "duration": 3.42}, {"text": "and I'm going to multiply it by", "start": 609.66, "duration": 4.98}, {"text": "a matrix which is also three by three we", "start": 612.36, "duration": 4.56}, {"text": "can think of this as a filter or a", "start": 614.64, "duration": 3.66}, {"text": "template like we're trying to match a", "start": 616.92, "duration": 3.539}, {"text": "template and so this particular Matrix", "start": 618.3, "duration": 5.52}, {"text": "is looking for a combination of", "start": 620.459, "duration": 5.82}, {"text": "um a combination of values where it's", "start": 623.82, "duration": 4.98}, {"text": "small as you go left to right it goes", "start": 626.279, "duration": 5.701}, {"text": "from small numbers to larger numbers", "start": 628.8, "duration": 5.28}, {"text": "we can call this our weight Matrix for", "start": 631.98, "duration": 4.34}, {"text": "example", "start": 634.08, "duration": 2.24}, {"text": "and we're going to multiply it element", "start": 637.92, "duration": 3.84}, {"text": "wise times our little patch and then", "start": 639.18, "duration": 4.56}, {"text": "we're going to put the sum into this new", "start": 641.76, "duration": 5.46}, {"text": "window this new output map", "start": 643.74, "duration": 5.52}, {"text": "and then we're going to slide our patch", "start": 647.22, "duration": 3.78}, {"text": "we're going to repeat the operation", "start": 649.26, "duration": 3.42}, {"text": "we're going to put the sum in a new cell", "start": 651.0, "duration": 3.3}, {"text": "of the output map and we're going to", "start": 652.68, "duration": 3.719}, {"text": "keep going like this sliding the filter", "start": 654.3, "duration": 4.26}, {"text": "is known as a convolution operation", "start": 656.399, "duration": 5.041}, {"text": "you're sliding and and a filter over", "start": 658.56, "duration": 6.06}, {"text": "something and doing addition operations", "start": 661.44, "duration": 6.24}, {"text": "and so we're creating a new output map", "start": 664.62, "duration": 4.38}, {"text": "and it's going to look something like", "start": 667.68, "duration": 2.279}, {"text": "this", "start": 669.0, "duration": 3.54}, {"text": "and the highest values in this output", "start": 669.959, "duration": 5.041}, {"text": "are essentially representing some kind", "start": 672.54, "duration": 5.22}, {"text": "of feature which in our case is", "start": 675.0, "duration": 5.04}, {"text": "some kind of something that matches the", "start": 677.76, "duration": 4.639}, {"text": "template", "start": 680.04, "duration": 2.359}, {"text": "and we might even go one step further", "start": 682.68, "duration": 3.599}, {"text": "and say you know we can we can do a", "start": 684.42, "duration": 4.8}, {"text": "maximization over little windows so this", "start": 686.279, "duration": 5.401}, {"text": "is called a Max pooling operation it", "start": 689.22, "duration": 5.04}, {"text": "helps downsize the sample and at the end", "start": 691.68, "duration": 5.159}, {"text": "our little patch here", "start": 694.26, "duration": 5.04}, {"text": "we start out with a 5x6 patch I've", "start": 696.839, "duration": 3.961}, {"text": "transformed it into a two by three", "start": 699.3, "duration": 3.659}, {"text": "feature map of that represents an edge", "start": 700.8, "duration": 4.5}, {"text": "gradient", "start": 702.959, "duration": 4.081}, {"text": "so that's the basic operation of a", "start": 705.3, "duration": 2.76}, {"text": "convolution", "start": 707.04, "duration": 3.18}, {"text": "and you know people have been doing this", "start": 708.06, "duration": 3.06}, {"text": "for a while", "start": 710.22, "duration": 3.299}, {"text": "but um but the problem is we had to come", "start": 711.12, "duration": 4.08}, {"text": "up with those those gradients we had to", "start": 713.519, "duration": 3.301}, {"text": "come up with that that filter that was", "start": 715.2, "duration": 3.36}, {"text": "we were feature Engineering in this case", "start": 716.82, "duration": 3.24}, {"text": "but we'd like to do feature Discovery we", "start": 718.56, "duration": 3.06}, {"text": "would it's it's kind of hard to know", "start": 720.06, "duration": 3.839}, {"text": "what features you always want to have in", "start": 721.62, "duration": 5.459}, {"text": "in some image processing classification", "start": 723.899, "duration": 5.601}, {"text": "tasks", "start": 727.079, "duration": 2.421}, {"text": "so if we think of this as wave", "start": 729.66, "duration": 3.06}, {"text": "parameters a convolution layer is a set", "start": 730.98, "duration": 4.2}, {"text": "of feature maps for each input map is", "start": 732.72, "duration": 4.32}, {"text": "derived from the convolution and in", "start": 735.18, "duration": 3.06}, {"text": "neural networks we're going to learn", "start": 737.04, "duration": 4.1}, {"text": "these parameters", "start": 738.24, "duration": 2.9}, {"text": "so the convolution operation is", "start": 741.779, "duration": 3.781}, {"text": "introducing uh you know some more hyper", "start": 743.399, "duration": 3.12}, {"text": "parameters that we have to think about", "start": 745.56, "duration": 2.76}, {"text": "the size of the filter and in general", "start": 746.519, "duration": 3.901}, {"text": "smaller filters are better", "start": 748.32, "duration": 3.9}, {"text": "and in fact people have found that you", "start": 750.42, "duration": 4.2}, {"text": "know if it's it's really good to just", "start": 752.22, "duration": 4.5}, {"text": "start with the three by three it's kind", "start": 754.62, "duration": 4.32}, {"text": "of like the smallest filter you can and", "start": 756.72, "duration": 4.859}, {"text": "just build layers", "start": 758.94, "duration": 3.3}, {"text": "um", "start": 761.579, "duration": 2.161}, {"text": "the number of pixels to slide over", "start": 762.24, "duration": 3.48}, {"text": "usually slide over one or two you know", "start": 763.74, "duration": 3.839}, {"text": "as long as you have computational power", "start": 765.72, "duration": 3.84}, {"text": "you can you can pretty much do three by", "start": 767.579, "duration": 4.801}, {"text": "three filters and slide over one", "start": 769.56, "duration": 4.68}, {"text": "um pixel at a time and then you don't", "start": 772.38, "duration": 4.98}, {"text": "lose any granularity", "start": 774.24, "duration": 6.12}, {"text": "uh people typically to do a Max pooling", "start": 777.36, "duration": 5.46}, {"text": "it helps to downsize your", "start": 780.36, "duration": 4.74}, {"text": "um your feature Maps", "start": 782.82, "duration": 3.78}, {"text": "but now the number of filters the number", "start": 785.1, "duration": 3.66}, {"text": "of filters is now a new parameter where", "start": 786.6, "duration": 3.78}, {"text": "it depends on the task it's not obvious", "start": 788.76, "duration": 3.66}, {"text": "how many filters you should have", "start": 790.38, "duration": 3.66}, {"text": "so we'll get a feel for that when we go", "start": 792.42, "duration": 5.88}, {"text": "to to talk more about this uh example", "start": 794.04, "duration": 7.859}, {"text": "so let's think about how this scales up", "start": 798.3, "duration": 6.18}, {"text": "um so imagine I have not in not a black", "start": 801.899, "duration": 4.141}, {"text": "and white image but now I have a color", "start": 804.48, "duration": 3.799}, {"text": "image", "start": 806.04, "duration": 4.979}, {"text": "and we would crop it to a certain size", "start": 808.279, "duration": 5.8}, {"text": "like in this example it's a two 224", "start": 811.019, "duration": 6.421}, {"text": "height 224 width by three three for the", "start": 814.079, "duration": 5.041}, {"text": "channels", "start": 817.44, "duration": 4.139}, {"text": "and so a filter we have if I build a", "start": 819.12, "duration": 6.24}, {"text": "three by three filter in in height and", "start": 821.579, "duration": 6.901}, {"text": "width I also have to take a", "start": 825.36, "duration": 4.919}, {"text": "and take into account the depth so in", "start": 828.48, "duration": 3.479}, {"text": "this case because I have three channels", "start": 830.279, "duration": 3.06}, {"text": "I'm going to have a three by three by", "start": 831.959, "duration": 3.301}, {"text": "three filter", "start": 833.339, "duration": 4.921}, {"text": "so my and then my feature map uh output", "start": 835.26, "duration": 4.44}, {"text": "is going to depend on the filter size", "start": 838.26, "duration": 3.24}, {"text": "and the slide parameter so I'm using I'm", "start": 839.7, "duration": 3.72}, {"text": "usually you know dropping off some of", "start": 841.5, "duration": 4.019}, {"text": "the pixels on the edges", "start": 843.42, "duration": 3.9}, {"text": "sometimes people do a padding and so", "start": 845.519, "duration": 3.601}, {"text": "they don't they don't have to you don't", "start": 847.32, "duration": 4.86}, {"text": "have to make a smaller map but typically", "start": 849.12, "duration": 5.88}, {"text": "we're downsizing as we go along", "start": 852.18, "duration": 5.04}, {"text": "and we repeat this for many filters", "start": 855.0, "duration": 4.139}, {"text": "and each filter is a three by three and", "start": 857.22, "duration": 3.78}, {"text": "you and at the end you stack all the", "start": 859.139, "duration": 4.14}, {"text": "output feature maps and so you end up", "start": 861.0, "duration": 5.639}, {"text": "with a convolution layer the thickness", "start": 863.279, "duration": 4.8}, {"text": "of the layer is the number of feature", "start": 866.639, "duration": 3.481}, {"text": "Maps sometimes it's called channels or", "start": 868.079, "duration": 3.721}, {"text": "number of filters or we just might call", "start": 870.12, "duration": 3.6}, {"text": "it depth", "start": 871.8, "duration": 3.0}, {"text": "and then we're going to add another", "start": 873.72, "duration": 3.359}, {"text": "convolution layer and that feature that", "start": 874.8, "duration": 3.9}, {"text": "filter size is going to be a height and", "start": 877.079, "duration": 3.961}, {"text": "a width times the number of filters of", "start": 878.7, "duration": 3.84}, {"text": "the previous layer", "start": 881.04, "duration": 4.08}, {"text": "or we might add or we might add a a", "start": 882.54, "duration": 5.88}, {"text": "layer of Max pooling to down sample", "start": 885.12, "duration": 5.219}, {"text": "and we could continue this operation and", "start": 888.42, "duration": 3.84}, {"text": "sort of do this and build up", "start": 890.339, "duration": 3.56}, {"text": "a larger network", "start": 892.26, "duration": 5.4}, {"text": "of set of convolution layers and as we", "start": 893.899, "duration": 6.161}, {"text": "go from left to right", "start": 897.66, "duration": 5.039}, {"text": "we are downsizing so we're getting", "start": 900.06, "duration": 4.26}, {"text": "smaller and smaller feature Maps but", "start": 902.699, "duration": 4.32}, {"text": "we're also increasing in our filters so", "start": 904.32, "duration": 5.16}, {"text": "we're producing more and more feature", "start": 907.019, "duration": 4.76}, {"text": "Maps", "start": 909.48, "duration": 2.299}, {"text": "um okay and then at the end and then our", "start": 912.959, "duration": 2.88}, {"text": "last layers is going to be the", "start": 914.699, "duration": 2.88}, {"text": "classification part and this looks like", "start": 915.839, "duration": 3.541}, {"text": "a multi-layer perceptron in this case", "start": 917.579, "duration": 4.921}, {"text": "it's it's got two layers uh two hidden", "start": 919.38, "duration": 6.06}, {"text": "unit layers one output layer this was an", "start": 922.5, "duration": 5.699}, {"text": "example from the um", "start": 925.44, "duration": 5.04}, {"text": "uh Vision uh I can't remember the name", "start": 928.199, "duration": 4.621}, {"text": "of the conference islvr conference where", "start": 930.48, "duration": 7.44}, {"text": "they predict 1000 object classes and", "start": 932.82, "duration": 6.42}, {"text": "um", "start": 937.92, "duration": 3.12}, {"text": "and it's a big classification problem", "start": 939.24, "duration": 4.14}, {"text": "and so this was the the kind of networks", "start": 941.04, "duration": 3.419}, {"text": "and the kind of work that was being done", "start": 943.38, "duration": 4.22}, {"text": "that really Advanced the field", "start": 944.459, "duration": 3.141}, {"text": "so what are these convolution layers", "start": 949.5, "duration": 3.779}, {"text": "look like well here's some of the the", "start": 951.06, "duration": 5.279}, {"text": "first layer of that previous Network and", "start": 953.279, "duration": 5.521}, {"text": "you can see that these are the filters", "start": 956.339, "duration": 4.981}, {"text": "so these are basically the weights", "start": 958.8, "duration": 4.2}, {"text": "um", "start": 961.32, "duration": 3.36}, {"text": "and I can't remember the size of these", "start": 963.0, "duration": 3.36}, {"text": "filters but if you look if you look at", "start": 964.68, "duration": 2.94}, {"text": "them you kind of look they kind of look", "start": 966.36, "duration": 2.64}, {"text": "like maybe they're doing some kind of", "start": 967.62, "duration": 3.06}, {"text": "edge detection because there's patterns", "start": 969.0, "duration": 5.639}, {"text": "of of higher values or lower values and", "start": 970.68, "duration": 5.94}, {"text": "and higher values", "start": 974.639, "duration": 4.081}, {"text": "um but they're in different orientations", "start": 976.62, "duration": 3.6}, {"text": "and then some of the filters look like", "start": 978.72, "duration": 3.239}, {"text": "patches of color", "start": 980.22, "duration": 3.84}, {"text": "so it's hard to you know say exactly", "start": 981.959, "duration": 3.901}, {"text": "what these filters are doing or why", "start": 984.06, "duration": 4.26}, {"text": "they're at certain angles but presumably", "start": 985.86, "duration": 4.56}, {"text": "these are low level features of of the", "start": 988.32, "duration": 3.959}, {"text": "1000 kinds of objects that they're", "start": 990.42, "duration": 3.84}, {"text": "trying to classify", "start": 992.279, "duration": 5.161}, {"text": "and if we look at the activations so", "start": 994.26, "duration": 5.579}, {"text": "here's a picture of a car as you go from", "start": 997.44, "duration": 4.199}, {"text": "left to right you're going from the", "start": 999.839, "duration": 3.961}, {"text": "lower layers up to the upper the higher", "start": 1001.639, "duration": 5.221}, {"text": "layers as we go from left to right we", "start": 1003.8, "duration": 7.92}, {"text": "see that the activations are still", "start": 1006.86, "duration": 6.779}, {"text": "um kind of looking like the car except", "start": 1011.72, "duration": 3.84}, {"text": "certain features are highlighted maybe", "start": 1013.639, "duration": 3.781}, {"text": "it's an edge maybe it's the front maybe", "start": 1015.56, "duration": 3.899}, {"text": "it's a part of a wheel who knows it's", "start": 1017.42, "duration": 4.56}, {"text": "hard to say as you go from left to right", "start": 1019.459, "duration": 4.561}, {"text": "also the the feature maps are getting", "start": 1021.98, "duration": 3.3}, {"text": "smaller", "start": 1024.02, "duration": 3.0}, {"text": "and so these", "start": 1025.28, "duration": 4.38}, {"text": "features are thought to represent", "start": 1027.02, "duration": 5.279}, {"text": "something more abstract about a car so", "start": 1029.66, "duration": 4.44}, {"text": "we're losing you know spatial resolution", "start": 1032.299, "duration": 3.841}, {"text": "but hopefully we're gaining conceptual", "start": 1034.1, "duration": 5.339}, {"text": "or classif or class information", "start": 1036.14, "duration": 6.0}, {"text": "and in this example the final output", "start": 1039.439, "duration": 4.561}, {"text": "predictions is strong prediction for car", "start": 1042.14, "duration": 3.779}, {"text": "a little bit of truck airplane ship and", "start": 1044.0, "duration": 4.699}, {"text": "for some reason horse", "start": 1045.919, "duration": 2.78}, {"text": "okay so the summary here is that you", "start": 1050.9, "duration": 4.62}, {"text": "know convolution were accomplishing", "start": 1053.54, "duration": 4.379}, {"text": "networks worked because the layers you", "start": 1055.52, "duration": 3.539}, {"text": "know the layers are essentially special", "start": 1057.919, "duration": 3.601}, {"text": "architecture they're essentially built", "start": 1059.059, "duration": 4.201}, {"text": "to pull out features from an image", "start": 1061.52, "duration": 3.899}, {"text": "they're essentially biasing the kind of", "start": 1063.26, "duration": 4.08}, {"text": "thing a network can learn", "start": 1065.419, "duration": 4.38}, {"text": "uh the lower layers have you know", "start": 1067.34, "duration": 5.4}, {"text": "typically uh less filters they represent", "start": 1069.799, "duration": 4.62}, {"text": "simple local features that are", "start": 1072.74, "duration": 3.96}, {"text": "presumably relevant for all classes and", "start": 1074.419, "duration": 4.741}, {"text": "you could do you know more exam people", "start": 1076.7, "duration": 4.2}, {"text": "have done more", "start": 1079.16, "duration": 3.66}, {"text": "um in-depth examination of what these", "start": 1080.9, "duration": 4.32}, {"text": "features look like for example", "start": 1082.82, "duration": 4.2}, {"text": "higher layers have more filters that", "start": 1085.22, "duration": 4.5}, {"text": "cover larger regions and they represent", "start": 1087.02, "duration": 4.5}, {"text": "you know something presumably about an", "start": 1089.72, "duration": 4.04}, {"text": "object", "start": 1091.52, "duration": 2.24}, {"text": "so now we can also say a little bit", "start": 1095.12, "duration": 2.7}, {"text": "about what deep learning is deep", "start": 1096.679, "duration": 3.601}, {"text": "learning really refers to learning", "start": 1097.82, "duration": 4.979}, {"text": "complex and varied transformations of", "start": 1100.28, "duration": 3.779}, {"text": "the input", "start": 1102.799, "duration": 3.361}, {"text": "that's one way to think of it deep", "start": 1104.059, "duration": 4.62}, {"text": "learning refers to discovering features", "start": 1106.16, "duration": 6.12}, {"text": "of the input and this is you know I like", "start": 1108.679, "duration": 4.801}, {"text": "to think of it this way especially", "start": 1112.28, "duration": 4.259}, {"text": "because one of the successes of deep", "start": 1113.48, "duration": 4.199}, {"text": "learning networks is dealing with", "start": 1116.539, "duration": 4.441}, {"text": "unstructured data raw text a bunch of", "start": 1117.679, "duration": 4.38}, {"text": "images", "start": 1120.98, "duration": 3.18}, {"text": "uh you know sound", "start": 1122.059, "duration": 4.681}, {"text": "um audio to sound", "start": 1124.16, "duration": 4.56}, {"text": "and so it's really pulling out features", "start": 1126.74, "duration": 3.6}, {"text": "for us we don't have to say we don't", "start": 1128.72, "duration": 2.94}, {"text": "have to pre-process the image we don't", "start": 1130.34, "duration": 4.92}, {"text": "have to be Engineers to you know take an", "start": 1131.66, "duration": 7.379}, {"text": "image and and create our own features", "start": 1135.26, "duration": 6.12}, {"text": "uh practically speaking for a lot of", "start": 1139.039, "duration": 3.721}, {"text": "people deep learning is just a neural", "start": 1141.38, "duration": 3.9}, {"text": "network with many layers um", "start": 1142.76, "duration": 5.159}, {"text": "and in that sense it's you know building", "start": 1145.28, "duration": 5.639}, {"text": "up these Transformations and maybe doing", "start": 1147.919, "duration": 4.201}, {"text": "like", "start": 1150.919, "duration": 4.321}, {"text": "uh you know a hierarchy of of", "start": 1152.12, "duration": 5.88}, {"text": "um of features", "start": 1155.24, "duration": 4.319}, {"text": "so sometimes people talk about it in", "start": 1158.0, "duration": 3.919}, {"text": "that sense", "start": 1159.559, "duration": 2.36}, {"text": "okay so I wanted to do so you typically", "start": 1162.32, "duration": 4.44}, {"text": "do a network demo but because this is an", "start": 1164.72, "duration": 4.8}, {"text": "hour long I um I'm just going to give", "start": 1166.76, "duration": 5.76}, {"text": "you some slides of what it looks like uh", "start": 1169.52, "duration": 7.5}, {"text": "when we do this uh digit classification", "start": 1172.52, "duration": 6.659}, {"text": "so again here's the set of you know the", "start": 1177.02, "duration": 3.6}, {"text": "kind of data we have", "start": 1179.179, "duration": 3.961}, {"text": "we're going to use the Keras", "start": 1180.62, "duration": 5.52}, {"text": "app that Keras package which is part of", "start": 1183.14, "duration": 6.2}, {"text": "tensorflow and which is the Google", "start": 1186.14, "duration": 6.24}, {"text": "module the Google library for doing fast", "start": 1189.34, "duration": 4.9}, {"text": "computations and it's a python-based", "start": 1192.38, "duration": 4.039}, {"text": "library", "start": 1194.24, "duration": 2.179}, {"text": "um on expands for example so this is the", "start": 1198.02, "duration": 5.58}, {"text": "our local HPC system which is part of", "start": 1200.84, "duration": 5.339}, {"text": "the exceed I'm sorry prior to the NSF", "start": 1203.6, "duration": 3.959}, {"text": "um", "start": 1206.179, "duration": 5.461}, {"text": "um set the NSF set of HPC systems uh for", "start": 1207.559, "duration": 6.801}, {"text": "research and support", "start": 1211.64, "duration": 2.72}, {"text": "and in the we have a um an interface", "start": 1214.94, "duration": 5.64}, {"text": "around a Jupiter notebook we it's our", "start": 1218.419, "duration": 4.62}, {"text": "expanse portal if you go to the expanse", "start": 1220.58, "duration": 5.88}, {"text": "website uh it'll have a link for the", "start": 1223.039, "duration": 5.461}, {"text": "portal when you go into the portal It'll", "start": 1226.46, "duration": 5.88}, {"text": "ask you to log in and and then it'll ask", "start": 1228.5, "duration": 5.28}, {"text": "if you want to run interactive", "start": 1232.34, "duration": 3.6}, {"text": "applications so here I've selected", "start": 1233.78, "duration": 3.36}, {"text": "Jupiter", "start": 1235.94, "duration": 2.7}, {"text": "and depending on your account", "start": 1237.14, "duration": 2.88}, {"text": "information depending on what you want", "start": 1238.64, "duration": 3.72}, {"text": "to run you can run on GPU interactively", "start": 1240.02, "duration": 4.5}, {"text": "or you can run on a compute node", "start": 1242.36, "duration": 4.38}, {"text": "interactively which is a CPU you could", "start": 1244.52, "duration": 4.26}, {"text": "use different partitions", "start": 1246.74, "duration": 3.72}, {"text": "so I have an example here the kind of", "start": 1248.78, "duration": 5.12}, {"text": "thing you're going to want to enter", "start": 1250.46, "duration": 3.44}, {"text": "um", "start": 1254.6, "duration": 3.42}, {"text": "we typically this is this was done for a", "start": 1255.74, "duration": 3.54}, {"text": "workshop so you wouldn't have a", "start": 1258.02, "duration": 2.659}, {"text": "reservation but", "start": 1259.28, "duration": 3.6}, {"text": "but this other information is pretty", "start": 1260.679, "duration": 3.221}, {"text": "much the kind of thing you want and", "start": 1262.88, "duration": 3.299}, {"text": "there's more details about this on the", "start": 1263.9, "duration": 4.38}, {"text": "website", "start": 1266.179, "duration": 4.921}, {"text": "by the way I tried this just before the", "start": 1268.28, "duration": 5.7}, {"text": "just before our session but uh", "start": 1271.1, "duration": 4.199}, {"text": "apparently there's some work going on in", "start": 1273.98, "duration": 2.64}, {"text": "expand so it's actually not working at", "start": 1275.299, "duration": 2.521}, {"text": "the moment so", "start": 1276.62, "duration": 3.419}, {"text": "I can't show you a live demo but here's", "start": 1277.82, "duration": 4.08}, {"text": "what it looks like uh basically we're", "start": 1280.039, "duration": 3.781}, {"text": "running a Jupiter notebook uh", "start": 1281.9, "duration": 3.72}, {"text": "interactively with the expanse HPC", "start": 1283.82, "duration": 5.46}, {"text": "system uh you want to find where your", "start": 1285.62, "duration": 5.22}, {"text": "um you know your python notebooks are", "start": 1289.28, "duration": 4.5}, {"text": "here I'm showing a bunch of notebooks if", "start": 1290.84, "duration": 4.5}, {"text": "anybody wants to get these notebooks I", "start": 1293.78, "duration": 3.36}, {"text": "believe we have these accessible from a", "start": 1295.34, "duration": 5.28}, {"text": "GitHub repo these are notebooks I did", "start": 1297.14, "duration": 7.279}, {"text": "for our workshop last summer", "start": 1300.62, "duration": 3.799}, {"text": "and in the notebook there is you know", "start": 1304.64, "duration": 3.48}, {"text": "it's a typical python code there's", "start": 1306.62, "duration": 3.539}, {"text": "pre-processing data loading data", "start": 1308.12, "duration": 3.96}, {"text": "preprocessing data", "start": 1310.159, "duration": 4.14}, {"text": "um and we're and", "start": 1312.08, "duration": 4.14}, {"text": "the heart of the", "start": 1314.299, "duration": 4.801}, {"text": "the code for convolution network is this", "start": 1316.22, "duration": 4.38}, {"text": "Keras code where we build a model so", "start": 1319.1, "duration": 3.78}, {"text": "Keras has these tools or these these", "start": 1320.6, "duration": 3.959}, {"text": "functions to help us build a model", "start": 1322.88, "duration": 3.6}, {"text": "the sequential model is kind of the", "start": 1324.559, "duration": 4.441}, {"text": "straightforward feed one layer feeds", "start": 1326.48, "duration": 4.439}, {"text": "into the next layer model there's other", "start": 1329.0, "duration": 4.86}, {"text": "kinds of ways to build more flexible", "start": 1330.919, "duration": 5.821}, {"text": "models called the functional API this is", "start": 1333.86, "duration": 5.16}, {"text": "using the sequential API", "start": 1336.74, "duration": 5.819}, {"text": "I declare a model I add a layer in this", "start": 1339.02, "duration": 6.18}, {"text": "case I'm adding a convolution layer I'm", "start": 1342.559, "duration": 4.021}, {"text": "giving it information about the filter", "start": 1345.2, "duration": 3.599}, {"text": "and the stride and the activation the", "start": 1346.58, "duration": 3.8}, {"text": "input shape", "start": 1348.799, "duration": 4.021}, {"text": "and in this example I'm adding a backs", "start": 1350.38, "duration": 4.06}, {"text": "pulley layer or maybe I would add", "start": 1352.82, "duration": 4.2}, {"text": "another convolution layer", "start": 1354.44, "duration": 4.859}, {"text": "when we go from our convolution layers", "start": 1357.02, "duration": 4.68}, {"text": "to our classification layers we have to", "start": 1359.299, "duration": 4.081}, {"text": "do a transformation and this and this", "start": 1361.7, "duration": 3.3}, {"text": "would be called a flatten in Kara so", "start": 1363.38, "duration": 4.08}, {"text": "we're taking our little maps at the end", "start": 1365.0, "duration": 5.4}, {"text": "and for little maps at their last", "start": 1367.46, "duration": 4.5}, {"text": "convolution layer and we're transforming", "start": 1370.4, "duration": 3.6}, {"text": "this into a vector so the nice thing", "start": 1371.96, "duration": 4.38}, {"text": "about Keras is you can build these", "start": 1374.0, "duration": 5.28}, {"text": "sequence of layers and these layers are", "start": 1376.34, "duration": 5.4}, {"text": "functions and some of these are actual", "start": 1379.28, "duration": 4.259}, {"text": "like hidden layer functions with", "start": 1381.74, "duration": 3.12}, {"text": "parameters and some of these are just", "start": 1383.539, "duration": 5.181}, {"text": "functions like the flatten operation", "start": 1384.86, "duration": 3.86}, {"text": "um and finally I have these", "start": 1388.82, "duration": 2.82}, {"text": "classification layers and these are", "start": 1390.32, "duration": 4.02}, {"text": "called dense layers in in Terrace", "start": 1391.64, "duration": 5.46}, {"text": "and of course my last output because I", "start": 1394.34, "duration": 6.42}, {"text": "have 10 digits we use the softmax output", "start": 1397.1, "duration": 5.64}, {"text": "activation and that is accompanied with", "start": 1400.76, "duration": 4.2}, {"text": "a category of crawl cross entropy loss", "start": 1402.74, "duration": 3.419}, {"text": "function", "start": 1404.96, "duration": 3.36}, {"text": "so if you if you're not", "start": 1406.159, "duration": 4.561}, {"text": "um familiar with the softmax from you", "start": 1408.32, "duration": 3.479}, {"text": "know from things like logistic", "start": 1410.72, "duration": 4.5}, {"text": "regression or or cross entropy terms uh", "start": 1411.799, "duration": 5.401}, {"text": "you know the the cares apis and the", "start": 1415.22, "duration": 5.04}, {"text": "Keras has good the Keras documentation", "start": 1417.2, "duration": 4.92}, {"text": "is good tutorials that that will explain", "start": 1420.26, "duration": 3.659}, {"text": "some of this", "start": 1422.12, "duration": 3.72}, {"text": "uh the last line here I have an", "start": 1423.919, "duration": 4.38}, {"text": "Optimizer I like to use the atom", "start": 1425.84, "duration": 4.5}, {"text": "Optimizer other people like", "start": 1428.299, "duration": 5.301}, {"text": "stochasticating descent with a a", "start": 1430.34, "duration": 5.28}, {"text": "adaptive learning rate", "start": 1433.6, "duration": 4.0}, {"text": "those are two that I see are used the", "start": 1435.62, "duration": 3.9}, {"text": "most but there's other ones", "start": 1437.6, "duration": 3.12}, {"text": "um", "start": 1439.52, "duration": 2.7}, {"text": "and it's hard to know which one is", "start": 1440.72, "duration": 2.819}, {"text": "better but we'll talk a little bit more", "start": 1442.22, "duration": 3.74}, {"text": "about that", "start": 1443.539, "duration": 2.421}, {"text": "okay and the thing to remember about the", "start": 1446.179, "duration": 4.921}, {"text": "way Karis does it is you know every", "start": 1447.86, "duration": 6.6}, {"text": "layer has some input output and it's not", "start": 1451.1, "duration": 5.1}, {"text": "obvious when you're doing this what is", "start": 1454.46, "duration": 3.62}, {"text": "the input output shape", "start": 1456.2, "duration": 4.8}, {"text": "Karis will try to figure it out so I'm", "start": 1458.08, "duration": 4.719}, {"text": "not actually saying when I talk about", "start": 1461.0, "duration": 3.96}, {"text": "the max pool layer I'm just you know", "start": 1462.799, "duration": 5.041}, {"text": "saying add this layer I'm not telling it", "start": 1464.96, "duration": 4.74}, {"text": "what the output shape is going to be", "start": 1467.84, "duration": 4.44}, {"text": "only with the dents with the last layer", "start": 1469.7, "duration": 4.32}, {"text": "of the or the dense layers I'm telling", "start": 1472.28, "duration": 3.54}, {"text": "it this is the number of hidden units so", "start": 1474.02, "duration": 5.899}, {"text": "you're going to have 32 units as output", "start": 1475.82, "duration": 4.099}, {"text": "okay", "start": 1480.02, "duration": 2.72}, {"text": "um okay and this is the convolution", "start": 1485.24, "duration": 3.12}, {"text": "layer as I said before just zoomed in a", "start": 1486.44, "duration": 3.119}, {"text": "little bit this is what the parameters", "start": 1488.36, "duration": 3.48}, {"text": "look like", "start": 1489.559, "duration": 4.62}, {"text": "uh and if you run this if you train a", "start": 1491.84, "duration": 4.56}, {"text": "network on this data if you if you look", "start": 1494.179, "duration": 3.36}, {"text": "at that demo you're going to get", "start": 1496.4, "duration": 3.54}, {"text": "something like this if you in that in", "start": 1497.539, "duration": 4.26}, {"text": "that particular notebook I have", "start": 1499.94, "duration": 5.82}, {"text": "I'm listening I'm I am plotting on the", "start": 1501.799, "duration": 4.921}, {"text": "left", "start": 1505.76, "duration": 3.12}, {"text": "three by three filters so I asked I ran", "start": 1506.72, "duration": 4.079}, {"text": "this network with three by three filters", "start": 1508.88, "duration": 4.98}, {"text": "16 of them so these are 16 3x3 filters", "start": 1510.799, "duration": 4.74}, {"text": "and you look at this filter", "start": 1513.86, "duration": 4.74}, {"text": "uh it's not obvious what these filters", "start": 1515.539, "duration": 6.301}, {"text": "are doing but here's the activation", "start": 1518.6, "duration": 5.939}, {"text": "function the activation output for those", "start": 1521.84, "duration": 4.68}, {"text": "16 filters these remember the first", "start": 1524.539, "duration": 4.02}, {"text": "convolution layer", "start": 1526.52, "duration": 4.68}, {"text": "um and again it's taken the input of a", "start": 1528.559, "duration": 4.561}, {"text": "five and it's done some transformation", "start": 1531.2, "duration": 4.079}, {"text": "some of this is not doing much of a", "start": 1533.12, "duration": 3.9}, {"text": "transformation some of this is doing a", "start": 1535.279, "duration": 3.841}, {"text": "really kind of suggestive transformation", "start": 1537.02, "duration": 4.86}, {"text": "so this bottom left", "start": 1539.12, "duration": 6.2}, {"text": "is looking like it's highlighting maybe", "start": 1541.88, "duration": 6.06}, {"text": "part of the edge", "start": 1545.32, "duration": 5.44}, {"text": "of the middle part of a five and if we", "start": 1547.94, "duration": 4.92}, {"text": "look at that filter", "start": 1550.76, "duration": 5.1}, {"text": "there is a white square in the upper", "start": 1552.86, "duration": 6.059}, {"text": "right and then darker squares below so", "start": 1555.86, "duration": 5.48}, {"text": "this filter what would match this filter", "start": 1558.919, "duration": 5.941}, {"text": "exactly this kind of edge", "start": 1561.34, "duration": 5.8}, {"text": "okay I hope that sort of makes intuitive", "start": 1564.86, "duration": 3.54}, {"text": "sense", "start": 1567.14, "duration": 1.98}, {"text": "um", "start": 1568.4, "duration": 2.279}, {"text": "and so it's hard to know what all these", "start": 1569.12, "duration": 2.88}, {"text": "filters do and how to do what all these", "start": 1570.679, "duration": 2.581}, {"text": "Transformations are doing but sometimes", "start": 1572.0, "duration": 3.659}, {"text": "you can sort of you know", "start": 1573.26, "duration": 6.299}, {"text": "uh uh get some sense about it", "start": 1575.659, "duration": 6.241}, {"text": "but what if we change the filter size so", "start": 1579.559, "duration": 4.801}, {"text": "here I S I said let's use a nine by nine", "start": 1581.9, "duration": 3.779}, {"text": "as the first convolution layer so", "start": 1584.36, "duration": 2.46}, {"text": "instead of three by three where I'm", "start": 1585.679, "duration": 2.641}, {"text": "looking for granular stuff now I'm", "start": 1586.82, "duration": 4.2}, {"text": "looking for nine by nine stuff", "start": 1588.32, "duration": 6.06}, {"text": "so the outputs or the filters themselves", "start": 1591.02, "duration": 5.94}, {"text": "again hard to interpret it's tempting to", "start": 1594.38, "duration": 3.899}, {"text": "say oh it's looking for an edge this", "start": 1596.96, "duration": 2.52}, {"text": "direction looking for an edge that", "start": 1598.279, "duration": 4.741}, {"text": "direction uh certainly if we look at", "start": 1599.48, "duration": 5.699}, {"text": "these Transformations", "start": 1603.02, "duration": 4.259}, {"text": "um you know it's very suggestive like", "start": 1605.179, "duration": 4.201}, {"text": "it's trying to highlight some section of", "start": 1607.279, "duration": 4.201}, {"text": "a five instead of just a edge Now with", "start": 1609.38, "duration": 4.919}, {"text": "an entire line segment with a certain", "start": 1611.48, "duration": 4.26}, {"text": "thickness", "start": 1614.299, "duration": 4.98}, {"text": "so as you get to larger filter sizes", "start": 1615.74, "duration": 5.76}, {"text": "you're looking at presumably and", "start": 1619.279, "duration": 3.961}, {"text": "intuitively and we can see it to some", "start": 1621.5, "duration": 3.299}, {"text": "extent larger", "start": 1623.24, "duration": 4.28}, {"text": "features", "start": 1624.799, "duration": 2.721}, {"text": "I hope that makes a little bit of sense", "start": 1627.799, "duration": 4.321}, {"text": "um because really you know you can spend", "start": 1630.5, "duration": 2.88}, {"text": "a lot of time thinking about what these", "start": 1632.12, "duration": 2.72}, {"text": "convolutions or layers are doing", "start": 1633.38, "duration": 3.72}, {"text": "obviously with the large networks it's", "start": 1634.84, "duration": 5.92}, {"text": "you know uh", "start": 1637.1, "duration": 5.22}, {"text": "people do spend some time trying to", "start": 1640.76, "duration": 4.279}, {"text": "interpret those things", "start": 1642.32, "duration": 2.719}, {"text": "okay so that's the basic convolution", "start": 1645.2, "duration": 4.5}, {"text": "Network you know nowadays there's all", "start": 1646.94, "duration": 3.66}, {"text": "kinds of", "start": 1649.7, "duration": 3.0}, {"text": "um uh deep learning models for specific", "start": 1650.6, "duration": 4.319}, {"text": "kind of tasks a lot you know the success", "start": 1652.7, "duration": 3.9}, {"text": "of deep learning models are handling", "start": 1654.919, "duration": 3.721}, {"text": "unstructured data and so you're going to", "start": 1656.6, "duration": 4.559}, {"text": "have similar kinds of you know", "start": 1658.64, "duration": 4.62}, {"text": "conceptually strategies to when you're", "start": 1661.159, "duration": 3.481}, {"text": "building a network whether it's a vision", "start": 1663.26, "duration": 4.14}, {"text": "Network or sequence Network or you know", "start": 1664.64, "duration": 4.5}, {"text": "these large language models there's some", "start": 1667.4, "duration": 3.6}, {"text": "feature processing going on in the lower", "start": 1669.14, "duration": 3.419}, {"text": "layers", "start": 1671.0, "duration": 2.94}, {"text": "um", "start": 1672.559, "duration": 3.961}, {"text": "and uh you know some maybe some ad Club", "start": 1673.94, "duration": 4.8}, {"text": "classification at the output layers", "start": 1676.52, "duration": 5.519}, {"text": "or some other tasks", "start": 1678.74, "duration": 4.919}, {"text": "all right so let me let me switch to", "start": 1682.039, "duration": 3.301}, {"text": "like some guidelines of how you might", "start": 1683.659, "duration": 3.961}, {"text": "run this on HPC or how how you might run", "start": 1685.34, "duration": 4.699}, {"text": "these in general", "start": 1687.62, "duration": 2.419}, {"text": "okay so there's like things to think", "start": 1690.2, "duration": 4.62}, {"text": "about in your project", "start": 1692.539, "duration": 4.681}, {"text": "uh so the you know the first thing", "start": 1694.82, "duration": 4.08}, {"text": "really is choosing hyper parameters we", "start": 1697.22, "duration": 3.839}, {"text": "as I said before layers there's there's", "start": 1698.9, "duration": 3.84}, {"text": "the architecture I prepare we'll talk a", "start": 1701.059, "duration": 3.36}, {"text": "little bit about that the the basic idea", "start": 1702.74, "duration": 2.58}, {"text": "is you're going to do a little bit of", "start": 1704.419, "duration": 2.461}, {"text": "exploration and exploitation if you're", "start": 1705.32, "duration": 3.3}, {"text": "really with those terms", "start": 1706.88, "duration": 3.659}, {"text": "you need to figure out an efficient job", "start": 1708.62, "duration": 4.32}, {"text": "workflow that's that's something that's", "start": 1710.539, "duration": 3.481}, {"text": "really", "start": 1712.94, "duration": 3.0}, {"text": "you know some in some sense it's your", "start": 1714.02, "duration": 3.3}, {"text": "personal preference sometimes you got to", "start": 1715.94, "duration": 4.08}, {"text": "look at what other people's done", "start": 1717.32, "duration": 3.78}, {"text": "um", "start": 1720.02, "duration": 3.12}, {"text": "uh sometimes the CPU works fine for a", "start": 1721.1, "duration": 4.439}, {"text": "lot of cases uh especially if you're", "start": 1723.14, "duration": 3.96}, {"text": "using small data and you want to just", "start": 1725.539, "duration": 3.361}, {"text": "test things if you're if you're taking a", "start": 1727.1, "duration": 3.54}, {"text": "pre-trained model sometimes you can just", "start": 1728.9, "duration": 3.06}, {"text": "run those on a CPU if you're not", "start": 1730.64, "duration": 3.0}, {"text": "training but you just want to run the", "start": 1731.96, "duration": 3.06}, {"text": "calculations and see test it with", "start": 1733.64, "duration": 2.82}, {"text": "different data", "start": 1735.02, "duration": 2.759}, {"text": "uh you're definitely going to want to", "start": 1736.46, "duration": 3.599}, {"text": "use gpus for large models or large data", "start": 1737.779, "duration": 3.481}, {"text": "sets but then you have to just be", "start": 1740.059, "duration": 4.641}, {"text": "careful about the memory trade-offs", "start": 1741.26, "duration": 3.44}, {"text": "um", "start": 1745.1, "duration": 2.52}, {"text": "you know you can do model saving and", "start": 1746.059, "duration": 3.661}, {"text": "checkpoint that that's all fine in", "start": 1747.62, "duration": 6.12}, {"text": "tensorflow uh tensorboard is a tool that", "start": 1749.72, "duration": 6.0}, {"text": "helps you monitor", "start": 1753.74, "duration": 4.919}, {"text": "your performance as it's running and we", "start": 1755.72, "duration": 5.52}, {"text": "do have that set up to run on expanse", "start": 1758.659, "duration": 4.981}, {"text": "but there's some uh but to run it", "start": 1761.24, "duration": 4.38}, {"text": "securely we have to run it you can't", "start": 1763.64, "duration": 3.36}, {"text": "just run it directly", "start": 1765.62, "duration": 3.419}, {"text": "so we do have a", "start": 1767.0, "duration": 4.74}, {"text": "a procedure to do that and I don't have", "start": 1769.039, "duration": 4.5}, {"text": "it I don't have a slide for it but if", "start": 1771.74, "duration": 2.939}, {"text": "you wanted if you want to use", "start": 1773.539, "duration": 5.161}, {"text": "tensorboard I we can we can set that up", "start": 1774.679, "duration": 7.681}, {"text": "I find that it's not necessary for me", "start": 1778.7, "duration": 5.82}, {"text": "like I don't particularly get much out", "start": 1782.36, "duration": 4.439}, {"text": "of tension board but I do not on a GPU", "start": 1784.52, "duration": 4.32}, {"text": "if you're trying to debug certain things", "start": 1786.799, "duration": 3.661}, {"text": "of GPU or get it more efficiently then", "start": 1788.84, "duration": 3.12}, {"text": "time support has some tools that could", "start": 1790.46, "duration": 3.86}, {"text": "be nice", "start": 1791.96, "duration": 2.36}, {"text": "okay", "start": 1796.179, "duration": 3.581}, {"text": "all right let's talk about juicing hyper", "start": 1797.899, "duration": 3.241}, {"text": "parameters so in general you know we", "start": 1799.76, "duration": 3.0}, {"text": "have architectural related hyper", "start": 1801.14, "duration": 3.36}, {"text": "parameters algorithm related hyper", "start": 1802.76, "duration": 2.82}, {"text": "parameters", "start": 1804.5, "duration": 3.419}, {"text": "uh other parameters to help you just", "start": 1805.58, "duration": 3.839}, {"text": "learn more efficiently", "start": 1807.919, "duration": 4.021}, {"text": "I'm not going to talk about all these", "start": 1809.419, "duration": 4.201}, {"text": "um but when you're building your network", "start": 1811.94, "duration": 4.38}, {"text": "you know just try to keep in mind uh", "start": 1813.62, "duration": 4.86}, {"text": "what are the things you know some of", "start": 1816.32, "duration": 4.02}, {"text": "these things you don't you probably want", "start": 1818.48, "duration": 4.02}, {"text": "to look at what other people did and", "start": 1820.34, "duration": 3.66}, {"text": "really everything you want to look at", "start": 1822.5, "duration": 4.74}, {"text": "how other people have have set this up", "start": 1824.0, "duration": 4.919}, {"text": "um", "start": 1827.24, "duration": 3.419}, {"text": "some options are sort of determined by", "start": 1828.919, "duration": 3.181}, {"text": "the task like your loss function if", "start": 1830.659, "duration": 2.52}, {"text": "you're doing classification versus", "start": 1832.1, "duration": 2.939}, {"text": "regression", "start": 1833.179, "duration": 4.081}, {"text": "and again use what works see if you know", "start": 1835.039, "duration": 4.02}, {"text": "you can find related work from from", "start": 1837.26, "duration": 3.6}, {"text": "other um", "start": 1839.059, "duration": 5.12}, {"text": "from papers and so forth", "start": 1840.86, "duration": 3.319}, {"text": "so another note about a hyper painters", "start": 1844.46, "duration": 3.9}, {"text": "so it can take a long time this is this", "start": 1846.62, "duration": 4.02}, {"text": "is where you could get stuck finding you", "start": 1848.36, "duration": 4.38}, {"text": "know good set of parameters it's hard to", "start": 1850.64, "duration": 3.84}, {"text": "find a global Optimum", "start": 1852.74, "duration": 3.539}, {"text": "so if that's you know if you're trying", "start": 1854.48, "duration": 3.299}, {"text": "to squeeze out unless you're doing a", "start": 1856.279, "duration": 4.741}, {"text": "competition uh squeezing out you know", "start": 1857.779, "duration": 5.461}, {"text": "very small criminal improvements maybe", "start": 1861.02, "duration": 4.44}, {"text": "that's not the the most important thing", "start": 1863.24, "duration": 4.319}, {"text": "I always get to try to start with small", "start": 1865.46, "duration": 3.78}, {"text": "data to get a sense of what good", "start": 1867.559, "duration": 3.24}, {"text": "parameters are values what good", "start": 1869.24, "duration": 3.12}, {"text": "parameter values are sometimes it's good", "start": 1870.799, "duration": 3.24}, {"text": "to start with small data", "start": 1872.36, "duration": 4.5}, {"text": "make sure you have a network that can at", "start": 1874.039, "duration": 4.921}, {"text": "least overfit if you have trouble", "start": 1876.86, "duration": 3.36}, {"text": "running your network and you feel like", "start": 1878.96, "duration": 3.12}, {"text": "it's not learning anything it's just", "start": 1880.22, "duration": 4.16}, {"text": "producing like random output", "start": 1882.08, "duration": 4.74}, {"text": "uh you know try to do a little bit", "start": 1884.38, "duration": 4.48}, {"text": "smaller data set and just have make sure", "start": 1886.82, "duration": 3.54}, {"text": "you have enough resources and at least", "start": 1888.86, "duration": 3.36}, {"text": "see if you can overfit so I had a I had", "start": 1890.36, "duration": 4.02}, {"text": "a case just a few weeks ago I", "start": 1892.22, "duration": 5.1}, {"text": "I had mislabeled my output and the", "start": 1894.38, "duration": 5.039}, {"text": "network was like learning nothing it was", "start": 1897.32, "duration": 4.859}, {"text": "just producing random output so I made a", "start": 1899.419, "duration": 4.321}, {"text": "smaller data set now that at least I", "start": 1902.179, "duration": 3.24}, {"text": "should make sure I can", "start": 1903.74, "duration": 5.28}, {"text": "over fit the training data and and sure", "start": 1905.419, "duration": 5.461}, {"text": "enough I could learn the training data", "start": 1909.02, "duration": 3.84}, {"text": "but still it generalized to my", "start": 1910.88, "duration": 4.32}, {"text": "validation data just with random", "start": 1912.86, "duration": 4.86}, {"text": "guessing so that told me okay maybe I", "start": 1915.2, "duration": 4.68}, {"text": "had a either you know I didn't think the", "start": 1917.72, "duration": 5.1}, {"text": "problem was so complicated", "start": 1919.88, "duration": 5.039}, {"text": "um so that that suggest so then I was", "start": 1922.82, "duration": 3.839}, {"text": "able to go and figure out I had you know", "start": 1924.919, "duration": 5.0}, {"text": "I had some data problems", "start": 1926.659, "duration": 3.26}, {"text": "okay", "start": 1930.02, "duration": 2.879}, {"text": "and the easy thing to do with Hyper", "start": 1931.46, "duration": 2.76}, {"text": "parameter search is to do a grid search", "start": 1932.899, "duration": 4.201}, {"text": "tick out space values like you know one", "start": 1934.22, "duration": 4.86}, {"text": "layer two layer four layers eight layers", "start": 1937.1, "duration": 3.6}, {"text": "for example if you're doing multi-layer", "start": 1939.08, "duration": 3.66}, {"text": "perceptron stuff if you're doing or even", "start": 1940.7, "duration": 4.62}, {"text": "if you're doing convolution stuff", "start": 1942.74, "duration": 4.319}, {"text": "um you know figure out a way to space", "start": 1945.32, "duration": 4.62}, {"text": "out your hyper parameter values", "start": 1947.059, "duration": 3.961}, {"text": "um", "start": 1949.94, "duration": 3.119}, {"text": "and and", "start": 1951.02, "duration": 3.6}, {"text": "you know when I say exploration versus", "start": 1953.059, "duration": 3.181}, {"text": "exploitation what I mean is you know if", "start": 1954.62, "duration": 2.939}, {"text": "you start with a small data set you get", "start": 1956.24, "duration": 2.64}, {"text": "a good idea of what kind of parameters", "start": 1957.559, "duration": 4.321}, {"text": "work and then you could do a grid search", "start": 1958.88, "duration": 5.22}, {"text": "and then you could say well look I have", "start": 1961.88, "duration": 4.26}, {"text": "good values here and maybe some good", "start": 1964.1, "duration": 4.559}, {"text": "combination of values here uh I'm gonna", "start": 1966.14, "duration": 4.62}, {"text": "focus in on those values and do maybe a", "start": 1968.659, "duration": 4.081}, {"text": "more focused grid search", "start": 1970.76, "duration": 4.519}, {"text": "there is a tool called", "start": 1972.74, "duration": 5.28}, {"text": "Keras has a tool called the", "start": 1975.279, "duration": 5.321}, {"text": "we're looking at the slide hyperband and", "start": 1978.02, "duration": 4.379}, {"text": "uh it's a tuner it has some tuner", "start": 1980.6, "duration": 2.699}, {"text": "functions that help with the white", "start": 1982.399, "duration": 3.26}, {"text": "search", "start": 1983.299, "duration": 2.36}, {"text": "uh okay so it's called hyper tuner sorry", "start": 1985.82, "duration": 4.859}, {"text": "not hyperband uh and so this implements", "start": 1988.039, "duration": 4.201}, {"text": "several strategies hyperband is one of", "start": 1990.679, "duration": 2.88}, {"text": "the strategies and that's like running a", "start": 1992.24, "duration": 3.12}, {"text": "little tournament so you can have a lot", "start": 1993.559, "duration": 3.84}, {"text": "of configurations of hyper parameters", "start": 1995.36, "duration": 4.679}, {"text": "and it'll help you weed weed out which", "start": 1997.399, "duration": 4.981}, {"text": "ones to focus on it'll sort of I'll show", "start": 2000.039, "duration": 4.98}, {"text": "you what that looks like in a second", "start": 2002.38, "duration": 4.139}, {"text": "um a random search a bit I haven't used", "start": 2005.019, "duration": 4.02}, {"text": "the Bayesian optimization but this one", "start": 2006.519, "duration": 4.321}, {"text": "is like doing kind of a function", "start": 2009.039, "duration": 4.62}, {"text": "approximization so it's it's looking at", "start": 2010.84, "duration": 5.88}, {"text": "combinations of hyper parameters that", "start": 2013.659, "duration": 4.681}, {"text": "where it hasn't searched before and", "start": 2016.72, "duration": 2.699}, {"text": "it'll like it'll like put together", "start": 2018.34, "duration": 3.6}, {"text": "combinations for you", "start": 2019.419, "duration": 4.64}, {"text": "um", "start": 2021.94, "duration": 2.119}, {"text": "but this is let me do let me show you", "start": 2024.7, "duration": 3.12}, {"text": "what the hyperband one looks like so", "start": 2026.38, "duration": 4.08}, {"text": "each it sort of sets up a tournament and", "start": 2027.82, "duration": 5.04}, {"text": "each round it's gonna take some Network", "start": 2030.46, "duration": 4.319}, {"text": "configurations for a small number of", "start": 2032.86, "duration": 3.299}, {"text": "models", "start": 2034.779, "duration": 3.961}, {"text": "I'm sorry small number of epics", "start": 2036.159, "duration": 4.681}, {"text": "and then it's going to run let's say the", "start": 2038.74, "duration": 4.38}, {"text": "first round with you know here I have", "start": 2040.84, "duration": 4.679}, {"text": "like eight networks in round one for", "start": 2043.12, "duration": 4.08}, {"text": "just a few epics and then round two it's", "start": 2045.519, "duration": 3.241}, {"text": "going to run less networks but for", "start": 2047.2, "duration": 3.6}, {"text": "longer epics and round three is it's", "start": 2048.76, "duration": 3.659}, {"text": "weeding out there so it's now it's only", "start": 2050.8, "duration": 4.14}, {"text": "like four maybe networks at round three", "start": 2052.419, "duration": 5.161}, {"text": "and then round four it's only you know", "start": 2054.94, "duration": 4.26}, {"text": "the last two networks and one of them is", "start": 2057.58, "duration": 2.519}, {"text": "a winner", "start": 2059.2, "duration": 2.699}, {"text": "so it's almost like setting up a turn in", "start": 2060.099, "duration": 3.78}, {"text": "it but it's not going all the way with", "start": 2061.899, "duration": 3.361}, {"text": "all the number of epics so you have to", "start": 2063.879, "duration": 3.72}, {"text": "set up the parameters so you can set up", "start": 2065.26, "duration": 5.119}, {"text": "this little tournament", "start": 2067.599, "duration": 2.78}, {"text": "uh and just to give you a feel for what", "start": 2071.5, "duration": 4.26}, {"text": "that looks like if you have some code", "start": 2073.3, "duration": 4.319}, {"text": "and you're doing some model and you have", "start": 2075.76, "duration": 4.139}, {"text": "a you know it's a good idea to make your", "start": 2077.619, "duration": 4.26}, {"text": "model a function to build have a build", "start": 2079.899, "duration": 3.541}, {"text": "model function and in this build model", "start": 2081.879, "duration": 3.181}, {"text": "function you have your", "start": 2083.44, "duration": 3.179}, {"text": "you know your parameter choices maybe", "start": 2085.06, "duration": 3.059}, {"text": "it's an activation Choice maybe it's a", "start": 2086.619, "duration": 2.941}, {"text": "number of filters", "start": 2088.119, "duration": 3.961}, {"text": "and these are your hyper parameters and", "start": 2089.56, "duration": 4.44}, {"text": "then to run this with a tuner you're", "start": 2092.08, "duration": 3.299}, {"text": "going to surround that you're going to", "start": 2094.0, "duration": 4.14}, {"text": "have another build model function that", "start": 2095.379, "duration": 5.46}, {"text": "wraps around the fir your first one with", "start": 2098.14, "duration": 6.24}, {"text": "your hyper parameter your hyper tuner", "start": 2100.839, "duration": 5.461}, {"text": "parameters so this is basically your", "start": 2104.38, "duration": 3.239}, {"text": "choice information so you're going to", "start": 2106.3, "duration": 3.9}, {"text": "say okay my number of filters are going", "start": 2107.619, "duration": 6.48}, {"text": "to go between 8 and 32.", "start": 2110.2, "duration": 4.56}, {"text": "um", "start": 2114.099, "duration": 4.26}, {"text": "and then you have a tuner object in", "start": 2114.76, "duration": 6.18}, {"text": "which you are telling this hyperband", "start": 2118.359, "duration": 5.641}, {"text": "function to use the wrapper build model", "start": 2120.94, "duration": 5.76}, {"text": "use the wrapper build model everything", "start": 2124.0, "duration": 4.02}, {"text": "and there are all these other parameters", "start": 2126.7, "duration": 3.659}, {"text": "are like your fit function parameters", "start": 2128.02, "duration": 5.099}, {"text": "so this way their hyperband function can", "start": 2130.359, "duration": 5.781}, {"text": "create this tournament", "start": 2133.119, "duration": 3.021}, {"text": "um oh just let me so I find it's kind of", "start": 2138.28, "duration": 4.62}, {"text": "useful to help me do a wide search to to", "start": 2140.68, "duration": 5.159}, {"text": "do something like this but uh in the end", "start": 2142.9, "duration": 5.58}, {"text": "I I always like once I find a good set", "start": 2145.839, "duration": 4.861}, {"text": "of parameters uh I'll end up doing a", "start": 2148.48, "duration": 3.72}, {"text": "grid search and a smaller you know into", "start": 2150.7, "duration": 4.22}, {"text": "a smaller set", "start": 2152.2, "duration": 2.72}, {"text": "what about workflow so", "start": 2155.26, "duration": 4.859}, {"text": "um I think the typical workflow you're", "start": 2158.079, "duration": 3.301}, {"text": "gonna you're gonna probably want to do", "start": 2160.119, "duration": 3.0}, {"text": "is you know you're going to have a job", "start": 2161.38, "duration": 3.54}, {"text": "and for each job you're going to run", "start": 2163.119, "duration": 4.5}, {"text": "some model and each model you're going", "start": 2164.92, "duration": 4.62}, {"text": "to Loop through maybe cross validation", "start": 2167.619, "duration": 3.781}, {"text": "data sets maybe a couple different you", "start": 2169.54, "duration": 3.299}, {"text": "know just maybe not a full cross", "start": 2171.4, "duration": 3.179}, {"text": "validation but like training validation", "start": 2172.839, "duration": 4.74}, {"text": "tests with a few parameters", "start": 2174.579, "duration": 3.661}, {"text": "um", "start": 2177.579, "duration": 2.76}, {"text": "so this is the typical kind of thing if", "start": 2178.24, "duration": 4.379}, {"text": "you had like super large data maybe you", "start": 2180.339, "duration": 3.121}, {"text": "would", "start": 2182.619, "duration": 2.761}, {"text": "you know maybe you would just load the", "start": 2183.46, "duration": 3.36}, {"text": "data into memory and then run a bunch of", "start": 2185.38, "duration": 2.82}, {"text": "models so you could imagine situations", "start": 2186.82, "duration": 3.84}, {"text": "where these last two levels are reversed", "start": 2188.2, "duration": 4.26}, {"text": "so you have to sort of consider how long", "start": 2190.66, "duration": 4.8}, {"text": "a model runs for one configuration", "start": 2192.46, "duration": 6.06}, {"text": "of hyper parameters for one data set", "start": 2195.46, "duration": 4.8}, {"text": "and you want to organize your jobs into", "start": 2198.52, "duration": 4.079}, {"text": "reasonable chunks of work", "start": 2200.26, "duration": 3.3}, {"text": "um", "start": 2202.599, "duration": 3.361}, {"text": "and of course if you have large models", "start": 2203.56, "duration": 3.779}, {"text": "or large jobs you ought to consider", "start": 2205.96, "duration": 4.34}, {"text": "model checkpoints", "start": 2207.339, "duration": 2.961}, {"text": "uh configurations about also be talking", "start": 2211.06, "duration": 3.96}, {"text": "about configurations of hyper parameters", "start": 2213.52, "duration": 3.12}, {"text": "and somehow you want to set that up so", "start": 2215.02, "duration": 3.72}, {"text": "your model is you know you can sort of", "start": 2216.64, "duration": 4.02}, {"text": "set that up you want to set it up so at", "start": 2218.74, "duration": 4.26}, {"text": "the job level you can easily change what", "start": 2220.66, "duration": 4.08}, {"text": "those configurations look like", "start": 2223.0, "duration": 4.14}, {"text": "and that could be done through arguments", "start": 2224.74, "duration": 3.96}, {"text": "in Python you know python has this nice", "start": 2227.14, "duration": 3.959}, {"text": "tool to handle arguments a lot of people", "start": 2228.7, "duration": 4.2}, {"text": "like to use yaml files so here's what it", "start": 2231.099, "duration": 3.421}, {"text": "looks like to use a yamlify you just", "start": 2232.9, "duration": 6.08}, {"text": "it's basically a text file with some uh", "start": 2234.52, "duration": 8.64}, {"text": "parameter parameters and values and you", "start": 2238.98, "duration": 6.82}, {"text": "basically load that text file and you", "start": 2243.16, "duration": 5.52}, {"text": "can extract the particular parameters in", "start": 2245.8, "duration": 4.62}, {"text": "the text file for your parameters in the", "start": 2248.68, "duration": 3.24}, {"text": "program so it's like reading the", "start": 2250.42, "duration": 2.939}, {"text": "dictionary and that's kind of nice", "start": 2251.92, "duration": 3.659}, {"text": "because it's a text file you can you can", "start": 2253.359, "duration": 4.74}, {"text": "run your job you save the text file you", "start": 2255.579, "duration": 4.561}, {"text": "can modify a text file and you know just", "start": 2258.099, "duration": 5.601}, {"text": "make sure you keep track of stuff there", "start": 2260.14, "duration": 3.56}, {"text": "um on something like expanse you could", "start": 2264.16, "duration": 4.679}, {"text": "uh you know if you have a text file you", "start": 2266.32, "duration": 4.14}, {"text": "could actually in your job just have", "start": 2268.839, "duration": 4.441}, {"text": "statements to set up those parameters so", "start": 2270.46, "duration": 4.2}, {"text": "every time you run a job everything", "start": 2273.28, "duration": 3.54}, {"text": "could be right in your job script", "start": 2274.66, "duration": 4.679}, {"text": "uh so here's here's an example I I was", "start": 2276.82, "duration": 4.44}, {"text": "sort of playing around with", "start": 2279.339, "duration": 3.721}, {"text": "so sometimes people like it this way I", "start": 2281.26, "duration": 3.18}, {"text": "don't know I it's sort of your personal", "start": 2283.06, "duration": 3.86}, {"text": "preference", "start": 2284.44, "duration": 2.48}, {"text": "okay so I've been talking about python", "start": 2287.44, "duration": 4.679}, {"text": "notebooks in our demos and but you know", "start": 2289.24, "duration": 4.8}, {"text": "when you run this for after a while you", "start": 2292.119, "duration": 3.301}, {"text": "kind of get you really don't want to", "start": 2294.04, "duration": 3.84}, {"text": "always have a notebook maybe you do if", "start": 2295.42, "duration": 3.6}, {"text": "you want to keep things in your notebook", "start": 2297.88, "duration": 4.26}, {"text": "you can and then you can use a Jupiter", "start": 2299.02, "duration": 5.64}, {"text": "convert command and right there in your", "start": 2302.14, "duration": 4.02}, {"text": "JavaScript for example you could convert", "start": 2304.66, "duration": 5.04}, {"text": "it to um to a script convert your python", "start": 2306.16, "duration": 5.34}, {"text": "notebook into a script and then run your", "start": 2309.7, "duration": 3.48}, {"text": "script the only thing you have to do is", "start": 2311.5, "duration": 3.0}, {"text": "be careful to turn off your plot", "start": 2313.18, "duration": 3.72}, {"text": "displays or maybe save your plots in a", "start": 2314.5, "duration": 4.8}, {"text": "file and and then set up your you know", "start": 2316.9, "duration": 3.48}, {"text": "have some good way to set up your", "start": 2319.3, "duration": 3.539}, {"text": "parameters paper mill is another tool", "start": 2320.38, "duration": 5.699}, {"text": "that people use it takes a little bit of", "start": 2322.839, "duration": 5.701}, {"text": "um uh other programming so wrap your", "start": 2326.079, "duration": 5.161}, {"text": "your output your your plots a certain", "start": 2328.54, "duration": 4.86}, {"text": "way and a couple other things so you so", "start": 2331.24, "duration": 3.54}, {"text": "that's not a that's that's another", "start": 2333.4, "duration": 3.78}, {"text": "option if you want to do that", "start": 2334.78, "duration": 4.319}, {"text": "uh if you're doing this over and over", "start": 2337.18, "duration": 3.659}, {"text": "again you know maybe you just take your", "start": 2339.099, "duration": 3.901}, {"text": "notebook and put in a python script and", "start": 2340.839, "duration": 5.661}, {"text": "that's probably what most people do", "start": 2343.0, "duration": 3.5}, {"text": "let's talk a little about GPU nodes", "start": 2346.66, "duration": 7.8}, {"text": "okay oops sorry about that here we are", "start": 2350.26, "duration": 6.06}, {"text": "um Okay so", "start": 2354.46, "duration": 4.379}, {"text": "if you're not familiar with gpus GPU has", "start": 2356.32, "duration": 4.5}, {"text": "multiple typically a GPU node has", "start": 2358.839, "duration": 4.321}, {"text": "multiple devices", "start": 2360.82, "duration": 6.06}, {"text": "Now by default tensorflow will run uh if", "start": 2363.16, "duration": 5.4}, {"text": "it's running on a GPU", "start": 2366.88, "duration": 4.02}, {"text": "it's going to look and see if there's a", "start": 2368.56, "duration": 4.5}, {"text": "GPU and it has these commands in fact I", "start": 2370.9, "duration": 5.4}, {"text": "have a little snippet of code that shows", "start": 2373.06, "duration": 5.4}, {"text": "you the kind of commands tensorflow has", "start": 2376.3, "duration": 3.9}, {"text": "to look and see if it's you know if", "start": 2378.46, "duration": 4.02}, {"text": "there's a GPU device so if there's a GPU", "start": 2380.2, "duration": 4.32}, {"text": "device it'll run on this the first GPU", "start": 2382.48, "duration": 3.96}, {"text": "device or the zero", "start": 2384.52, "duration": 4.319}, {"text": "if there's a GPU otherwise it'll use", "start": 2386.44, "duration": 4.02}, {"text": "your CPU cores", "start": 2388.839, "duration": 3.661}, {"text": "and remember this is all those dot all", "start": 2390.46, "duration": 5.82}, {"text": "those dot products that we saw in the", "start": 2392.5, "duration": 5.64}, {"text": "forward propagation of backup floor", "start": 2396.28, "duration": 3.42}, {"text": "propagation back propagation steps", "start": 2398.14, "duration": 2.939}, {"text": "that's really what it's going to be", "start": 2399.7, "duration": 4.28}, {"text": "using the GPU for", "start": 2401.079, "duration": 2.901}, {"text": "okay", "start": 2404.8, "duration": 4.26}, {"text": "in general", "start": 2407.079, "duration": 3.601}, {"text": "uh so here's a little test I did it", "start": 2409.06, "duration": 3.0}, {"text": "generally your GPU is always going to be", "start": 2410.68, "duration": 2.46}, {"text": "faster so", "start": 2412.06, "duration": 2.7}, {"text": "this was a test where I had a", "start": 2413.14, "duration": 3.479}, {"text": "multi-layer perceptron with 4 to 12", "start": 2414.76, "duration": 3.38}, {"text": "layers", "start": 2416.619, "duration": 4.74}, {"text": "512 to a thousand hidden units and the", "start": 2418.14, "duration": 6.459}, {"text": "GPU the time it takes to process 25", "start": 2421.359, "duration": 7.381}, {"text": "epics for example in this with this data", "start": 2424.599, "duration": 6.541}, {"text": "set you know the GPU is just a linear", "start": 2428.74, "duration": 3.66}, {"text": "increase", "start": 2431.14, "duration": 3.66}, {"text": "whereas the CPU it starts to have a", "start": 2432.4, "duration": 4.02}, {"text": "non-linear increase after you know after", "start": 2434.8, "duration": 3.48}, {"text": "you get up into the 12th layer", "start": 2436.42, "duration": 4.56}, {"text": "so these are not terribly large Networks", "start": 2438.28, "duration": 4.44}, {"text": "it's not that you can't run in the TPU", "start": 2440.98, "duration": 3.24}, {"text": "but you're waiting longer So eventually", "start": 2442.72, "duration": 4.2}, {"text": "uh so in this example one I had more", "start": 2444.22, "duration": 4.92}, {"text": "data it was just like I'm waiting a long", "start": 2446.92, "duration": 3.0}, {"text": "time", "start": 2449.14, "duration": 3.0}, {"text": "the downside for GPU", "start": 2449.92, "duration": 5.1}, {"text": "is that there's less gpus on our HPC", "start": 2452.14, "duration": 4.56}, {"text": "systems and so you might be waiting in", "start": 2455.02, "duration": 3.0}, {"text": "the cube more", "start": 2456.7, "duration": 5.18}, {"text": "so that's the trade-off you have", "start": 2458.02, "duration": 3.86}, {"text": "okay", "start": 2462.64, "duration": 2.959}, {"text": "let's talk about the parallelization on", "start": 2466.24, "duration": 3.78}, {"text": "a GPU node let's see what that looks", "start": 2468.52, "duration": 4.079}, {"text": "like so the main approach to paralyze", "start": 2470.02, "duration": 3.78}, {"text": "your training", "start": 2472.599, "duration": 3.361}, {"text": "is called the data parallel approach in", "start": 2473.8, "duration": 4.62}, {"text": "this scheme you take your data you split", "start": 2475.96, "duration": 7.139}, {"text": "it up so we're making our data uh um", "start": 2478.42, "duration": 6.36}, {"text": "into let's say if we're going to run on", "start": 2483.099, "duration": 3.301}, {"text": "you know whatever number of devices we", "start": 2484.78, "duration": 2.76}, {"text": "can split up our data in those number", "start": 2486.4, "duration": 3.24}, {"text": "devices and we're going to launch our", "start": 2487.54, "duration": 4.74}, {"text": "python script our neural network script", "start": 2489.64, "duration": 4.8}, {"text": "on each device", "start": 2492.28, "duration": 4.68}, {"text": "and each device is going to train they", "start": 2494.44, "duration": 4.62}, {"text": "have the same model you're trying to", "start": 2496.96, "duration": 3.659}, {"text": "copy of the model with their part of the", "start": 2499.06, "duration": 2.46}, {"text": "data", "start": 2500.619, "duration": 3.24}, {"text": "and then those parameter updates have to", "start": 2501.52, "duration": 6.9}, {"text": "be aggregated and and broadcast", "start": 2503.859, "duration": 7.141}, {"text": "for across those model instances so", "start": 2508.42, "duration": 4.5}, {"text": "somehow there has to be a procedure so", "start": 2511.0, "duration": 4.079}, {"text": "that the model parameters are kept the", "start": 2512.92, "duration": 4.86}, {"text": "same but each model on each on each", "start": 2515.079, "duration": 4.741}, {"text": "device is running with a separate piece", "start": 2517.78, "duration": 4.14}, {"text": "of data", "start": 2519.82, "duration": 4.259}, {"text": "so that's that's the main idea of data", "start": 2521.92, "duration": 4.38}, {"text": "parallel", "start": 2524.079, "duration": 6.181}, {"text": "Karis has a a library or a tool that's", "start": 2526.3, "duration": 6.6}, {"text": "called a strategy and there's some", "start": 2530.26, "duration": 5.099}, {"text": "functions to help you you know Define", "start": 2532.9, "duration": 4.58}, {"text": "the strategy", "start": 2535.359, "duration": 5.341}, {"text": "there's another tool called harovod", "start": 2537.48, "duration": 5.859}, {"text": "which uses NPI as a wrapper or wraps", "start": 2540.7, "duration": 4.919}, {"text": "around some of your functions to handle", "start": 2543.339, "duration": 4.321}, {"text": "this stuff", "start": 2545.619, "duration": 4.561}, {"text": "so that's for the data parallel there's", "start": 2547.66, "duration": 4.38}, {"text": "another approach called Model parallel", "start": 2550.18, "duration": 3.659}, {"text": "so if you maybe you have a really large", "start": 2552.04, "duration": 4.68}, {"text": "model you could in principle split the", "start": 2553.839, "duration": 3.78}, {"text": "model", "start": 2556.72, "duration": 3.06}, {"text": "and each each", "start": 2557.619, "duration": 4.561}, {"text": "mob each part of a model is running on", "start": 2559.78, "duration": 4.559}, {"text": "the device", "start": 2562.18, "duration": 3.659}, {"text": "um and maybe it's processing all the", "start": 2564.339, "duration": 3.0}, {"text": "data or maybe you have a combination of", "start": 2565.839, "duration": 4.041}, {"text": "model and data parallelism", "start": 2567.339, "duration": 4.861}, {"text": "so these last two approaches I haven't", "start": 2569.88, "duration": 3.82}, {"text": "actually used these last two approaches", "start": 2572.2, "duration": 4.02}, {"text": "I know with the large like the GPT model", "start": 2573.7, "duration": 5.58}, {"text": "uh now there's a tool called Deep speed", "start": 2576.22, "duration": 5.16}, {"text": "and that's supposed to help you set up a", "start": 2579.28, "duration": 4.1}, {"text": "model of data parallel", "start": 2581.38, "duration": 4.739}, {"text": "implementation for for parallelizing the", "start": 2583.38, "duration": 4.78}, {"text": "GPT training for example", "start": 2586.119, "duration": 4.141}, {"text": "another thing you can do another issue", "start": 2588.16, "duration": 4.98}, {"text": "that comes up is memory because the GPU", "start": 2590.26, "duration": 4.98}, {"text": "doesn't have as much memory as a CPU", "start": 2593.14, "duration": 4.979}, {"text": "um system so you can do things like", "start": 2595.24, "duration": 5.16}, {"text": "using mixed Precision you sort of have", "start": 2598.119, "duration": 3.061}, {"text": "to", "start": 2600.4, "duration": 2.76}, {"text": "it's not that hard to set up but you", "start": 2601.18, "duration": 3.84}, {"text": "have to kind of make sure you're not", "start": 2603.16, "duration": 5.76}, {"text": "losing um accuracy", "start": 2605.02, "duration": 5.64}, {"text": "so typically we might do mixed Precision", "start": 2608.92, "duration": 3.06}, {"text": "for hidden layers but maybe not the", "start": 2610.66, "duration": 4.199}, {"text": "output layer for example", "start": 2611.98, "duration": 5.22}, {"text": "all right so let's talk about so as I", "start": 2614.859, "duration": 3.781}, {"text": "said the data parallel is the most", "start": 2617.2, "duration": 2.7}, {"text": "common approach so let's talk about this", "start": 2618.64, "duration": 2.699}, {"text": "some more", "start": 2619.9, "duration": 5.48}, {"text": "so in Keras and tensorflow", "start": 2621.339, "duration": 4.041}, {"text": "if you're running on a single GPU node", "start": 2625.78, "duration": 5.1}, {"text": "so single GP node is going to have", "start": 2629.5, "duration": 3.42}, {"text": "several devices you can set up what's", "start": 2630.88, "duration": 3.84}, {"text": "called a mirror strategy", "start": 2632.92, "duration": 4.14}, {"text": "and so there's a distribute library or", "start": 2634.72, "duration": 4.379}, {"text": "distributed module and there's a", "start": 2637.06, "duration": 3.72}, {"text": "mirrored strategy function and you tell", "start": 2639.099, "duration": 5.641}, {"text": "it let's mirror my model on the these", "start": 2640.78, "duration": 6.18}, {"text": "GPU devices so on expands for example", "start": 2644.74, "duration": 4.74}, {"text": "our gpus have four", "start": 2646.96, "duration": 5.52}, {"text": "um our GPU node has four devices four", "start": 2649.48, "duration": 5.839}, {"text": "GPU devices", "start": 2652.48, "duration": 2.839}, {"text": "so you tell Keras okay I'm going to use", "start": 2655.359, "duration": 5.881}, {"text": "a marriage strategy and uh when you", "start": 2657.76, "duration": 5.339}, {"text": "build a model you build it inside the", "start": 2661.24, "duration": 4.079}, {"text": "scope of a marriage strategy so Karis", "start": 2663.099, "duration": 4.381}, {"text": "knows that okay this is a model that's", "start": 2665.319, "duration": 4.321}, {"text": "going to be built and I'm going to copy", "start": 2667.48, "duration": 5.76}, {"text": "this into the all the devices so your", "start": 2669.64, "duration": 5.58}, {"text": "build model so basically your build", "start": 2673.24, "duration": 5.839}, {"text": "model is is copied", "start": 2675.22, "duration": 6.42}, {"text": "and then you just train as normal", "start": 2679.079, "duration": 4.181}, {"text": "and when you do training whether you're", "start": 2681.64, "duration": 4.08}, {"text": "using GPU or CPU well for sure when", "start": 2683.26, "duration": 4.5}, {"text": "you're using GPU you want to try to do", "start": 2685.72, "duration": 5.04}, {"text": "things in batches of 32 or at least even", "start": 2687.76, "duration": 4.099}, {"text": "the", "start": 2690.76, "duration": 5.66}, {"text": "the numbers that divide into 32 like 2 4", "start": 2691.859, "duration": 8.26}, {"text": "8 for example two four eight sixteen", "start": 2696.42, "duration": 6.64}, {"text": "and that's because gpus vectorize things", "start": 2700.119, "duration": 6.121}, {"text": "into sizes of 32.", "start": 2703.06, "duration": 5.58}, {"text": "so always try to it always runs a little", "start": 2706.24, "duration": 3.96}, {"text": "bit better if you use you know something", "start": 2708.64, "duration": 3.9}, {"text": "that could be divided into 32.", "start": 2710.2, "duration": 3.84}, {"text": "uh", "start": 2712.54, "duration": 4.559}, {"text": "so a multi so the mirror strategy with", "start": 2714.04, "duration": 4.92}, {"text": "multiple devices on one GPU that's", "start": 2717.099, "duration": 4.561}, {"text": "pretty straightforward", "start": 2718.96, "duration": 4.32}, {"text": "but what if we want to go what if you", "start": 2721.66, "duration": 2.699}, {"text": "have a bigger problem and you want to", "start": 2723.28, "duration": 3.42}, {"text": "use multi-node so this is now a little", "start": 2724.359, "duration": 4.681}, {"text": "bit more complicated", "start": 2726.7, "duration": 6.659}, {"text": "Karis has a multi-worker strategy", "start": 2729.04, "duration": 6.96}, {"text": "but it requires setting up config files", "start": 2733.359, "duration": 4.561}, {"text": "with IP address so if you're on a static", "start": 2736.0, "duration": 3.359}, {"text": "machine you always know what the IP", "start": 2737.92, "duration": 3.419}, {"text": "addresses you could use the multi-worker", "start": 2739.359, "duration": 7.5}, {"text": "strategy uh it's it has a a concept of a", "start": 2741.339, "duration": 7.681}, {"text": "chief and a bunch of workers", "start": 2746.859, "duration": 4.621}, {"text": "and you have to just say Okay this this", "start": 2749.02, "duration": 4.2}, {"text": "IP address is your Chief here and", "start": 2751.48, "duration": 2.94}, {"text": "everybody knows what the other IP", "start": 2753.22, "duration": 3.599}, {"text": "addresses are every device every device", "start": 2754.42, "duration": 3.6}, {"text": "you run on", "start": 2756.819, "duration": 3.54}, {"text": "but on our HPC systems you know", "start": 2758.02, "duration": 4.02}, {"text": "resources are shared that means your IP", "start": 2760.359, "duration": 3.841}, {"text": "addresses are Dynamic it's not that easy", "start": 2762.04, "duration": 4.319}, {"text": "to set up you could set up a", "start": 2764.2, "duration": 4.379}, {"text": "multi-worker strategy but I you have to", "start": 2766.359, "duration": 4.141}, {"text": "go through you know you have to go and", "start": 2768.579, "duration": 3.841}, {"text": "pick out slurp information and it's a", "start": 2770.5, "duration": 3.9}, {"text": "little bit tricky", "start": 2772.42, "duration": 2.699}, {"text": "um", "start": 2774.4, "duration": 2.699}, {"text": "but it's a lot easier to use horovod so", "start": 2775.119, "duration": 3.181}, {"text": "horovod", "start": 2777.099, "duration": 4.441}, {"text": "sets up some wrappers in your code and", "start": 2778.3, "duration": 5.64}, {"text": "handles the MPI for you so you would do", "start": 2781.54, "duration": 3.779}, {"text": "this in a slurred batch shop and that's", "start": 2783.94, "duration": 4.159}, {"text": "what I want to show next", "start": 2785.319, "duration": 2.78}, {"text": "so on expands for example if you're", "start": 2789.22, "duration": 3.98}, {"text": "running single node single device", "start": 2790.9, "duration": 4.74}, {"text": "we are typically using Singularity", "start": 2793.2, "duration": 4.74}, {"text": "containers so there's going to be a", "start": 2795.64, "duration": 4.8}, {"text": "singularity command in your batch script", "start": 2797.94, "duration": 3.58}, {"text": "and that's going to run your python", "start": 2800.44, "duration": 3.72}, {"text": "script again your python script like", "start": 2801.52, "duration": 4.5}, {"text": "your typical Python scripts low data", "start": 2804.16, "duration": 4.38}, {"text": "build data training", "start": 2806.02, "duration": 5.579}, {"text": "with MPI we're gonna", "start": 2808.54, "duration": 5.4}, {"text": "perform first of the MPI run command", "start": 2811.599, "duration": 4.441}, {"text": "give it the number of tasks so the", "start": 2813.94, "duration": 3.6}, {"text": "number of tasks is going to be if you", "start": 2816.04, "duration": 3.84}, {"text": "run down one GPU node it could be four", "start": 2817.54, "duration": 3.6}, {"text": "it doesn't have to be four it could be", "start": 2819.88, "duration": 2.459}, {"text": "two", "start": 2821.14, "duration": 3.06}, {"text": "if you're running two GPU nodes it would", "start": 2822.339, "duration": 4.381}, {"text": "be eight for example", "start": 2824.2, "duration": 5.28}, {"text": "uh so we'd run an MP right command and", "start": 2826.72, "duration": 4.26}, {"text": "that execute that launches The", "start": 2829.48, "duration": 2.94}, {"text": "Singularity command which is going to", "start": 2830.98, "duration": 3.48}, {"text": "run the python script", "start": 2832.42, "duration": 5.22}, {"text": "and so this python script get with you", "start": 2834.46, "duration": 4.7}, {"text": "know the singularity", "start": 2837.64, "duration": 4.199}, {"text": "container gets distributed across nodes", "start": 2839.16, "duration": 7.06}, {"text": "by your MPI and your slurm job", "start": 2841.839, "duration": 6.421}, {"text": "now what Harvard is doing is is", "start": 2846.22, "duration": 3.78}, {"text": "essentially this you have to initialize", "start": 2848.26, "duration": 3.9}, {"text": "the back-end communication to be telling", "start": 2850.0, "duration": 4.8}, {"text": "you're initializing uh if you're", "start": 2852.16, "duration": 4.38}, {"text": "familiar with MPI you're familiarizing", "start": 2854.8, "duration": 2.819}, {"text": "you know the communication information", "start": 2856.54, "duration": 2.76}, {"text": "stuff", "start": 2857.619, "duration": 4.2}, {"text": "and you have uh NPI rank information in", "start": 2859.3, "duration": 5.84}, {"text": "your script in your program", "start": 2861.819, "duration": 3.321}, {"text": "then you're gonna after you load the", "start": 2865.3, "duration": 4.08}, {"text": "data you're gonna do some sharding or", "start": 2868.0, "duration": 3.18}, {"text": "splitting or maybe you're maybe you're", "start": 2869.38, "duration": 4.08}, {"text": "smart maybe you have uh procedures to", "start": 2871.18, "duration": 4.5}, {"text": "load the data so it's already", "start": 2873.46, "duration": 3.119}, {"text": "um", "start": 2875.68, "duration": 2.82}, {"text": "split up for example", "start": 2876.579, "duration": 3.361}, {"text": "so you could do that with rank", "start": 2878.5, "duration": 3.96}, {"text": "information uh you could do that with", "start": 2879.94, "duration": 5.7}, {"text": "tensorflow data set in a module you", "start": 2882.46, "duration": 4.92}, {"text": "could do that with numpy if you're if", "start": 2885.64, "duration": 5.3}, {"text": "you have the data set up the right way", "start": 2887.38, "duration": 3.56}, {"text": "and then you're going to set up a", "start": 2891.94, "duration": 2.879}, {"text": "distributed Optimizer and so again this", "start": 2893.14, "duration": 4.14}, {"text": "is going to be a horrified function uh", "start": 2894.819, "duration": 4.381}, {"text": "and it's going to have in part of your", "start": 2897.28, "duration": 3.6}, {"text": "model it's going to instead of having a", "start": 2899.2, "duration": 2.82}, {"text": "regular Optimizer it's going to", "start": 2900.88, "duration": 3.06}, {"text": "basically wrap around that Optimizer", "start": 2902.02, "duration": 4.92}, {"text": "with some distributed optimizer", "start": 2903.94, "duration": 6.54}, {"text": "and then in your Trading there's going", "start": 2906.94, "duration": 5.399}, {"text": "to be a batch aggregate", "start": 2910.48, "duration": 4.02}, {"text": "so after each I'm sorry after each batch", "start": 2912.339, "duration": 4.561}, {"text": "process that is processed there's going", "start": 2914.5, "duration": 3.599}, {"text": "to be some aggregation of your weight", "start": 2916.9, "duration": 4.08}, {"text": "updates and", "start": 2918.099, "duration": 3.921}, {"text": "um", "start": 2920.98, "duration": 4.02}, {"text": "there has to be some kind of way to let", "start": 2922.02, "duration": 4.18}, {"text": "every model knows what those weight", "start": 2925.0, "duration": 4.5}, {"text": "updates are so typically your your zero", "start": 2926.2, "duration": 5.879}, {"text": "rank process is handling these weight", "start": 2929.5, "duration": 4.68}, {"text": "updates and it would also in your zero", "start": 2932.079, "duration": 3.841}, {"text": "rank process you're going to handle", "start": 2934.18, "duration": 4.52}, {"text": "checkpoints so only one of your model", "start": 2935.92, "duration": 7.88}, {"text": "instances should be doing checkpoints", "start": 2938.7, "duration": 5.1}, {"text": "all right and just to be clear so the", "start": 2945.04, "duration": 3.6}, {"text": "Empire run command", "start": 2947.14, "duration": 4.199}, {"text": "says launch this number of tasks each", "start": 2948.64, "duration": 4.14}, {"text": "task is running on a different device", "start": 2951.339, "duration": 2.581}, {"text": "but", "start": 2952.78, "duration": 3.48}, {"text": "as far as those programs know they're", "start": 2953.92, "duration": 4.56}, {"text": "all running on GPU device zero so", "start": 2956.26, "duration": 4.079}, {"text": "there's a little bit of", "start": 2958.48, "duration": 3.24}, {"text": "but at least that's how it works on", "start": 2960.339, "duration": 3.061}, {"text": "expands it may not be it may not be that", "start": 2961.72, "duration": 4.879}, {"text": "way in other systems I'm not sure", "start": 2963.4, "duration": 3.199}, {"text": "and so you have a bunch of instances of", "start": 2966.94, "duration": 3.419}, {"text": "your script and but they all have", "start": 2969.04, "duration": 3.059}, {"text": "different they all know what you know", "start": 2970.359, "duration": 2.941}, {"text": "their piece is because they have a", "start": 2972.099, "duration": 3.661}, {"text": "different rank information", "start": 2973.3, "duration": 4.86}, {"text": "and then during training there's going", "start": 2975.76, "duration": 4.74}, {"text": "to be some all reduce", "start": 2978.16, "duration": 4.38}, {"text": "which is going to do the aggregation of", "start": 2980.5, "duration": 3.42}, {"text": "weight updates and there's going to be", "start": 2982.54, "duration": 2.94}, {"text": "some broadcast", "start": 2983.92, "duration": 3.12}, {"text": "which", "start": 2985.48, "duration": 4.08}, {"text": "which shares the information about what", "start": 2987.04, "duration": 4.02}, {"text": "the new weight values are", "start": 2989.56, "duration": 3.539}, {"text": "and so in general bigger batch sizes are", "start": 2991.06, "duration": 4.019}, {"text": "better because then you don't have to do", "start": 2993.099, "duration": 5.461}, {"text": "this communication as much but a bigger", "start": 2995.079, "duration": 5.28}, {"text": "batch size also means that you're going", "start": 2998.56, "duration": 4.7}, {"text": "to be using more memory", "start": 3000.359, "duration": 2.901}, {"text": "okay so just to give you a sense of what", "start": 3009.0, "duration": 2.88}, {"text": "the code looks like it's actually not", "start": 3010.68, "duration": 2.1}, {"text": "that it's actually pretty", "start": 3011.88, "duration": 3.36}, {"text": "straightforward I got this you can go to", "start": 3012.78, "duration": 4.559}, {"text": "the horovid", "start": 3015.24, "duration": 5.22}, {"text": "um a website and the example I have", "start": 3017.339, "duration": 7.5}, {"text": "there pretty much Works uh as as is I", "start": 3020.46, "duration": 5.94}, {"text": "don't think I did anything particularly", "start": 3024.839, "duration": 4.561}, {"text": "different in my tests and when I've used", "start": 3026.4, "duration": 5.19}, {"text": "Harvard on other systems", "start": 3029.4, "duration": 2.58}, {"text": "[Music]", "start": 3031.59, "duration": 2.19}, {"text": "um", "start": 3031.98, "duration": 3.78}, {"text": "and it goes something like this so the", "start": 3033.78, "duration": 3.299}, {"text": "communication you have to initialize", "start": 3035.76, "duration": 3.059}, {"text": "your rank it's just an init statement", "start": 3037.079, "duration": 3.661}, {"text": "you're sharing and splitting data is", "start": 3038.819, "duration": 3.0}, {"text": "probably the most confusing thing", "start": 3040.74, "duration": 2.339}, {"text": "because it really depends on what your", "start": 3041.819, "duration": 3.961}, {"text": "data looks like that you have you could", "start": 3043.079, "duration": 6.301}, {"text": "use a tensorflow uh data set module API", "start": 3045.78, "duration": 7.319}, {"text": "but you could also use numpy so but it", "start": 3049.38, "duration": 4.979}, {"text": "depends on how much you have to read in", "start": 3053.099, "duration": 3.301}, {"text": "and when if it's split already it", "start": 3054.359, "duration": 3.121}, {"text": "depends on whether or not you have a lot", "start": 3056.4, "duration": 3.0}, {"text": "of files of images and you want to use a", "start": 3057.48, "duration": 4.379}, {"text": "tensorflow data set to as a generator", "start": 3059.4, "duration": 4.8}, {"text": "for example so those are all the issues", "start": 3061.859, "duration": 3.901}, {"text": "you have to come up you have to sort of", "start": 3064.2, "duration": 3.3}, {"text": "work through", "start": 3065.76, "duration": 3.96}, {"text": "the distributed Optimizer just one line", "start": 3067.5, "duration": 4.079}, {"text": "of code say it's you know horva", "start": 3069.72, "duration": 3.42}, {"text": "distribute your Optimizer you tell it", "start": 3071.579, "duration": 3.721}, {"text": "what the optimizer is and then there's", "start": 3073.14, "duration": 3.84}, {"text": "some adjustment you want to make it's", "start": 3075.3, "duration": 3.18}, {"text": "recommended you make some adjustment to", "start": 3076.98, "duration": 2.879}, {"text": "your learning rate to handle the fact", "start": 3078.48, "duration": 2.76}, {"text": "that", "start": 3079.859, "duration": 5.301}, {"text": "um you know you splitting up your data", "start": 3081.24, "duration": 3.92}, {"text": "and then", "start": 3085.619, "duration": 4.561}, {"text": "the batch updates and the aggregation", "start": 3087.3, "duration": 4.559}, {"text": "and the broadcasting of the weights", "start": 3090.18, "duration": 3.419}, {"text": "they're actually handled for you all you", "start": 3091.859, "duration": 4.081}, {"text": "got to do is you know you do a normal", "start": 3093.599, "duration": 5.941}, {"text": "model compile that's normal Keras", "start": 3095.94, "duration": 7.04}, {"text": "statement but you have this other option", "start": 3099.54, "duration": 6.12}, {"text": "and the experimental run tensorflow", "start": 3102.98, "duration": 4.24}, {"text": "function okay so this just basically", "start": 3105.66, "duration": 3.24}, {"text": "tells", "start": 3107.22, "duration": 7.08}, {"text": "um Keras to make sure it it it uses this", "start": 3108.9, "duration": 8.52}, {"text": "distributed optimizer", "start": 3114.3, "duration": 4.62}, {"text": "that's a little bit that's a little bit", "start": 3117.42, "duration": 3.24}, {"text": "of I'm probably", "start": 3118.92, "duration": 3.419}, {"text": "mischaracterizing that a little bit but", "start": 3120.66, "duration": 3.659}, {"text": "the uh", "start": 3122.339, "duration": 3.601}, {"text": "um the website explains that a little", "start": 3124.319, "duration": 3.661}, {"text": "bit more", "start": 3125.94, "duration": 4.08}, {"text": "okay so that's it and when you're", "start": 3127.98, "duration": 4.379}, {"text": "running on a Jeep oops sorry", "start": 3130.02, "duration": 4.68}, {"text": "this is almost my last slide okay and", "start": 3132.359, "duration": 4.5}, {"text": "when you're running a GPU node you can", "start": 3134.7, "duration": 4.68}, {"text": "always SSH into that node or it's a CPU", "start": 3136.859, "duration": 4.681}, {"text": "node you can SSH into that node you can", "start": 3139.38, "duration": 4.26}, {"text": "see what's running using the top command", "start": 3141.54, "duration": 4.2}, {"text": "on a CPU node if you if you are running", "start": 3143.64, "duration": 4.979}, {"text": "on a GPU device you can SSH into those", "start": 3145.74, "duration": 5.339}, {"text": "nodes and run the Nvidia SMI command and", "start": 3148.619, "duration": 4.621}, {"text": "it's going to show you how much memory", "start": 3151.079, "duration": 5.161}, {"text": "is getting used and what kind of", "start": 3153.24, "duration": 3.78}, {"text": "um", "start": 3156.24, "duration": 2.52}, {"text": "you know what kind of uses you have on", "start": 3157.02, "duration": 3.539}, {"text": "the GPU devices so I'm only showing you", "start": 3158.76, "duration": 4.14}, {"text": "part of the output from a Nvidia Dash", "start": 3160.559, "duration": 3.661}, {"text": "SMI command", "start": 3162.9, "duration": 2.82}, {"text": "so that's something else you could do", "start": 3164.22, "duration": 3.06}, {"text": "when you're debugging when you're", "start": 3165.72, "duration": 3.18}, {"text": "monitoring these things you know you're", "start": 3167.28, "duration": 2.819}, {"text": "you know one of your issues is going to", "start": 3168.9, "duration": 3.719}, {"text": "be am I running out of memory uh for", "start": 3170.099, "duration": 6.24}, {"text": "example and you can look at that", "start": 3172.619, "duration": 5.94}, {"text": "okay so where do you go from here", "start": 3176.339, "duration": 3.961}, {"text": "I always tell people you know start with", "start": 3178.559, "duration": 3.841}, {"text": "relevant examples tensorflow has pretty", "start": 3180.3, "duration": 5.64}, {"text": "good tutorials a lot of the blogs you", "start": 3182.4, "duration": 6.0}, {"text": "see they basically take the tensorflow", "start": 3185.94, "duration": 4.2}, {"text": "tutorials they run it they modify a", "start": 3188.4, "duration": 3.419}, {"text": "little bit and they put it on their blog", "start": 3190.14, "duration": 4.02}, {"text": "uh so I like I like to go to tensorflow", "start": 3191.819, "duration": 3.901}, {"text": "for a lot of you know to start with a", "start": 3194.16, "duration": 2.64}, {"text": "lot of things sometimes they don't have", "start": 3195.72, "duration": 2.94}, {"text": "great explanations", "start": 3196.8, "duration": 4.44}, {"text": "um but sometimes you know they do", "start": 3198.66, "duration": 5.04}, {"text": "or I find I find they're pretty good", "start": 3201.24, "duration": 5.9}, {"text": "all right and that's it", "start": 3203.7, "duration": 3.44}]