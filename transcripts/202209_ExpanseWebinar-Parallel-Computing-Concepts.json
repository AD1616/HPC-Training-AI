[{"text": "um thank you again for joining us and", "start": 0.24, "duration": 4.079}, {"text": "for your patience i would now like to", "start": 2.24, "duration": 4.72}, {"text": "introduce our speaker for today bob", "start": 4.319, "duration": 3.761}, {"text": "sinkovitz", "start": 6.96, "duration": 3.92}, {"text": "um he leads the scientific applications", "start": 8.08, "duration": 4.8}, {"text": "efforts at sdsc and has collaborated", "start": 10.88, "duration": 4.0}, {"text": "with researchers across many fields such", "start": 12.88, "duration": 4.479}, {"text": "as physics chemistry astronomy climate", "start": 14.88, "duration": 4.64}, {"text": "and social sciences just to name a few", "start": 17.359, "duration": 4.961}, {"text": "and as always with an emphasis on making", "start": 19.52, "duration": 5.36}, {"text": "the most effort most effective use of", "start": 22.32, "duration": 5.279}, {"text": "high-end computing resources so please", "start": 24.88, "duration": 6.84}, {"text": "join me in welcoming bob", "start": 27.599, "duration": 4.121}, {"text": "hey good morning and good afternoon", "start": 34.399, "duration": 4.401}, {"text": "everybody um cindy just just quick check", "start": 36.239, "duration": 5.201}, {"text": "the video and audio is okay", "start": 38.8, "duration": 3.68}, {"text": "yes", "start": 41.44, "duration": 2.799}, {"text": "all right so i'm going to go ahead and", "start": 42.48, "duration": 4.72}, {"text": "share my screen", "start": 44.239, "duration": 2.961}, {"text": "there there we go all right so that", "start": 47.52, "duration": 3.84}, {"text": "thank you everybody for for joining me", "start": 49.36, "duration": 3.44}, {"text": "for the", "start": 51.36, "duration": 4.08}, {"text": "introduction to parallel computing", "start": 52.8, "duration": 4.399}, {"text": "concepts this is a talk that i've given", "start": 55.44, "duration": 4.16}, {"text": "a few times it's it's very basic talk", "start": 57.199, "duration": 4.801}, {"text": "where you don't assume any", "start": 59.6, "duration": 4.0}, {"text": "any background in high performance", "start": 62.0, "duration": 3.04}, {"text": "computing", "start": 63.6, "duration": 3.199}, {"text": "but we we felt that this presentation", "start": 65.04, "duration": 3.2}, {"text": "was something that sort of filled in the", "start": 66.799, "duration": 4.961}, {"text": "gap in our in our training", "start": 68.24, "duration": 4.879}, {"text": "we have a", "start": 71.76, "duration": 2.96}, {"text": "um that the slides are going to be", "start": 73.119, "duration": 5.04}, {"text": "posted and we also have a github repo", "start": 74.72, "duration": 7.2}, {"text": "with just a few simple examples in there", "start": 78.159, "duration": 5.761}, {"text": "of python code that i use to generate", "start": 81.92, "duration": 2.96}, {"text": "some of the figures for this", "start": 83.92, "duration": 2.879}, {"text": "presentation and that you may find", "start": 84.88, "duration": 4.8}, {"text": "useful as you're developing your own", "start": 86.799, "duration": 5.921}, {"text": "allocation requests", "start": 89.68, "duration": 3.04}, {"text": "yeah so an overview of the talk i'm", "start": 94.0, "duration": 2.56}, {"text": "going to be giving a little bit of an", "start": 95.68, "duration": 3.04}, {"text": "introduction then i'm going to get into", "start": 96.56, "duration": 5.68}, {"text": "processes threads mpi and openmp", "start": 98.72, "duration": 5.92}, {"text": "and don't be um don't be scared by the", "start": 102.24, "duration": 4.159}, {"text": "last two topics we are not going to be", "start": 104.64, "duration": 4.24}, {"text": "doing any programming um we're assuming", "start": 106.399, "duration": 3.921}, {"text": "that you are", "start": 108.88, "duration": 3.84}, {"text": "essentially non-programmers and users of", "start": 110.32, "duration": 4.079}, {"text": "the supercomputer centers but these are", "start": 112.72, "duration": 3.28}, {"text": "still topics that you need to be aware", "start": 114.399, "duration": 4.241}, {"text": "of we'll talk about hybrid applications", "start": 116.0, "duration": 5.6}, {"text": "which involve you know multiple types of", "start": 118.64, "duration": 4.4}, {"text": "parallelization", "start": 121.6, "duration": 4.159}, {"text": "amdahl's law which is the fundamental", "start": 123.04, "duration": 6.079}, {"text": "law describing the speed up that you can", "start": 125.759, "duration": 5.601}, {"text": "get for power parallel application you", "start": 129.119, "duration": 5.281}, {"text": "know other limits on the scalability", "start": 131.36, "duration": 5.12}, {"text": "how to run parallel applications and", "start": 134.4, "duration": 5.36}, {"text": "perform a scaling study and then finally", "start": 136.48, "duration": 5.28}, {"text": "where to go next for some more more", "start": 139.76, "duration": 4.72}, {"text": "information and some conclusions", "start": 141.76, "duration": 5.44}, {"text": "and as cindy mentioned earlier um you", "start": 144.48, "duration": 4.56}, {"text": "know feel free to post your questions", "start": 147.2, "duration": 4.0}, {"text": "into the chat it's kind of hard for me", "start": 149.04, "duration": 3.44}, {"text": "to follow", "start": 151.2, "duration": 3.44}, {"text": "the questions as we go along so", "start": 152.48, "duration": 4.96}, {"text": "you know cindy susan feel free to", "start": 154.64, "duration": 5.2}, {"text": "you know interrupt me at a", "start": 157.44, "duration": 4.96}, {"text": "um you know at a good point if you see", "start": 159.84, "duration": 4.8}, {"text": "fit", "start": 162.4, "duration": 2.24}, {"text": "so a little bit of a you know a little", "start": 165.519, "duration": 2.8}, {"text": "bit of background a little bit of", "start": 167.28, "duration": 3.2}, {"text": "introduction you know what why are you", "start": 168.319, "duration": 3.28}, {"text": "here", "start": 170.48, "duration": 2.56}, {"text": "um", "start": 171.599, "duration": 3.201}, {"text": "so", "start": 173.04, "duration": 4.32}, {"text": "and i realized i had i had modified this", "start": 174.8, "duration": 4.799}, {"text": "talk slightly for a", "start": 177.36, "duration": 3.519}, {"text": "slightly different audience but the", "start": 179.599, "duration": 4.0}, {"text": "context is still the same but ignore", "start": 180.879, "duration": 4.08}, {"text": "ignore the part where i say machine", "start": 183.599, "duration": 3.92}, {"text": "learning workloads um think of this as", "start": 184.959, "duration": 3.841}, {"text": "any workload it could be high", "start": 187.519, "duration": 3.041}, {"text": "performance computing data intensive", "start": 188.8, "duration": 3.68}, {"text": "computing machine learning but", "start": 190.56, "duration": 3.759}, {"text": "essentially the computing that you're", "start": 192.48, "duration": 4.399}, {"text": "doing the data management and so on it's", "start": 194.319, "duration": 4.481}, {"text": "grown to the point where you can no", "start": 196.879, "duration": 4.881}, {"text": "longer run them on your local resources", "start": 198.8, "duration": 5.68}, {"text": "you can no longer say use your laptop or", "start": 201.76, "duration": 4.72}, {"text": "your desktop or maybe even your", "start": 204.48, "duration": 4.0}, {"text": "maybe even your lab cluster so", "start": 206.48, "duration": 4.08}, {"text": "specifically you may need to kind of", "start": 208.48, "duration": 4.24}, {"text": "graduate to using large-scale parallel", "start": 210.56, "duration": 3.599}, {"text": "computing", "start": 212.72, "duration": 3.2}, {"text": "so", "start": 214.159, "duration": 3.921}, {"text": "you know as i said that this was um a", "start": 215.92, "duration": 4.56}, {"text": "more general talk with a few tweaks for", "start": 218.08, "duration": 4.48}, {"text": "for machine learning", "start": 220.48, "duration": 4.16}, {"text": "okay so this session is intended for", "start": 222.56, "duration": 4.399}, {"text": "anyone who's currently running plans to", "start": 224.64, "duration": 4.08}, {"text": "run or is thinking about running", "start": 226.959, "duration": 4.56}, {"text": "applications on parallel computers", "start": 228.72, "duration": 5.76}, {"text": "if you write proposals for computer time", "start": 231.519, "duration": 3.761}, {"text": "so", "start": 234.48, "duration": 2.88}, {"text": "you know i suspect that we might have a", "start": 235.28, "duration": 3.84}, {"text": "few folks in here who are a little more", "start": 237.36, "duration": 4.0}, {"text": "senior who are writing their previously", "start": 239.12, "duration": 5.119}, {"text": "exceed and now allocate or now access", "start": 241.36, "duration": 5.04}, {"text": "allocation requests so if you're writing", "start": 244.239, "duration": 4.161}, {"text": "a proposal for time i think these are", "start": 246.4, "duration": 3.44}, {"text": "some concepts that you'll find very", "start": 248.4, "duration": 3.6}, {"text": "helpful", "start": 249.84, "duration": 4.0}, {"text": "if you're purchasing time on compute", "start": 252.0, "duration": 3.519}, {"text": "resources you want to maximize the", "start": 253.84, "duration": 3.519}, {"text": "return on your investment", "start": 255.519, "duration": 3.12}, {"text": "you know i know a lot of you are going", "start": 257.359, "duration": 3.201}, {"text": "to be computing in the cloud or maybe", "start": 258.639, "duration": 4.081}, {"text": "you have a workload that spans say the", "start": 260.56, "duration": 4.72}, {"text": "nationally allocated resources and the", "start": 262.72, "duration": 4.32}, {"text": "cloud when you're paying real money you", "start": 265.28, "duration": 3.919}, {"text": "want to make sure that you're maximizing", "start": 267.04, "duration": 4.719}, {"text": "that return on investment", "start": 269.199, "duration": 4.081}, {"text": "you might be considering purchasing", "start": 271.759, "duration": 3.201}, {"text": "hardware for your lab", "start": 273.28, "duration": 3.44}, {"text": "or you're", "start": 274.96, "duration": 3.519}, {"text": "equally good a good reason for being", "start": 276.72, "duration": 3.199}, {"text": "here is that you're just curious about", "start": 278.479, "duration": 3.201}, {"text": "parallel computing you know what what it", "start": 279.919, "duration": 3.921}, {"text": "is what what can it do for you what are", "start": 281.68, "duration": 5.28}, {"text": "the limitations", "start": 283.84, "duration": 3.12}, {"text": "so the motivation for this talk is we", "start": 287.44, "duration": 4.96}, {"text": "look back at the training", "start": 289.919, "duration": 5.041}, {"text": "that we do here at sdsc more broadly in", "start": 292.4, "duration": 6.4}, {"text": "exceed and you know other contexts", "start": 294.96, "duration": 3.84}, {"text": "and", "start": 299.12, "duration": 2.4}, {"text": "much of the training", "start": 299.919, "duration": 3.84}, {"text": "is really targeted people who write", "start": 301.52, "duration": 4.64}, {"text": "their own parallel applications and it", "start": 303.759, "duration": 5.361}, {"text": "focuses on a lot on programmer topics", "start": 306.16, "duration": 5.52}, {"text": "they're things like mpi and openmp and", "start": 309.12, "duration": 5.12}, {"text": "cuda and profiling applications and", "start": 311.68, "duration": 4.959}, {"text": "performance tuning", "start": 314.24, "duration": 4.64}, {"text": "so as a consequence those of you who are", "start": 316.639, "duration": 3.761}, {"text": "end users", "start": 318.88, "duration": 3.28}, {"text": "rather than developers you know you", "start": 320.4, "duration": 4.4}, {"text": "rarely get a proper introduction to", "start": 322.16, "duration": 4.879}, {"text": "parallel computing and when i say proper", "start": 324.8, "duration": 4.399}, {"text": "this is going to be a very very quick", "start": 327.039, "duration": 3.6}, {"text": "proper introduction but hopefully it", "start": 329.199, "duration": 3.361}, {"text": "gives you enough of the fundamentals", "start": 330.639, "duration": 4.56}, {"text": "so even if you don't write code which is", "start": 332.56, "duration": 5.52}, {"text": "becoming more and more common", "start": 335.199, "duration": 4.881}, {"text": "it's so important that you understand", "start": 338.08, "duration": 3.76}, {"text": "some of the basic principles of parallel", "start": 340.08, "duration": 3.44}, {"text": "computing so you can make the most", "start": 341.84, "duration": 3.359}, {"text": "effective use of advanced cyber", "start": 343.52, "duration": 3.6}, {"text": "infrastructure you know over the years", "start": 345.199, "duration": 2.881}, {"text": "i've", "start": 347.12, "duration": 2.72}, {"text": "talked people either on tours of our", "start": 348.08, "duration": 3.92}, {"text": "data center or you know just casually at", "start": 349.84, "duration": 4.16}, {"text": "parties and they say wow so you have a", "start": 352.0, "duration": 3.759}, {"text": "super computer with a with a million", "start": 354.0, "duration": 3.759}, {"text": "cores or a hundred thousand cores or ten", "start": 355.759, "duration": 3.761}, {"text": "thousand cores so you could just put", "start": 357.759, "duration": 3.44}, {"text": "your code on there and you get a you'll", "start": 359.52, "duration": 3.92}, {"text": "get an answer ten thousand times faster", "start": 361.199, "duration": 3.761}, {"text": "i say whoa", "start": 363.44, "duration": 3.52}, {"text": "there are a few caveats there so we'll", "start": 364.96, "duration": 3.44}, {"text": "talk about them you know you can", "start": 366.96, "duration": 3.92}, {"text": "definitely improve your turnaround do", "start": 368.4, "duration": 4.96}, {"text": "larger calculations but", "start": 370.88, "duration": 4.08}, {"text": "um you know for the most part you're", "start": 373.36, "duration": 3.92}, {"text": "probably not going to be scaling at", "start": 374.96, "duration": 4.799}, {"text": "at that kind of a level", "start": 377.28, "duration": 3.68}, {"text": "so", "start": 379.759, "duration": 2.88}, {"text": "before before we you know kind of dive", "start": 380.96, "duration": 3.28}, {"text": "in more into the technical content i", "start": 382.639, "duration": 3.28}, {"text": "just want to address a few of the myths", "start": 384.24, "duration": 4.56}, {"text": "of parallel computing um some some of", "start": 385.919, "duration": 4.881}, {"text": "you are probably aware already that", "start": 388.8, "duration": 5.28}, {"text": "these are myths but i still find um you", "start": 390.8, "duration": 4.48}, {"text": "know when i talk to researchers from", "start": 394.08, "duration": 3.44}, {"text": "other fields that they do", "start": 395.28, "duration": 4.8}, {"text": "that the these um misconceptions still", "start": 397.52, "duration": 4.32}, {"text": "do persist", "start": 400.08, "duration": 3.839}, {"text": "so one of them is that parallel", "start": 401.84, "duration": 4.16}, {"text": "computing is for the astrophysicists and", "start": 403.919, "duration": 4.481}, {"text": "engineers and climate modelers and", "start": 406.0, "duration": 4.08}, {"text": "others who are working in traditionally", "start": 408.4, "duration": 4.239}, {"text": "math intensive fields", "start": 410.08, "duration": 4.88}, {"text": "and while this might have been partially", "start": 412.639, "duration": 5.201}, {"text": "true decades ago today nearly every", "start": 414.96, "duration": 4.56}, {"text": "field of research makes this parallel", "start": 417.84, "duration": 4.24}, {"text": "computing and this includes the social", "start": 419.52, "duration": 5.2}, {"text": "sciences life sciences in fact", "start": 422.08, "duration": 5.44}, {"text": "on our computers on expanse and comet", "start": 424.72, "duration": 5.199}, {"text": "before that um it was life sciences and", "start": 427.52, "duration": 4.88}, {"text": "biomedical research that was really the", "start": 429.919, "duration": 5.361}, {"text": "largest piece of our usage", "start": 432.4, "duration": 4.72}, {"text": "we're even seeing um", "start": 435.28, "duration": 3.6}, {"text": "some uptake from from areas like the", "start": 437.12, "duration": 4.0}, {"text": "arts and humanities and of course this", "start": 438.88, "duration": 4.08}, {"text": "is in addition to all of the usual", "start": 441.12, "duration": 4.16}, {"text": "suspects like physics chemistry", "start": 442.96, "duration": 7.079}, {"text": "engineering material science and so on", "start": 445.28, "duration": 4.759}, {"text": "okay i kind of touched on this earlier", "start": 450.96, "duration": 4.56}, {"text": "you know there's this myth that throwing", "start": 454.0, "duration": 3.039}, {"text": "more hardware at a problem will", "start": 455.52, "duration": 2.959}, {"text": "automatically reduce the time to", "start": 457.039, "duration": 3.28}, {"text": "solution", "start": 458.479, "duration": 3.521}, {"text": "and parallel computing is only going to", "start": 460.319, "duration": 2.801}, {"text": "help you if you already have an", "start": 462.0, "duration": 2.639}, {"text": "application that has been written to", "start": 463.12, "duration": 4.079}, {"text": "take advantage of parallel hardware and", "start": 464.639, "duration": 4.721}, {"text": "even if you do have a parallel code", "start": 467.199, "duration": 3.521}, {"text": "there is an inherent limit on", "start": 469.36, "duration": 3.6}, {"text": "scalability in fact", "start": 470.72, "duration": 4.0}, {"text": "for a given problem size if you keep", "start": 472.96, "duration": 4.239}, {"text": "throwing more and more and more hardware", "start": 474.72, "duration": 4.08}, {"text": "problem you may actually get to the", "start": 477.199, "duration": 4.56}, {"text": "point where the code runs slower than", "start": 478.8, "duration": 5.28}, {"text": "running on fewer numbers of cores gpus", "start": 481.759, "duration": 4.801}, {"text": "or nodes", "start": 484.08, "duration": 2.48}, {"text": "and then there's just one caveat here um", "start": 486.72, "duration": 4.319}, {"text": "there's something called high throughput", "start": 489.52, "duration": 3.6}, {"text": "computing that it uses parallel", "start": 491.039, "duration": 4.56}, {"text": "computing to run many many single core", "start": 493.12, "duration": 4.32}, {"text": "or single gpu in", "start": 495.599, "duration": 4.401}, {"text": "instances of an application to achieve", "start": 497.44, "duration": 4.56}, {"text": "near perfect scaling this is what we", "start": 500.0, "duration": 3.759}, {"text": "used to call embarrassingly parallel", "start": 502.0, "duration": 3.84}, {"text": "computing it's a term that we're trying", "start": 503.759, "duration": 3.761}, {"text": "trying to get away from because there's", "start": 505.84, "duration": 3.6}, {"text": "nothing wrong with having a workload", "start": 507.52, "duration": 3.92}, {"text": "that consists of many many small", "start": 509.44, "duration": 4.24}, {"text": "problems that could be run independently", "start": 511.44, "duration": 4.399}, {"text": "but in this case if you do have that", "start": 513.68, "duration": 3.919}, {"text": "high through competing high throughput", "start": 515.839, "duration": 3.281}, {"text": "computing workload", "start": 517.599, "duration": 3.601}, {"text": "say where you need to do ten thousand a", "start": 519.12, "duration": 4.24}, {"text": "hundred thousand maybe even a million", "start": 521.2, "duration": 5.12}, {"text": "very very similar small calculations you", "start": 523.36, "duration": 6.32}, {"text": "can get near perfect scaling", "start": 526.32, "duration": 5.199}, {"text": "and then finally and and this is", "start": 529.68, "duration": 3.52}, {"text": "something i hope to dispel for for this", "start": 531.519, "duration": 3.601}, {"text": "group is that you need to be a", "start": 533.2, "duration": 4.0}, {"text": "programmer or software developer to make", "start": 535.12, "duration": 5.36}, {"text": "use of parallel computing", "start": 537.2, "duration": 3.28}, {"text": "a few decades ago this was kind of", "start": 541.12, "duration": 5.6}, {"text": "partially true but now most users of", "start": 543.6, "duration": 5.52}, {"text": "parallel computers are not programmers", "start": 546.72, "duration": 4.559}, {"text": "instead you're using mature third-party", "start": 549.12, "duration": 4.0}, {"text": "software that had been developed", "start": 551.279, "duration": 3.841}, {"text": "elsewhere and then made available to the", "start": 553.12, "duration": 4.56}, {"text": "community", "start": 555.12, "duration": 2.56}, {"text": "so you know to say that another way you", "start": 557.839, "duration": 4.161}, {"text": "know many if not most of you are going", "start": 559.92, "duration": 4.64}, {"text": "to be using somebody else's code", "start": 562.0, "duration": 4.72}, {"text": "if you're doing climate and weather you", "start": 564.56, "duration": 4.56}, {"text": "might use um a code called wharf you", "start": 566.72, "duration": 4.799}, {"text": "know very very widely used for", "start": 569.12, "duration": 4.719}, {"text": "everything from research to to hurricane", "start": 571.519, "duration": 4.88}, {"text": "tracking if you're doing if you're doing", "start": 573.839, "duration": 4.801}, {"text": "molecular biology you'll probably be", "start": 576.399, "duration": 6.241}, {"text": "using codes like amber or gromacs or or", "start": 578.64, "duration": 6.56}, {"text": "namdi to do the simulations", "start": 582.64, "duration": 4.4}, {"text": "if you're constructing phylogenetic", "start": 585.2, "duration": 5.199}, {"text": "trees you might use rxml or beast or", "start": 587.04, "duration": 5.68}, {"text": "other other um widely used codes or if", "start": 590.399, "duration": 3.841}, {"text": "you're doing electronic structure you", "start": 592.72, "duration": 4.64}, {"text": "might be using cp2k or vasp so even", "start": 594.24, "duration": 5.36}, {"text": "though you're doing parallel computing", "start": 597.36, "duration": 3.84}, {"text": "you're doing science you're really not", "start": 599.6, "duration": 3.76}, {"text": "writing code", "start": 601.2, "duration": 3.92}, {"text": "so you think you know why", "start": 603.36, "duration": 3.28}, {"text": "um you know", "start": 605.12, "duration": 3.04}, {"text": "what why do i need to know about", "start": 606.64, "duration": 4.24}, {"text": "parallel computing if i'm not actually", "start": 608.16, "duration": 4.0}, {"text": "writing the code and we're going to get", "start": 610.88, "duration": 2.399}, {"text": "into that into the rest of the", "start": 612.16, "duration": 3.919}, {"text": "presentation but first just a little bit", "start": 613.279, "duration": 5.761}, {"text": "of um overview of what parallel", "start": 616.079, "duration": 4.241}, {"text": "computers are", "start": 619.04, "duration": 2.96}, {"text": "so", "start": 620.32, "duration": 2.959}, {"text": "you know if you if you look at the", "start": 622.0, "duration": 3.839}, {"text": "history of computing for for a long time", "start": 623.279, "duration": 4.8}, {"text": "we were we were doing serial computing", "start": 625.839, "duration": 4.161}, {"text": "we had a single processor we're trying", "start": 628.079, "duration": 4.481}, {"text": "to make it faster or more powerful but", "start": 630.0, "duration": 4.0}, {"text": "over the years we found out that we'd", "start": 632.56, "duration": 3.76}, {"text": "really really run out of steam you can", "start": 634.0, "duration": 4.16}, {"text": "only you can only increase the clock", "start": 636.32, "duration": 5.84}, {"text": "speed so much you can um only make the", "start": 638.16, "duration": 6.48}, {"text": "instruction buffers so wide so we had to", "start": 642.16, "duration": 4.4}, {"text": "start going parallel", "start": 644.64, "duration": 3.92}, {"text": "so our modern clusters and parallel", "start": 646.56, "duration": 4.0}, {"text": "computers they consist of multiple", "start": 648.56, "duration": 3.76}, {"text": "compute nodes connected together by a", "start": 650.56, "duration": 3.76}, {"text": "fast network", "start": 652.32, "duration": 4.24}, {"text": "each of the cpu nodes in a modern", "start": 654.32, "duration": 5.28}, {"text": "supercomputer typically contains one or", "start": 656.56, "duration": 3.92}, {"text": "more", "start": 659.6, "duration": 3.6}, {"text": "most often two multi-core processes", "start": 660.48, "duration": 5.28}, {"text": "while gpu nodes kind of the standard now", "start": 663.2, "duration": 5.759}, {"text": "is to get four gpus in the node", "start": 665.76, "duration": 4.8}, {"text": "so in order to effectively use this", "start": 668.959, "duration": 3.521}, {"text": "hardware we need applications when", "start": 670.56, "duration": 3.839}, {"text": "paralyzed so they can run on multiple", "start": 672.48, "duration": 5.44}, {"text": "cores gpus both within a node and across", "start": 674.399, "duration": 5.44}, {"text": "nodes and i'm not going to go into the", "start": 677.92, "duration": 4.0}, {"text": "details of the figure here on the right", "start": 679.839, "duration": 4.881}, {"text": "this is just a high level view of comet", "start": 681.92, "duration": 4.96}, {"text": "showing how it's built from nodes which", "start": 684.72, "duration": 3.76}, {"text": "are assembled into racks and then the", "start": 686.88, "duration": 4.639}, {"text": "racks are networked together into a", "start": 688.48, "duration": 5.28}, {"text": "into a full supercomputer and then we", "start": 691.519, "duration": 4.88}, {"text": "have connections to", "start": 693.76, "duration": 7.44}, {"text": "external networks and to storage systems", "start": 696.399, "duration": 7.68}, {"text": "bob yes question in the chat um it's it", "start": 701.2, "duration": 5.68}, {"text": "asks are there studies on how many gpus", "start": 704.079, "duration": 5.521}, {"text": "per cpu", "start": 706.88, "duration": 3.68}, {"text": "oh", "start": 709.6, "duration": 2.72}, {"text": "you know that that that's a really great", "start": 710.56, "duration": 3.76}, {"text": "question and if i'm interpreting it", "start": 712.32, "duration": 4.56}, {"text": "correctly it's a hard answer", "start": 714.32, "duration": 3.68}, {"text": "because", "start": 716.88, "duration": 3.84}, {"text": "there are some applications", "start": 718.0, "duration": 4.56}, {"text": "for example if you're doing molecular", "start": 720.72, "duration": 5.119}, {"text": "dynamics and use amber where the cpu is", "start": 722.56, "duration": 6.8}, {"text": "actually used very very very little", "start": 725.839, "duration": 5.841}, {"text": "and we find that", "start": 729.36, "duration": 4.08}, {"text": "for for each node that we're running on", "start": 731.68, "duration": 4.08}, {"text": "even if we're doing multiple multiple", "start": 733.44, "duration": 5.92}, {"text": "gpus that we only need one cpu", "start": 735.76, "duration": 4.8}, {"text": "um", "start": 739.36, "duration": 3.279}, {"text": "you know sticking in like in the area of", "start": 740.56, "duration": 4.64}, {"text": "molecular dynamics um another code i", "start": 742.639, "duration": 3.681}, {"text": "believe", "start": 745.2, "duration": 3.28}, {"text": "i can remember it was um gromacs or", "start": 746.32, "duration": 4.16}, {"text": "games i believe it was sorry chromax and", "start": 748.48, "duration": 4.4}, {"text": "amd i believe it was gromacs actually", "start": 750.48, "duration": 5.359}, {"text": "makes much much heavier use of the cpu", "start": 752.88, "duration": 4.959}, {"text": "because they haven't taken all of the", "start": 755.839, "duration": 4.641}, {"text": "functionality and ported it to the gpus", "start": 757.839, "duration": 4.401}, {"text": "now in all fairness", "start": 760.48, "duration": 4.0}, {"text": "um amber um", "start": 762.24, "duration": 4.719}, {"text": "amber grow max namdi they all work a", "start": 764.48, "duration": 4.08}, {"text": "little bit differently amber is", "start": 766.959, "duration": 3.521}, {"text": "typically used for something called", "start": 768.56, "duration": 4.0}, {"text": "replica exchange where you essentially", "start": 770.48, "duration": 3.919}, {"text": "have multiple", "start": 772.56, "duration": 3.839}, {"text": "independent md simulations going on at", "start": 774.399, "duration": 4.321}, {"text": "the same time with a little bit of of", "start": 776.399, "duration": 4.961}, {"text": "infrequent exchange data so yeah that", "start": 778.72, "duration": 4.88}, {"text": "was a long way of saying that", "start": 781.36, "duration": 5.12}, {"text": "it varies it really really varies a lot", "start": 783.6, "duration": 5.919}, {"text": "by the by by the application so and how", "start": 786.48, "duration": 4.96}, {"text": "much of the code had actually been", "start": 789.519, "duration": 4.801}, {"text": "ported to to run on the gpus", "start": 791.44, "duration": 5.12}, {"text": "good all right", "start": 794.32, "duration": 3.6}, {"text": "all right so now we're going to start", "start": 796.56, "duration": 2.719}, {"text": "getting into", "start": 797.92, "duration": 2.479}, {"text": "i'm going to call maybe a little bit of", "start": 799.279, "duration": 2.8}, {"text": "the nerdier topics we're going to", "start": 800.399, "duration": 7.361}, {"text": "discuss processes threads mpi and openmp", "start": 802.079, "duration": 8.241}, {"text": "so threads and processes are both", "start": 807.76, "duration": 5.28}, {"text": "independent sequences of execution", "start": 810.32, "duration": 5.04}, {"text": "so a process you could think of as an", "start": 813.04, "duration": 4.88}, {"text": "instance of a program with access to its", "start": 815.36, "duration": 5.12}, {"text": "own memory to its own state and file", "start": 817.92, "duration": 4.32}, {"text": "descriptors so we can open and close", "start": 820.48, "duration": 2.88}, {"text": "files", "start": 822.24, "duration": 3.52}, {"text": "and then threads are lightweight", "start": 823.36, "duration": 5.12}, {"text": "entities that execute within a process", "start": 825.76, "duration": 4.56}, {"text": "so every process is going to have at", "start": 828.48, "duration": 4.0}, {"text": "least one thread you get one thread by", "start": 830.32, "duration": 4.959}, {"text": "default and threads within a process can", "start": 832.48, "duration": 4.56}, {"text": "access shared memory and there are", "start": 835.279, "duration": 4.24}, {"text": "advantages and disadvantages both to", "start": 837.04, "duration": 5.039}, {"text": "both the threading and", "start": 839.519, "duration": 5.12}, {"text": "using threads and processes", "start": 842.079, "duration": 5.361}, {"text": "so if you look online", "start": 844.639, "duration": 4.64}, {"text": "yeah most of the resources that describe", "start": 847.44, "duration": 3.04}, {"text": "the difference between threads and", "start": 849.279, "duration": 3.521}, {"text": "processes they tend to be geared toward", "start": 850.48, "duration": 4.4}, {"text": "computer scientists but i think that the", "start": 852.8, "duration": 4.159}, {"text": "following two from doing my research i", "start": 854.88, "duration": 4.16}, {"text": "think do a great job", "start": 856.959, "duration": 2.961}, {"text": "um", "start": 859.04, "duration": 3.84}, {"text": "there's a really nice thread on stack", "start": 859.92, "duration": 5.68}, {"text": "overflow that goes pretty deep into what", "start": 862.88, "duration": 4.88}, {"text": "threads and processes are", "start": 865.6, "duration": 4.56}, {"text": "um but you might want to start if you", "start": 867.76, "duration": 4.0}, {"text": "don't come from a composite background", "start": 870.16, "duration": 4.0}, {"text": "maybe start with the second link", "start": 871.76, "duration": 4.56}, {"text": "um wait which which is more informal and", "start": 874.16, "duration": 4.16}, {"text": "gives you a much higher level um", "start": 876.32, "duration": 6.48}, {"text": "description of threads versus processors", "start": 878.32, "duration": 4.48}, {"text": "okay so process um the downside is that", "start": 883.279, "duration": 5.92}, {"text": "they occur more overhead a process is a", "start": 886.639, "duration": 3.921}, {"text": "full-blown", "start": 889.199, "duration": 3.521}, {"text": "instance of the program", "start": 890.56, "duration": 4.399}, {"text": "but they're also more flexible multiple", "start": 892.72, "duration": 4.4}, {"text": "processes can be run within a compute", "start": 894.959, "duration": 4.56}, {"text": "node using shared memory but they could", "start": 897.12, "duration": 3.839}, {"text": "also be", "start": 899.519, "duration": 3.68}, {"text": "run across multiple compute nodes", "start": 900.959, "duration": 4.24}, {"text": "distributed memory so if you really", "start": 903.199, "duration": 4.561}, {"text": "really want to scale up to do large", "start": 905.199, "duration": 5.64}, {"text": "scale parallel computing you need to use", "start": 907.76, "duration": 5.68}, {"text": "processes threads incur much less", "start": 910.839, "duration": 3.8}, {"text": "overhead", "start": 913.44, "duration": 4.399}, {"text": "threaded codes can also use less memory", "start": 914.639, "duration": 5.841}, {"text": "can since threads within a process have", "start": 917.839, "duration": 4.8}, {"text": "access to the same data structure and if", "start": 920.48, "duration": 4.24}, {"text": "you're a programmer you often find in", "start": 922.639, "duration": 4.32}, {"text": "many cases that it's easier to program", "start": 924.72, "duration": 4.4}, {"text": "for threads than for processes", "start": 926.959, "duration": 3.601}, {"text": "the problem though is that they're less", "start": 929.12, "duration": 3.839}, {"text": "flexible multiple threads within a", "start": 930.56, "duration": 5.36}, {"text": "process can only be run within a compute", "start": 932.959, "duration": 5.041}, {"text": "node within a single node", "start": 935.92, "duration": 5.84}, {"text": "taking advantage of that shared memory", "start": 938.0, "duration": 6.24}, {"text": "now there are a few caveats there um in", "start": 941.76, "duration": 4.8}, {"text": "theory any programming model could be", "start": 944.24, "duration": 5.279}, {"text": "applied to any hardware but in practice", "start": 946.56, "duration": 5.76}, {"text": "if you try to run multiple threads", "start": 949.519, "duration": 5.521}, {"text": "across this across multiple nodes", "start": 952.32, "duration": 4.879}, {"text": "distributed memory um your performance", "start": 955.04, "duration": 3.599}, {"text": "is going to be terrible and most", "start": 957.199, "duration": 4.241}, {"text": "applications aren't written to do that", "start": 958.639, "duration": 4.721}, {"text": "so why do you care about processes and", "start": 961.44, "duration": 2.959}, {"text": "threads", "start": 963.36, "duration": 2.64}, {"text": "the type of parallelization is going to", "start": 964.399, "duration": 4.081}, {"text": "determine where and how you run your", "start": 966.0, "duration": 3.92}, {"text": "code", "start": 968.48, "duration": 2.88}, {"text": "so if you have a distributed memory", "start": 969.92, "duration": 2.88}, {"text": "application", "start": 971.36, "duration": 3.52}, {"text": "you can um you know with multiple", "start": 972.8, "duration": 4.32}, {"text": "processes again multiple instances of", "start": 974.88, "duration": 5.04}, {"text": "program these can be run on one or more", "start": 977.12, "duration": 5.04}, {"text": "nodes and when i say multiple instances", "start": 979.92, "duration": 4.4}, {"text": "of a program even when i run an mpi", "start": 982.16, "duration": 3.599}, {"text": "application which we'll be talking about", "start": 984.32, "duration": 2.56}, {"text": "shortly", "start": 985.759, "duration": 4.08}, {"text": "you're really having identical copies of", "start": 986.88, "duration": 4.959}, {"text": "that program running but they'll be", "start": 989.839, "duration": 4.161}, {"text": "communicating through the mpi library", "start": 991.839, "duration": 4.081}, {"text": "and their each process is distinguished", "start": 994.0, "duration": 4.24}, {"text": "by a process id", "start": 995.92, "duration": 4.96}, {"text": "um shared memory applications can or", "start": 998.24, "duration": 5.36}, {"text": "should be run on on a single node", "start": 1000.88, "duration": 5.519}, {"text": "and this is also going to affect", "start": 1003.6, "duration": 4.96}, {"text": "your decision of where to run", "start": 1006.399, "duration": 3.841}, {"text": "some some supercomputers will have what", "start": 1008.56, "duration": 3.92}, {"text": "we call fatter nodes there's more memory", "start": 1010.24, "duration": 4.88}, {"text": "more cores per node so if you have a", "start": 1012.48, "duration": 4.24}, {"text": "shared a purely shared memory", "start": 1015.12, "duration": 3.6}, {"text": "application you may want to look for a", "start": 1016.72, "duration": 4.559}, {"text": "machine that has a lot of cores per node", "start": 1018.72, "duration": 6.16}, {"text": "like expense or bridges um hybrid", "start": 1021.279, "duration": 4.961}, {"text": "applications this is where it gets a", "start": 1024.88, "duration": 3.6}, {"text": "little more complicated they can be run", "start": 1026.24, "duration": 5.199}, {"text": "on more one or more nodes but you should", "start": 1028.48, "duration": 4.64}, {"text": "consider the balance between threads and", "start": 1031.439, "duration": 3.281}, {"text": "processes this is where it gets a little", "start": 1033.12, "duration": 3.28}, {"text": "a little more complicated", "start": 1034.72, "duration": 3.119}, {"text": "and in all cases you may need to", "start": 1036.4, "duration": 3.439}, {"text": "consider how processes and threads are", "start": 1037.839, "duration": 3.84}, {"text": "mapped and bound to cores", "start": 1039.839, "duration": 3.681}, {"text": "and i won't be getting into though into", "start": 1041.679, "duration": 3.841}, {"text": "those details here today", "start": 1043.52, "duration": 4.0}, {"text": "but we have some documentation on the", "start": 1045.52, "duration": 3.6}, {"text": "expense web page", "start": 1047.52, "duration": 3.519}, {"text": "and our um", "start": 1049.12, "duration": 3.919}, {"text": "user services staff you use a support", "start": 1051.039, "duration": 4.0}, {"text": "staff um you know really understands", "start": 1053.039, "duration": 4.0}, {"text": "this very deeply how to make sure that", "start": 1055.039, "duration": 4.64}, {"text": "say individual threads remain bound to", "start": 1057.039, "duration": 3.841}, {"text": "cores", "start": 1059.679, "duration": 2.801}, {"text": "so in addition to being aware of threads", "start": 1060.88, "duration": 3.36}, {"text": "and processes", "start": 1062.48, "duration": 2.88}, {"text": "you know this is going to help you", "start": 1064.24, "duration": 3.36}, {"text": "understand your code how it's utilizing", "start": 1065.36, "duration": 4.48}, {"text": "the hardware and identifying common", "start": 1067.6, "duration": 5.12}, {"text": "common problems", "start": 1069.84, "duration": 2.88}, {"text": "so we're going to talk very very briefly", "start": 1073.2, "duration": 4.719}, {"text": "about message pass and interface or mpi", "start": 1075.039, "duration": 4.321}, {"text": "we're not going to get into programming", "start": 1077.919, "duration": 3.281}, {"text": "this this is not a programming talk but", "start": 1079.36, "duration": 4.8}, {"text": "you do need to be aware of it so mpi is", "start": 1081.2, "duration": 4.56}, {"text": "you know what we call the de facto", "start": 1084.16, "duration": 4.48}, {"text": "standard for paralyzing c c plus plus", "start": 1085.76, "duration": 4.88}, {"text": "and four trend codes to run on", "start": 1088.64, "duration": 4.159}, {"text": "distributed memory multiple compute node", "start": 1090.64, "duration": 4.32}, {"text": "systems so as i said while it's not", "start": 1092.799, "duration": 4.88}, {"text": "officially adopted by any major standard", "start": 1094.96, "duration": 5.04}, {"text": "bodies it is so widely used that it's", "start": 1097.679, "duration": 5.281}, {"text": "really the de facto standard", "start": 1100.0, "duration": 5.36}, {"text": "now where it gets a little confusing is", "start": 1102.96, "duration": 3.599}, {"text": "you may hear", "start": 1105.36, "duration": 4.8}, {"text": "about things like openmpi and mvapg and", "start": 1106.559, "duration": 6.0}, {"text": "m pitch and even then there are vendor", "start": 1110.16, "duration": 5.12}, {"text": "support versions like intel mpi these", "start": 1112.559, "duration": 5.681}, {"text": "are just implementations of mpi they", "start": 1115.28, "duration": 5.279}, {"text": "should all have the same functionality", "start": 1118.24, "duration": 4.24}, {"text": "in some cases you may want to build your", "start": 1120.559, "duration": 5.441}, {"text": "code with one version or another for for", "start": 1122.48, "duration": 5.6}, {"text": "for performance reasons but they should", "start": 1126.0, "duration": 4.16}, {"text": "all work for your application", "start": 1128.08, "duration": 4.479}, {"text": "mpi applications can be run within a", "start": 1130.16, "duration": 5.28}, {"text": "single shared memory node so all widely", "start": 1132.559, "duration": 5.921}, {"text": "used mpi implementations take advantage", "start": 1135.44, "duration": 6.72}, {"text": "of that faster intranode communications", "start": 1138.48, "duration": 5.68}, {"text": "mpi is portable and you can use it", "start": 1142.16, "duration": 3.44}, {"text": "anywhere you could use it on any", "start": 1144.16, "duration": 3.6}, {"text": "supercomputer any cluster", "start": 1145.6, "duration": 4.24}, {"text": "and then finally although i said it's a", "start": 1147.76, "duration": 4.24}, {"text": "de facto standard it's become kind of", "start": 1149.84, "duration": 4.24}, {"text": "synonymous with distributed memory", "start": 1152.0, "duration": 4.32}, {"text": "parallelization there are other options", "start": 1154.08, "duration": 4.0}, {"text": "out there someone like called something", "start": 1156.32, "duration": 4.479}, {"text": "called charm plus plus which is used in", "start": 1158.08, "duration": 5.2}, {"text": "the namdi molecular dynamics package in", "start": 1160.799, "duration": 6.24}, {"text": "fact our previous director at sdsc", "start": 1163.28, "duration": 6.72}, {"text": "is using charm plus plus to", "start": 1167.039, "duration": 5.841}, {"text": "paralyze his state-of-the-art", "start": 1170.0, "duration": 6.24}, {"text": "cosmology code something called upc um", "start": 1172.88, "duration": 4.72}, {"text": "universal", "start": 1176.24, "duration": 3.679}, {"text": "i believe it's universal parallel c um", "start": 1177.6, "duration": 4.4}, {"text": "x10 so there's a few things a few other", "start": 1179.919, "duration": 3.201}, {"text": "things out there you might want to keep", "start": 1182.0, "duration": 3.679}, {"text": "an eye on", "start": 1183.12, "duration": 4.96}, {"text": "okay so although we're discussing mpi", "start": 1185.679, "duration": 4.161}, {"text": "the same principles are going to apply", "start": 1188.08, "duration": 3.28}, {"text": "when parallelizing deep learning", "start": 1189.84, "duration": 4.16}, {"text": "applications so if you're coming from a", "start": 1191.36, "duration": 4.88}, {"text": "machine learning or ai background there", "start": 1194.0, "duration": 4.64}, {"text": "are things out there like horovod this", "start": 1196.24, "duration": 4.96}, {"text": "is a distributed deep learning framework", "start": 1198.64, "duration": 5.52}, {"text": "for tensorflow keras pytorch and apache", "start": 1201.2, "duration": 4.08}, {"text": "mxnet", "start": 1204.16, "duration": 3.04}, {"text": "there's also something called nickel", "start": 1205.28, "duration": 3.759}, {"text": "this is the nvidia communicate", "start": 1207.2, "duration": 4.24}, {"text": "collective communication library which", "start": 1209.039, "duration": 5.321}, {"text": "is used for multi-gpu and", "start": 1211.44, "duration": 5.52}, {"text": "multi-multi-node communications again", "start": 1214.36, "duration": 4.199}, {"text": "these are things that you are probably", "start": 1216.96, "duration": 3.28}, {"text": "not going to be directly programming", "start": 1218.559, "duration": 3.601}, {"text": "with but you should be aware that there", "start": 1220.24, "duration": 4.319}, {"text": "are these um", "start": 1222.16, "duration": 4.56}, {"text": "the these distributed memory approaches", "start": 1224.559, "duration": 5.681}, {"text": "for for machine learning codes", "start": 1226.72, "duration": 3.52}, {"text": "so", "start": 1230.4, "duration": 2.8}, {"text": "you're just going to emphasize one more", "start": 1231.679, "duration": 5.12}, {"text": "time that this is not a programming talk", "start": 1233.2, "duration": 6.16}, {"text": "if you are not a programmer or an", "start": 1236.799, "duration": 4.721}, {"text": "application developer you do not need to", "start": 1239.36, "duration": 4.4}, {"text": "know mpi but i think everybody should be", "start": 1241.52, "duration": 4.159}, {"text": "aware of what it is", "start": 1243.76, "duration": 4.399}, {"text": "mpi applications could be pretty dense", "start": 1245.679, "duration": 4.641}, {"text": "it's written at a low level if you were", "start": 1248.159, "duration": 4.561}, {"text": "a c programmer", "start": 1250.32, "duration": 4.4}, {"text": "it will probably make sense if you're", "start": 1252.72, "duration": 3.68}, {"text": "not a programmer if you've only", "start": 1254.72, "duration": 3.76}, {"text": "programmed in higher level languages it", "start": 1256.4, "duration": 4.24}, {"text": "may look what we call a little too close", "start": 1258.48, "duration": 5.199}, {"text": "to the metal data is explicitly", "start": 1260.64, "duration": 5.279}, {"text": "communicated between processes", "start": 1263.679, "duration": 5.12}, {"text": "using calls to mpi library routines", "start": 1265.919, "duration": 3.681}, {"text": "so", "start": 1268.799, "duration": 2.481}, {"text": "again if you're not a programmer at all", "start": 1269.6, "duration": 3.28}, {"text": "you could kind of zone out for the next", "start": 1271.28, "duration": 3.92}, {"text": "minute", "start": 1272.88, "duration": 2.32}, {"text": "but if you are programmer you may have", "start": 1275.44, "duration": 3.68}, {"text": "seen a code that looks something like", "start": 1277.679, "duration": 4.0}, {"text": "this this is the", "start": 1279.12, "duration": 5.039}, {"text": "first code that you will write as a c or", "start": 1281.679, "duration": 5.12}, {"text": "a c plus plus programmer just the hello", "start": 1284.159, "duration": 3.601}, {"text": "world", "start": 1286.799, "duration": 3.76}, {"text": "we um include a header", "start": 1287.76, "duration": 3.919}, {"text": "so we could", "start": 1290.559, "duration": 4.561}, {"text": "get the i o routines i i o functions we", "start": 1291.679, "duration": 5.201}, {"text": "declare a main function", "start": 1295.12, "duration": 3.52}, {"text": "and then we have one executable", "start": 1296.88, "duration": 3.679}, {"text": "statement printf", "start": 1298.64, "duration": 2.64}, {"text": "now", "start": 1300.559, "duration": 2.48}, {"text": "what the mpi application is going to", "start": 1301.28, "duration": 5.279}, {"text": "look like and all of the mpi stuff is", "start": 1303.039, "duration": 6.241}, {"text": "highlighted in bold and purple", "start": 1306.559, "duration": 6.48}, {"text": "is you need to add a header you need to", "start": 1309.28, "duration": 6.32}, {"text": "initialize the mpi environment you need", "start": 1313.039, "duration": 4.88}, {"text": "to get the number of processes the rank", "start": 1315.6, "duration": 5.04}, {"text": "of each process get a name associated", "start": 1317.919, "duration": 4.961}, {"text": "with each process and now here i've", "start": 1320.64, "duration": 5.039}, {"text": "modified the print statement to say", "start": 1322.88, "duration": 6.0}, {"text": "to be to write out hello world from each", "start": 1325.679, "duration": 5.841}, {"text": "process so what we've done now is we've", "start": 1328.88, "duration": 5.52}, {"text": "created identical copies of this program", "start": 1331.52, "duration": 4.88}, {"text": "that we're going to be running", "start": 1334.4, "duration": 4.24}, {"text": "and at this point the only thing that", "start": 1336.4, "duration": 4.56}, {"text": "distinguishes them is there is their", "start": 1338.64, "duration": 4.56}, {"text": "rank so again if you're not a programmer", "start": 1340.96, "duration": 3.92}, {"text": "you don't need to know how to write mpi", "start": 1343.2, "duration": 3.12}, {"text": "but i just wanted to show you what it", "start": 1344.88, "duration": 3.039}, {"text": "looks like", "start": 1346.32, "duration": 3.68}, {"text": "okay openmp", "start": 1347.919, "duration": 4.24}, {"text": "switching gears a little bit is an api", "start": 1350.0, "duration": 4.64}, {"text": "for shared memory programming and used", "start": 1352.159, "duration": 5.841}, {"text": "in c c plus and fortran", "start": 1354.64, "duration": 5.039}, {"text": "it provides a collection of compiler", "start": 1358.0, "duration": 3.52}, {"text": "directives library routines and", "start": 1359.679, "duration": 3.601}, {"text": "environment variables", "start": 1361.52, "duration": 3.6}, {"text": "it's supported by all the major", "start": 1363.28, "duration": 3.2}, {"text": "compilers", "start": 1365.12, "duration": 3.2}, {"text": "in fact of all the compilers i've worked", "start": 1366.48, "duration": 4.319}, {"text": "with i've never seen a modern compiler", "start": 1368.32, "duration": 4.88}, {"text": "that does not support openmp so if your", "start": 1370.799, "duration": 4.88}, {"text": "code's been paralyzed using openmp you", "start": 1373.2, "duration": 4.0}, {"text": "should be able to build it and run it", "start": 1375.679, "duration": 2.721}, {"text": "anywhere", "start": 1377.2, "duration": 2.959}, {"text": "and i know for sure that it's supported", "start": 1378.4, "duration": 5.2}, {"text": "by ibm intel gcc", "start": 1380.159, "duration": 5.52}, {"text": "pgi", "start": 1383.6, "duration": 4.559}, {"text": "amd optimizing compiler collection which", "start": 1385.679, "duration": 4.88}, {"text": "is what we use on expanse so again it's", "start": 1388.159, "duration": 5.041}, {"text": "portable and you can use it anywhere", "start": 1390.559, "duration": 4.561}, {"text": "and like mpi", "start": 1393.2, "duration": 4.479}, {"text": "it's often synonymous with shared memory", "start": 1395.12, "duration": 4.799}, {"text": "parallelization but there are a lot of", "start": 1397.679, "duration": 4.401}, {"text": "other options out there there's the silk", "start": 1399.919, "duration": 4.0}, {"text": "programming language which looks a lot", "start": 1402.08, "duration": 4.32}, {"text": "like c with a few extensions there's", "start": 1403.919, "duration": 4.081}, {"text": "posix threads which is a little bit", "start": 1406.4, "duration": 3.68}, {"text": "lower level and there are specialized", "start": 1408.0, "duration": 4.799}, {"text": "libraries for python r and other", "start": 1410.08, "duration": 5.599}, {"text": "programming languages", "start": 1412.799, "duration": 3.841}, {"text": "so", "start": 1415.679, "duration": 3.441}, {"text": "same caveats apply here to the to the", "start": 1416.64, "duration": 4.72}, {"text": "mpi example if you're not a programmer", "start": 1419.12, "duration": 4.799}, {"text": "or an application developer you do not", "start": 1421.36, "duration": 4.72}, {"text": "need to know openmp", "start": 1423.919, "duration": 4.481}, {"text": "but if you are just pay attention for a", "start": 1426.08, "duration": 4.959}, {"text": "minute this is a simple bit of c code", "start": 1428.4, "duration": 5.92}, {"text": "that initializes two arrays a and b", "start": 1431.039, "duration": 6.961}, {"text": "so so the first loop here initializes um", "start": 1434.32, "duration": 5.92}, {"text": "initializes those arrays and then the", "start": 1438.0, "duration": 4.88}, {"text": "second loop will add them element by", "start": 1440.24, "duration": 5.36}, {"text": "element and then store them in a third", "start": 1442.88, "duration": 4.88}, {"text": "array c", "start": 1445.6, "duration": 5.679}, {"text": "so if i want to um parallelize that code", "start": 1447.76, "duration": 5.36}, {"text": "you know i have a little bit of header", "start": 1451.279, "duration": 4.321}, {"text": "information up here in the beginning", "start": 1453.12, "duration": 4.4}, {"text": "the main main thing is that i've added", "start": 1455.6, "duration": 4.64}, {"text": "what we call a pragma to the code", "start": 1457.52, "duration": 4.0}, {"text": "and we're not going to get into the", "start": 1460.24, "duration": 3.679}, {"text": "syntax but essentially this says i want", "start": 1461.52, "duration": 4.08}, {"text": "to take all those iterations of that", "start": 1463.919, "duration": 3.841}, {"text": "next loop and i'm going to break them up", "start": 1465.6, "duration": 4.72}, {"text": "across thread and each of those threads", "start": 1467.76, "duration": 4.72}, {"text": "is going to get a chunk of work in this", "start": 1470.32, "duration": 5.76}, {"text": "case my chunk size is 100 it's going to", "start": 1472.48, "duration": 5.28}, {"text": "finish its", "start": 1476.08, "duration": 3.68}, {"text": "portion of portion of the work and then", "start": 1477.76, "duration": 3.039}, {"text": "it's going to sit there and wait and say", "start": 1479.76, "duration": 3.76}, {"text": "okay i'm ready i want some more work i", "start": 1480.799, "duration": 5.12}, {"text": "want some more iterations to work on so", "start": 1483.52, "duration": 3.2}, {"text": "again", "start": 1485.919, "duration": 1.601}, {"text": "it's", "start": 1486.72, "duration": 3.12}, {"text": "openmp code tends to be a little bit", "start": 1487.52, "duration": 3.84}, {"text": "more readable", "start": 1489.84, "duration": 3.36}, {"text": "if you're doing very basic things easier", "start": 1491.36, "duration": 3.36}, {"text": "to write", "start": 1493.2, "duration": 3.12}, {"text": "and all you need to be aware of if", "start": 1494.72, "duration": 3.6}, {"text": "you're building your code is that you", "start": 1496.32, "duration": 4.64}, {"text": "use the appropriate compiler flags", "start": 1498.32, "duration": 5.12}, {"text": "and unfortunately this is not standard", "start": 1500.96, "duration": 5.599}, {"text": "across compilers depending on which", "start": 1503.44, "duration": 6.239}, {"text": "which compiler using intel um", "start": 1506.559, "duration": 6.0}, {"text": "aocc or others you know you might find", "start": 1509.679, "duration": 6.561}, {"text": "that it's dash f openmp dash q openmp", "start": 1512.559, "duration": 5.36}, {"text": "dash mp", "start": 1516.24, "duration": 4.4}, {"text": "and others and this is all documented in", "start": 1517.919, "duration": 5.281}, {"text": "the expense user guide i know i'm being", "start": 1520.64, "duration": 5.36}, {"text": "a little expanse centric but if you are", "start": 1523.2, "duration": 5.2}, {"text": "using", "start": 1526.0, "duration": 4.72}, {"text": "any of the other major previously exceed", "start": 1528.4, "duration": 5.44}, {"text": "and now access allocated resources if", "start": 1530.72, "duration": 5.52}, {"text": "you go to your to their user guides they", "start": 1533.84, "duration": 4.8}, {"text": "will all give you um description of the", "start": 1536.24, "duration": 4.559}, {"text": "compiler flags that you need to use in", "start": 1538.64, "duration": 4.8}, {"text": "order to to build your code", "start": 1540.799, "duration": 4.961}, {"text": "bob we have a question in the chat yes", "start": 1543.44, "duration": 4.08}, {"text": "um could you oh", "start": 1545.76, "duration": 3.76}, {"text": "yeah i'll go ahead and um", "start": 1547.52, "duration": 4.159}, {"text": "i could read it okay", "start": 1549.52, "duration": 3.68}, {"text": "um", "start": 1551.679, "duration": 2.961}, {"text": "i should know it's directly to me so i", "start": 1553.2, "duration": 4.56}, {"text": "have to read it to you oh okay", "start": 1554.64, "duration": 5.36}, {"text": "could you give an example on when we", "start": 1557.76, "duration": 5.12}, {"text": "should choose openmp over mpi and vice", "start": 1560.0, "duration": 4.64}, {"text": "versa", "start": 1562.88, "duration": 2.56}, {"text": "oh", "start": 1564.64, "duration": 3.2}, {"text": "that's um", "start": 1565.44, "duration": 4.239}, {"text": "that's kind of tough end like my", "start": 1567.84, "duration": 3.68}, {"text": "previous answer it's going to depend on", "start": 1569.679, "duration": 4.961}, {"text": "the on the application so i would say", "start": 1571.52, "duration": 5.44}, {"text": "um", "start": 1574.64, "duration": 2.32}, {"text": "if you", "start": 1577.039, "duration": 2.561}, {"text": "well if you're an end user and you're", "start": 1578.0, "duration": 3.44}, {"text": "not writing your own applications you're", "start": 1579.6, "duration": 3.52}, {"text": "kind of stuck with", "start": 1581.44, "duration": 4.239}, {"text": "with the software that's available but", "start": 1583.12, "duration": 4.4}, {"text": "more generally i would say if you have", "start": 1585.679, "duration": 4.88}, {"text": "an application that's gonna need to run", "start": 1587.52, "duration": 6.159}, {"text": "across a large number of cars or nodes", "start": 1590.559, "duration": 5.201}, {"text": "you know you need to you need to run on", "start": 1593.679, "duration": 4.401}, {"text": "a thousand or ten thousand or a hundred", "start": 1595.76, "duration": 5.279}, {"text": "thousand cores you have no choice but to", "start": 1598.08, "duration": 6.24}, {"text": "go with openm sorry to go with mpi", "start": 1601.039, "duration": 5.441}, {"text": "if we go with openmp", "start": 1604.32, "duration": 4.0}, {"text": "you're gonna be limited", "start": 1606.48, "duration": 4.48}, {"text": "just to running within a single node now", "start": 1608.32, "duration": 5.04}, {"text": "sometimes that's not so bad these nodes", "start": 1610.96, "duration": 6.319}, {"text": "now have so much memory 256 gigabytes um", "start": 1613.36, "duration": 7.6}, {"text": "100 you know in our case 128 cores", "start": 1617.279, "duration": 5.601}, {"text": "that you can do a lot of computing on", "start": 1620.96, "duration": 4.24}, {"text": "there so basically mpi if you want to go", "start": 1622.88, "duration": 4.0}, {"text": "really big", "start": 1625.2, "duration": 3.359}, {"text": "if you're staying within a node you'll", "start": 1626.88, "duration": 4.32}, {"text": "probably be more efficient using using", "start": 1628.559, "duration": 5.681}, {"text": "openmp since threads are lighter weight", "start": 1631.2, "duration": 3.92}, {"text": "but", "start": 1634.24, "duration": 2.4}, {"text": "in the next couple of slides though i'm", "start": 1635.12, "duration": 3.52}, {"text": "going to talk about hybrid codes and", "start": 1636.64, "duration": 3.12}, {"text": "this is where it gets a little more", "start": 1638.64, "duration": 2.72}, {"text": "complicated and then i'll", "start": 1639.76, "duration": 4.08}, {"text": "answer your i think", "start": 1641.36, "duration": 4.16}, {"text": "be answering your question a little more", "start": 1643.84, "duration": 4.16}, {"text": "thoroughly", "start": 1645.52, "duration": 2.48}, {"text": "and if you just want to give um", "start": 1650.48, "duration": 5.52}, {"text": "cindy um a thumbs up that that answered", "start": 1652.84, "duration": 5.24}, {"text": "the question okay good i see that okay", "start": 1656.0, "duration": 3.52}, {"text": "and there's one other question can", "start": 1658.08, "duration": 3.839}, {"text": "shared memory programs cause memory", "start": 1659.52, "duration": 4.08}, {"text": "corruption", "start": 1661.919, "duration": 4.88}, {"text": "ooh shared memory programs", "start": 1663.6, "duration": 6.64}, {"text": "they even though they're easier to write", "start": 1666.799, "duration": 6.801}, {"text": "there um they can get you into trouble", "start": 1670.24, "duration": 4.319}, {"text": "um", "start": 1673.6, "duration": 2.48}, {"text": "you need to be a little more careful", "start": 1674.559, "duration": 3.921}, {"text": "that that threads aren't writing into", "start": 1676.08, "duration": 4.079}, {"text": "each other's memory", "start": 1678.48, "duration": 5.199}, {"text": "um so i'm going to say you you you have", "start": 1680.159, "duration": 5.601}, {"text": "to you really have to be aware of of", "start": 1683.679, "duration": 4.161}, {"text": "what's going on in your code again if", "start": 1685.76, "duration": 4.24}, {"text": "you're an end user", "start": 1687.84, "duration": 5.12}, {"text": "of a well-written code if you're using", "start": 1690.0, "duration": 5.679}, {"text": "the the vasps and gromacs and ambers and", "start": 1692.96, "duration": 5.04}, {"text": "namdies and wharfs and there's other", "start": 1695.679, "duration": 4.321}, {"text": "codes this is probably", "start": 1698.0, "duration": 3.2}, {"text": "this is probably not going to be a", "start": 1700.0, "duration": 4.399}, {"text": "problem but yes it can cause", "start": 1701.2, "duration": 5.44}, {"text": "it can cause memory that memory", "start": 1704.399, "duration": 4.88}, {"text": "corruption if it's not written properly", "start": 1706.64, "duration": 5.84}, {"text": "whereas mpi goes since you're working", "start": 1709.279, "duration": 5.441}, {"text": "with multiple processes that those", "start": 1712.48, "duration": 4.16}, {"text": "processes are more isolated and they", "start": 1714.72, "duration": 3.52}, {"text": "just kind of work in their in their own", "start": 1716.64, "duration": 4.24}, {"text": "sandbox", "start": 1718.24, "duration": 2.64}, {"text": "and rebate did that um answer your", "start": 1720.96, "duration": 4.88}, {"text": "question", "start": 1723.2, "duration": 2.64}, {"text": "excellent thank you and and feel free to", "start": 1727.36, "duration": 3.52}, {"text": "follow up with me with me later if you", "start": 1729.039, "duration": 3.36}, {"text": "like", "start": 1730.88, "duration": 4.24}, {"text": "um okay so mpi and openmp the big", "start": 1732.399, "duration": 5.681}, {"text": "picture on the left we talk about mpi on", "start": 1735.12, "duration": 5.76}, {"text": "the right we talk about openmp", "start": 1738.08, "duration": 4.319}, {"text": "mpi", "start": 1740.88, "duration": 3.76}, {"text": "um", "start": 1742.399, "duration": 4.241}, {"text": "is used when you when you're managing", "start": 1744.64, "duration": 4.88}, {"text": "multiple processes and then mpi is", "start": 1746.64, "duration": 5.44}, {"text": "implemented in the libraries in this end", "start": 1749.52, "duration": 4.96}, {"text": "pitch in open mpi and vendor", "start": 1752.08, "duration": 5.44}, {"text": "implementations and envapidge threads", "start": 1754.48, "duration": 6.319}, {"text": "are managed using openmp and openmp is", "start": 1757.52, "duration": 5.92}, {"text": "implemented by compilers", "start": 1760.799, "duration": 3.841}, {"text": "okay so now we're going to get into", "start": 1763.44, "duration": 3.68}, {"text": "hybrid applications and this is going to", "start": 1764.64, "duration": 4.72}, {"text": "get a little more into answering that", "start": 1767.12, "duration": 4.08}, {"text": "the previous question about when should", "start": 1769.36, "duration": 2.799}, {"text": "i use", "start": 1771.2, "duration": 1.76}, {"text": "red", "start": 1772.159, "duration": 4.64}, {"text": "open and p when should i use processes", "start": 1772.96, "duration": 5.28}, {"text": "mpi", "start": 1776.799, "duration": 4.0}, {"text": "so a lot of modern applications are", "start": 1778.24, "duration": 4.799}, {"text": "built using a hybrid approach to take", "start": 1780.799, "duration": 4.641}, {"text": "advantage of both distributed and shared", "start": 1783.039, "duration": 3.52}, {"text": "memory", "start": 1785.44, "duration": 3.44}, {"text": "and this will typically involve", "start": 1786.559, "duration": 5.921}, {"text": "especially for hpc applications um mpi", "start": 1788.88, "duration": 5.6}, {"text": "and openmp although as i mentioned", "start": 1792.48, "duration": 2.88}, {"text": "earlier there are some other", "start": 1794.48, "duration": 3.12}, {"text": "combinations possible", "start": 1795.36, "duration": 4.0}, {"text": "so hybrid codes have an advantage over", "start": 1797.6, "duration": 3.6}, {"text": "the purely share to disturb your memory", "start": 1799.36, "duration": 3.679}, {"text": "apps in a couple of ways", "start": 1801.2, "duration": 4.24}, {"text": "so shared memory apps as i just said", "start": 1803.039, "duration": 4.481}, {"text": "they have limited scalability you pretty", "start": 1805.44, "duration": 4.959}, {"text": "much need to run within that node rather", "start": 1807.52, "duration": 4.879}, {"text": "than across multiple nodes", "start": 1810.399, "duration": 4.241}, {"text": "now distributed memory applications they", "start": 1812.399, "duration": 3.921}, {"text": "may have higher memory", "start": 1814.64, "duration": 3.759}, {"text": "memory requirements i'll", "start": 1816.32, "duration": 3.599}, {"text": "give an example of that in a few more", "start": 1818.399, "duration": 4.081}, {"text": "slides and they also introduce some more", "start": 1819.919, "duration": 3.841}, {"text": "overhead", "start": 1822.48, "duration": 2.96}, {"text": "so what you're going to need to do and", "start": 1823.76, "duration": 2.799}, {"text": "this is where it gets a little", "start": 1825.44, "duration": 3.839}, {"text": "complicated you may need to think about", "start": 1826.559, "duration": 4.641}, {"text": "the balance between", "start": 1829.279, "duration": 4.4}, {"text": "threads and processes to get to get the", "start": 1831.2, "duration": 4.56}, {"text": "best performance", "start": 1833.679, "duration": 3.921}, {"text": "so i'm going to go to a", "start": 1835.76, "duration": 4.399}, {"text": "very very simplified view of a parallel", "start": 1837.6, "duration": 3.84}, {"text": "computer", "start": 1840.159, "duration": 3.201}, {"text": "you'll never see anything this small or", "start": 1841.44, "duration": 4.479}, {"text": "this simple but let's say that we have a", "start": 1843.36, "duration": 4.08}, {"text": "we built a really simple parallel", "start": 1845.919, "duration": 4.24}, {"text": "computer it consists of two nodes each", "start": 1847.44, "duration": 4.959}, {"text": "of them have 16 cores and we have some", "start": 1850.159, "duration": 4.24}, {"text": "kind of interconnect or network joining", "start": 1852.399, "duration": 4.801}, {"text": "them so that could be ethernet or", "start": 1854.399, "duration": 4.561}, {"text": "um", "start": 1857.2, "duration": 3.76}, {"text": "or infiniband", "start": 1858.96, "duration": 4.16}, {"text": "so the big box that the two big boxes", "start": 1860.96, "duration": 4.0}, {"text": "are nodes", "start": 1863.12, "duration": 4.72}, {"text": "and i've indicated the cores as the", "start": 1864.96, "duration": 6.16}, {"text": "small blue boxes", "start": 1867.84, "duration": 3.28}, {"text": "so if we have a purely message pass an", "start": 1872.559, "duration": 3.921}, {"text": "application", "start": 1875.12, "duration": 2.64}, {"text": "so let's say we have an application", "start": 1876.48, "duration": 3.6}, {"text": "paralyzed using just message pass and we", "start": 1877.76, "duration": 4.96}, {"text": "want we want to run across", "start": 1880.08, "duration": 5.12}, {"text": "we want to use all 32 of those cores the", "start": 1882.72, "duration": 6.24}, {"text": "only way to do it is to run one process", "start": 1885.2, "duration": 6.24}, {"text": "per core so basically we have a process", "start": 1888.96, "duration": 6.079}, {"text": "associated with each of those cores", "start": 1891.44, "duration": 6.079}, {"text": "if we have a threaded application", "start": 1895.039, "duration": 4.161}, {"text": "now we're going to be restricted to it", "start": 1897.519, "duration": 3.76}, {"text": "to a single node", "start": 1899.2, "duration": 3.76}, {"text": "and in this case let's say we wanted to", "start": 1901.279, "duration": 4.161}, {"text": "use all um", "start": 1902.96, "duration": 5.36}, {"text": "oh all 16 of those cores in this case we", "start": 1905.44, "duration": 5.52}, {"text": "would be running a single process and", "start": 1908.32, "duration": 4.32}, {"text": "within that process we're going to have", "start": 1910.96, "duration": 4.319}, {"text": "16 threads um so one thread for each", "start": 1912.64, "duration": 5.44}, {"text": "core and i've alluded to this earlier", "start": 1915.279, "duration": 4.961}, {"text": "technically any programming model can be", "start": 1918.08, "duration": 4.719}, {"text": "mapped to any hardware but in practice", "start": 1920.24, "duration": 3.919}, {"text": "you really need to run threaded", "start": 1922.799, "duration": 5.201}, {"text": "applications within a single node", "start": 1924.159, "duration": 5.681}, {"text": "now hybrid applications is where things", "start": 1928.0, "duration": 4.159}, {"text": "get a little more interesting", "start": 1929.84, "duration": 4.079}, {"text": "so if we have a code that will say", "start": 1932.159, "duration": 4.481}, {"text": "paralyzed using mpi and openmp we can", "start": 1933.919, "duration": 5.281}, {"text": "use any combination of threads and", "start": 1936.64, "duration": 3.919}, {"text": "processes", "start": 1939.2, "duration": 4.24}, {"text": "such that they um such that they", "start": 1940.559, "duration": 4.48}, {"text": "you know add up to the number of cores", "start": 1943.44, "duration": 4.079}, {"text": "we want to use so in this case i'm", "start": 1945.039, "duration": 5.281}, {"text": "running two processes per node and eight", "start": 1947.519, "duration": 5.601}, {"text": "processes sorry and eight threads per", "start": 1950.32, "duration": 5.599}, {"text": "per process", "start": 1953.12, "duration": 2.799}, {"text": "a typical scenario i'm going to say more", "start": 1957.279, "duration": 5.52}, {"text": "typical be set b before the number of", "start": 1960.0, "duration": 4.48}, {"text": "cores in the node", "start": 1962.799, "duration": 3.681}, {"text": "really started growing", "start": 1964.48, "duration": 5.28}, {"text": "was to use threads within a node", "start": 1966.48, "duration": 6.4}, {"text": "and then sorry sorry run one process per", "start": 1969.76, "duration": 4.159}, {"text": "node", "start": 1972.88, "duration": 3.6}, {"text": "and then use threads within the node but", "start": 1973.919, "duration": 4.801}, {"text": "this is becoming a little less common as", "start": 1976.48, "duration": 3.84}, {"text": "these nodes are getting really fat you", "start": 1978.72, "duration": 3.52}, {"text": "know currently we have", "start": 1980.32, "duration": 5.52}, {"text": "um two 64 core processors for a total of", "start": 1982.24, "duration": 6.64}, {"text": "128 cores within expense", "start": 1985.84, "duration": 5.92}, {"text": "i believe that if you follow the amd", "start": 1988.88, "duration": 4.639}, {"text": "roadmap there's probably going to be", "start": 1991.76, "duration": 4.0}, {"text": "processors coming out you know in the", "start": 1993.519, "duration": 4.321}, {"text": "next few years that might have 96 cores", "start": 1995.76, "duration": 3.36}, {"text": "so these note", "start": 1997.84, "duration": 2.959}, {"text": "these nodes are getting really really", "start": 1999.12, "duration": 4.08}, {"text": "fat but this was kind of the typical way", "start": 2000.799, "duration": 4.561}, {"text": "that these hybrid applications were run", "start": 2003.2, "duration": 4.319}, {"text": "one process per node and then threads", "start": 2005.36, "duration": 5.919}, {"text": "within the within the process", "start": 2007.519, "duration": 5.28}, {"text": "but here's where it gets more", "start": 2011.279, "duration": 2.88}, {"text": "complicated or i'm going to say", "start": 2012.799, "duration": 3.841}, {"text": "complicated er", "start": 2014.159, "duration": 4.961}, {"text": "you could do anything in between", "start": 2016.64, "duration": 4.72}, {"text": "so on the left hand side", "start": 2019.12, "duration": 5.2}, {"text": "i'm doing one process per node and then", "start": 2021.36, "duration": 4.799}, {"text": "a number of threads equal to the number", "start": 2024.32, "duration": 2.959}, {"text": "of cores", "start": 2026.159, "duration": 3.76}, {"text": "all the way to the other extreme", "start": 2027.279, "duration": 4.481}, {"text": "where i'm running when i'm running one", "start": 2029.919, "duration": 3.441}, {"text": "process per core", "start": 2031.76, "duration": 4.0}, {"text": "and you could do anything in between", "start": 2033.36, "duration": 4.24}, {"text": "so in this case", "start": 2035.76, "duration": 4.48}, {"text": "um the question is you know how many", "start": 2037.6, "duration": 4.88}, {"text": "cores how many processes should i use", "start": 2040.24, "duration": 4.08}, {"text": "i'm going to say it's complicated and", "start": 2042.48, "duration": 3.919}, {"text": "the only way that you can really figure", "start": 2044.32, "duration": 4.079}, {"text": "this out is going to be it's going to be", "start": 2046.399, "duration": 4.401}, {"text": "highly highly application and possibly", "start": 2048.399, "duration": 4.881}, {"text": "problem dependent is that you just need", "start": 2050.8, "duration": 5.359}, {"text": "to benchmark it you'll need to", "start": 2053.28, "duration": 5.599}, {"text": "you'll just need to run a small problem", "start": 2056.159, "duration": 6.801}, {"text": "using um using different configurations", "start": 2058.879, "duration": 5.841}, {"text": "and again here i'm not i don't have time", "start": 2062.96, "duration": 3.76}, {"text": "to go into the details but if you look", "start": 2064.72, "duration": 4.32}, {"text": "at the expanse user guide and the user", "start": 2066.72, "duration": 4.399}, {"text": "guides at the other major supercomputer", "start": 2069.04, "duration": 5.119}, {"text": "centers they'll explain how you can how", "start": 2071.119, "duration": 6.641}, {"text": "you can run the hybrid applications", "start": 2074.159, "duration": 3.601}, {"text": "okay so now we're going to get into", "start": 2078.24, "duration": 3.119}, {"text": "amdahl's law we're going to talk a", "start": 2079.76, "duration": 3.04}, {"text": "little bit about the limits on", "start": 2081.359, "duration": 4.24}, {"text": "scalability", "start": 2082.8, "duration": 2.799}, {"text": "so", "start": 2085.919, "duration": 2.48}, {"text": "amdahl's law", "start": 2086.72, "duration": 4.24}, {"text": "describes the absolute limit on the", "start": 2088.399, "duration": 5.44}, {"text": "speed up of a code as a function of how", "start": 2090.96, "duration": 5.52}, {"text": "much of that code can be paralyzed", "start": 2093.839, "duration": 4.961}, {"text": "and the number of processes so if you", "start": 2096.48, "duration": 3.2}, {"text": "know", "start": 2098.8, "duration": 4.0}, {"text": "just one law of parallel computing this", "start": 2099.68, "duration": 6.24}, {"text": "is the one to know this is um", "start": 2102.8, "duration": 5.6}, {"text": "you know th this this is this is the law", "start": 2105.92, "duration": 3.919}, {"text": "i had a", "start": 2108.4, "duration": 3.199}, {"text": "you know a graphic in one of my earlier", "start": 2109.839, "duration": 3.921}, {"text": "presentations where", "start": 2111.599, "duration": 4.561}, {"text": "it was charlton heston coming down with", "start": 2113.76, "duration": 3.52}, {"text": "the coming down with the ten", "start": 2116.16, "duration": 3.04}, {"text": "commandments and i had", "start": 2117.28, "duration": 5.28}, {"text": "pasted um amdahl's law onto the onto the", "start": 2119.2, "duration": 4.72}, {"text": "tablets so", "start": 2122.56, "duration": 3.12}, {"text": "yeah if if you're", "start": 2123.92, "duration": 4.0}, {"text": "if you need one one fundamental thing", "start": 2125.68, "duration": 4.56}, {"text": "that you know it's amdahl's law so let's", "start": 2127.92, "duration": 4.48}, {"text": "say that p is the fraction the code that", "start": 2130.24, "duration": 3.76}, {"text": "can be paralyzed", "start": 2132.4, "duration": 3.92}, {"text": "and s is the fraction the code that must", "start": 2134.0, "duration": 4.16}, {"text": "be run sequentially", "start": 2136.32, "duration": 4.08}, {"text": "so together the serial fraction plus", "start": 2138.16, "duration": 4.72}, {"text": "parallel fractions can equal one and n", "start": 2140.4, "duration": 4.64}, {"text": "is the number of processes", "start": 2142.88, "duration": 3.92}, {"text": "the absolute", "start": 2145.04, "duration": 3.68}, {"text": "best speed up that you can get from your", "start": 2146.8, "duration": 3.039}, {"text": "code", "start": 2148.72, "duration": 2.72}, {"text": "is as a function of the number of", "start": 2149.839, "duration": 4.801}, {"text": "processors is given by by this formula 1", "start": 2151.44, "duration": 6.8}, {"text": "over 1 minus p plus p over n so you can", "start": 2154.64, "duration": 8.08}, {"text": "see 1 minus p represents the", "start": 2158.24, "duration": 4.48}, {"text": "represents the um serial fraction the", "start": 2163.44, "duration": 4.96}, {"text": "code and p over n", "start": 2165.92, "duration": 4.88}, {"text": "represents the parallel fraction divided", "start": 2168.4, "duration": 4.32}, {"text": "by the number of processes so this is", "start": 2170.8, "duration": 4.64}, {"text": "assuming that as we keep doubling the", "start": 2172.72, "duration": 5.92}, {"text": "number of number processors that we have", "start": 2175.44, "duration": 5.36}, {"text": "that we're having the run time", "start": 2178.64, "duration": 4.479}, {"text": "cutting the runtime in half for", "start": 2180.8, "duration": 6.08}, {"text": "all of the parallel content", "start": 2183.119, "duration": 6.24}, {"text": "so in the limit where we let n go to", "start": 2186.88, "duration": 4.64}, {"text": "infinity when we run on an infinitely", "start": 2189.359, "duration": 4.161}, {"text": "big supercomputer", "start": 2191.52, "duration": 4.24}, {"text": "the limit on our speed up is going to be", "start": 2193.52, "duration": 6.48}, {"text": "1 over s so 1 over the serial content so", "start": 2195.76, "duration": 5.839}, {"text": "you might be looking this to say hey", "start": 2200.0, "duration": 3.44}, {"text": "that doesn't seem so bad but i'm going", "start": 2201.599, "duration": 3.201}, {"text": "to show you the next slide that it", "start": 2203.44, "duration": 3.679}, {"text": "doesn't take much serial content to", "start": 2204.8, "duration": 5.84}, {"text": "quickly impact the speed up", "start": 2207.119, "duration": 3.521}, {"text": "so what i'm showing here is um you know", "start": 2210.8, "duration": 5.84}, {"text": "the the amdahl's law equation", "start": 2214.32, "duration": 5.44}, {"text": "plotted as a function of number of cores", "start": 2216.64, "duration": 6.16}, {"text": "on on the x-axis and speed up on the on", "start": 2219.76, "duration": 5.28}, {"text": "the y-axis and i'm showing this for", "start": 2222.8, "duration": 6.64}, {"text": "parallel content of 50 75 90 and 95", "start": 2225.04, "duration": 5.44}, {"text": "percent", "start": 2229.44, "duration": 4.0}, {"text": "so if we have parallel content of 50", "start": 2230.48, "duration": 4.96}, {"text": "you know and by parallel content i don't", "start": 2233.44, "duration": 4.0}, {"text": "mean the number of lines of code but the", "start": 2235.44, "duration": 4.56}, {"text": "amount of time spent in the code", "start": 2237.44, "duration": 3.919}, {"text": "so if we could take that parallel", "start": 2240.0, "duration": 2.72}, {"text": "content", "start": 2241.359, "duration": 2.881}, {"text": "divide it across many many many", "start": 2242.72, "duration": 3.76}, {"text": "processes until it vanishes", "start": 2244.24, "duration": 3.839}, {"text": "until the time spent parallel content", "start": 2246.48, "duration": 3.2}, {"text": "vanishes we're left with that little bit", "start": 2248.079, "duration": 3.441}, {"text": "of serial content and that's going to", "start": 2249.68, "duration": 4.24}, {"text": "drive determine what our maximum speed", "start": 2251.52, "duration": 5.68}, {"text": "up is so you can see with um 50 parallel", "start": 2253.92, "duration": 4.72}, {"text": "content the best we're going to do is", "start": 2257.2, "duration": 7.2}, {"text": "the 2x 75 of 4x 90 10x and 95", "start": 2258.64, "duration": 7.6}, {"text": "a 20x", "start": 2264.4, "duration": 5.84}, {"text": "and even if we go out to code with 99", "start": 2266.24, "duration": 5.839}, {"text": "parallel content", "start": 2270.24, "duration": 3.52}, {"text": "the very very best speed up we're going", "start": 2272.079, "duration": 5.201}, {"text": "to get is is 100x", "start": 2273.76, "duration": 5.599}, {"text": "and i should add that the situation is", "start": 2277.28, "duration": 4.079}, {"text": "even a little more dire than i make you", "start": 2279.359, "duration": 3.681}, {"text": "seem here because you're not going to", "start": 2281.359, "duration": 4.48}, {"text": "keep throwing more and more um hardware", "start": 2283.04, "duration": 5.76}, {"text": "at the problem to get incrementally um", "start": 2285.839, "duration": 5.28}, {"text": "incrementally better better performance", "start": 2288.8, "duration": 4.319}, {"text": "so i'm shown here on this top curve with", "start": 2291.119, "duration": 5.761}, {"text": "the arrow this is the 99 parallel", "start": 2293.119, "duration": 6.72}, {"text": "content or one percent serial content", "start": 2296.88, "duration": 5.199}, {"text": "that", "start": 2299.839, "duration": 5.601}, {"text": "even if we have a code with 99 parallel", "start": 2302.079, "duration": 6.561}, {"text": "paralyzable content you still wouldn't", "start": 2305.44, "duration": 6.24}, {"text": "want to use all of the cores on one note", "start": 2308.64, "duration": 5.68}, {"text": "of expense in this case if we're using", "start": 2311.68, "duration": 5.2}, {"text": "all 128 cores you'd see that we we're", "start": 2314.32, "duration": 5.2}, {"text": "only getting a speed up of 60x so we're", "start": 2316.88, "duration": 6.239}, {"text": "really wasting a lot of hardware", "start": 2319.52, "duration": 3.599}, {"text": "so at this point you might be getting a", "start": 2323.359, "duration": 3.281}, {"text": "little depressed", "start": 2325.28, "duration": 2.4}, {"text": "um", "start": 2326.64, "duration": 2.88}, {"text": "about what parallel computing can do for", "start": 2327.68, "duration": 4.56}, {"text": "you but i'll talk a little bit later how", "start": 2329.52, "duration": 3.52}, {"text": "um", "start": 2332.24, "duration": 2.879}, {"text": "how how the situation isn't as bad as it", "start": 2333.04, "duration": 3.92}, {"text": "looks but first i'm going to talk about", "start": 2335.119, "duration": 4.801}, {"text": "some other limits in scalability so", "start": 2336.96, "duration": 5.119}, {"text": "emphasize a few times amdahl's law sets", "start": 2339.92, "duration": 4.56}, {"text": "an absolute theoretical upper limit in", "start": 2342.079, "duration": 4.961}, {"text": "speed up but there are other factors", "start": 2344.48, "duration": 4.48}, {"text": "so there's communications overhead it", "start": 2347.04, "duration": 4.559}, {"text": "takes some time to move data be between", "start": 2348.96, "duration": 4.8}, {"text": "processes or move it across the network", "start": 2351.599, "duration": 4.161}, {"text": "between nodes", "start": 2353.76, "duration": 3.44}, {"text": "there's going to be", "start": 2355.76, "duration": 3.359}, {"text": "issues with problem size if you have a", "start": 2357.2, "duration": 4.32}, {"text": "small enough problem you can only cut it", "start": 2359.119, "duration": 4.48}, {"text": "up into so many pieces and then there's", "start": 2361.52, "duration": 4.64}, {"text": "going to be uneven load balancing", "start": 2363.599, "duration": 4.561}, {"text": "so in these real life applications that", "start": 2366.16, "duration": 3.439}, {"text": "involve communications and", "start": 2368.16, "duration": 3.12}, {"text": "synchronization", "start": 2369.599, "duration": 3.601}, {"text": "you know where the threads the processes", "start": 2371.28, "duration": 3.52}, {"text": "have to all complete their share of the", "start": 2373.2, "duration": 2.879}, {"text": "work", "start": 2374.8, "duration": 3.2}, {"text": "before before proceeding or if we have", "start": 2376.079, "duration": 4.321}, {"text": "irregular problems non-cartesian grids", "start": 2378.0, "duration": 4.4}, {"text": "and so on the speed up can be much less", "start": 2380.4, "duration": 5.679}, {"text": "than predicted by amdahl's law", "start": 2382.4, "duration": 6.08}, {"text": "and i see a question in the chat this", "start": 2386.079, "duration": 3.681}, {"text": "would actually be a good time to answer", "start": 2388.48, "duration": 2.8}, {"text": "it", "start": 2389.76, "duration": 4.16}, {"text": "okay so yeah great question from and i", "start": 2391.28, "duration": 4.96}, {"text": "hope i'm getting the name right rabaya", "start": 2393.92, "duration": 3.84}, {"text": "this may be a weird question but how do", "start": 2396.24, "duration": 3.599}, {"text": "you determine what part of your code is", "start": 2397.76, "duration": 4.079}, {"text": "paralyzable", "start": 2399.839, "duration": 3.76}, {"text": "i can think of some examples like sensor", "start": 2401.839, "duration": 3.52}, {"text": "data reading can be paralyzed from", "start": 2403.599, "duration": 4.401}, {"text": "processing but is there a rule of thumb", "start": 2405.359, "duration": 5.441}, {"text": "and this is really really hard um", "start": 2408.0, "duration": 5.359}, {"text": "amdahl's law is", "start": 2410.8, "duration": 5.279}, {"text": "it's very theoretical", "start": 2413.359, "duration": 5.681}, {"text": "um you know it it you're going into it", "start": 2416.079, "duration": 5.04}, {"text": "assuming that you know what the parallel", "start": 2419.04, "duration": 4.64}, {"text": "content is or what the serial content is", "start": 2421.119, "duration": 5.121}, {"text": "but it's really really hard to to", "start": 2423.68, "duration": 4.64}, {"text": "measure that directly so what i'm going", "start": 2426.24, "duration": 3.76}, {"text": "to say and i'll get into this into the", "start": 2428.32, "duration": 4.24}, {"text": "next few slides is what it really comes", "start": 2430.0, "duration": 3.92}, {"text": "down to is you have to do some", "start": 2432.56, "duration": 3.84}, {"text": "benchmarking studies and from that", "start": 2433.92, "duration": 3.84}, {"text": "you're going to start getting a feel for", "start": 2436.4, "duration": 3.28}, {"text": "how much is paralyzable and how many", "start": 2437.76, "duration": 4.72}, {"text": "cars you should be running on but yeah a", "start": 2439.68, "duration": 5.84}, {"text": "fantastic question if you had a", "start": 2442.48, "duration": 6.16}, {"text": "very very very simple code that just did", "start": 2445.52, "duration": 4.48}, {"text": "a couple of things", "start": 2448.64, "duration": 3.52}, {"text": "you could", "start": 2450.0, "duration": 4.32}, {"text": "you might be able to um figure it out", "start": 2452.16, "duration": 4.56}, {"text": "but for real applications it's almost", "start": 2454.32, "duration": 4.4}, {"text": "impossible to just look at the code and", "start": 2456.72, "duration": 4.72}, {"text": "determine that", "start": 2458.72, "duration": 2.72}, {"text": "all right so i'm gonna you know talk", "start": 2462.0, "duration": 2.96}, {"text": "about just a few of the things that", "start": 2463.76, "duration": 3.68}, {"text": "impact your perilous scalability first", "start": 2464.96, "duration": 4.32}, {"text": "of all paralyzing the code requires that", "start": 2467.44, "duration": 3.44}, {"text": "you divide the computational work into", "start": 2469.28, "duration": 2.96}, {"text": "chunks that can be executed", "start": 2470.88, "duration": 2.64}, {"text": "independently", "start": 2472.24, "duration": 3.04}, {"text": "and if the work cannot be distributed", "start": 2473.52, "duration": 4.319}, {"text": "evenly then process is going to sit idle", "start": 2475.28, "duration": 4.48}, {"text": "waiting for the longest chunk to finish", "start": 2477.839, "duration": 4.321}, {"text": "so here's a case where we're running on", "start": 2479.76, "duration": 4.559}, {"text": "you know one of four cores for cpus it", "start": 2482.16, "duration": 4.24}, {"text": "could also be gpus", "start": 2484.319, "duration": 3.04}, {"text": "where", "start": 2486.4, "duration": 2.959}, {"text": "the initial chunk of work", "start": 2487.359, "duration": 5.601}, {"text": "is largest for cpu um", "start": 2489.359, "duration": 6.96}, {"text": "what largest for cp cpu two and a little", "start": 2492.96, "duration": 5.76}, {"text": "bit shorter for zero one and three so", "start": 2496.319, "duration": 4.241}, {"text": "what's going to happen is zero one and", "start": 2498.72, "duration": 4.08}, {"text": "three are gonna complete their work", "start": 2500.56, "duration": 3.92}, {"text": "and if we have that synchronization", "start": 2502.8, "duration": 3.2}, {"text": "point that indicate with the arrow and", "start": 2504.48, "duration": 3.52}, {"text": "the vertical line they just have to sit", "start": 2506.0, "duration": 4.96}, {"text": "there idle waiting for waiting for cpu2", "start": 2508.0, "duration": 4.0}, {"text": "to finish", "start": 2510.96, "duration": 2.56}, {"text": "and then in the in the next chunk of", "start": 2512.0, "duration": 4.079}, {"text": "work cpu one was given that given the", "start": 2513.52, "duration": 4.72}, {"text": "most work and the other processes or", "start": 2516.079, "duration": 5.121}, {"text": "threads need to need to sit idle", "start": 2518.24, "duration": 5.68}, {"text": "we also have communications overhead um", "start": 2521.2, "duration": 4.32}, {"text": "if you're doing computational fluid", "start": 2523.92, "duration": 3.84}, {"text": "dynamics magnetohydrodynamics", "start": 2525.52, "duration": 4.4}, {"text": "climate weather anything involving", "start": 2527.76, "duration": 3.52}, {"text": "systems of partial differential", "start": 2529.92, "duration": 3.12}, {"text": "equations you're probably going to be", "start": 2531.28, "duration": 4.319}, {"text": "doing a simulation on a on a discrete", "start": 2533.04, "duration": 3.68}, {"text": "grid", "start": 2535.599, "duration": 2.561}, {"text": "so let's say and this is an", "start": 2536.72, "duration": 3.84}, {"text": "unrealistically small problem but let's", "start": 2538.16, "duration": 5.6}, {"text": "say we have a 16 by 16 grid we divide", "start": 2540.56, "duration": 5.039}, {"text": "that into four into four eight by eight", "start": 2543.76, "duration": 4.72}, {"text": "chunks and we distribute that across our", "start": 2545.599, "duration": 5.121}, {"text": "four processes", "start": 2548.48, "duration": 4.24}, {"text": "and let's assume that the value of each", "start": 2550.72, "duration": 4.0}, {"text": "cell depends on the value of the", "start": 2552.72, "duration": 5.84}, {"text": "neighboring cells we might have um", "start": 2554.72, "duration": 6.16}, {"text": "you know we might have um", "start": 2558.56, "duration": 4.72}, {"text": "a material flowing across across those", "start": 2560.88, "duration": 5.199}, {"text": "cell boundaries or we need to", "start": 2563.28, "duration": 5.36}, {"text": "make updates for for diffusion or", "start": 2566.079, "duration": 4.641}, {"text": "temperature gradients and so on", "start": 2568.64, "duration": 3.52}, {"text": "so for cells that are within the", "start": 2570.72, "duration": 3.84}, {"text": "interior of each chunk calculations can", "start": 2572.16, "duration": 4.48}, {"text": "be done within each process", "start": 2574.56, "duration": 4.96}, {"text": "but for cells at the boundaries", "start": 2576.64, "duration": 4.479}, {"text": "we need data from the neighboring", "start": 2579.52, "duration": 4.24}, {"text": "processors so now we have to add ghost", "start": 2581.119, "duration": 3.601}, {"text": "cells", "start": 2583.76, "duration": 3.2}, {"text": "around the around those chunks", "start": 2584.72, "duration": 4.08}, {"text": "so that adds a little bit of overhead it", "start": 2586.96, "duration": 3.879}, {"text": "requires more memory", "start": 2588.8, "duration": 6.08}, {"text": "um yeah want more storage", "start": 2590.839, "duration": 7.0}, {"text": "so oh sorry let me go back um", "start": 2594.88, "duration": 5.36}, {"text": "and that's going to to impact your um", "start": 2597.839, "duration": 4.081}, {"text": "scalability", "start": 2600.24, "duration": 3.68}, {"text": "all right so i hope i didn't scare", "start": 2601.92, "duration": 4.8}, {"text": "everybody off thinking that whoa there's", "start": 2603.92, "duration": 4.48}, {"text": "so many things that can go wrong with", "start": 2606.72, "duration": 3.84}, {"text": "parallel computing and limit my per", "start": 2608.4, "duration": 4.719}, {"text": "limit my scalability so between the hard", "start": 2610.56, "duration": 5.039}, {"text": "limits imposed by amdahl's law and all", "start": 2613.119, "duration": 3.841}, {"text": "of the other factors that affect", "start": 2615.599, "duration": 3.921}, {"text": "scalability how does anyone ever use all", "start": 2616.96, "duration": 4.639}, {"text": "the cores on a single modern compute", "start": 2619.52, "duration": 4.319}, {"text": "node let alone the full power of large", "start": 2621.599, "duration": 4.641}, {"text": "supercomputers", "start": 2623.839, "duration": 4.961}, {"text": "so first of all and i forgot to put this", "start": 2626.24, "duration": 4.32}, {"text": "bullet on here", "start": 2628.8, "duration": 3.84}, {"text": "often the real complications that you", "start": 2630.56, "duration": 5.44}, {"text": "have to do re involve very very", "start": 2632.64, "duration": 5.28}, {"text": "depending on the nature of them very", "start": 2636.0, "duration": 4.48}, {"text": "very little serial content", "start": 2637.92, "duration": 5.76}, {"text": "so the um parallel content could be much", "start": 2640.48, "duration": 7.119}, {"text": "higher could be 99.9 99.99", "start": 2643.68, "duration": 5.76}, {"text": "and so on percent", "start": 2647.599, "duration": 3.121}, {"text": "but first of all the reality is that", "start": 2649.44, "duration": 3.6}, {"text": "most parallel applications do not scale", "start": 2650.72, "duration": 4.639}, {"text": "to thousands or even hundreds of cores", "start": 2653.04, "duration": 4.48}, {"text": "you could do a lot of great parallel", "start": 2655.359, "duration": 4.161}, {"text": "computing within a single within a", "start": 2657.52, "duration": 4.4}, {"text": "single node and we see this in certain", "start": 2659.52, "duration": 4.48}, {"text": "fields like phylogenetic constructing", "start": 2661.92, "duration": 3.76}, {"text": "phylogenetic trees and a lot of", "start": 2664.0, "duration": 3.839}, {"text": "computational chemistry", "start": 2665.68, "duration": 4.399}, {"text": "now the applications that achieve high", "start": 2667.839, "duration": 3.841}, {"text": "scalability", "start": 2670.079, "duration": 4.721}, {"text": "they employ several strategies again as", "start": 2671.68, "duration": 5.439}, {"text": "end users rather than programmers you", "start": 2674.8, "duration": 4.08}, {"text": "don't need to worry so much about how", "start": 2677.119, "duration": 4.321}, {"text": "this is done in practice", "start": 2678.88, "duration": 4.88}, {"text": "but first of all we often grow the", "start": 2681.44, "duration": 5.04}, {"text": "problem size with the number of cores or", "start": 2683.76, "duration": 4.88}, {"text": "nodes this is something um", "start": 2686.48, "duration": 4.16}, {"text": "called gustafson's law", "start": 2688.64, "duration": 4.32}, {"text": "so instead of", "start": 2690.64, "duration": 5.199}, {"text": "you know say working with a 16 by 16", "start": 2692.96, "duration": 3.76}, {"text": "grid", "start": 2695.839, "duration": 2.961}, {"text": "and dividing across 4", "start": 2696.72, "duration": 5.84}, {"text": "or 8 or 16 or 32 or so on processes", "start": 2698.8, "duration": 5.76}, {"text": "instead we're going to solve a much much", "start": 2702.56, "duration": 4.96}, {"text": "larger problem so that each of those", "start": 2704.56, "duration": 4.96}, {"text": "threads or processes is going to have a", "start": 2707.52, "duration": 4.48}, {"text": "lot more work to do", "start": 2709.52, "duration": 4.88}, {"text": "we can also overlap communications with", "start": 2712.0, "duration": 4.72}, {"text": "computation so while we're waiting for", "start": 2714.4, "duration": 4.56}, {"text": "data to be exchanged we could actually", "start": 2716.72, "duration": 4.639}, {"text": "be doing computing in the background we", "start": 2718.96, "duration": 4.72}, {"text": "could do dynamic load balancing to sign", "start": 2721.359, "duration": 4.72}, {"text": "work decors as they become idle in fact", "start": 2723.68, "duration": 4.32}, {"text": "i did this in an application of my own", "start": 2726.079, "duration": 4.0}, {"text": "in computational economics a few years", "start": 2728.0, "duration": 2.96}, {"text": "ago", "start": 2730.079, "duration": 4.161}, {"text": "where we were analyzing um", "start": 2730.96, "duration": 7.52}, {"text": "the activity of entire stock exchanges", "start": 2734.24, "duration": 8.079}, {"text": "and the dynamic range for for analyzing", "start": 2738.48, "duration": 4.96}, {"text": "each stock which could be done", "start": 2742.319, "duration": 4.481}, {"text": "independently range from minutes to less", "start": 2743.44, "duration": 4.879}, {"text": "than a second", "start": 2746.8, "duration": 4.24}, {"text": "but by dynamically assigning the work", "start": 2748.319, "duration": 4.081}, {"text": "starting with the biggest chunks", "start": 2751.04, "duration": 3.36}, {"text": "distributing them and then as processes", "start": 2752.4, "duration": 4.32}, {"text": "become become free assigning them pieces", "start": 2754.4, "duration": 4.439}, {"text": "of work we're able to get near perfect", "start": 2756.72, "duration": 4.96}, {"text": "scalability and you can also increase", "start": 2758.839, "duration": 4.601}, {"text": "the ratio of the computation to", "start": 2761.68, "duration": 4.88}, {"text": "communication if you're doing more work", "start": 2763.44, "duration": 6.56}, {"text": "a perfect example is reactive flow", "start": 2766.56, "duration": 5.44}, {"text": "so studying combustion", "start": 2770.0, "duration": 3.359}, {"text": "you know this is an extension of", "start": 2772.0, "duration": 3.839}, {"text": "computational flow dynamics all of that", "start": 2773.359, "duration": 4.161}, {"text": "work that you're doing solving the", "start": 2775.839, "duration": 2.881}, {"text": "chemistry", "start": 2777.52, "duration": 3.839}, {"text": "is going to overwhelm a lot of the a lot", "start": 2778.72, "duration": 4.16}, {"text": "of the other computations and", "start": 2781.359, "duration": 3.841}, {"text": "communications so now you've increased", "start": 2782.88, "duration": 3.92}, {"text": "that ratio of computation and", "start": 2785.2, "duration": 4.0}, {"text": "communication and your scalability is", "start": 2786.8, "duration": 5.12}, {"text": "going to look a lot better", "start": 2789.2, "duration": 5.119}, {"text": "so we've got about 10 minutes left and i", "start": 2791.92, "duration": 4.32}, {"text": "think i'm going to get into the the most", "start": 2794.319, "duration": 5.76}, {"text": "practical topic in this talk and that is", "start": 2796.24, "duration": 4.8}, {"text": "you know running the parallel", "start": 2800.079, "duration": 3.201}, {"text": "applications we've covered the basics of", "start": 2801.04, "duration": 4.24}, {"text": "parallel computing hardware threads", "start": 2803.28, "duration": 5.44}, {"text": "processes zomdous lawn scalability", "start": 2805.28, "duration": 4.96}, {"text": "so you got a lot of theory a lot of", "start": 2808.72, "duration": 4.399}, {"text": "background but this comes but what we", "start": 2810.24, "duration": 5.04}, {"text": "ultimately want to know as end users of", "start": 2813.119, "duration": 4.881}, {"text": "supercomputers is all right how many", "start": 2815.28, "duration": 5.52}, {"text": "cpus or how many gpus should i use when", "start": 2818.0, "duration": 5.839}, {"text": "i'm running my parallel application", "start": 2820.8, "duration": 5.92}, {"text": "and the only definitive way to answer", "start": 2823.839, "duration": 6.881}, {"text": "this is to perform a scaling study using", "start": 2826.72, "duration": 5.92}, {"text": "a representative problem", "start": 2830.72, "duration": 4.32}, {"text": "run on different number of processors or", "start": 2832.64, "duration": 3.76}, {"text": "gpus", "start": 2835.04, "duration": 3.12}, {"text": "and by a representative problem i mean", "start": 2836.4, "duration": 3.679}, {"text": "one with the same size the grid", "start": 2838.16, "duration": 4.0}, {"text": "dimensions number of particles number of", "start": 2840.079, "duration": 5.441}, {"text": "images number of genomes and complexity", "start": 2842.16, "duration": 6.0}, {"text": "the level of theory the type of analysis", "start": 2845.52, "duration": 4.799}, {"text": "all of the physics that you include as", "start": 2848.16, "duration": 3.52}, {"text": "the research problem that you want to", "start": 2850.319, "duration": 2.561}, {"text": "solve", "start": 2851.68, "duration": 2.48}, {"text": "so", "start": 2852.88, "duration": 3.12}, {"text": "this doesn't mean that you need to solve", "start": 2854.16, "duration": 3.76}, {"text": "your entire research problem if you're", "start": 2856.0, "duration": 3.839}, {"text": "doing something like molecular dynamics", "start": 2857.92, "duration": 4.399}, {"text": "and let's say you need to run for", "start": 2859.839, "duration": 4.721}, {"text": "you know for a million time steps you", "start": 2862.319, "duration": 5.121}, {"text": "could benchmark that using using a", "start": 2864.56, "duration": 5.2}, {"text": "thousand or ten thousand times steps if", "start": 2867.44, "duration": 3.76}, {"text": "you're doing", "start": 2869.76, "duration": 3.92}, {"text": "computational fluid dynamics you don't", "start": 2871.2, "duration": 5.04}, {"text": "need to keep going out um", "start": 2873.68, "duration": 4.159}, {"text": "you know for a large number of steps", "start": 2876.24, "duration": 2.72}, {"text": "just enough that you're getting", "start": 2877.839, "duration": 2.961}, {"text": "something that's representative of the", "start": 2878.96, "duration": 3.44}, {"text": "problem and that you're not being", "start": 2880.8, "duration": 4.559}, {"text": "overwhelmed say by the startup time say", "start": 2882.4, "duration": 6.8}, {"text": "reading and reading and writing data", "start": 2885.359, "duration": 5.361}, {"text": "so", "start": 2889.2, "duration": 3.119}, {"text": "this is probably going to be the most", "start": 2890.72, "duration": 4.32}, {"text": "important thing to know if you're", "start": 2892.319, "duration": 4.241}, {"text": "writing", "start": 2895.04, "duration": 3.52}, {"text": "requests for for time on the", "start": 2896.56, "duration": 4.08}, {"text": "supercomputers or even if you want to", "start": 2898.56, "duration": 5.84}, {"text": "make good use of your own resources", "start": 2900.64, "duration": 5.84}, {"text": "so what i'm showing here are scaling", "start": 2904.4, "duration": 4.159}, {"text": "plots", "start": 2906.48, "duration": 5.28}, {"text": "runtime on the y-axis number of cores on", "start": 2908.559, "duration": 5.601}, {"text": "the x-axis", "start": 2911.76, "duration": 5.12}, {"text": "for two different codes", "start": 2914.16, "duration": 3.6}, {"text": "and", "start": 2916.88, "duration": 2.959}, {"text": "without reading the head", "start": 2917.76, "duration": 4.24}, {"text": "now see if you can look at these and", "start": 2919.839, "duration": 4.24}, {"text": "decide which one has better scaling", "start": 2922.0, "duration": 4.24}, {"text": "behavior", "start": 2924.079, "duration": 3.681}, {"text": "and i'm going to say it's kind of hard", "start": 2926.24, "duration": 2.48}, {"text": "to tell", "start": 2927.76, "duration": 3.2}, {"text": "since the timings at large core counts", "start": 2928.72, "duration": 4.24}, {"text": "are indistinguishable so what i've done", "start": 2930.96, "duration": 4.399}, {"text": "here is i've plotted the runtime as a", "start": 2932.96, "duration": 4.8}, {"text": "function a number of cores on linear", "start": 2935.359, "duration": 5.76}, {"text": "axes and this is a terrible thing to do", "start": 2937.76, "duration": 5.76}, {"text": "because i cannot", "start": 2941.119, "duration": 4.641}, {"text": "i really can't tell how much speed up i", "start": 2943.52, "duration": 6.64}, {"text": "got going say from 64 cores to 128 cores", "start": 2945.76, "duration": 5.839}, {"text": "and these two codes they look", "start": 2950.16, "duration": 3.76}, {"text": "indistinguishable to me", "start": 2951.599, "duration": 4.0}, {"text": "so the right way to present scaling", "start": 2953.92, "duration": 3.04}, {"text": "results", "start": 2955.599, "duration": 4.081}, {"text": "is to use log axes", "start": 2956.96, "duration": 4.96}, {"text": "so i have um", "start": 2959.68, "duration": 4.639}, {"text": "logarithmic for the time logarithmic for", "start": 2961.92, "duration": 4.56}, {"text": "the for the number of cores", "start": 2964.319, "duration": 5.361}, {"text": "doesn't matter if you use log 2 log 10", "start": 2966.48, "duration": 4.24}, {"text": "it's it's", "start": 2969.68, "duration": 3.52}, {"text": "pretty common to to use log 2 for for", "start": 2970.72, "duration": 5.52}, {"text": "the number of cores but as long as you", "start": 2973.2, "duration": 5.919}, {"text": "know it's a it's a log axis", "start": 2976.24, "duration": 5.68}, {"text": "so now i'm showing the exact same data", "start": 2979.119, "duration": 4.561}, {"text": "from the previous slide", "start": 2981.92, "duration": 3.919}, {"text": "that's with linear axes that that's with", "start": 2983.68, "duration": 4.08}, {"text": "log axes and you can see that the", "start": 2985.839, "duration": 5.361}, {"text": "situation is completely different", "start": 2987.76, "duration": 4.24}, {"text": "so", "start": 2991.2, "duration": 1.76}, {"text": "um", "start": 2992.0, "duration": 2.88}, {"text": "you know the red line is showing the", "start": 2992.96, "duration": 3.84}, {"text": "showing the run time as the function of", "start": 2994.88, "duration": 5.28}, {"text": "the number of cores i've added in a", "start": 2996.8, "duration": 5.519}, {"text": "little guideline here to show you what", "start": 3000.16, "duration": 4.64}, {"text": "what what linear um scaling should look", "start": 3002.319, "duration": 5.361}, {"text": "like and also i think it's very handy to", "start": 3004.8, "duration": 4.64}, {"text": "show the", "start": 3007.68, "duration": 4.56}, {"text": "um that the parallel efficiency so", "start": 3009.44, "duration": 5.36}, {"text": "basically this is the speed up divided", "start": 3012.24, "duration": 4.48}, {"text": "by the number of curves so if you have", "start": 3014.8, "duration": 4.0}, {"text": "an efficiency of one that means every", "start": 3016.72, "duration": 4.08}, {"text": "time you double the number of cores you", "start": 3018.8, "duration": 4.08}, {"text": "cut the runtime in half", "start": 3020.8, "duration": 4.559}, {"text": "um on the left hand side we can see that", "start": 3022.88, "duration": 4.959}, {"text": "we're doing pretty well in the beginning", "start": 3025.359, "duration": 5.041}, {"text": "um you know drops off a little bit", "start": 3027.839, "duration": 4.72}, {"text": "you know the smaller core counts and", "start": 3030.4, "duration": 4.0}, {"text": "then it kind of plunges", "start": 3032.559, "duration": 4.0}, {"text": "so this gives us a lot more insight and", "start": 3034.4, "duration": 4.159}, {"text": "this is what the reviewers will want to", "start": 3036.559, "duration": 3.841}, {"text": "see when you submit an allocation", "start": 3038.559, "duration": 4.321}, {"text": "request", "start": 3040.4, "duration": 2.48}, {"text": "so now the question is where should i be", "start": 3042.96, "duration": 3.84}, {"text": "on that scaling curve", "start": 3044.64, "duration": 4.4}, {"text": "should i you know run at a very high", "start": 3046.8, "duration": 4.0}, {"text": "parallel efficiency", "start": 3049.04, "duration": 3.44}, {"text": "but it's going to take my code longer to", "start": 3050.8, "duration": 4.08}, {"text": "run or should i go to lower parallel", "start": 3052.48, "duration": 3.76}, {"text": "efficiency", "start": 3054.88, "duration": 2.88}, {"text": "which will be much faster but we'll be", "start": 3056.24, "duration": 3.28}, {"text": "wasting resources", "start": 3057.76, "duration": 3.68}, {"text": "so i'm going to say if your work", "start": 3059.52, "duration": 3.68}, {"text": "is not particularly sensitive to the", "start": 3061.44, "duration": 3.76}, {"text": "time to complete a single run", "start": 3063.2, "duration": 4.8}, {"text": "consider using cpu or gpu accounts that", "start": 3065.2, "duration": 5.68}, {"text": "give you something at or very close 100", "start": 3068.0, "duration": 4.079}, {"text": "efficiency", "start": 3070.88, "duration": 2.88}, {"text": "sometimes even if that means running a", "start": 3072.079, "duration": 3.76}, {"text": "single core", "start": 3073.76, "duration": 3.839}, {"text": "um this isn't always practical there may", "start": 3075.839, "duration": 3.76}, {"text": "be some calculations that just take too", "start": 3077.599, "duration": 4.96}, {"text": "long or require too much memory but if", "start": 3079.599, "duration": 5.921}, {"text": "you could try to be um you know try try", "start": 3082.559, "duration": 5.601}, {"text": "to stay at a high um parallel efficiency", "start": 3085.52, "duration": 4.079}, {"text": "this especially makes sense for", "start": 3088.16, "duration": 3.679}, {"text": "parameter sweep workloads where you're", "start": 3089.599, "duration": 4.561}, {"text": "doing the same calculation with many", "start": 3091.839, "duration": 6.0}, {"text": "times through different sets of inputs", "start": 3094.16, "duration": 5.84}, {"text": "okay you can go out a little bit further", "start": 3097.839, "duration": 4.0}, {"text": "on the scaling curve if the job would", "start": 3100.0, "duration": 4.559}, {"text": "take an unreasonably long time", "start": 3101.839, "duration": 4.48}, {"text": "to complete at the lower and the lower", "start": 3104.559, "duration": 3.52}, {"text": "core counts or if shorter time", "start": 3106.319, "duration": 3.76}, {"text": "dissolution helps make progress in your", "start": 3108.079, "duration": 4.0}, {"text": "research like let's say you're doing a", "start": 3110.079, "duration": 4.24}, {"text": "molecular dynamic simulation and you", "start": 3112.079, "duration": 3.921}, {"text": "really don't want to wait all week for", "start": 3114.319, "duration": 3.04}, {"text": "that to complete you might want", "start": 3116.0, "duration": 3.44}, {"text": "something complete in an afternoon or a", "start": 3117.359, "duration": 3.441}, {"text": "day it's going to make you more", "start": 3119.44, "duration": 3.119}, {"text": "scientific it's going to make you more", "start": 3120.8, "duration": 5.759}, {"text": "um productive as a researcher", "start": 3122.559, "duration": 5.841}, {"text": "if your code does not have checkpoint", "start": 3126.559, "duration": 3.601}, {"text": "restart capabilities and the runtime", "start": 3128.4, "duration": 3.76}, {"text": "would exceed the q limits you'll have no", "start": 3130.16, "duration": 4.399}, {"text": "choice for running higher core counts i", "start": 3132.16, "duration": 4.56}, {"text": "see we've got a question here", "start": 3134.559, "duration": 3.841}, {"text": "oh so from", "start": 3136.72, "duration": 4.08}, {"text": "xiaofei how to calculate parallel", "start": 3138.4, "duration": 5.36}, {"text": "efficiency um parallel efficiency is", "start": 3140.8, "duration": 5.039}, {"text": "just the", "start": 3143.76, "duration": 2.88}, {"text": "um", "start": 3145.839, "duration": 2.801}, {"text": "it is just the", "start": 3146.64, "duration": 5.439}, {"text": "speed up divided by the number of cores", "start": 3148.64, "duration": 6.56}, {"text": "so for example if i got a", "start": 3152.079, "duration": 6.48}, {"text": "16x speedup on 16 cores my parallel", "start": 3155.2, "duration": 5.2}, {"text": "efficiency would be one", "start": 3158.559, "duration": 4.961}, {"text": "if i had a um", "start": 3160.4, "duration": 6.08}, {"text": "if i had a um 8x speed up and 16 cores", "start": 3163.52, "duration": 5.44}, {"text": "my efficiency would be", "start": 3166.48, "duration": 3.839}, {"text": "what would be 50", "start": 3168.96, "duration": 3.68}, {"text": "and also in the", "start": 3170.319, "duration": 4.721}, {"text": "um in the accompanying", "start": 3172.64, "duration": 3.679}, {"text": "um", "start": 3175.04, "duration": 3.36}, {"text": "github repo i have an example where i", "start": 3176.319, "duration": 4.641}, {"text": "calculate the parallel efficiency", "start": 3178.4, "duration": 5.679}, {"text": "and again feel free to um feel free to", "start": 3180.96, "duration": 4.56}, {"text": "follow up with me later if you have any", "start": 3184.079, "duration": 3.201}, {"text": "more questions", "start": 3185.52, "duration": 3.839}, {"text": "all right we're almost done", "start": 3187.28, "duration": 3.039}, {"text": "um", "start": 3189.359, "duration": 2.881}, {"text": "and when can i run further out on the", "start": 3190.319, "duration": 3.361}, {"text": "scaling curve", "start": 3192.24, "duration": 3.599}, {"text": "so let's say", "start": 3193.68, "duration": 3.919}, {"text": "time to solution is really really", "start": 3195.839, "duration": 4.401}, {"text": "critical then it's okay to run at lower", "start": 3197.599, "duration": 5.921}, {"text": "efficiency but be sure to justify this", "start": 3200.24, "duration": 5.359}, {"text": "in any allocation request", "start": 3203.52, "duration": 3.52}, {"text": "so let's say that you need to do", "start": 3205.599, "duration": 3.361}, {"text": "calculations that are run on a regular", "start": 3207.04, "duration": 4.16}, {"text": "schedule say data is collected during", "start": 3208.96, "duration": 4.08}, {"text": "the day and you need to process it", "start": 3211.2, "duration": 4.96}, {"text": "overnight a perfect example of this is", "start": 3213.04, "duration": 4.24}, {"text": "the", "start": 3216.16, "duration": 3.439}, {"text": "atmospheric river simulations that are", "start": 3217.28, "duration": 3.76}, {"text": "done at the scripps institution of", "start": 3219.599, "duration": 3.52}, {"text": "oceanography they want to they want to", "start": 3221.04, "duration": 4.079}, {"text": "be able to have that prediction the next", "start": 3223.119, "duration": 4.561}, {"text": "um the next morning", "start": 3225.119, "duration": 4.24}, {"text": "um sometimes", "start": 3227.68, "duration": 4.159}, {"text": "you know and we don't use our super", "start": 3229.359, "duration": 4.72}, {"text": "computers for anything quite so critical", "start": 3231.839, "duration": 5.041}, {"text": "but if you're in a um", "start": 3234.079, "duration": 4.321}, {"text": "if you work in an operational", "start": 3236.88, "duration": 2.88}, {"text": "environment", "start": 3238.4, "duration": 3.199}, {"text": "you know where you're tracking tornadoes", "start": 3239.76, "duration": 4.48}, {"text": "or tsunamis or hurricanes you can run", "start": 3241.599, "duration": 5.121}, {"text": "way down here you know much smaller you", "start": 3244.24, "duration": 5.04}, {"text": "know much worse parallel efficiency", "start": 3246.72, "duration": 5.839}, {"text": "if getting that answer really really", "start": 3249.28, "duration": 6.079}, {"text": "really quickly is important for example", "start": 3252.559, "duration": 4.881}, {"text": "i'd much rather that", "start": 3255.359, "duration": 4.48}, {"text": "somebody you know wasted some computer", "start": 3257.44, "duration": 3.2}, {"text": "time", "start": 3259.839, "duration": 3.28}, {"text": "to give me a heads up that a tonight", "start": 3260.64, "duration": 5.28}, {"text": "tornado is coming um you know", "start": 3263.119, "duration": 4.881}, {"text": "not knowing say five minutes in advance", "start": 3265.92, "duration": 3.76}, {"text": "rather than one minute in advance you", "start": 3268.0, "duration": 3.839}, {"text": "know kind of an extreme case but most of", "start": 3269.68, "duration": 4.0}, {"text": "you are going to be further out here", "start": 3271.839, "duration": 4.0}, {"text": "better parallel efficiency", "start": 3273.68, "duration": 4.48}, {"text": "um one other consideration and i'll just", "start": 3275.839, "duration": 4.72}, {"text": "talk about this very very quickly", "start": 3278.16, "duration": 4.8}, {"text": "sometimes if you have a application with", "start": 3280.559, "duration": 4.961}, {"text": "a very very large memory footprint", "start": 3282.96, "duration": 3.84}, {"text": "you'll need to get you'll need to", "start": 3285.52, "duration": 3.52}, {"text": "request more cores", "start": 3286.8, "duration": 4.559}, {"text": "just to get the just get memory normally", "start": 3289.04, "duration": 3.44}, {"text": "you will get", "start": 3291.359, "duration": 2.881}, {"text": "um", "start": 3292.48, "duration": 4.24}, {"text": "number of cores proportional to the", "start": 3294.24, "duration": 5.92}, {"text": "proportion of the memory so let's see on", "start": 3296.72, "duration": 6.32}, {"text": "on expanse you get just under", "start": 3300.16, "duration": 5.12}, {"text": "the right you get", "start": 3303.04, "duration": 4.559}, {"text": "just under two gigabytes per core but if", "start": 3305.28, "duration": 3.52}, {"text": "you have someone with a really really", "start": 3307.599, "duration": 3.041}, {"text": "big memory footprint that doesn't", "start": 3308.8, "duration": 4.24}, {"text": "special that doesn't scale well it's", "start": 3310.64, "duration": 5.84}, {"text": "okay just make sure that you just fight", "start": 3313.04, "duration": 5.2}, {"text": "and then finally we've got about two", "start": 3316.48, "duration": 3.68}, {"text": "minutes left you know where to go next", "start": 3318.24, "duration": 4.079}, {"text": "we only scratched the surface", "start": 3320.16, "duration": 4.399}, {"text": "and i sorry i realized that this slide", "start": 3322.319, "duration": 5.201}, {"text": "is still exceed centric you know the new", "start": 3324.559, "duration": 4.641}, {"text": "access program is still getting up to", "start": 3327.52, "duration": 4.24}, {"text": "speed but i believe that the exceed web", "start": 3329.2, "duration": 5.359}, {"text": "pages are still available um you know", "start": 3331.76, "duration": 5.28}, {"text": "xseed and scsc they have many training", "start": 3334.559, "duration": 4.161}, {"text": "resources covering a wide range of", "start": 3337.04, "duration": 2.88}, {"text": "topics", "start": 3338.72, "duration": 3.76}, {"text": "and then for um", "start": 3339.92, "duration": 4.48}, {"text": "you know for the nationally allocated", "start": 3342.48, "duration": 3.599}, {"text": "resources", "start": 3344.4, "duration": 4.08}, {"text": "they have user guides that", "start": 3346.079, "duration": 3.921}, {"text": "um", "start": 3348.48, "duration": 2.879}, {"text": "you know that have a lot of practical", "start": 3350.0, "duration": 3.599}, {"text": "information on job submission accounting", "start": 3351.359, "duration": 4.641}, {"text": "compilation data movement available", "start": 3353.599, "duration": 4.881}, {"text": "software and other site specific content", "start": 3356.0, "duration": 4.96}, {"text": "and i promise to get this updated once", "start": 3358.48, "duration": 5.44}, {"text": "um access is a little bit more up and up", "start": 3360.96, "duration": 4.72}, {"text": "and running", "start": 3363.92, "duration": 4.639}, {"text": "so just a few conclusions absolute last", "start": 3365.68, "duration": 5.2}, {"text": "slide parallel competence for everyone", "start": 3368.559, "duration": 4.161}, {"text": "who wants accomplished more research", "start": 3370.88, "duration": 3.84}, {"text": "solve more challenging problems you", "start": 3372.72, "duration": 4.079}, {"text": "don't need to be a programmer said this", "start": 3374.72, "duration": 4.24}, {"text": "many times already but you do need to", "start": 3376.799, "duration": 4.481}, {"text": "know some of the fundamentals in order", "start": 3378.96, "duration": 4.56}, {"text": "to effectively use parallel computer", "start": 3381.28, "duration": 4.64}, {"text": "processes or instances of programs", "start": 3383.52, "duration": 4.64}, {"text": "threads run within a process and access", "start": 3385.92, "duration": 7.28}, {"text": "shared data mpi and openmp are just um", "start": 3388.16, "duration": 5.84}, {"text": "you know", "start": 3393.2, "duration": 3.599}, {"text": "libraries compiler directives and so on", "start": 3394.0, "duration": 5.28}, {"text": "use the parallel codes amdahl's law", "start": 3396.799, "duration": 3.921}, {"text": "gives you that upper limit of", "start": 3399.28, "duration": 3.519}, {"text": "scalability but be aware of the other", "start": 3400.72, "duration": 4.8}, {"text": "factors and then finally when you are", "start": 3402.799, "duration": 6.161}, {"text": "requesting time through access be sure", "start": 3405.52, "duration": 4.559}, {"text": "that you know how to display your", "start": 3408.96, "duration": 2.639}, {"text": "scaling data and choose your chord", "start": 3410.079, "duration": 3.76}, {"text": "counts um and i have some code here that", "start": 3411.599, "duration": 3.841}, {"text": "i use to generate the figures in this", "start": 3413.839, "duration": 3.76}, {"text": "presentation and in fact i would even", "start": 3415.44, "duration": 3.919}, {"text": "recommend if you were writing an", "start": 3417.599, "duration": 4.641}, {"text": "allocation request just go there use my", "start": 3419.359, "duration": 5.44}, {"text": "jupiter notebooks and plug in your own", "start": 3422.24, "duration": 4.079}, {"text": "data and if you want to give me any", "start": 3424.799, "duration": 3.201}, {"text": "feedback on them", "start": 3426.319, "duration": 4.0}, {"text": "i'd be happy to accommodate pull", "start": 3428.0, "duration": 3.599}, {"text": "requests", "start": 3430.319, "duration": 4.0}, {"text": "and i see one more question okay cindy", "start": 3431.599, "duration": 4.161}, {"text": "just posted that", "start": 3434.319, "duration": 4.0}, {"text": "and with that i am all done so i'm happy", "start": 3435.76, "duration": 3.839}, {"text": "to", "start": 3438.319, "duration": 2.881}, {"text": "take any questions i know we're at the", "start": 3439.599, "duration": 3.841}, {"text": "top of the hour but i can stick around", "start": 3441.2, "duration": 4.24}, {"text": "for just a few minutes for those of you", "start": 3443.44, "duration": 5.32}, {"text": "who are interested", "start": 3445.44, "duration": 3.32}]