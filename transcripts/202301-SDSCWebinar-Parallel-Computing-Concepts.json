[{"text": "I'd like to introduce", "start": 0.0, "duration": 4.98}, {"text": "um Bob sinkovitz who is our director of", "start": 2.399, "duration": 4.621}, {"text": "Education and Training and has a huge", "start": 4.98, "duration": 5.42}, {"text": "impressive background in all things HPC", "start": 7.02, "duration": 6.84}, {"text": "and uh today he'll talk about his view", "start": 10.4, "duration": 5.92}, {"text": "of HPC by talking about parallel", "start": 13.86, "duration": 5.28}, {"text": "Computing Concepts which I'm sure you'll", "start": 16.32, "duration": 6.9}, {"text": "find valuable Bob turn it over to you", "start": 19.14, "duration": 6.6}, {"text": "great then thank you so much Mary", "start": 23.22, "duration": 4.799}, {"text": "let me just get my", "start": 25.74, "duration": 5.66}, {"text": "slides shared", "start": 28.019, "duration": 3.381}, {"text": "and I see your talk", "start": 32.099, "duration": 6.421}, {"text": "all right look good looks good to me yes", "start": 35.579, "duration": 6.241}, {"text": "can everybody else see the talk", "start": 38.52, "duration": 6.32}, {"text": "and dear Bob", "start": 41.82, "duration": 3.02}, {"text": "yes all right great let's see thank you", "start": 45.48, "duration": 4.919}, {"text": "I will check things off um thank you so", "start": 48.0, "duration": 4.379}, {"text": "much for joining us today for the for", "start": 50.399, "duration": 4.34}, {"text": "the first expense webinar of the year", "start": 52.379, "duration": 4.741}, {"text": "that this is actually one of my favorite", "start": 54.739, "duration": 4.181}, {"text": "topics then in high performance", "start": 57.12, "duration": 3.36}, {"text": "Computing", "start": 58.92, "duration": 3.779}, {"text": "um it's a very very introductory level", "start": 60.48, "duration": 4.499}, {"text": "level talk but I'm hoping that you'll", "start": 62.699, "duration": 4.681}, {"text": "you'll still find it interesting", "start": 64.979, "duration": 5.281}, {"text": "um and the topic is parallel Computing", "start": 67.38, "duration": 6.3}, {"text": "Concepts and I have a GitHub repo that", "start": 70.26, "duration": 5.1}, {"text": "I'm going to be sharing with with the", "start": 73.68, "duration": 2.88}, {"text": "group at the end", "start": 75.36, "duration": 2.939}, {"text": "um with a few exercises that you can", "start": 76.56, "duration": 4.58}, {"text": "work through on your own", "start": 78.299, "duration": 2.841}, {"text": "so a quick overview of what I'm going to", "start": 81.96, "duration": 3.9}, {"text": "be talking about today I'll start off", "start": 84.479, "duration": 2.941}, {"text": "with an introduction", "start": 85.86, "duration": 4.82}, {"text": "and then I'll get into processes threads", "start": 87.42, "duration": 6.239}, {"text": "MPI and openmp", "start": 90.68, "duration": 4.72}, {"text": "we'll then talk a little bit about power", "start": 93.659, "duration": 3.721}, {"text": "about sorry about hybrid parallel", "start": 95.4, "duration": 3.24}, {"text": "applications", "start": 97.38, "duration": 4.5}, {"text": "and then amdahl's law other limits on", "start": 98.64, "duration": 5.88}, {"text": "the scalability of parallel applications", "start": 101.88, "duration": 4.919}, {"text": "how to run parallel application then", "start": 104.52, "duration": 4.26}, {"text": "complete scaling studies and finally", "start": 106.799, "duration": 4.201}, {"text": "where to go next and some conclusions a", "start": 108.78, "duration": 5.18}, {"text": "little wrap up from the top", "start": 111.0, "duration": 2.96}, {"text": "um so this session is intended for", "start": 118.02, "duration": 3.959}, {"text": "anybody who", "start": 120.119, "duration": 4.201}, {"text": "currently run plans to run or is", "start": 121.979, "duration": 4.32}, {"text": "thinking about running applications on", "start": 124.32, "duration": 4.939}, {"text": "Parallel computers", "start": 126.299, "duration": 2.96}, {"text": "if you write proposals for computer time", "start": 129.36, "duration": 4.8}, {"text": "on campus clusters nationally allocated", "start": 131.879, "duration": 5.281}, {"text": "systems or other resources", "start": 134.16, "duration": 5.34}, {"text": "if you purchase time on compute", "start": 137.16, "duration": 4.26}, {"text": "resources say go into the cloud and you", "start": 139.5, "duration": 3.959}, {"text": "want to maximize your return on", "start": 141.42, "duration": 4.2}, {"text": "investment", "start": 143.459, "duration": 3.901}, {"text": "if you're thinking about purchasing", "start": 145.62, "duration": 3.6}, {"text": "hardware for your lab", "start": 147.36, "duration": 4.019}, {"text": "and finally if you're just curious about", "start": 149.22, "duration": 5.239}, {"text": "parallel computing", "start": 151.379, "duration": 3.08}, {"text": "so a little bit of motivation for the", "start": 154.8, "duration": 4.14}, {"text": "talk is you know over the years we've", "start": 156.42, "duration": 4.38}, {"text": "done lots and lots of training here at", "start": 158.94, "duration": 6.06}, {"text": "the super computer center but here and I", "start": 160.8, "duration": 5.82}, {"text": "think we see the same pattern a lot of", "start": 165.0, "duration": 3.239}, {"text": "the other academics the Computing Center", "start": 166.62, "duration": 4.02}, {"text": "is that a lot of the training in", "start": 168.239, "duration": 4.681}, {"text": "parallel Computing is really targeted", "start": 170.64, "duration": 4.92}, {"text": "people who are programmers who are right", "start": 172.92, "duration": 4.92}, {"text": "in their own parallel application and", "start": 175.56, "duration": 5.759}, {"text": "the topics tend to be heavy on things", "start": 177.84, "duration": 7.38}, {"text": "like MPI and openmp Cuda profiling", "start": 181.319, "duration": 6.42}, {"text": "application Performance Tuning", "start": 185.22, "duration": 5.58}, {"text": "all very very important topics but again", "start": 187.739, "duration": 5.401}, {"text": "if you are not writing your own code if", "start": 190.8, "duration": 3.96}, {"text": "you're more of an end user of super", "start": 193.14, "duration": 3.42}, {"text": "computers you really don't need these", "start": 194.76, "duration": 4.16}, {"text": "topics", "start": 196.56, "duration": 2.36}, {"text": "um but as consequence if you're an end", "start": 200.04, "duration": 3.96}, {"text": "user if you're using somebody else's", "start": 201.84, "duration": 5.3}, {"text": "climate and weather or chemistry or", "start": 204.0, "duration": 5.879}, {"text": "phylogenetics or Material Science code", "start": 207.14, "duration": 5.98}, {"text": "you may not get a proper introduction to", "start": 209.879, "duration": 5.761}, {"text": "parallel computing", "start": 213.12, "duration": 5.28}, {"text": "so even if you don't write code it's", "start": 215.64, "duration": 4.739}, {"text": "still important that you understand at", "start": 218.4, "duration": 3.54}, {"text": "least some of the basic principles of", "start": 220.379, "duration": 3.72}, {"text": "parallel Computing so that you make the", "start": 221.94, "duration": 4.019}, {"text": "most effective use of this valuable", "start": 224.099, "duration": 5.121}, {"text": "Advance infrastructure", "start": 225.959, "duration": 3.261}, {"text": "so I want to dispel a few of the myths", "start": 230.4, "duration": 5.52}, {"text": "the surrounding parallel Computing", "start": 233.519, "duration": 4.381}, {"text": "um and the first one is that parallel", "start": 235.92, "duration": 3.599}, {"text": "Computing is for astrophysicists", "start": 237.9, "duration": 4.44}, {"text": "Engineers climate modelers and others", "start": 239.519, "duration": 4.8}, {"text": "who are working in traditionally math", "start": 242.34, "duration": 4.14}, {"text": "intensive fields", "start": 244.319, "duration": 4.5}, {"text": "and while this might have been partially", "start": 246.48, "duration": 5.16}, {"text": "true decades ago today nearly every", "start": 248.819, "duration": 4.441}, {"text": "field of research makes the use of", "start": 251.64, "duration": 4.379}, {"text": "parallel computing and this includes the", "start": 253.26, "duration": 5.099}, {"text": "social sciences Life Sciences Arts and", "start": 256.019, "duration": 4.5}, {"text": "Humanities in addition to physics", "start": 258.359, "duration": 4.141}, {"text": "chemistry engineering Material Science", "start": 260.519, "duration": 4.501}, {"text": "and the other domains that we refer to", "start": 262.5, "duration": 5.16}, {"text": "as the usual suspect", "start": 265.02, "duration": 4.8}, {"text": "so to share a story from about 10 years", "start": 267.66, "duration": 5.4}, {"text": "ago I was leading a tour of our data", "start": 269.82, "duration": 4.08}, {"text": "center", "start": 273.06, "duration": 3.419}, {"text": "and one of the people on the tour was um", "start": 273.9, "duration": 3.72}, {"text": "what with the political science", "start": 276.479, "duration": 4.381}, {"text": "department here at UC San Diego", "start": 277.62, "duration": 5.639}, {"text": "and he was amazed with the tour looking", "start": 280.86, "duration": 4.44}, {"text": "all the hardware and he said boy I wish", "start": 283.259, "duration": 4.021}, {"text": "I could have access to this", "start": 285.3, "duration": 3.78}, {"text": "and we realized that was a little bit of", "start": 287.28, "duration": 3.96}, {"text": "a failing in our part to really", "start": 289.08, "duration": 4.619}, {"text": "communicate that these are widely", "start": 291.24, "duration": 4.08}, {"text": "available resources and that they're", "start": 293.699, "duration": 4.701}, {"text": "available to everybody", "start": 295.32, "duration": 3.08}, {"text": "okay another myth that we have of", "start": 299.28, "duration": 3.6}, {"text": "parallel Computing is that throwing more", "start": 300.96, "duration": 3.66}, {"text": "Hardware a problem will automatically", "start": 302.88, "duration": 4.92}, {"text": "reduce the time to solution", "start": 304.62, "duration": 5.22}, {"text": "so parallel Computing is only going to", "start": 307.8, "duration": 3.899}, {"text": "help if you already have an application", "start": 309.84, "duration": 4.139}, {"text": "that has been written to take advantage", "start": 311.699, "duration": 4.801}, {"text": "of parallel Hardware we've had cases", "start": 313.979, "duration": 4.381}, {"text": "here in the past when somebody had a", "start": 316.5, "duration": 4.139}, {"text": "Serial application and they tried", "start": 318.36, "duration": 3.96}, {"text": "running it in parallel on a", "start": 320.639, "duration": 3.0}, {"text": "supercomputer", "start": 322.32, "duration": 4.26}, {"text": "what they ended up doing was running the", "start": 323.639, "duration": 5.701}, {"text": "same code the same problem the same data", "start": 326.58, "duration": 5.1}, {"text": "multiple times essentially they were", "start": 329.34, "duration": 4.44}, {"text": "repeating their work rather than rather", "start": 331.68, "duration": 4.739}, {"text": "than running in parallel so if you do", "start": 333.78, "duration": 4.68}, {"text": "want to use parallel Computing you do", "start": 336.419, "duration": 4.201}, {"text": "have to have a code that's been", "start": 338.46, "duration": 4.38}, {"text": "um that that's been paralyzed", "start": 340.62, "duration": 4.44}, {"text": "and then finally even if you do have a", "start": 342.84, "duration": 4.079}, {"text": "parallel cone there's going to be an", "start": 345.06, "duration": 3.9}, {"text": "inherent limit on scalability and we'll", "start": 346.919, "duration": 3.361}, {"text": "be talking about that a little bit later", "start": 348.96, "duration": 3.86}, {"text": "in the talk", "start": 350.28, "duration": 2.54}, {"text": "okay one caveat though about um about", "start": 358.56, "duration": 5.1}, {"text": "thrown more Hardware at the at your", "start": 361.44, "duration": 4.199}, {"text": "problem is that there's things called", "start": 363.66, "duration": 4.62}, {"text": "high throughput Computing workloads that", "start": 365.639, "duration": 4.861}, {"text": "can use parallel Computing to run many", "start": 368.28, "duration": 4.56}, {"text": "single core GPU instances of an", "start": 370.5, "duration": 4.44}, {"text": "application so that you achieve near", "start": 372.84, "duration": 4.799}, {"text": "perfect scaling this is a this is more", "start": 374.94, "duration": 4.8}, {"text": "of a special case imagine that you're", "start": 377.639, "duration": 3.06}, {"text": "doing", "start": 379.74, "duration": 2.1}, {"text": "um", "start": 380.699, "duration": 4.161}, {"text": "say many many many similar calculations", "start": 381.84, "duration": 5.22}, {"text": "except each with a different set of", "start": 384.86, "duration": 4.059}, {"text": "inputs so you're doing a parameter study", "start": 387.06, "duration": 3.54}, {"text": "and this is a case where you really can", "start": 388.919, "duration": 3.541}, {"text": "scale", "start": 390.6, "duration": 3.96}, {"text": "um you can scale up very very far as", "start": 392.46, "duration": 3.72}, {"text": "long as you have enough work as long as", "start": 394.56, "duration": 3.12}, {"text": "there are enough individual problems to", "start": 396.18, "duration": 3.78}, {"text": "be solved", "start": 397.68, "duration": 4.68}, {"text": "and then finally and the the most", "start": 399.96, "duration": 3.6}, {"text": "important", "start": 402.36, "duration": 2.94}, {"text": "I'll admit that I want to expel for this", "start": 403.56, "duration": 4.02}, {"text": "group is that you need to be a", "start": 405.3, "duration": 4.32}, {"text": "programmer or a software developer to", "start": 407.58, "duration": 4.32}, {"text": "make use of parallel computing", "start": 409.62, "duration": 5.28}, {"text": "so most uses of parallel computers are", "start": 411.9, "duration": 5.22}, {"text": "not programmed this is different than", "start": 414.9, "duration": 4.44}, {"text": "the situation say 20 or 30 years ago", "start": 417.12, "duration": 4.199}, {"text": "where if you're doing parallel Computing", "start": 419.34, "duration": 3.54}, {"text": "you've pretty much had to write your own", "start": 421.319, "duration": 3.961}, {"text": "but now in many many areas there's", "start": 422.88, "duration": 4.8}, {"text": "mature third-party software out there", "start": 425.28, "duration": 4.56}, {"text": "that's been developed elsewhere and then", "start": 427.68, "duration": 5.42}, {"text": "made available to the community", "start": 429.84, "duration": 3.26}, {"text": "so many of you in fact most of our users", "start": 433.38, "duration": 6.06}, {"text": "are now running somebody else's code so", "start": 436.02, "duration": 5.579}, {"text": "if you're doing say weather modeling you", "start": 439.44, "duration": 4.68}, {"text": "might use more if you're doing molecular", "start": 441.599, "duration": 4.681}, {"text": "Dynamic stimulations of biological", "start": 444.12, "duration": 5.519}, {"text": "systems you may use Amber or bromax or", "start": 446.28, "duration": 5.699}, {"text": "or namdi or other packages that are out", "start": 449.639, "duration": 5.34}, {"text": "there if you're doing phylogenetics this", "start": 451.979, "duration": 5.481}, {"text": "is the construction of", "start": 454.979, "duration": 4.141}, {"text": "phylogenetic trees showing the", "start": 457.46, "duration": 3.88}, {"text": "relationship between organisms there's a", "start": 459.12, "duration": 3.9}, {"text": "whole host of software out there that we", "start": 461.34, "duration": 3.72}, {"text": "make available through a interface", "start": 463.02, "duration": 2.94}, {"text": "called", "start": 465.06, "duration": 3.539}, {"text": "we'll call it Cypress science Gateway if", "start": 465.96, "duration": 4.26}, {"text": "you're doing electronic structure", "start": 468.599, "duration": 5.16}, {"text": "calculations you might use cp2k vast and", "start": 470.22, "duration": 6.479}, {"text": "another software so overall most of you", "start": 473.759, "duration": 4.261}, {"text": "are probably going to be using somebody", "start": 476.699, "duration": 4.041}, {"text": "else's code", "start": 478.02, "duration": 2.72}, {"text": "now parallel computers I'm sure a lot of", "start": 481.56, "duration": 3.9}, {"text": "you are familiar with this already but", "start": 483.72, "duration": 3.66}, {"text": "modern clusters and parallel computers", "start": 485.46, "duration": 3.9}, {"text": "consist of multiple compute nodes", "start": 487.38, "duration": 4.68}, {"text": "connected by a fast Network now the", "start": 489.36, "duration": 5.04}, {"text": "hardware that goes into these nodes is", "start": 492.06, "duration": 4.38}, {"text": "probably familiar to you already it's", "start": 494.4, "duration": 4.68}, {"text": "the it's the CPUs the gpus that you may", "start": 496.44, "duration": 5.46}, {"text": "have in your desktop or laptop system", "start": 499.08, "duration": 5.16}, {"text": "typically each of these CPU nodes is", "start": 501.9, "duration": 4.5}, {"text": "going to contain one or more typically", "start": 504.24, "duration": 5.22}, {"text": "two multi-core processes and the GPU", "start": 506.4, "duration": 5.22}, {"text": "nodes will usually contain four", "start": 509.46, "duration": 4.319}, {"text": "and in order to use this Hardware we", "start": 511.62, "duration": 3.599}, {"text": "need applications that have been", "start": 513.779, "duration": 3.421}, {"text": "paralyzed so they can run in multiple", "start": 515.219, "duration": 4.081}, {"text": "cores or multiple gpus", "start": 517.2, "duration": 4.92}, {"text": "and we're interested both in running", "start": 519.3, "duration": 5.46}, {"text": "across multiple cores or GPS within a", "start": 522.12, "duration": 5.64}, {"text": "node and across multiple node so and", "start": 524.76, "duration": 4.32}, {"text": "what I'm showing here on the right is", "start": 527.76, "duration": 3.84}, {"text": "just a diagram high level diagram of", "start": 529.08, "duration": 4.68}, {"text": "expands our current supercomputer if", "start": 531.6, "duration": 3.78}, {"text": "you're showing the overall organization", "start": 533.76, "duration": 5.6}, {"text": "into nodes racks and so on", "start": 535.38, "duration": 3.98}, {"text": "okay so I'm going to soak it into a", "start": 542.16, "duration": 3.299}, {"text": "little bit of a more technical content", "start": 544.019, "duration": 3.481}, {"text": "we're going to shift gear we're going to", "start": 545.459, "duration": 5.341}, {"text": "talk about processing thread MPI and", "start": 547.5, "duration": 6.08}, {"text": "openmp", "start": 550.8, "duration": 2.78}, {"text": "so you've probably heard about", "start": 554.16, "duration": 3.84}, {"text": "processing the threads before this is um", "start": 555.3, "duration": 5.58}, {"text": "often a fuzzy concept for most folks who", "start": 558.0, "duration": 3.899}, {"text": "aren't", "start": 560.88, "duration": 2.76}, {"text": "um deep into parallel Computing or", "start": 561.899, "duration": 4.341}, {"text": "computer science so these are both", "start": 563.64, "duration": 5.759}, {"text": "independent sequences of execution", "start": 566.24, "duration": 5.74}, {"text": "you could think of a process as an", "start": 569.399, "duration": 5.341}, {"text": "instance of a program with access to its", "start": 571.98, "duration": 5.82}, {"text": "own memory State and file descriptor so", "start": 574.74, "duration": 4.98}, {"text": "when we are running an MPI application", "start": 577.8, "duration": 3.3}, {"text": "which will be talking about in a few", "start": 579.72, "duration": 2.22}, {"text": "minutes", "start": 581.1, "duration": 3.0}, {"text": "you could think of this as being many", "start": 581.94, "duration": 4.98}, {"text": "independent copies of that program", "start": 584.1, "duration": 4.44}, {"text": "running on their own and then", "start": 586.92, "duration": 4.5}, {"text": "occasionally communicating", "start": 588.54, "duration": 4.56}, {"text": "threads on the other hand are very", "start": 591.42, "duration": 3.72}, {"text": "lightweight entities that execute inside", "start": 593.1, "duration": 3.299}, {"text": "a process", "start": 595.14, "duration": 3.3}, {"text": "so every process is going to have at", "start": 596.399, "duration": 4.261}, {"text": "least one thread and then those threads", "start": 598.44, "duration": 4.98}, {"text": "within a process can access memory and", "start": 600.66, "duration": 4.38}, {"text": "we're going to find that that threads", "start": 603.42, "duration": 3.359}, {"text": "and processes are both very very", "start": 605.04, "duration": 4.44}, {"text": "powerful when we're trying to um run a", "start": 606.779, "duration": 4.861}, {"text": "parallel application", "start": 609.48, "duration": 3.96}, {"text": "so if you want to go a little bit deeper", "start": 611.64, "duration": 4.56}, {"text": "into threads and processes I found a few", "start": 613.44, "duration": 5.1}, {"text": "online resources describing the", "start": 616.2, "duration": 4.38}, {"text": "difference and when would you when you", "start": 618.54, "duration": 4.2}, {"text": "would use one or the other", "start": 620.58, "duration": 4.86}, {"text": "um to two that I particularly like is", "start": 622.74, "duration": 6.12}, {"text": "one hair from from stack overflow", "start": 625.44, "duration": 5.519}, {"text": "um that really discusses in depth the", "start": 628.86, "duration": 3.72}, {"text": "difference between a threat and a", "start": 630.959, "duration": 3.721}, {"text": "process I'll warn you this is stack", "start": 632.58, "duration": 4.56}, {"text": "Overflow so the description can be a", "start": 634.68, "duration": 5.279}, {"text": "little bit nerdier maybe aimed more to", "start": 637.14, "duration": 4.5}, {"text": "computer science Community but what's", "start": 639.959, "duration": 3.361}, {"text": "still pretty useful", "start": 641.64, "duration": 4.56}, {"text": "and then the the second link", "start": 643.32, "duration": 4.92}, {"text": "um it gives a much more informal", "start": 646.2, "duration": 6.259}, {"text": "description of the process and threads", "start": 648.24, "duration": 4.219}, {"text": "so comparing a little bit more comparing", "start": 653.339, "duration": 4.981}, {"text": "processes and thread processes a", "start": 655.56, "duration": 5.04}, {"text": "downside of a process is that it occurs", "start": 658.32, "duration": 3.9}, {"text": "more overhead", "start": 660.6, "duration": 3.299}, {"text": "but on the other hand they're more", "start": 662.22, "duration": 4.14}, {"text": "flexible multiple processes can be run", "start": 663.899, "duration": 4.38}, {"text": "within a compute node or more", "start": 666.36, "duration": 4.14}, {"text": "importantly you can run multiple", "start": 668.279, "duration": 5.701}, {"text": "processes processes across multiple", "start": 670.5, "duration": 5.459}, {"text": "compute nodes what we call distributed", "start": 673.98, "duration": 4.32}, {"text": "memory parallel programming", "start": 675.959, "duration": 4.681}, {"text": "if you really want to take advantage of", "start": 678.3, "duration": 5.64}, {"text": "the full power of a of a large", "start": 680.64, "duration": 5.58}, {"text": "supercomputer you are going to you you", "start": 683.94, "duration": 4.56}, {"text": "will need to use processes and some sort", "start": 686.22, "duration": 4.92}, {"text": "of parallelization scheme such as MPI", "start": 688.5, "duration": 4.74}, {"text": "now threads I mentioned earlier they", "start": 691.14, "duration": 5.22}, {"text": "incur less overhead when we use threaded", "start": 693.24, "duration": 6.0}, {"text": "codes we can use less memory essential", "start": 696.36, "duration": 5.099}, {"text": "threads within a process can have access", "start": 699.24, "duration": 4.32}, {"text": "to the same data structure", "start": 701.459, "duration": 6.0}, {"text": "now from a programming point of view", "start": 703.56, "duration": 5.76}, {"text": "um", "start": 707.459, "duration": 4.5}, {"text": "red level programming can be a little", "start": 709.32, "duration": 4.44}, {"text": "more complicated or sorry not more", "start": 711.959, "duration": 3.481}, {"text": "complicated but there there's some more", "start": 713.76, "duration": 4.86}, {"text": "more nuances that that could um that", "start": 715.44, "duration": 5.22}, {"text": "could result in incorrect code but", "start": 718.62, "duration": 3.6}, {"text": "that's really a burden on the part of", "start": 720.66, "duration": 3.119}, {"text": "the developer", "start": 722.22, "duration": 3.78}, {"text": "um the big downside of threads though is", "start": 723.779, "duration": 3.841}, {"text": "that they're less flexible than", "start": 726.0, "duration": 4.74}, {"text": "processes so multiple threads within a", "start": 727.62, "duration": 5.76}, {"text": "process can only be run within a compute", "start": 730.74, "duration": 6.2}, {"text": "node within a shared memory node", "start": 733.38, "duration": 3.56}, {"text": "and let me just check the chat real", "start": 738.72, "duration": 3.84}, {"text": "quick see if there are any questions in", "start": 740.94, "duration": 3.74}, {"text": "there", "start": 742.56, "duration": 2.12}, {"text": "okay oh um question here for from Joni", "start": 747.66, "duration": 5.58}, {"text": "Powell how about books on the issue", "start": 750.18, "duration": 4.8}, {"text": "um Johnny I will follow up with that", "start": 753.24, "duration": 4.02}, {"text": "later I don't have any resources off the", "start": 754.98, "duration": 3.479}, {"text": "top of my head but I'll do a little bit", "start": 757.26, "duration": 3.0}, {"text": "of research of that after after I'm", "start": 758.459, "duration": 3.961}, {"text": "going with my talk", "start": 760.26, "duration": 3.72}, {"text": "okay so a little bit more about", "start": 762.42, "duration": 3.78}, {"text": "processing the threads and why do I care", "start": 763.98, "duration": 3.96}, {"text": "about the distinction", "start": 766.2, "duration": 3.6}, {"text": "so the type of parallelization will", "start": 767.94, "duration": 3.959}, {"text": "determine how and where you can run your", "start": 769.8, "duration": 4.2}, {"text": "code so the distributed memory", "start": 771.899, "duration": 4.62}, {"text": "applications again multiple processes", "start": 774.0, "duration": 4.98}, {"text": "multiple instances of a program this can", "start": 776.519, "duration": 5.401}, {"text": "be run on one or more nodes the shared", "start": 778.98, "duration": 5.4}, {"text": "memory application have to be run within", "start": 781.92, "duration": 4.8}, {"text": "a single node now we also have what we", "start": 784.38, "duration": 4.38}, {"text": "call hybrid applications that use a", "start": 786.72, "duration": 5.46}, {"text": "combination of threads and processes", "start": 788.76, "duration": 5.579}, {"text": "they can be run on one or more nodes but", "start": 792.18, "duration": 3.779}, {"text": "we need to consider the balance between", "start": 794.339, "duration": 3.841}, {"text": "threads and process", "start": 795.959, "duration": 4.5}, {"text": "and then finally in all cases we need to", "start": 798.18, "duration": 4.32}, {"text": "be aware how crosses and threads are", "start": 800.459, "duration": 3.901}, {"text": "mapped in Bound to cores", "start": 802.5, "duration": 4.079}, {"text": "so in addition to being so in addition", "start": 804.36, "duration": 4.08}, {"text": "to all that being aware of threads and", "start": 806.579, "duration": 3.601}, {"text": "processes this isn't going to help you", "start": 808.44, "duration": 3.78}, {"text": "understand how your code is utilizing", "start": 810.18, "duration": 3.959}, {"text": "the hardware and identify some common", "start": 812.22, "duration": 4.46}, {"text": "problems", "start": 814.139, "duration": 2.541}, {"text": "so I know that this is not a um not a", "start": 819.0, "duration": 5.639}, {"text": "presentation for programmers this is for", "start": 822.779, "duration": 3.961}, {"text": "for end use as a super computers but I", "start": 824.639, "duration": 4.081}, {"text": "do want to talk a little bit about MPI", "start": 826.74, "duration": 5.399}, {"text": "the message passing interface and openmp", "start": 828.72, "duration": 7.14}, {"text": "so MPI is a standard for paralyzing C C", "start": 832.139, "duration": 6.361}, {"text": "plus plus and Fortran code so they can", "start": 835.86, "duration": 5.52}, {"text": "run under on a distributed memory", "start": 838.5, "duration": 5.82}, {"text": "basically multiple compute nodes system", "start": 841.38, "duration": 5.519}, {"text": "so while it's not officially adopted by", "start": 844.32, "duration": 4.38}, {"text": "any of the major standard bodies it's", "start": 846.899, "duration": 4.321}, {"text": "really de facto standard for a lot of", "start": 848.7, "duration": 4.92}, {"text": "large-scale parallel Computing de facto", "start": 851.22, "duration": 4.679}, {"text": "basically means that it's um pretty much", "start": 853.62, "duration": 4.5}, {"text": "used everywhere", "start": 855.899, "duration": 4.021}, {"text": "now there's a little bit of confusion", "start": 858.12, "duration": 5.04}, {"text": "once we start talking about openmpi and", "start": 859.92, "duration": 5.219}, {"text": "vapage and mpitch", "start": 863.16, "duration": 5.04}, {"text": "these are just multiple open source", "start": 865.139, "duration": 6.121}, {"text": "implementations of MPI so basically MPI", "start": 868.2, "duration": 5.699}, {"text": "is a standard open MPI and that page and", "start": 871.26, "duration": 5.16}, {"text": "pitch and these other groups along with", "start": 873.899, "duration": 4.861}, {"text": "vendor support versions will Implement", "start": 876.42, "duration": 4.26}, {"text": "MPI", "start": 878.76, "duration": 3.78}, {"text": "when you build when you build a parallel", "start": 880.68, "duration": 3.12}, {"text": "application", "start": 882.54, "duration": 3.84}, {"text": "that's based the database and MP", "start": 883.8, "duration": 5.58}, {"text": "you could select which MPI Library", "start": 886.38, "duration": 4.44}, {"text": "you're going to use", "start": 889.38, "duration": 3.54}, {"text": "now MPI application types that already", "start": 890.82, "duration": 5.16}, {"text": "can be run within a shared memory node", "start": 892.92, "duration": 6.12}, {"text": "and when they do that all of the major", "start": 895.98, "duration": 5.64}, {"text": "MPI implementations are optimized to", "start": 899.04, "duration": 4.979}, {"text": "take advantage of that faster internal", "start": 901.62, "duration": 3.659}, {"text": "communication", "start": 904.019, "duration": 3.781}, {"text": "MPI is portable it can be used pretty", "start": 905.279, "duration": 3.841}, {"text": "much anywhere", "start": 907.8, "duration": 3.659}, {"text": "and although MPI synonymous with", "start": 909.12, "duration": 4.68}, {"text": "distributed memory parallelization there", "start": 911.459, "duration": 4.44}, {"text": "are other options out there that are", "start": 913.8, "duration": 4.08}, {"text": "gaining some adoption like charm plus", "start": 915.899, "duration": 5.461}, {"text": "plus which is used in the um namdi", "start": 917.88, "duration": 5.48}, {"text": "molecular Dynamics package", "start": 921.36, "duration": 6.2}, {"text": "UPC and X10", "start": 923.36, "duration": 4.2}, {"text": "and um just a little by diversion for", "start": 929.82, "duration": 4.199}, {"text": "those of you who are doing who are doing", "start": 932.22, "duration": 3.479}, {"text": "deep learning so although we're", "start": 934.019, "duration": 3.841}, {"text": "discussing MPI the same principles are", "start": 935.699, "duration": 3.661}, {"text": "going to apply when you're paralyzed in", "start": 937.86, "duration": 3.779}, {"text": "deep learning applications say using the", "start": 939.36, "duration": 4.62}, {"text": "horovod which is a distributed deep", "start": 941.639, "duration": 4.5}, {"text": "learning training framework that could", "start": 943.98, "duration": 4.44}, {"text": "be used with tensorflow carers pie torch", "start": 946.139, "duration": 4.44}, {"text": "and Apache and x-net", "start": 948.42, "duration": 4.2}, {"text": "um and nickel which is the Nvidia", "start": 950.579, "duration": 4.5}, {"text": "Collective communication Library which", "start": 952.62, "duration": 5.159}, {"text": "implements multiple GPU and multi-node", "start": 955.079, "duration": 4.56}, {"text": "communication Primitives that are", "start": 957.779, "duration": 5.661}, {"text": "optimized for the Nvidia GPU", "start": 959.639, "duration": 3.801}, {"text": "foreign", "start": 963.48, "duration": 5.7}, {"text": "quick Lance at an MPI application we'll", "start": 965.839, "duration": 6.101}, {"text": "start with a really simple code snippet", "start": 969.18, "duration": 4.86}, {"text": "again if you're not a programmer you", "start": 971.94, "duration": 3.78}, {"text": "don't need to worry about the details I", "start": 974.04, "duration": 3.12}, {"text": "just wanted to give you a chance to see", "start": 975.72, "duration": 3.66}, {"text": "what MPI looks like", "start": 977.16, "duration": 5.28}, {"text": "so this is a um you know if you've ever", "start": 979.38, "duration": 5.22}, {"text": "taken a c or C plus plus programming", "start": 982.44, "duration": 3.66}, {"text": "class you probably started off with", "start": 984.6, "duration": 4.38}, {"text": "hello world I have a main program it", "start": 986.1, "duration": 4.62}, {"text": "prints out it prints the hello world", "start": 988.98, "duration": 3.479}, {"text": "message and then it and then it", "start": 990.72, "duration": 3.239}, {"text": "terminates", "start": 992.459, "duration": 4.141}, {"text": "so NPI applications they're pretty dense", "start": 993.959, "duration": 5.041}, {"text": "they're written at a low level if you're", "start": 996.6, "duration": 4.44}, {"text": "a c programmer", "start": 999.0, "duration": 4.139}, {"text": "MPI will look", "start": 1001.04, "duration": 3.359}, {"text": "um", "start": 1003.139, "duration": 3.181}, {"text": "you guys don't say it would look not too", "start": 1004.399, "duration": 2.94}, {"text": "bad", "start": 1006.32, "duration": 2.34}, {"text": "um but if you've been working with", "start": 1007.339, "duration": 3.3}, {"text": "higher level languages such as such as", "start": 1008.66, "duration": 4.44}, {"text": "python or Java it may seem a little", "start": 1010.639, "duration": 3.541}, {"text": "abstract", "start": 1013.1, "duration": 4.2}, {"text": "NPI is basically used to explicitly", "start": 1014.18, "duration": 6.0}, {"text": "communicate data between processes using", "start": 1017.3, "duration": 5.459}, {"text": "calls in the MPI Library routine", "start": 1020.18, "duration": 4.32}, {"text": "so if you're not a programmer an", "start": 1022.759, "duration": 4.021}, {"text": "application developer you do not need to", "start": 1024.5, "duration": 5.28}, {"text": "know MPI you just need to be aware that", "start": 1026.78, "duration": 4.679}, {"text": "it exists and that if you're building", "start": 1029.78, "duration": 3.84}, {"text": "your executable you'll need to use the", "start": 1031.459, "duration": 4.941}, {"text": "appropriate reference", "start": 1033.62, "duration": 2.78}, {"text": "so to show you what a MPI version that", "start": 1037.16, "duration": 4.259}, {"text": "code looks like", "start": 1039.98, "duration": 4.5}, {"text": "in this case we we've added", "start": 1041.419, "duration": 5.04}, {"text": "um we've added the code and the", "start": 1044.48, "duration": 4.439}, {"text": "directives that we'll need in order to", "start": 1046.459, "duration": 5.821}, {"text": "run hello world in parallel so instead", "start": 1048.919, "duration": 4.981}, {"text": "of just simply Pronounce hello world", "start": 1052.28, "duration": 3.24}, {"text": "we're going to have", "start": 1053.9, "duration": 4.44}, {"text": "each process", "start": 1055.52, "duration": 5.399}, {"text": "right hello world and it's going to tell", "start": 1058.34, "duration": 5.04}, {"text": "us which process number it is out of how", "start": 1060.919, "duration": 3.841}, {"text": "many", "start": 1063.38, "duration": 3.659}, {"text": "so that this is uh", "start": 1064.76, "duration": 3.48}, {"text": "um this might be a little bit of an", "start": 1067.039, "duration": 5.52}, {"text": "extreme case and the ratio of MPI to um", "start": 1068.24, "duration": 6.72}, {"text": "to the original code because we had to", "start": 1072.559, "duration": 4.74}, {"text": "implement all of the initialization", "start": 1074.96, "duration": 4.38}, {"text": "routine we had to create the MPI", "start": 1077.299, "duration": 4.62}, {"text": "environment get the number of processes", "start": 1079.34, "duration": 5.28}, {"text": "get the rank of the process get the name", "start": 1081.919, "duration": 5.88}, {"text": "of each processor and and so on", "start": 1084.62, "duration": 5.58}, {"text": "um but every MPI application is going to", "start": 1087.799, "duration": 4.141}, {"text": "has them in this code again if you're", "start": 1090.2, "duration": 4.08}, {"text": "not a programmer you don't need to you", "start": 1091.94, "duration": 4.08}, {"text": "don't need to know about the details of", "start": 1094.28, "duration": 3.48}, {"text": "NPI you just need to know that it is", "start": 1096.02, "duration": 3.2}, {"text": "this", "start": 1097.76, "duration": 4.86}, {"text": "now openmp is an application programming", "start": 1099.22, "duration": 5.5}, {"text": "interface or API for shared memory", "start": 1102.62, "duration": 3.72}, {"text": "programming", "start": 1104.72, "duration": 5.16}, {"text": "and it's also available for C C plus", "start": 1106.34, "duration": 6.78}, {"text": "plus and Fortran so openmp gives you a", "start": 1109.88, "duration": 4.74}, {"text": "collection of compiler directives", "start": 1113.12, "duration": 3.24}, {"text": "Library retains and environment", "start": 1114.62, "duration": 4.5}, {"text": "variables it is supported by all of the", "start": 1116.36, "duration": 6.179}, {"text": "major compilers including IBM Intel GCC", "start": 1119.12, "duration": 8.1}, {"text": "PGI and the AMD optimize name compiler", "start": 1122.539, "duration": 6.961}, {"text": "collection or aocc", "start": 1127.22, "duration": 5.16}, {"text": "it's portable it can be used anywhere", "start": 1129.5, "duration": 5.46}, {"text": "and they'll although openmp is often", "start": 1132.38, "duration": 4.08}, {"text": "synonymous with shared memory", "start": 1134.96, "duration": 3.719}, {"text": "parallelization there are other options", "start": 1136.46, "duration": 3.42}, {"text": "out there", "start": 1138.679, "duration": 2.941}, {"text": "there's a um", "start": 1139.88, "duration": 4.32}, {"text": "a language which is a you could think of", "start": 1141.62, "duration": 5.22}, {"text": "as an extension to see called Silk that", "start": 1144.2, "duration": 4.32}, {"text": "there's posix threads which is a little", "start": 1146.84, "duration": 3.12}, {"text": "bit lower level", "start": 1148.52, "duration": 2.82}, {"text": "um and there are specialized libraries", "start": 1149.96, "duration": 5.959}, {"text": "for python R and other program languages", "start": 1151.34, "duration": 4.579}, {"text": "so like we did for for MPI where we're", "start": 1155.96, "duration": 4.44}, {"text": "not going to get into the details but I", "start": 1158.299, "duration": 3.961}, {"text": "just want to show you what an openmp", "start": 1160.4, "duration": 4.44}, {"text": "application looks like if you are not a", "start": 1162.26, "duration": 4.38}, {"text": "programmer you could you could ignore", "start": 1164.84, "duration": 3.3}, {"text": "this content", "start": 1166.64, "duration": 2.399}, {"text": "um if you have a little bit of", "start": 1168.14, "duration": 3.72}, {"text": "programming experience though this is a", "start": 1169.039, "duration": 7.14}, {"text": "c implementation of", "start": 1171.86, "duration": 7.14}, {"text": "um of a simple Loop of a couple of", "start": 1176.179, "duration": 4.921}, {"text": "simple Loops where we take two arrays A", "start": 1179.0, "duration": 4.14}, {"text": "and B and we initialize them and then we", "start": 1181.1, "duration": 3.959}, {"text": "add them element by element and store", "start": 1183.14, "duration": 4.76}, {"text": "them in and erase the", "start": 1185.059, "duration": 5.281}, {"text": "application paralyzes these in using", "start": 1187.9, "duration": 4.48}, {"text": "openmp tend to be more readable because", "start": 1190.34, "duration": 4.02}, {"text": "then they're often involved relatively", "start": 1192.38, "duration": 4.62}, {"text": "minor changes to the code again if", "start": 1194.36, "duration": 4.74}, {"text": "you're not a programmer you do not need", "start": 1197.0, "duration": 5.22}, {"text": "to need to know openmp but you do need", "start": 1199.1, "duration": 4.92}, {"text": "to be aware that if you have a code", "start": 1202.22, "duration": 4.44}, {"text": "that's been paralyzes in openmp and you", "start": 1204.02, "duration": 4.68}, {"text": "need to build your own executable that", "start": 1206.66, "duration": 3.24}, {"text": "you're going to need to use the", "start": 1208.7, "duration": 3.599}, {"text": "appropriate compiler flight and", "start": 1209.9, "duration": 4.68}, {"text": "unfortunately these vary from compiler", "start": 1212.299, "duration": 4.88}, {"text": "to compiler", "start": 1214.58, "duration": 2.599}, {"text": "the next slide we share what it looks", "start": 1217.22, "duration": 3.78}, {"text": "like when we parallel it paralyze this", "start": 1219.08, "duration": 3.3}, {"text": "code", "start": 1221.0, "duration": 3.84}, {"text": "um this is a you know really a toy", "start": 1222.38, "duration": 3.9}, {"text": "example", "start": 1224.84, "duration": 5.459}, {"text": "um at adding one 1000 I'm sorry yeah add", "start": 1226.28, "duration": 5.46}, {"text": "in one thousand", "start": 1230.299, "duration": 3.421}, {"text": "um elements of an array element by", "start": 1231.74, "duration": 3.9}, {"text": "element isn't going to take much time we", "start": 1233.72, "duration": 3.54}, {"text": "would never need to paralyze this code", "start": 1235.64, "duration": 4.08}, {"text": "but if we did and we're showing here how", "start": 1237.26, "duration": 5.88}, {"text": "we could add a few directives or in case", "start": 1239.72, "duration": 6.18}, {"text": "of C and C plus plus pragmas in order to", "start": 1243.14, "duration": 4.74}, {"text": "paralyze that loop again you don't need", "start": 1245.9, "duration": 4.139}, {"text": "to know the details you just need to be", "start": 1247.88, "duration": 5.1}, {"text": "aware if your code is paralyzed using", "start": 1250.039, "duration": 7.281}, {"text": "openmp so that you build it correctly", "start": 1252.98, "duration": 4.34}, {"text": "so the big picture MPI and openmp on the", "start": 1258.98, "duration": 5.76}, {"text": "left we talk about processes and open", "start": 1262.76, "duration": 5.7}, {"text": "MPS sorry processes and MPI so multiple", "start": 1264.74, "duration": 6.96}, {"text": "processes are managed using MPI and MPI", "start": 1268.46, "duration": 5.82}, {"text": "is implemented in libraries so if you're", "start": 1271.7, "duration": 4.8}, {"text": "building an MPI application you just", "start": 1274.28, "duration": 4.019}, {"text": "need to um", "start": 1276.5, "duration": 3.84}, {"text": "you just need to link the appropriate", "start": 1278.299, "duration": 4.62}, {"text": "library and pitch and that page openmp", "start": 1280.34, "duration": 5.16}, {"text": "or vendor implementation", "start": 1282.919, "duration": 6.12}, {"text": "friends on the other hand are our main", "start": 1285.5, "duration": 5.34}, {"text": "multiple threads are managed using", "start": 1289.039, "duration": 5.401}, {"text": "openmp and openmp is implemented by the", "start": 1290.84, "duration": 8.339}, {"text": "compiler so again GCC Intel PGI aocc in", "start": 1294.44, "duration": 6.359}, {"text": "fact over the years I've never seen a", "start": 1299.179, "duration": 3.901}, {"text": "compiler that does not support", "start": 1300.799, "duration": 5.721}, {"text": "um does not support openmp", "start": 1303.08, "duration": 3.44}, {"text": "okay so we're going to move on now to", "start": 1306.86, "duration": 4.319}, {"text": "hybrid application", "start": 1308.84, "duration": 4.74}, {"text": "so many modern parallel applications are", "start": 1311.179, "duration": 4.38}, {"text": "built using a hybrid approach to take", "start": 1313.58, "duration": 4.02}, {"text": "advantage of both distributed and shared", "start": 1315.559, "duration": 4.74}, {"text": "memory and this often involves some", "start": 1317.6, "duration": 5.4}, {"text": "combination of say MPI and openmp", "start": 1320.299, "duration": 5.341}, {"text": "although other combinations are possible", "start": 1323.0, "duration": 5.1}, {"text": "so hybrid codes have advantages over", "start": 1325.64, "duration": 4.68}, {"text": "purely shared or purely distributed", "start": 1328.1, "duration": 3.78}, {"text": "memory applications", "start": 1330.32, "duration": 4.68}, {"text": "so shared memory apps can have limited", "start": 1331.88, "duration": 5.46}, {"text": "scalability within a look within a node", "start": 1335.0, "duration": 5.94}, {"text": "and cannot be run across multiple nodes", "start": 1337.34, "duration": 5.64}, {"text": "distributed memory applications on their", "start": 1340.94, "duration": 4.44}, {"text": "hand may have higher memory requirements", "start": 1342.98, "duration": 4.62}, {"text": "and more overhead when running within a", "start": 1345.38, "duration": 5.039}, {"text": "node so we'll find especially as the", "start": 1347.6, "duration": 4.8}, {"text": "number of cores in a processor has grown", "start": 1350.419, "duration": 3.481}, {"text": "over the years that these hybrid", "start": 1352.4, "duration": 3.06}, {"text": "applications are becoming more and more", "start": 1353.9, "duration": 3.74}, {"text": "common", "start": 1355.46, "duration": 2.18}, {"text": "so we're going to look at a very", "start": 1357.799, "duration": 3.901}, {"text": "simplified parallel computer", "start": 1359.539, "duration": 5.221}, {"text": "so we're going to imagine a very very", "start": 1361.7, "duration": 5.339}, {"text": "stripped down cluster consisting of just", "start": 1364.76, "duration": 5.1}, {"text": "two nodes each of these with 16 cores", "start": 1367.039, "duration": 5.281}, {"text": "and they're joined by a network so so", "start": 1369.86, "duration": 4.559}, {"text": "the large gray box is going to be our", "start": 1372.32, "duration": 4.5}, {"text": "node those little blue boxes are going", "start": 1374.419, "duration": 4.201}, {"text": "to be the cores within a node and we", "start": 1376.82, "duration": 3.359}, {"text": "have some kind of interconnect it could", "start": 1378.62, "duration": 5.78}, {"text": "be Ethernet or infinibac", "start": 1380.179, "duration": 4.221}, {"text": "so if we have a pure message passed an", "start": 1385.1, "duration": 6.12}, {"text": "application so say just paralyze using", "start": 1388.22, "duration": 5.64}, {"text": "MPI", "start": 1391.22, "duration": 5.52}, {"text": "um we would run typically one process", "start": 1393.86, "duration": 6.78}, {"text": "per core and this example shows how an", "start": 1396.74, "duration": 6.6}, {"text": "application running 32 processes could", "start": 1400.64, "duration": 5.88}, {"text": "be executed onto 16 core nodes so again", "start": 1403.34, "duration": 7.579}, {"text": "we have one process running on each core", "start": 1406.52, "duration": 4.399}, {"text": "if we have a threaded application on the", "start": 1411.02, "duration": 3.48}, {"text": "other hand", "start": 1413.299, "duration": 3.36}, {"text": "a pure threaded application we would", "start": 1414.5, "duration": 4.98}, {"text": "have just a single process and all of", "start": 1416.659, "duration": 5.101}, {"text": "those threads would be executing within", "start": 1419.48, "duration": 4.74}, {"text": "that process and then applications have", "start": 1421.76, "duration": 4.02}, {"text": "been paralyzed using threading only", "start": 1424.22, "duration": 3.12}, {"text": "they're going to be restricted to", "start": 1425.78, "duration": 3.72}, {"text": "running to running within a single node", "start": 1427.34, "duration": 3.839}, {"text": "and here we share with Reddit app", "start": 1429.5, "duration": 4.62}, {"text": "running on running 16th Reds and 16", "start": 1431.179, "duration": 4.681}, {"text": "Accords", "start": 1434.12, "duration": 4.2}, {"text": "now I have an asterisk here says that", "start": 1435.86, "duration": 4.38}, {"text": "you know technically any programming", "start": 1438.32, "duration": 4.62}, {"text": "model can be mapped to any hardware in", "start": 1440.24, "duration": 4.98}, {"text": "fact we had", "start": 1442.94, "duration": 3.0}, {"text": "um", "start": 1445.22, "duration": 2.88}, {"text": "the doneness on one of our", "start": 1445.94, "duration": 4.26}, {"text": "supercomputers a few Generations ago if", "start": 1448.1, "duration": 5.28}, {"text": "you go back to the Gordon here and we", "start": 1450.2, "duration": 7.08}, {"text": "use something called vsnp which gave you", "start": 1453.38, "duration": 5.82}, {"text": "the illusion of a large shared memory", "start": 1457.28, "duration": 3.66}, {"text": "space but for the most part the", "start": 1459.2, "duration": 4.68}, {"text": "performance is pretty poor and", "start": 1460.94, "duration": 5.4}, {"text": "I don't know of any you know really good", "start": 1463.88, "duration": 4.56}, {"text": "implementations of shared memory", "start": 1466.34, "duration": 4.62}, {"text": "parallelism across or threaded threaded", "start": 1468.44, "duration": 4.2}, {"text": "parallelism across multiple nodes", "start": 1470.96, "duration": 2.94}, {"text": "anymore so essentially you're going to", "start": 1472.64, "duration": 4.38}, {"text": "be running on a single node", "start": 1473.9, "duration": 5.1}, {"text": "now if we have a hybrid application", "start": 1477.02, "duration": 5.519}, {"text": "again typically MPI plus openmp we can", "start": 1479.0, "duration": 5.64}, {"text": "use any combination of threads and", "start": 1482.539, "duration": 5.221}, {"text": "processes within a node so this example", "start": 1484.64, "duration": 5.519}, {"text": "shows two processes per node and eight", "start": 1487.76, "duration": 4.2}, {"text": "threads per process", "start": 1490.159, "duration": 4.201}, {"text": "so you can see that on each node we have", "start": 1491.96, "duration": 4.74}, {"text": "two of those armed rectangles which are", "start": 1494.36, "duration": 5.699}, {"text": "processes and then are eight threads", "start": 1496.7, "duration": 6.68}, {"text": "within within the process", "start": 1500.059, "duration": 3.321}, {"text": "so for hybrid applications a typical", "start": 1505.52, "duration": 5.279}, {"text": "scenario is to run one process per node", "start": 1508.34, "duration": 5.28}, {"text": "and use threads within a node although", "start": 1510.799, "duration": 5.101}, {"text": "this is becoming a little less common as", "start": 1513.62, "duration": 4.2}, {"text": "the number of cores that now continues", "start": 1515.9, "duration": 4.22}, {"text": "to grow", "start": 1517.82, "duration": 2.3}, {"text": "so really we can go we can um to tone", "start": 1520.159, "duration": 4.861}, {"text": "the dial anywhere from one process per", "start": 1522.86, "duration": 5.04}, {"text": "node to one process per core if we have", "start": 1525.02, "duration": 4.68}, {"text": "these hybrid applications", "start": 1527.9, "duration": 3.0}, {"text": "and this is where it gets a little", "start": 1529.7, "duration": 3.42}, {"text": "tricky you know trying to figure out how", "start": 1530.9, "duration": 4.74}, {"text": "we choose the balance between processes", "start": 1533.12, "duration": 4.26}, {"text": "and threads given that we have this", "start": 1535.64, "duration": 3.24}, {"text": "range of options", "start": 1537.38, "duration": 3.84}, {"text": "and the only way to really know for sure", "start": 1538.88, "duration": 4.86}, {"text": "is to do a benchmarking study you might", "start": 1541.22, "duration": 4.439}, {"text": "want to try this um with a different", "start": 1543.74, "duration": 5.64}, {"text": "number of of thread per process look at", "start": 1545.659, "duration": 6.421}, {"text": "the run times and the memory usage so", "start": 1549.38, "duration": 5.34}, {"text": "again we can tune this anywhere from", "start": 1552.08, "duration": 5.459}, {"text": "one process per node and one thread per", "start": 1554.72, "duration": 5.579}, {"text": "core all the way down to one process per", "start": 1557.539, "duration": 5.061}, {"text": "core", "start": 1560.299, "duration": 2.301}, {"text": "and let me just check the check real", "start": 1562.64, "duration": 3.899}, {"text": "quick", "start": 1565.4, "duration": 5.1}, {"text": "oh so and a um question here from Iman", "start": 1566.539, "duration": 7.02}, {"text": "Mandel our core and thread the same so", "start": 1570.5, "duration": 5.64}, {"text": "no they're not a core is", "start": 1573.559, "duration": 5.581}, {"text": "of course a hardware constant so so it's", "start": 1576.14, "duration": 4.26}, {"text": "one of the", "start": 1579.14, "duration": 3.899}, {"text": "it's one of the execution units within", "start": 1580.4, "duration": 5.04}, {"text": "the processor it's a CPU", "start": 1583.039, "duration": 5.341}, {"text": "um a thread is a is a programming", "start": 1585.44, "duration": 5.82}, {"text": "concept it's a threat of execution and", "start": 1588.38, "duration": 4.44}, {"text": "as I mentioned near near the beginning", "start": 1591.26, "duration": 4.56}, {"text": "of the talk multiple threads can", "start": 1592.82, "duration": 6.12}, {"text": "um Can can access the the same shared", "start": 1595.82, "duration": 5.9}, {"text": "memory structures", "start": 1598.94, "duration": 2.78}, {"text": "so we're going to move on to to ondo's", "start": 1604.159, "duration": 3.661}, {"text": "law we'll talk a little bit about the", "start": 1606.26, "duration": 4.14}, {"text": "scalability of parallel applications", "start": 1607.82, "duration": 6.0}, {"text": "so amdel's law describes the absolute", "start": 1610.4, "duration": 5.639}, {"text": "limit on the speed up of a code", "start": 1613.82, "duration": 4.32}, {"text": "as a function of the proportion of code", "start": 1616.039, "duration": 4.5}, {"text": "that can be paralyzed and the number of", "start": 1618.14, "duration": 4.68}, {"text": "processes so this is really the most", "start": 1620.539, "duration": 5.52}, {"text": "fundamental law of parallel computing", "start": 1622.82, "duration": 5.76}, {"text": "select let P be the fraction of the work", "start": 1626.059, "duration": 5.281}, {"text": "that can be paralyzed and S be the", "start": 1628.58, "duration": 4.38}, {"text": "fraction of the work that must be run", "start": 1631.34, "duration": 4.079}, {"text": "sequentially so you can imagine a lot of", "start": 1632.96, "duration": 4.14}, {"text": "workloads where", "start": 1635.419, "duration": 3.481}, {"text": "okay there's a certain amount of Serial", "start": 1637.1, "duration": 3.9}, {"text": "work it might be the might be the input", "start": 1638.9, "duration": 4.139}, {"text": "or output it might be the aggregation", "start": 1641.0, "duration": 3.779}, {"text": "results from multiple threads or", "start": 1643.039, "duration": 4.26}, {"text": "processes that really just can't be it", "start": 1644.779, "duration": 4.981}, {"text": "can't be paralyzed whereas P again is", "start": 1647.299, "duration": 4.141}, {"text": "the fraction that we can take", "start": 1649.76, "duration": 3.72}, {"text": "fraction work that we take that we can", "start": 1651.44, "duration": 4.56}, {"text": "divide up across multiple processes", "start": 1653.48, "duration": 5.04}, {"text": "and then n is going to be the number of", "start": 1656.0, "duration": 4.88}, {"text": "process", "start": 1658.52, "duration": 2.36}, {"text": "so our speed up is going to be 1 over", "start": 1661.34, "duration": 7.28}, {"text": "what 1 minus P plus p over n", "start": 1663.86, "duration": 4.76}, {"text": "so in the limit as the number of", "start": 1670.88, "duration": 4.32}, {"text": "processes goes to Infinity the", "start": 1672.799, "duration": 4.5}, {"text": "theoretical speed up is going to depend", "start": 1675.2, "duration": 4.56}, {"text": "only on the proportion of the parallel", "start": 1677.299, "duration": 5.221}, {"text": "content so again here we show n go into", "start": 1679.76, "duration": 3.899}, {"text": "Infinity", "start": 1682.52, "duration": 4.62}, {"text": "the P Over N term that vanishes we were", "start": 1683.659, "duration": 7.561}, {"text": "left with 1 over 1 minus P or 1 over s", "start": 1687.14, "duration": 5.7}, {"text": "so you might be looking at this and say", "start": 1691.22, "duration": 3.78}, {"text": "that doesn't seem so bad but we'll show", "start": 1692.84, "duration": 3.9}, {"text": "in the next slide that it doesn't take", "start": 1695.0, "duration": 4.5}, {"text": "much zero content to quickly impact the", "start": 1696.74, "duration": 5.24}, {"text": "speed up", "start": 1699.5, "duration": 2.48}, {"text": "so this is a plot that I generated I'm", "start": 1702.32, "duration": 4.14}, {"text": "showing abdall's law", "start": 1704.36, "duration": 4.38}, {"text": "for for code with", "start": 1706.46, "duration": 7.56}, {"text": "um 50 75 90 and 95 parallel content", "start": 1708.74, "duration": 7.559}, {"text": "um so on the y-axis we are showing the", "start": 1714.02, "duration": 3.72}, {"text": "speed of the speed up of that", "start": 1716.299, "duration": 5.281}, {"text": "application and on the x-axis they're", "start": 1717.74, "duration": 6.0}, {"text": "the number of chords", "start": 1721.58, "duration": 4.02}, {"text": "you can see it for the bottom curve", "start": 1723.74, "duration": 4.679}, {"text": "which is a", "start": 1725.6, "duration": 3.959}, {"text": "um", "start": 1728.419, "duration": 3.301}, {"text": "code weight with parent width 50", "start": 1729.559, "duration": 5.1}, {"text": "parallel content that we very very", "start": 1731.72, "duration": 5.579}, {"text": "quickly level out of course we are going", "start": 1734.659, "duration": 4.681}, {"text": "to um", "start": 1737.299, "duration": 5.461}, {"text": "not get any better speed up than m2x", "start": 1739.34, "duration": 6.12}, {"text": "um the green curve shows 75 content", "start": 1742.76, "duration": 5.76}, {"text": "again we we quickly level off", "start": 1745.46, "duration": 5.28}, {"text": "our maximum speed up is going to be four", "start": 1748.52, "duration": 5.94}, {"text": "but look at that even even with 95", "start": 1750.74, "duration": 5.939}, {"text": "um parallel content we can only get a", "start": 1754.46, "duration": 4.74}, {"text": "maximum speed up of 20.", "start": 1756.679, "duration": 5.401}, {"text": "if we'll go out further on this case I", "start": 1759.2, "duration": 5.64}, {"text": "added added a curve showing the code", "start": 1762.08, "duration": 6.3}, {"text": "width 99 paralyzable content", "start": 1764.84, "duration": 7.62}, {"text": "even there if we're running on a single", "start": 1768.38, "duration": 7.74}, {"text": "node with 128 cores that's typically", "start": 1772.46, "duration": 6.0}, {"text": "what you'll find in on unexpensed", "start": 1776.12, "duration": 4.26}, {"text": "bridges that the Pittsburgh super", "start": 1778.46, "duration": 5.699}, {"text": "computer center and other clusters", "start": 1780.38, "duration": 6.84}, {"text": "even with the code with 99 parallel", "start": 1784.159, "duration": 5.581}, {"text": "paralyzable content even though we could", "start": 1787.22, "duration": 4.5}, {"text": "get a maximum speed up", "start": 1789.74, "duration": 6.059}, {"text": "of 100. if we're using all 120 of those", "start": 1791.72, "duration": 6.6}, {"text": "course we're making terrible use of that", "start": 1795.799, "duration": 3.36}, {"text": "note", "start": 1798.32, "duration": 4.5}, {"text": "so basically where we would be using 128", "start": 1799.159, "duration": 6.061}, {"text": "cores but only getting a speed up of", "start": 1802.82, "duration": 5.359}, {"text": "about 60.", "start": 1805.22, "duration": 2.959}, {"text": "Bob there's a question of in the chat", "start": 1809.0, "duration": 5.46}, {"text": "about threads", "start": 1812.24, "duration": 4.34}, {"text": "yeah", "start": 1814.46, "duration": 2.12}, {"text": "um yes so a question for from Peter", "start": 1816.62, "duration": 4.799}, {"text": "um so what would be correct to say that", "start": 1819.26, "duration": 4.56}, {"text": "Reds are executed on the course yes so", "start": 1821.419, "duration": 4.38}, {"text": "we would take we would take threads and", "start": 1823.82, "duration": 4.16}, {"text": "we would map them to core", "start": 1825.799, "duration": 6.781}, {"text": "and typically you would execute One Core", "start": 1827.98, "duration": 5.559}, {"text": "um", "start": 1832.58, "duration": 4.319}, {"text": "one thread on each core although you can", "start": 1833.539, "duration": 6.0}, {"text": "map multiple threads per core but it", "start": 1836.899, "duration": 5.16}, {"text": "will typically", "start": 1839.539, "duration": 6.02}, {"text": "um and impact the performance", "start": 1842.059, "duration": 3.5}, {"text": "there's a question about our cores and", "start": 1845.72, "duration": 5.579}, {"text": "threads the same oh and yeah Mary and I", "start": 1848.299, "duration": 5.221}, {"text": "addressed that one earlier yeah okay so", "start": 1851.299, "duration": 4.441}, {"text": "sorry about that no no problem yeah so", "start": 1853.52, "duration": 5.1}, {"text": "core is a um", "start": 1855.74, "duration": 5.22}, {"text": "the dimension is a is a hardware concept", "start": 1858.62, "duration": 5.52}, {"text": "whereas thread is a programming", "start": 1860.96, "duration": 5.599}, {"text": "construct", "start": 1864.14, "duration": 2.419}, {"text": "okay so we're going to talk about some", "start": 1867.44, "duration": 4.619}, {"text": "of the other limits in scalability", "start": 1869.24, "duration": 5.46}, {"text": "so amdel's law we've just discussed", "start": 1872.059, "duration": 4.74}, {"text": "that's a theoretical upper limit on the", "start": 1874.7, "duration": 3.66}, {"text": "speed up but they're going to be other", "start": 1876.799, "duration": 4.201}, {"text": "factors that affect scalability there's", "start": 1878.36, "duration": 4.86}, {"text": "going to be Communications overhead", "start": 1881.0, "duration": 4.62}, {"text": "there's going to be limitations based on", "start": 1883.22, "duration": 4.86}, {"text": "the problem size and we're also going to", "start": 1885.62, "duration": 5.58}, {"text": "be impacted by uneven load balancing how", "start": 1888.08, "duration": 4.92}, {"text": "well we could take the work and divide", "start": 1891.2, "duration": 5.82}, {"text": "it up evenly across the processing units", "start": 1893.0, "duration": 6.539}, {"text": "so in real life applications involve", "start": 1897.02, "duration": 5.58}, {"text": "Communications and synchronization they", "start": 1899.539, "duration": 5.041}, {"text": "when all the threads of the processes", "start": 1902.6, "duration": 3.42}, {"text": "have to complete their work before", "start": 1904.58, "duration": 4.02}, {"text": "proceeding or irregular problems", "start": 1906.02, "duration": 5.1}, {"text": "non-cartesian grids the speed up can be", "start": 1908.6, "duration": 4.199}, {"text": "much less than what was predicted by", "start": 1911.12, "duration": 4.46}, {"text": "abdall's law", "start": 1912.799, "duration": 2.781}, {"text": "so we'll start off with with load", "start": 1915.86, "duration": 3.299}, {"text": "balancing", "start": 1917.779, "duration": 3.421}, {"text": "so when we paralyze the code we need to", "start": 1919.159, "duration": 4.14}, {"text": "take the work we need to divide it up", "start": 1921.2, "duration": 4.26}, {"text": "into chunks that can be executed", "start": 1923.299, "duration": 3.901}, {"text": "independently either by threads or", "start": 1925.46, "duration": 4.74}, {"text": "processes or a combination of two now if", "start": 1927.2, "duration": 4.859}, {"text": "the work cannot be distributed evenly", "start": 1930.2, "duration": 4.199}, {"text": "then processes are going to sit idle", "start": 1932.059, "duration": 4.441}, {"text": "waiting for the longest chunk to finish", "start": 1934.399, "duration": 4.741}, {"text": "and this is going to be a really common", "start": 1936.5, "duration": 5.22}, {"text": "situation when you're working with with", "start": 1939.14, "duration": 6.06}, {"text": "irregular domains um more more complex", "start": 1941.72, "duration": 5.819}, {"text": "geometries and so on", "start": 1945.2, "duration": 4.68}, {"text": "so let's say that we're running on a", "start": 1947.539, "duration": 5.52}, {"text": "system that has four CPUs or cores we're", "start": 1949.88, "duration": 4.98}, {"text": "going to label them here zero one two", "start": 1953.059, "duration": 3.72}, {"text": "and three", "start": 1954.86, "duration": 3.419}, {"text": "we're going to start off we're going to", "start": 1956.779, "duration": 3.301}, {"text": "take the available work and we're going", "start": 1958.279, "duration": 4.441}, {"text": "to divide it up across those across", "start": 1960.08, "duration": 4.979}, {"text": "those four um CPUs", "start": 1962.72, "duration": 5.4}, {"text": "the blue regions or when or when that", "start": 1965.059, "duration": 6.541}, {"text": "core is busy and the red hatched raisins", "start": 1968.12, "duration": 5.82}, {"text": "regions are when it's idle", "start": 1971.6, "duration": 4.86}, {"text": "so we'll see that if we have load if we", "start": 1973.94, "duration": 6.06}, {"text": "had a load in Balance say cpu2 was given", "start": 1976.46, "duration": 6.0}, {"text": "a larger chunk of work than zero one or", "start": 1980.0, "duration": 4.38}, {"text": "three", "start": 1982.46, "duration": 2.819}, {"text": "um", "start": 1984.38, "duration": 2.399}, {"text": "it's going to take it's going to take", "start": 1985.279, "duration": 4.201}, {"text": "longer to finish and the other CPU are", "start": 1986.779, "duration": 3.721}, {"text": "going to be waiting they're going to be", "start": 1989.48, "duration": 4.079}, {"text": "idle waiting for CPU to finish its work", "start": 1990.5, "duration": 4.5}, {"text": "when you get until we get to that", "start": 1993.559, "duration": 3.24}, {"text": "synchronization point", "start": 1995.0, "duration": 4.08}, {"text": "now we take the next chunk of work we", "start": 1996.799, "duration": 4.62}, {"text": "divide it up in this case it was CPU one", "start": 1999.08, "duration": 4.86}, {"text": "who got the largest chunk", "start": 2001.419, "duration": 5.701}, {"text": "in this case 0 2 and 3 are going to be", "start": 2003.94, "duration": 5.76}, {"text": "sitting idle so this is going to affect", "start": 2007.12, "duration": 4.2}, {"text": "the scalability of your of your", "start": 2009.7, "duration": 3.599}, {"text": "application", "start": 2011.32, "duration": 4.82}, {"text": "we have", "start": 2013.299, "duration": 2.841}, {"text": "all right we also have the issue of", "start": 2018.36, "duration": 3.76}, {"text": "communications overhead", "start": 2020.5, "duration": 4.08}, {"text": "this is an example showing the you know", "start": 2022.12, "duration": 4.799}, {"text": "a very idealized case though being a", "start": 2024.58, "duration": 4.079}, {"text": "system of partial differential equation", "start": 2026.919, "duration": 4.14}, {"text": "so if you're doing computational flow", "start": 2028.659, "duration": 3.681}, {"text": "Dynamics", "start": 2031.059, "duration": 3.901}, {"text": "magnetohydrodynamics climate weather a", "start": 2032.34, "duration": 4.179}, {"text": "lot of other types of simulations they", "start": 2034.96, "duration": 3.959}, {"text": "involve solving systems of partial", "start": 2036.519, "duration": 5.16}, {"text": "differential equations on a grade", "start": 2038.919, "duration": 5.341}, {"text": "and that grid is then distributed across", "start": 2041.679, "duration": 5.941}, {"text": "processes to achieve parallelization", "start": 2044.26, "duration": 6.54}, {"text": "so this is uh obviously a toy problem", "start": 2047.62, "duration": 5.7}, {"text": "very very simplified we're solving a", "start": 2050.8, "duration": 7.68}, {"text": "system of pdes on a 16 by 16 grid and if", "start": 2053.32, "duration": 8.819}, {"text": "we wanted to run this on four processes", "start": 2058.48, "duration": 5.28}, {"text": "we're going to divide that into four", "start": 2062.139, "duration": 4.861}, {"text": "even chunk for evenly sized eight by", "start": 2063.76, "duration": 4.74}, {"text": "eight chunks", "start": 2067.0, "duration": 4.26}, {"text": "we'll call this process zero one two and", "start": 2068.5, "duration": 4.94}, {"text": "three", "start": 2071.26, "duration": 2.18}, {"text": "now if we assume that each cell is", "start": 2075.76, "duration": 3.78}, {"text": "updated using the values in the four", "start": 2077.859, "duration": 3.721}, {"text": "neighboring cells", "start": 2079.54, "duration": 4.619}, {"text": "then the cells that are within the", "start": 2081.58, "duration": 4.68}, {"text": "interior of each chunk can be done", "start": 2084.159, "duration": 5.161}, {"text": "completely within the process", "start": 2086.26, "duration": 5.22}, {"text": "but when we get to the cells of the", "start": 2089.32, "duration": 4.14}, {"text": "boundary they're going to need data", "start": 2091.48, "duration": 3.899}, {"text": "belonging to a different process and", "start": 2093.46, "duration": 3.6}, {"text": "this is when we need to explicitly", "start": 2095.379, "duration": 4.801}, {"text": "explicitly communicate that data so it", "start": 2097.06, "duration": 5.039}, {"text": "accommodate this first of all we need to", "start": 2100.18, "duration": 4.26}, {"text": "add a Halo of ghost cells and then the", "start": 2102.099, "duration": 4.561}, {"text": "data has to be communicated between this", "start": 2104.44, "duration": 3.36}, {"text": "process", "start": 2106.66, "duration": 3.0}, {"text": "so the data movement", "start": 2107.8, "duration": 3.48}, {"text": "the amount of time it takes to do this", "start": 2109.66, "duration": 4.02}, {"text": "is going to depend on the latency and", "start": 2111.28, "duration": 4.079}, {"text": "the bandwidth and network and this is", "start": 2113.68, "duration": 2.76}, {"text": "going to introduce Communications", "start": 2115.359, "duration": 4.081}, {"text": "overhead so again as a end user rather", "start": 2116.44, "duration": 4.86}, {"text": "than a programmer you're not going to be", "start": 2119.44, "duration": 4.02}, {"text": "responsible for implementing this this", "start": 2121.3, "duration": 3.96}, {"text": "is going to be done all under the hood", "start": 2123.46, "duration": 3.72}, {"text": "in the application but you just need to", "start": 2125.26, "duration": 3.359}, {"text": "be aware of the impact that this is", "start": 2127.18, "duration": 3.54}, {"text": "going to have", "start": 2128.619, "duration": 4.141}, {"text": "so for the cells that are the boundaries", "start": 2130.72, "duration": 3.899}, {"text": "of the chunk data belong to the", "start": 2132.76, "duration": 4.14}, {"text": "neighborhood process is needed", "start": 2134.619, "duration": 4.801}, {"text": "and I'm showing here the pattern of", "start": 2136.9, "duration": 4.5}, {"text": "communication that we're going to have", "start": 2139.42, "duration": 4.86}, {"text": "so each each process is going to be", "start": 2141.4, "duration": 4.86}, {"text": "sending data and receiving data from its", "start": 2144.28, "duration": 3.42}, {"text": "neighbors", "start": 2146.26, "duration": 4.5}, {"text": "let me just check the chat let's see", "start": 2147.7, "duration": 5.12}, {"text": "um", "start": 2150.76, "duration": 2.06}, {"text": "view here", "start": 2153.579, "duration": 2.78}, {"text": "all right so", "start": 2159.7, "duration": 4.7}, {"text": "um", "start": 2162.339, "duration": 2.061}, {"text": "okay so going back to to Lauren yes so", "start": 2164.56, "duration": 4.98}, {"text": "martya answer that really so as a user I", "start": 2167.32, "duration": 3.72}, {"text": "don't want to tell my parallel software", "start": 2169.54, "duration": 3.0}, {"text": "to use more threads than the total", "start": 2171.04, "duration": 4.02}, {"text": "number and typically that that that's", "start": 2172.54, "duration": 4.74}, {"text": "correct", "start": 2175.06, "duration": 4.5}, {"text": "um and then Johnny asked Johnny Powell", "start": 2177.28, "duration": 4.5}, {"text": "asked perhaps you'll get the chin get to", "start": 2179.56, "duration": 3.96}, {"text": "this question but how does one manage", "start": 2181.78, "duration": 4.799}, {"text": "uneven load and and say in charm plus", "start": 2183.52, "duration": 5.76}, {"text": "plus so I'm not terribly familiar with", "start": 2186.579, "duration": 5.401}, {"text": "chime plus plus but in general whether", "start": 2189.28, "duration": 6.18}, {"text": "it's openmp MPI charm plus plus you're", "start": 2191.98, "duration": 5.52}, {"text": "going to do some Dynamic scheduling of", "start": 2195.46, "duration": 2.94}, {"text": "the work", "start": 2197.5, "duration": 2.7}, {"text": "it's always an extreme example I had", "start": 2198.4, "duration": 3.959}, {"text": "worked on an application years ago in", "start": 2200.2, "duration": 5.12}, {"text": "computational finance where we were", "start": 2202.359, "duration": 5.401}, {"text": "analyzing all the stocks that were", "start": 2205.32, "duration": 4.18}, {"text": "traded within a given day", "start": 2207.76, "duration": 3.9}, {"text": "and there's a", "start": 2209.5, "duration": 4.2}, {"text": "um order of the magnitude difference in", "start": 2211.66, "duration": 4.26}, {"text": "the amount of work that was required for", "start": 2213.7, "duration": 4.32}, {"text": "preheat stock say something that traded", "start": 2215.92, "duration": 4.08}, {"text": "heavily like Apple or Google required a", "start": 2218.02, "duration": 4.319}, {"text": "lot of work others didn't require much", "start": 2220.0, "duration": 4.32}, {"text": "so what we would do is we would start", "start": 2222.339, "duration": 4.141}, {"text": "with the largest chunks of work we would", "start": 2224.32, "duration": 4.68}, {"text": "hand it out to cars or processes and", "start": 2226.48, "duration": 4.5}, {"text": "then when they were done they would grab", "start": 2229.0, "duration": 3.839}, {"text": "the next available piece of work so", "start": 2230.98, "duration": 3.24}, {"text": "depending on the problem you could do", "start": 2232.839, "duration": 3.721}, {"text": "this Dynamic load balancing", "start": 2234.22, "duration": 5.28}, {"text": "and then um", "start": 2236.56, "duration": 5.519}, {"text": "the um summer round had followed up wait", "start": 2239.5, "duration": 5.599}, {"text": "with Lauren's question", "start": 2242.079, "duration": 3.02}, {"text": "yeah so and then Marty said yes", "start": 2247.119, "duration": 4.201}, {"text": "sometimes is the operative word yeah", "start": 2248.98, "duration": 4.139}, {"text": "that's um", "start": 2251.32, "duration": 4.62}, {"text": "sometimes you can you can use more", "start": 2253.119, "duration": 6.841}, {"text": "threads than cores especially if you're", "start": 2255.94, "duration": 4.919}, {"text": "um", "start": 2259.96, "duration": 3.24}, {"text": "if you have random patterns of patterns", "start": 2260.859, "duration": 4.021}, {"text": "of memory access and you're really", "start": 2263.2, "duration": 3.36}, {"text": "waiting to spend a lot of time waiting", "start": 2264.88, "duration": 3.719}, {"text": "for data but in general in HPC", "start": 2266.56, "duration": 3.24}, {"text": "applications they're probably going to", "start": 2268.599, "duration": 4.821}, {"text": "stick to to one thread per core", "start": 2269.8, "duration": 3.62}, {"text": "um so it is a Caitlyn Hetherington asked", "start": 2274.359, "duration": 6.121}, {"text": "is a process the same thing as a node", "start": 2277.96, "duration": 4.139}, {"text": "and I'm sorry I'm kind of mixing up my", "start": 2280.48, "duration": 3.48}, {"text": "terminology a little bit no no you can", "start": 2282.099, "duration": 4.141}, {"text": "think of a node as being a computer like", "start": 2283.96, "duration": 4.68}, {"text": "the whole box it can contain multiple", "start": 2286.24, "duration": 4.98}, {"text": "processes each with multiple cores of", "start": 2288.64, "duration": 4.199}, {"text": "CPU", "start": 2291.22, "duration": 3.54}, {"text": "and then finally", "start": 2292.839, "duration": 4.081}, {"text": "um Shen Feng Ma", "start": 2294.76, "duration": 4.74}, {"text": "for openmp is there any historic reason", "start": 2296.92, "duration": 6.0}, {"text": "to call it open multi-process instead of", "start": 2299.5, "duration": 5.82}, {"text": "open multi-threading", "start": 2302.92, "duration": 6.36}, {"text": "now I have to be honest I've never heard", "start": 2305.32, "duration": 6.84}, {"text": "um oh openmp referred to refer to either", "start": 2309.28, "duration": 5.1}, {"text": "of those um I've only ever heard a card", "start": 2312.16, "duration": 4.56}, {"text": "called openmp so I need to look into", "start": 2314.38, "duration": 5.06}, {"text": "that a little bit more", "start": 2316.72, "duration": 2.72}, {"text": "okay so moving on so we talked about we", "start": 2320.2, "duration": 4.5}, {"text": "talked about the limits imposed by", "start": 2323.079, "duration": 4.02}, {"text": "andal's law and then these all these", "start": 2324.7, "duration": 4.44}, {"text": "other factors that are going to affect", "start": 2327.099, "duration": 3.421}, {"text": "scalability", "start": 2329.14, "duration": 2.88}, {"text": "so by now you're probably wondering how", "start": 2330.52, "duration": 3.9}, {"text": "does anybody ever use all the cores on a", "start": 2332.02, "duration": 4.92}, {"text": "single modern compute node let alone the", "start": 2334.42, "duration": 5.04}, {"text": "full power of large supercomputer", "start": 2336.94, "duration": 5.28}, {"text": "so first of all the reality is that most", "start": 2339.46, "duration": 5.52}, {"text": "parallel applications do not scale to", "start": 2342.22, "duration": 5.04}, {"text": "thousands or even hundreds of cores", "start": 2344.98, "duration": 5.16}, {"text": "in fact on expanse we have many many", "start": 2347.26, "duration": 5.76}, {"text": "jobs that just run within a single node", "start": 2350.14, "duration": 5.82}, {"text": "or even use a fraction of that node", "start": 2353.02, "duration": 4.68}, {"text": "now if you do want to achieve higher", "start": 2355.96, "duration": 3.54}, {"text": "scalability though there are several", "start": 2357.7, "duration": 3.72}, {"text": "strategies you know we do have these", "start": 2359.5, "duration": 4.56}, {"text": "large machines out there sometimes we'll", "start": 2361.42, "duration": 4.86}, {"text": "be dedicated to a single large", "start": 2364.06, "duration": 5.76}, {"text": "calculation so first of all we can grow", "start": 2366.28, "duration": 5.7}, {"text": "the problem size with the numbers of", "start": 2369.82, "duration": 3.539}, {"text": "cores or nodes", "start": 2371.98, "duration": 2.52}, {"text": "um if you look into something called", "start": 2373.359, "duration": 4.98}, {"text": "augustus's law IT addresses the issue of", "start": 2374.5, "duration": 7.56}, {"text": "weak scaling where we where we allow the", "start": 2378.339, "duration": 7.141}, {"text": "problem side to go to grow so that each", "start": 2382.06, "duration": 7.44}, {"text": "red or um MPI process has the same", "start": 2385.48, "duration": 5.7}, {"text": "amount of work but we just throw the", "start": 2389.5, "duration": 4.74}, {"text": "problem as we grow the number of corns", "start": 2391.18, "duration": 5.28}, {"text": "we can overlap the communication with", "start": 2394.24, "duration": 4.14}, {"text": "the computation again this is something", "start": 2396.46, "duration": 4.74}, {"text": "that's being done by the developer", "start": 2398.38, "duration": 4.979}, {"text": "um if they're you know highly scaled", "start": 2401.2, "duration": 4.56}, {"text": "very very clever they can be doing work", "start": 2403.359, "duration": 3.72}, {"text": "while they're waiting for the", "start": 2405.76, "duration": 3.24}, {"text": "communications to finish I already", "start": 2407.079, "duration": 4.02}, {"text": "talked about the third bullet that we", "start": 2409.0, "duration": 4.02}, {"text": "could use Dynamic load balancing to", "start": 2411.099, "duration": 4.26}, {"text": "assign work decors as they become idle", "start": 2413.02, "duration": 4.68}, {"text": "and we can increase the ratio of", "start": 2415.359, "duration": 4.74}, {"text": "computation to communication so a good", "start": 2417.7, "duration": 4.379}, {"text": "example is if you're familiar with", "start": 2420.099, "duration": 4.561}, {"text": "computational fluid dynamics", "start": 2422.079, "duration": 5.28}, {"text": "if we extend that to combustion or", "start": 2424.66, "duration": 5.959}, {"text": "called reactive flow the chemistry", "start": 2427.359, "duration": 6.301}, {"text": "calculations within each cell are very", "start": 2430.619, "duration": 5.261}, {"text": "computationally expensive and that's", "start": 2433.66, "duration": 4.14}, {"text": "going to increase the computation of", "start": 2435.88, "duration": 4.02}, {"text": "communication and we can often get", "start": 2437.8, "duration": 5.72}, {"text": "better scaling in those situations", "start": 2439.9, "duration": 3.62}, {"text": "okay", "start": 2445.24, "duration": 2.42}, {"text": "so running parallel applications and", "start": 2447.7, "duration": 3.6}, {"text": "scale and studies this is going to be", "start": 2449.74, "duration": 5.64}, {"text": "the last major topic before we wrap up", "start": 2451.3, "duration": 6.24}, {"text": "so if we've covered the basics of", "start": 2455.38, "duration": 4.26}, {"text": "parallel Computing Hardware threads", "start": 2457.54, "duration": 4.74}, {"text": "processes hybrid application the", "start": 2459.64, "duration": 5.4}, {"text": "implementation of MPI and openmp amdos", "start": 2462.28, "duration": 4.2}, {"text": "law and other factors that affect", "start": 2465.04, "duration": 3.84}, {"text": "scalability", "start": 2466.48, "duration": 4.32}, {"text": "so we've got all this Theory and", "start": 2468.88, "duration": 4.5}, {"text": "background but how do we know how many", "start": 2470.8, "duration": 5.22}, {"text": "CPUs or gpus to use when we're running", "start": 2473.38, "duration": 4.979}, {"text": "our parallel application", "start": 2476.02, "duration": 4.98}, {"text": "and the only way to definitely answer", "start": 2478.359, "duration": 4.681}, {"text": "that question is to perform a scaling", "start": 2481.0, "duration": 5.099}, {"text": "study or a representative problem is run", "start": 2483.04, "duration": 6.539}, {"text": "on a different number of processes", "start": 2486.099, "duration": 5.52}, {"text": "and by a representative problem I mean", "start": 2489.579, "duration": 4.621}, {"text": "one with the same size say the grid", "start": 2491.619, "duration": 4.921}, {"text": "dimensions number of particle number of", "start": 2494.2, "duration": 5.82}, {"text": "images number of genomes cetera and the", "start": 2496.54, "duration": 5.819}, {"text": "complex the complexity the level of", "start": 2500.02, "duration": 4.5}, {"text": "theory the type of analysis the physics", "start": 2502.359, "duration": 4.021}, {"text": "as the research problem you want to", "start": 2504.52, "duration": 4.2}, {"text": "solve now this doesn't mean that you", "start": 2506.38, "duration": 5.34}, {"text": "need to run your entire production scale", "start": 2508.72, "duration": 5.76}, {"text": "calculation to do the scaling study", "start": 2511.72, "duration": 5.28}, {"text": "if you're doing say molecular Dynamics", "start": 2514.48, "duration": 4.8}, {"text": "or solving systems partial differential", "start": 2517.0, "duration": 4.38}, {"text": "equations", "start": 2519.28, "duration": 4.26}, {"text": "you could you could run just a few time", "start": 2521.38, "duration": 3.959}, {"text": "steps instead of a large number of time", "start": 2523.54, "duration": 2.76}, {"text": "steps", "start": 2525.339, "duration": 3.061}, {"text": "if you're doing High throughput", "start": 2526.3, "duration": 4.44}, {"text": "Computing you could run just a few", "start": 2528.4, "duration": 4.62}, {"text": "problems in your parameter Suite rather", "start": 2530.74, "duration": 4.5}, {"text": "than the entire Suite what you do want", "start": 2533.02, "duration": 4.98}, {"text": "to do a limited number of these", "start": 2535.24, "duration": 5.099}, {"text": "calculations just to figure out how much", "start": 2538.0, "duration": 4.5}, {"text": "faster your code is running as you use", "start": 2540.339, "duration": 5.541}, {"text": "more as you use more Hardware", "start": 2542.5, "duration": 3.38}, {"text": "so I'm going to talk about presenting", "start": 2546.64, "duration": 3.54}, {"text": "the scaling results and we're going to", "start": 2548.44, "duration": 3.84}, {"text": "start off with the wrong way to do this", "start": 2550.18, "duration": 4.08}, {"text": "so these two plots believe it or not", "start": 2552.28, "duration": 3.78}, {"text": "they are", "start": 2554.26, "duration": 5.28}, {"text": "um scaling curve for two different codes", "start": 2556.06, "duration": 5.76}, {"text": "and it's really hard to tell which one", "start": 2559.54, "duration": 4.559}, {"text": "has the better scaling Behavior", "start": 2561.82, "duration": 4.38}, {"text": "and the reason we can't tell is because", "start": 2564.099, "duration": 4.801}, {"text": "the timings at the large core cuts are", "start": 2566.2, "duration": 4.379}, {"text": "indistinguishable", "start": 2568.9, "duration": 5.459}, {"text": "so we see this a lot in the allocation", "start": 2570.579, "duration": 5.701}, {"text": "um in the allocation proposals asking", "start": 2574.359, "duration": 4.201}, {"text": "for time on our supercomputers we'll get", "start": 2576.28, "duration": 4.62}, {"text": "these plots on the on these linear axes", "start": 2578.56, "duration": 4.799}, {"text": "and it's really really hard hard for us", "start": 2580.9, "duration": 6.199}, {"text": "to um to tell if you are using the", "start": 2583.359, "duration": 7.701}, {"text": "parallel Hardware effectively", "start": 2587.099, "duration": 3.961}, {"text": "so a better way to do this the way that", "start": 2591.64, "duration": 4.52}, {"text": "we like to see scaling studies presented", "start": 2593.98, "duration": 7.339}, {"text": "is to plot the data on log X", "start": 2596.16, "duration": 5.159}, {"text": "um so now again these are these are the", "start": 2601.72, "duration": 3.899}, {"text": "same", "start": 2604.599, "duration": 2.76}, {"text": "um that this is these are the same dips", "start": 2605.619, "duration": 3.72}, {"text": "that's that showed on the previous slide", "start": 2607.359, "duration": 4.561}, {"text": "but note that the different", "start": 2609.339, "duration": 4.381}, {"text": "um", "start": 2611.92, "duration": 3.54}, {"text": "that the different scales for the left", "start": 2613.72, "duration": 5.34}, {"text": "axis on the on the two plots I've also", "start": 2615.46, "duration": 5.82}, {"text": "included a line that's that sharp black", "start": 2619.06, "duration": 4.62}, {"text": "line showing linear scaling and plotting", "start": 2621.28, "duration": 4.079}, {"text": "the parallel efficiency on the right", "start": 2623.68, "duration": 3.12}, {"text": "Axis", "start": 2625.359, "duration": 4.74}, {"text": "so in this case for a perfectly scaling", "start": 2626.8, "duration": 5.76}, {"text": "code which as shown left we'll see that", "start": 2630.099, "duration": 4.681}, {"text": "the efficiency is going to be", "start": 2632.56, "duration": 4.86}, {"text": "um is going to be pegged out of one and", "start": 2634.78, "duration": 5.52}, {"text": "if we look at the at runtime as a", "start": 2637.42, "duration": 4.74}, {"text": "function number of cores we have this", "start": 2640.3, "duration": 4.44}, {"text": "linear relationship and you see that it", "start": 2642.16, "duration": 4.98}, {"text": "has the same slope as the as the black", "start": 2644.74, "duration": 4.14}, {"text": "line that I added", "start": 2647.14, "duration": 3.9}, {"text": "now on the right I'm showing the the", "start": 2648.88, "duration": 5.28}, {"text": "code with a more limited scalability and", "start": 2651.04, "duration": 5.9}, {"text": "you'll see that the that the runtime", "start": 2654.16, "duration": 6.24}, {"text": "starts off decreasing linearly with the", "start": 2656.94, "duration": 5.2}, {"text": "number of cores but eventually it", "start": 2660.4, "duration": 4.199}, {"text": "flattens out and if we plot this in", "start": 2662.14, "duration": 4.14}, {"text": "terms of the efficiency we could see", "start": 2664.599, "duration": 5.52}, {"text": "that after we get past about about 16", "start": 2666.28, "duration": 6.12}, {"text": "cores that the efficiency is rapidly", "start": 2670.119, "duration": 4.881}, {"text": "dropping off", "start": 2672.4, "duration": 2.6}, {"text": "okay", "start": 2677.5, "duration": 2.96}, {"text": "so then then the question is where", "start": 2682.42, "duration": 3.96}, {"text": "should I be running on this scaling", "start": 2684.52, "duration": 4.2}, {"text": "curve so if your work is not", "start": 2686.38, "duration": 4.26}, {"text": "particularly sensitive to the time to", "start": 2688.72, "duration": 4.56}, {"text": "complete a single run consider using a", "start": 2690.64, "duration": 5.459}, {"text": "CPU or a GPU account that's at we're", "start": 2693.28, "duration": 5.819}, {"text": "very close to 100 efficiency and", "start": 2696.099, "duration": 5.101}, {"text": "sometimes this might mean running on a", "start": 2699.099, "duration": 4.621}, {"text": "single core or GPU", "start": 2701.2, "duration": 5.52}, {"text": "this is going to really make sense for a", "start": 2703.72, "duration": 4.8}, {"text": "parameter squeak workloads with the same", "start": 2706.72, "duration": 4.32}, {"text": "calculation is run many times with", "start": 2708.52, "duration": 5.059}, {"text": "different sets of inputs", "start": 2711.04, "duration": 4.86}, {"text": "now we can go a little bit further on", "start": 2713.579, "duration": 4.061}, {"text": "the scaling curve if the job would take", "start": 2715.9, "duration": 4.439}, {"text": "an unreasonably long time at lower core", "start": 2717.64, "duration": 5.16}, {"text": "counts or for shorter time to solution", "start": 2720.339, "duration": 4.801}, {"text": "it helps to make research in your", "start": 2722.8, "duration": 4.62}, {"text": "um sorry helps you make progress in your", "start": 2725.14, "duration": 3.42}, {"text": "research", "start": 2727.42, "duration": 3.06}, {"text": "so we don't we don't want to wait we", "start": 2728.56, "duration": 3.6}, {"text": "often don't want to wait days or", "start": 2730.48, "duration": 4.139}, {"text": "sometimes even hours for far results", "start": 2732.16, "duration": 4.02}, {"text": "so if", "start": 2734.619, "duration": 3.48}, {"text": "running at a little bit lower efficiency", "start": 2736.18, "duration": 4.62}, {"text": "makes you more effective if you do a", "start": 2738.099, "duration": 4.74}, {"text": "calculation you can come back in 15", "start": 2740.8, "duration": 4.44}, {"text": "minutes look at the results make an", "start": 2742.839, "duration": 4.201}, {"text": "adjustment run it again yeah you", "start": 2745.24, "duration": 3.9}, {"text": "definitely want to run at a lower", "start": 2747.04, "duration": 4.5}, {"text": "parallel efficiency so you get quicker", "start": 2749.14, "duration": 3.9}, {"text": "turnaround", "start": 2751.54, "duration": 3.6}, {"text": "um if your code does not have checkpoint", "start": 2753.04, "duration": 4.44}, {"text": "restart capabilities and the runtime", "start": 2755.14, "duration": 4.5}, {"text": "would ex would exceed the queue limits", "start": 2757.48, "duration": 4.02}, {"text": "you may have no choice but to run it", "start": 2759.64, "duration": 5.54}, {"text": "hard at higher courts", "start": 2761.5, "duration": 3.68}, {"text": "and then sometimes", "start": 2765.76, "duration": 3.9}, {"text": "the Timeless solution is absolutely", "start": 2767.8, "duration": 4.319}, {"text": "critical and in that case it's okay to", "start": 2769.66, "duration": 4.62}, {"text": "run at lower efficiency", "start": 2772.119, "duration": 4.141}, {"text": "so examples might include calculations", "start": 2774.28, "duration": 4.02}, {"text": "that need to run on a regular schedule", "start": 2776.26, "duration": 5.16}, {"text": "say you're collecting data each day it", "start": 2778.3, "duration": 4.68}, {"text": "needs to be processed and analyzed", "start": 2781.42, "duration": 3.419}, {"text": "overnight you need those results in the", "start": 2782.98, "duration": 5.22}, {"text": "morning or and we hope you wouldn't be", "start": 2784.839, "duration": 6.48}, {"text": "using our super computers for", "start": 2788.2, "duration": 5.1}, {"text": "um for this kind of work but if you're", "start": 2791.319, "duration": 4.141}, {"text": "doing", "start": 2793.3, "duration": 3.72}, {"text": "um if you go and work like severe", "start": 2795.46, "duration": 4.2}, {"text": "weather forecasting where you need an", "start": 2797.02, "duration": 4.98}, {"text": "almost immediate response say", "start": 2799.66, "duration": 5.04}, {"text": "we're looking at response to a um to a", "start": 2802.0, "duration": 5.04}, {"text": "tornado or tsunami or so on yeah then", "start": 2804.7, "duration": 4.26}, {"text": "you can keep using more and more cores", "start": 2807.04, "duration": 5.039}, {"text": "until as long as your runtime continues", "start": 2808.96, "duration": 5.42}, {"text": "the decrease", "start": 2812.079, "duration": 2.301}, {"text": "okay one other consideration is some", "start": 2814.9, "duration": 4.919}, {"text": "applications have a very large memory", "start": 2818.02, "duration": 3.42}, {"text": "footprint", "start": 2819.819, "duration": 4.681}, {"text": "so on resources like expense at spsd or", "start": 2821.44, "duration": 4.679}, {"text": "Bridges to it at Pittsburgh super", "start": 2824.5, "duration": 3.96}, {"text": "Computing Center they allow multiple", "start": 2826.119, "duration": 4.021}, {"text": "jobs to share a node", "start": 2828.46, "duration": 3.78}, {"text": "and these jobs can normally request", "start": 2830.14, "duration": 4.62}, {"text": "without extra charge memory in", "start": 2832.24, "duration": 4.68}, {"text": "proportion to the core account so for", "start": 2834.76, "duration": 6.0}, {"text": "example expense has 128 cores and 256", "start": 2836.92, "duration": 7.919}, {"text": "gigabytes of memory per node so you get", "start": 2840.76, "duration": 6.359}, {"text": "um without any extra charge to two", "start": 2844.839, "duration": 5.28}, {"text": "gigabyte per two gigabytes per core but", "start": 2847.119, "duration": 5.941}, {"text": "if memory is the limited resource you", "start": 2850.119, "duration": 5.881}, {"text": "may need to throw extra cores at it", "start": 2853.06, "duration": 5.0}, {"text": "um", "start": 2856.0, "duration": 2.06}, {"text": "and it could result in poor and poorer", "start": 2860.339, "duration": 4.541}, {"text": "scaling but do consider using", "start": 2863.079, "duration": 3.421}, {"text": "specialized large memory nodes when", "start": 2864.88, "duration": 3.92}, {"text": "they're available", "start": 2866.5, "duration": 2.3}, {"text": "and then finally it's going to wrap up", "start": 2869.319, "duration": 4.921}, {"text": "um we've only spread the surface sdsc", "start": 2871.96, "duration": 4.5}, {"text": "has many many training resources and a", "start": 2874.24, "duration": 4.619}, {"text": "wide on a wide range of topics", "start": 2876.46, "duration": 4.379}, {"text": "we also have user guides for the", "start": 2878.859, "duration": 3.661}, {"text": "nationally allocated resources they", "start": 2880.839, "duration": 3.24}, {"text": "contain a lot of information on job", "start": 2882.52, "duration": 4.26}, {"text": "submission accounting compilation data", "start": 2884.079, "duration": 4.321}, {"text": "movement available software and other", "start": 2886.78, "duration": 4.799}, {"text": "site-specific content so I have here the", "start": 2888.4, "duration": 6.12}, {"text": "link link to expense and also the link", "start": 2891.579, "duration": 4.74}, {"text": "to access", "start": 2894.52, "duration": 4.5}, {"text": "get access documentation if you're not", "start": 2896.319, "duration": 4.321}, {"text": "familiar already access is the phone", "start": 2899.02, "duration": 5.64}, {"text": "want to exceed and they have the user", "start": 2900.64, "duration": 6.9}, {"text": "guides for all the resource providers", "start": 2904.66, "duration": 5.459}, {"text": "so to wrap up parallel Computing is for", "start": 2907.54, "duration": 4.2}, {"text": "anyone who wants to accomplish more", "start": 2910.119, "duration": 3.301}, {"text": "research and solve more challenging", "start": 2911.74, "duration": 2.7}, {"text": "problems", "start": 2913.42, "duration": 3.06}, {"text": "you don't need to be a programmer but", "start": 2914.44, "duration": 3.54}, {"text": "you do need to know some of the", "start": 2916.48, "duration": 3.78}, {"text": "fundamentals to effectively use parallel", "start": 2917.98, "duration": 3.839}, {"text": "computers", "start": 2920.26, "duration": 3.66}, {"text": "remember that processes could be thought", "start": 2921.819, "duration": 5.221}, {"text": "of as instances of a program threads run", "start": 2923.92, "duration": 5.58}, {"text": "run within a process and access shared", "start": 2927.04, "duration": 5.7}, {"text": "data MPI and openmp these are just", "start": 2929.5, "duration": 5.28}, {"text": "implementations of parallelization", "start": 2932.74, "duration": 3.96}, {"text": "schemes that are used to generate the", "start": 2934.78, "duration": 4.02}, {"text": "parallel software", "start": 2936.7, "duration": 4.2}, {"text": "in terms of scalability", "start": 2938.8, "duration": 4.74}, {"text": "always be aware of omdahl's law", "start": 2940.9, "duration": 4.679}, {"text": "but also know that it gives you an upper", "start": 2943.54, "duration": 4.02}, {"text": "limit on the scalability and there are", "start": 2945.579, "duration": 4.381}, {"text": "other factors that can impact the", "start": 2947.56, "duration": 4.38}, {"text": "scalability such as load imbalance and", "start": 2949.96, "duration": 3.72}, {"text": "Communications overhead", "start": 2951.94, "duration": 3.96}, {"text": "and then finally know how to display", "start": 2953.68, "duration": 5.1}, {"text": "your scaling data and choose core cuts", "start": 2955.9, "duration": 4.56}, {"text": "and then finally if you're interested", "start": 2958.78, "duration": 4.5}, {"text": "and I will paste this into the chat I", "start": 2960.46, "duration": 5.04}, {"text": "have a python notebook that I use to", "start": 2963.28, "duration": 4.14}, {"text": "generate the figures in this in this", "start": 2965.5, "duration": 4.56}, {"text": "presentation in fact you may want to to", "start": 2967.42, "duration": 4.32}, {"text": "use a notebook to generate your own", "start": 2970.06, "duration": 6.18}, {"text": "scaling plots to um in in your next um", "start": 2971.74, "duration": 7.14}, {"text": "allocation request", "start": 2976.24, "duration": 4.98}, {"text": "and with that I'm going to wrap up and", "start": 2978.88, "duration": 4.8}, {"text": "I'm happy to take any question great", "start": 2981.22, "duration": 5.22}, {"text": "that thank you Johnny", "start": 2983.68, "duration": 4.939}, {"text": "um", "start": 2986.44, "duration": 2.179}, {"text": "so I'm going to do a quick demo of my", "start": 2988.66, "duration": 5.9}, {"text": "Jupiter notebooks", "start": 2991.599, "duration": 2.961}, {"text": "on the bottom here um does anybody have", "start": 2995.98, "duration": 4.7}, {"text": "any questions first", "start": 2997.9, "duration": 2.78}, {"text": "yeah", "start": 3004.28, "duration": 3.579}, {"text": "um hey there so a question that's maybe", "start": 3005.819, "duration": 4.921}, {"text": "about the cause versus threads confusion", "start": 3007.859, "duration": 6.361}, {"text": "if I do something like run h-top it", "start": 3010.74, "duration": 6.78}, {"text": "tells me I have you know 64 CPUs that", "start": 3014.22, "duration": 5.28}, {"text": "I'm sure my computer only actually has", "start": 3017.52, "duration": 6.12}, {"text": "32 cores what's the discrepancy there", "start": 3019.5, "duration": 5.88}, {"text": "oh", "start": 3023.64, "duration": 3.8}, {"text": "um", "start": 3025.38, "duration": 2.06}, {"text": "so some some process like Intel will", "start": 3027.48, "duration": 5.099}, {"text": "Implement what they call I believe an", "start": 3030.359, "duration": 3.841}, {"text": "Intel it's called hyper threading yeah", "start": 3032.579, "duration": 3.78}, {"text": "yeah and Marty put it in there where", "start": 3034.2, "duration": 6.3}, {"text": "it's executing two Hardware threads per", "start": 3036.359, "duration": 6.24}, {"text": "um per per core", "start": 3040.5, "duration": 4.88}, {"text": "and", "start": 3042.599, "duration": 2.781}, {"text": "performance for for a lot of business", "start": 3045.9, "duration": 5.219}, {"text": "application but we generally turn it off", "start": 3048.18, "duration": 5.82}, {"text": "on on our systems", "start": 3051.119, "duration": 5.641}, {"text": "yes and Marty added um yes launchable", "start": 3054.0, "duration": 4.68}, {"text": "versus physical course", "start": 3056.76, "duration": 4.14}, {"text": "okay so I kind of I stopped seeing any", "start": 3058.68, "duration": 4.139}, {"text": "kind of speed up once I get into that", "start": 3060.9, "duration": 4.86}, {"text": "like past the number of calls right", "start": 3062.819, "duration": 5.101}, {"text": "going to the number of threads but yeah", "start": 3065.76, "duration": 5.18}, {"text": "okay thank you yeah", "start": 3067.92, "duration": 3.02}, {"text": "yeah and yeah I'm trying Trevor added", "start": 3071.28, "duration": 3.6}, {"text": "yeah you can run 100", "start": 3073.26, "duration": 4.02}, {"text": "000 24 threads in 32 cores there will be", "start": 3074.88, "duration": 4.14}, {"text": "time sliced by the kernel and will not", "start": 3077.28, "duration": 3.299}, {"text": "run at full speed", "start": 3079.02, "duration": 4.14}, {"text": "so again we we typically turn off hyper", "start": 3080.579, "duration": 4.681}, {"text": "threading and HPC", "start": 3083.16, "duration": 5.04}, {"text": "in HPC environments but it's often", "start": 3085.26, "duration": 4.74}, {"text": "turned on for running business", "start": 3088.2, "duration": 4.22}, {"text": "applications", "start": 3090.0, "duration": 2.42}, {"text": "right so I'm going to um", "start": 3092.64, "duration": 4.26}, {"text": "you know I have a note here and I", "start": 3095.339, "duration": 3.841}, {"text": "realized that I um misspoke a little bit", "start": 3096.9, "duration": 3.9}, {"text": "when it's talking about large memory", "start": 3099.18, "duration": 4.679}, {"text": "when we're discussing", "start": 3100.8, "duration": 4.86}, {"text": "um", "start": 3103.859, "duration": 4.321}, {"text": "you know running head no no additional", "start": 3105.66, "duration": 4.08}, {"text": "cost it's actually a little bit more", "start": 3108.18, "duration": 3.06}, {"text": "complicated than that", "start": 3109.74, "duration": 3.8}, {"text": "um you can you can separately request", "start": 3111.24, "duration": 5.579}, {"text": "cores and memory and you are charged", "start": 3113.54, "duration": 6.64}, {"text": "that you'll be charged and", "start": 3116.819, "duration": 5.641}, {"text": "feel free you'll be charged in", "start": 3120.18, "duration": 4.98}, {"text": "proportion to the larger resource they", "start": 3122.46, "duration": 4.379}, {"text": "use so for example even if you only", "start": 3125.16, "duration": 4.8}, {"text": "request One Core", "start": 3126.839, "duration": 5.401}, {"text": "but you need half the memory on the Node", "start": 3129.96, "duration": 4.619}, {"text": "you you would be charged essentially for", "start": 3132.24, "duration": 4.859}, {"text": "using half of that note now what I did", "start": 3134.579, "duration": 4.861}, {"text": "mean to imply there though was that if", "start": 3137.099, "duration": 3.24}, {"text": "you", "start": 3139.44, "duration": 2.82}, {"text": "um if you do have a large memory", "start": 3140.339, "duration": 4.081}, {"text": "application and you're out in the regime", "start": 3142.26, "duration": 4.68}, {"text": "where the", "start": 3144.42, "duration": 4.5}, {"text": "um but what were you getting into", "start": 3146.94, "duration": 5.04}, {"text": "getting into lower efficiency that that", "start": 3148.92, "duration": 6.24}, {"text": "you may want to um let's say take", "start": 3151.98, "duration": 4.68}, {"text": "advantage of those cores that you're", "start": 3155.16, "duration": 4.08}, {"text": "essentially being charged for but we can", "start": 3156.66, "duration": 4.38}, {"text": "follow up on that", "start": 3159.24, "duration": 6.079}, {"text": "and then finally let me do a", "start": 3161.04, "duration": 4.279}, {"text": "this is a quick demo here", "start": 3166.079, "duration": 4.101}, {"text": "okay and Mary or semi can you confirm", "start": 3175.579, "duration": 5.561}, {"text": "that you're seeing my browser now", "start": 3178.5, "duration": 6.66}, {"text": "yes I can see your notebook okay so let", "start": 3181.14, "duration": 7.38}, {"text": "me just take this I'm going to", "start": 3185.16, "duration": 7.28}, {"text": "paste this into the chat", "start": 3188.52, "duration": 3.92}, {"text": "I meant to", "start": 3197.04, "duration": 6.42}, {"text": "meant to do this earlier", "start": 3200.22, "duration": 6.06}, {"text": "so so if you go to my repo I have a", "start": 3203.46, "duration": 4.8}, {"text": "Jupiter notebook", "start": 3206.28, "duration": 4.559}, {"text": "and then a few um a few figures that I", "start": 3208.26, "duration": 5.9}, {"text": "generated for this presentation", "start": 3210.839, "duration": 3.321}, {"text": "so if you're if you're familiar with", "start": 3214.92, "duration": 4.98}, {"text": "python you should be able to", "start": 3216.839, "duration": 6.061}, {"text": "um easily easily get my notebook to", "start": 3219.9, "duration": 4.8}, {"text": "generate your own", "start": 3222.9, "duration": 3.6}, {"text": "um your own scaling plots", "start": 3224.7, "duration": 2.94}, {"text": "um these are the ones that I want to", "start": 3226.5, "duration": 3.119}, {"text": "when you hit ignore that here I'm", "start": 3227.64, "duration": 3.78}, {"text": "showing that the scaling you're using", "start": 3229.619, "duration": 4.44}, {"text": "linear axes I scroll down a little bit", "start": 3231.42, "duration": 6.36}, {"text": "and I show the the proper scaling using", "start": 3234.059, "duration": 7.141}, {"text": "um using logarithmic to access these so", "start": 3237.78, "duration": 5.4}, {"text": "this was the figure they saw earlier for", "start": 3241.2, "duration": 4.74}, {"text": "a code with perfect scalability", "start": 3243.18, "duration": 5.7}, {"text": "and then a", "start": 3245.94, "duration": 5.1}, {"text": "a more realistic case than the code that", "start": 3248.88, "duration": 3.0}, {"text": "has", "start": 3251.04, "duration": 3.059}, {"text": "that has limited scalability", "start": 3251.88, "duration": 3.36}, {"text": "um if you want to play around with", "start": 3254.099, "duration": 3.841}, {"text": "omdols", "start": 3255.24, "duration": 4.44}, {"text": "um down here I have to code that I use", "start": 3257.94, "duration": 4.26}, {"text": "for for generating this plot and in fact", "start": 3259.68, "duration": 4.86}, {"text": "let's add one more", "start": 3262.2, "duration": 4.46}, {"text": "um", "start": 3264.54, "duration": 2.12}, {"text": "we'll add one more line in there", "start": 3268.74, "duration": 3.92}, {"text": "foreign", "start": 3274.74, "duration": 2.9}, {"text": "99.9 parallel content", "start": 3280.04, "duration": 4.38}, {"text": "yeah so if you want you can go in here", "start": 3304.5, "duration": 4.319}, {"text": "you can play around with um the", "start": 3306.599, "duration": 3.96}, {"text": "different ways of plotting a scaling", "start": 3308.819, "duration": 5.78}, {"text": "data and experiment with with all", "start": 3310.559, "duration": 4.04}, {"text": "I'm gonna go ahead and stop Sharon so", "start": 3315.9, "duration": 3.84}, {"text": "are there any questions before we wrap", "start": 3318.0, "duration": 3.92}, {"text": "up", "start": 3319.74, "duration": 2.18}, {"text": "all right if if not um thank you so much", "start": 3322.079, "duration": 3.961}, {"text": "for coming I'm going to hang things back", "start": 3324.3, "duration": 4.62}, {"text": "to back back to Mary Thomas to", "start": 3326.04, "duration": 5.96}, {"text": "um to close the session", "start": 3328.92, "duration": 3.08}]