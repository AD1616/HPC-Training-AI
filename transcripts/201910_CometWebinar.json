[{"text": "okay so my name is mine win and I'm the", "start": 0.179, "duration": 4.741}, {"text": "lead for data analytics here at the San", "start": 2.879, "duration": 4.741}, {"text": "Diego super computer center at UCSD", "start": 4.92, "duration": 5.82}, {"text": "uh and uh I'd like to thank you for", "start": 7.62, "duration": 6.019}, {"text": "joining us here today", "start": 10.74, "duration": 2.899}, {"text": "okay so here's the agenda", "start": 15.78, "duration": 4.14}, {"text": "um since", "start": 17.82, "duration": 3.66}, {"text": "deep learning models are essentially", "start": 19.92, "duration": 3.18}, {"text": "deep neural networks we're going to", "start": 21.48, "duration": 3.48}, {"text": "start out with some deep some neural", "start": 23.1, "duration": 3.66}, {"text": "network Basics and then we'll follow", "start": 24.96, "duration": 4.44}, {"text": "that up with uh fundamentals of deep", "start": 26.76, "duration": 4.8}, {"text": "learning and then we'll talk about", "start": 29.4, "duration": 4.62}, {"text": "transfer learning which is a technique", "start": 31.56, "duration": 5.7}, {"text": "used to speed up training and really has", "start": 34.02, "duration": 5.039}, {"text": "been a key factor in the success and", "start": 37.26, "duration": 5.28}, {"text": "Adoption of deep learning models", "start": 39.059, "duration": 5.401}, {"text": "okay so to start out with we should", "start": 42.54, "duration": 3.96}, {"text": "state that deep learning is a subfield", "start": 44.46, "duration": 4.259}, {"text": "of machine learning it's not a different", "start": 46.5, "duration": 3.84}, {"text": "thing it's not a different area it is a", "start": 48.719, "duration": 3.601}, {"text": "subfield of machine learning which in", "start": 50.34, "duration": 3.36}, {"text": "itself is a subfield of artificial", "start": 52.32, "duration": 2.759}, {"text": "intelligence", "start": 53.7, "duration": 3.6}, {"text": "and key to machine learning is the idea", "start": 55.079, "duration": 4.261}, {"text": "that you can build systems that can", "start": 57.3, "duration": 4.14}, {"text": "learn from data without being explicitly", "start": 59.34, "duration": 3.359}, {"text": "programmed", "start": 61.44, "duration": 3.84}, {"text": "deep learning then also focuses on", "start": 62.699, "duration": 4.441}, {"text": "systems that are data driven", "start": 65.28, "duration": 3.78}, {"text": "and that can discover hidden patterns", "start": 67.14, "duration": 3.659}, {"text": "and Trends in the data automatically", "start": 69.06, "duration": 5.18}, {"text": "through the training process", "start": 70.799, "duration": 3.441}, {"text": "and the Deep refers to the deep and deep", "start": 76.26, "duration": 5.399}, {"text": "learning refers to the many layers", "start": 79.26, "duration": 4.92}, {"text": "of interconnected processing units in a", "start": 81.659, "duration": 5.1}, {"text": "model so a conventional so-called", "start": 84.18, "duration": 5.16}, {"text": "shallow neural network what you see on", "start": 86.759, "duration": 3.72}, {"text": "the left there", "start": 89.34, "duration": 3.72}, {"text": "typically consists of only a few layers", "start": 90.479, "duration": 4.261}, {"text": "a deep model on the other hand that", "start": 93.06, "duration": 4.199}, {"text": "that's shown on the right can have many", "start": 94.74, "duration": 4.1}, {"text": "many more layers", "start": 97.259, "duration": 5.521}, {"text": "tens or even hundreds of layers and", "start": 98.84, "duration": 5.56}, {"text": "these many layers allow for the Deep", "start": 102.78, "duration": 3.42}, {"text": "learning model to learn representations", "start": 104.4, "duration": 4.859}, {"text": "of the data at multiple and increasingly", "start": 106.2, "duration": 5.699}, {"text": "complex levels of abstraction and this", "start": 109.259, "duration": 4.741}, {"text": "is what gives deep learning models their", "start": 111.899, "duration": 4.341}, {"text": "power", "start": 114.0, "duration": 2.24}, {"text": "and here is a", "start": 117.06, "duration": 3.9}, {"text": "partial list of applications of deep", "start": 119.1, "duration": 2.839}, {"text": "learning", "start": 120.96, "duration": 3.18}, {"text": "there's many many there are many many", "start": 121.939, "duration": 5.081}, {"text": "more applications so really any time", "start": 124.14, "duration": 4.679}, {"text": "that you have a problem that can be", "start": 127.02, "duration": 4.98}, {"text": "posed in a pattern recognition context", "start": 128.819, "duration": 7.14}, {"text": "you can apply deep learning to it", "start": 132.0, "duration": 5.52}, {"text": "okay so let's just start with some", "start": 135.959, "duration": 3.481}, {"text": "neural network basics", "start": 137.52, "duration": 5.16}, {"text": "this is the basic structure of a neural", "start": 139.44, "duration": 4.019}, {"text": "network", "start": 142.68, "duration": 3.54}, {"text": "so you have a hidden layer you have an", "start": 143.459, "duration": 5.881}, {"text": "input layer usually one sometimes two or", "start": 146.22, "duration": 4.739}, {"text": "three hidden layers and then an output", "start": 149.34, "duration": 2.52}, {"text": "layer", "start": 150.959, "duration": 3.601}, {"text": "so the hidden the the input layer is", "start": 151.86, "duration": 4.92}, {"text": "where the input data gets fed into the", "start": 154.56, "duration": 3.06}, {"text": "model", "start": 156.78, "duration": 2.64}, {"text": "the output layer is where the model", "start": 157.62, "duration": 3.66}, {"text": "outputs its predictions", "start": 159.42, "duration": 3.74}, {"text": "the hidden layer is usually anything", "start": 161.28, "duration": 5.099}, {"text": "between the the input and the output and", "start": 163.16, "duration": 4.719}, {"text": "the job of the Hidden layer is to", "start": 166.379, "duration": 3.901}, {"text": "transform the input into something that", "start": 167.879, "duration": 4.5}, {"text": "the output layer can use that is", "start": 170.28, "duration": 3.539}, {"text": "relevant for the task that the model is", "start": 172.379, "duration": 4.101}, {"text": "being trained on", "start": 173.819, "duration": 5.7}, {"text": "so a layer consists of several", "start": 176.48, "duration": 4.66}, {"text": "processing units in the processing units", "start": 179.519, "duration": 5.421}, {"text": "here are specified by the by the circles", "start": 181.14, "duration": 6.12}, {"text": "processing units are connected by", "start": 184.94, "duration": 4.9}, {"text": "weights and the weights are the", "start": 187.26, "duration": 4.199}, {"text": "parameters of the model", "start": 189.84, "duration": 4.14}, {"text": "and in a conventional shallow neural", "start": 191.459, "duration": 4.321}, {"text": "network the layers are fully connected", "start": 193.98, "duration": 3.96}, {"text": "meaning that each", "start": 195.78, "duration": 5.34}, {"text": "processing unit is connected to every", "start": 197.94, "duration": 5.28}, {"text": "Processing Unit in the next layer as", "start": 201.12, "duration": 5.119}, {"text": "shown as shown in this diagram", "start": 203.22, "duration": 6.9}, {"text": "so the neural network learns the mapping", "start": 206.239, "duration": 6.041}, {"text": "from the input and output based on the", "start": 210.12, "duration": 5.36}, {"text": "training data that it is given", "start": 212.28, "duration": 3.2}, {"text": "okay so here's a high level overview of", "start": 215.58, "duration": 5.159}, {"text": "what happens in a neural network so each", "start": 217.739, "duration": 5.821}, {"text": "unit processes its input to generate an", "start": 220.739, "duration": 4.981}, {"text": "output and an output is often referred", "start": 223.56, "duration": 4.319}, {"text": "to as an activation", "start": 225.72, "duration": 4.62}, {"text": "these activations then are propagated", "start": 227.879, "duration": 4.14}, {"text": "through all the layers", "start": 230.34, "duration": 3.6}, {"text": "and the activations at the output layer", "start": 232.019, "duration": 5.22}, {"text": "are the model's predictions", "start": 233.94, "duration": 6.54}, {"text": "the models outputs are compared to the", "start": 237.239, "duration": 4.801}, {"text": "ground truth Target", "start": 240.48, "duration": 3.6}, {"text": "and the errors between the outputs and", "start": 242.04, "duration": 4.38}, {"text": "the targets are computed using some sort", "start": 244.08, "duration": 3.78}, {"text": "of loss function", "start": 246.42, "duration": 3.899}, {"text": "and the model's weights are adjusted to", "start": 247.86, "duration": 4.079}, {"text": "minimize this loss function during the", "start": 250.319, "duration": 4.221}, {"text": "training process", "start": 251.939, "duration": 2.601}, {"text": "so all of the processing is done by", "start": 254.78, "duration": 4.359}, {"text": "these processing units so let's take a", "start": 257.579, "duration": 3.301}, {"text": "closer look at what happens inside a", "start": 259.139, "duration": 3.361}, {"text": "processing unit", "start": 260.88, "duration": 5.039}, {"text": "so here you have the processing unit the", "start": 262.5, "duration": 7.02}, {"text": "X's are its inputs and the W's are its", "start": 265.919, "duration": 5.101}, {"text": "weights right so the W's are the", "start": 269.52, "duration": 4.38}, {"text": "parameters of the neural network", "start": 271.02, "duration": 5.459}, {"text": "so this the steps that are performed by", "start": 273.9, "duration": 6.12}, {"text": "each processing unit is is as follows it", "start": 276.479, "duration": 6.841}, {"text": "first computes the dot product of inputs", "start": 280.02, "duration": 6.239}, {"text": "and the weights and then it adds a bias", "start": 283.32, "duration": 4.92}, {"text": "and a bias is just another parameter of", "start": 286.259, "duration": 3.961}, {"text": "the processing unit", "start": 288.24, "duration": 4.86}, {"text": "and then an activation function is", "start": 290.22, "duration": 5.52}, {"text": "applied so this F here is the activation", "start": 293.1, "duration": 4.8}, {"text": "function that's applied to the dot", "start": 295.74, "duration": 4.62}, {"text": "product with the inputs and the weights", "start": 297.9, "duration": 5.4}, {"text": "and then this output or the activation", "start": 300.36, "duration": 5.48}, {"text": "then is fed to the next layer of units", "start": 303.3, "duration": 5.28}, {"text": "and also to note is that processing unit", "start": 305.84, "duration": 5.62}, {"text": "is also referred to as node and I'll use", "start": 308.58, "duration": 6.38}, {"text": "those two terms interchangeably", "start": 311.46, "duration": 3.5}, {"text": "okay so the activation converts the", "start": 315.72, "duration": 5.72}, {"text": "input of the node to its output", "start": 318.419, "duration": 6.961}, {"text": "and to capture any complex arbitrary", "start": 321.44, "duration": 5.38}, {"text": "relationship between the input and", "start": 325.38, "duration": 3.72}, {"text": "output the activation function needs to", "start": 326.82, "duration": 3.96}, {"text": "be non-linear", "start": 329.1, "duration": 4.039}, {"text": "and it also needs to be differentiable", "start": 330.78, "duration": 4.979}, {"text": "in order to propagate the loss through", "start": 333.139, "duration": 4.301}, {"text": "the model in order to adjust the weights", "start": 335.759, "duration": 4.641}, {"text": "during the training process", "start": 337.44, "duration": 2.96}, {"text": "and here are some commonly used", "start": 340.74, "duration": 3.959}, {"text": "activation functions you have the", "start": 342.3, "duration": 5.58}, {"text": "sigmoid function in the upper left tan H", "start": 344.699, "duration": 6.301}, {"text": "or hyperbolic tangent is shown on the", "start": 347.88, "duration": 4.94}, {"text": "upper right", "start": 351.0, "duration": 5.34}, {"text": "relu is on the lower left and releus", "start": 352.82, "duration": 5.92}, {"text": "stands for rectified linear unit and", "start": 356.34, "duration": 3.919}, {"text": "then there's a soft Max", "start": 358.74, "duration": 4.28}, {"text": "activation function that's shown on the", "start": 360.259, "duration": 4.601}, {"text": "lower right", "start": 363.02, "duration": 3.76}, {"text": "so these are commonly used activation", "start": 364.86, "duration": 3.779}, {"text": "functions in a neural network as well as", "start": 366.78, "duration": 5.78}, {"text": "in a deep learning model", "start": 368.639, "duration": 3.921}, {"text": "okay so another important component of a", "start": 373.56, "duration": 5.4}, {"text": "neural network is the loss function", "start": 376.5, "duration": 4.979}, {"text": "so the last function allows for the", "start": 378.96, "duration": 4.739}, {"text": "model's parameters to be evaluated", "start": 381.479, "duration": 4.741}, {"text": "so it quantifies the agreement between", "start": 383.699, "duration": 4.681}, {"text": "what the model is predicting and the", "start": 386.22, "duration": 5.039}, {"text": "ground truth Target it provides a single", "start": 388.38, "duration": 4.56}, {"text": "scale of value that captures the", "start": 391.259, "duration": 3.0}, {"text": "performance of the model which can be", "start": 392.94, "duration": 4.56}, {"text": "very complex and it also allows for", "start": 394.259, "duration": 4.621}, {"text": "candidate Solutions in other words", "start": 397.5, "duration": 3.72}, {"text": "different sets of model weights to be", "start": 398.88, "duration": 4.379}, {"text": "ranked and compared", "start": 401.22, "duration": 3.9}, {"text": "and the last function is used to", "start": 403.259, "duration": 3.421}, {"text": "optimize the model parameters during", "start": 405.12, "duration": 4.139}, {"text": "training and other names that the loss", "start": 406.68, "duration": 5.4}, {"text": "function go goes by would be a cost", "start": 409.259, "duration": 6.5}, {"text": "function or objective function", "start": 412.08, "duration": 3.679}, {"text": "and here are some commonly used loss", "start": 415.8, "duration": 3.8}, {"text": "functions there's cross entropy which", "start": 417.6, "duration": 4.08}, {"text": "computes the average of differences", "start": 419.6, "duration": 4.74}, {"text": "between what the model predicts", "start": 421.68, "duration": 5.1}, {"text": "the the predicted and the target", "start": 424.34, "duration": 4.479}, {"text": "probability distributions", "start": 426.78, "duration": 5.1}, {"text": "the mean squared error is the average of", "start": 428.819, "duration": 5.461}, {"text": "the squared differences between the", "start": 431.88, "duration": 3.96}, {"text": "model's predictions and the ground truth", "start": 434.28, "duration": 4.38}, {"text": "targets and then mean absolute error is", "start": 435.84, "duration": 5.639}, {"text": "the absolute error of that so all three", "start": 438.66, "duration": 4.5}, {"text": "of these can be used for classification", "start": 441.479, "duration": 4.321}, {"text": "although the the cross entropy is", "start": 443.16, "duration": 4.439}, {"text": "probably the most commonly used one for", "start": 445.8, "duration": 4.019}, {"text": "classification and in the last two are", "start": 447.599, "duration": 5.781}, {"text": "commonly used for regression problems", "start": 449.819, "duration": 3.561}, {"text": "okay so the loss function is used to", "start": 456.0, "duration": 4.5}, {"text": "optimize the model's parameters", "start": 458.039, "duration": 4.5}, {"text": "um so what does this mean the loss", "start": 460.5, "duration": 4.259}, {"text": "function is is parameterized by the", "start": 462.539, "duration": 3.66}, {"text": "model's weights that means the loss", "start": 464.759, "duration": 3.0}, {"text": "motion is a function of the model's", "start": 466.199, "duration": 2.28}, {"text": "weight", "start": 467.759, "duration": 3.541}, {"text": "so by adjusting the weights we can", "start": 468.479, "duration": 5.701}, {"text": "change the value of the loss function", "start": 471.3, "duration": 4.739}, {"text": "so the goal of chaining the model is to", "start": 474.18, "duration": 3.359}, {"text": "optimize the weights in order to", "start": 476.039, "duration": 4.201}, {"text": "minimize the loss function", "start": 477.539, "duration": 4.5}, {"text": "now the challenge with this is that", "start": 480.24, "duration": 4.5}, {"text": "finding the best set of Weights is", "start": 482.039, "duration": 4.801}, {"text": "really really difficult and and can be", "start": 484.74, "duration": 6.06}, {"text": "impossible with real world problems or", "start": 486.84, "duration": 6.0}, {"text": "really any problem with any complexity", "start": 490.8, "duration": 3.839}, {"text": "and this is due to the high", "start": 492.84, "duration": 3.9}, {"text": "dimensionality and the complexity of the", "start": 494.639, "duration": 3.541}, {"text": "parameter space", "start": 496.74, "duration": 4.32}, {"text": "so the approach is to use an iterative", "start": 498.18, "duration": 4.799}, {"text": "algorithm", "start": 501.06, "duration": 4.919}, {"text": "so we can iterative we can iteratively", "start": 502.979, "duration": 5.4}, {"text": "adjust the model's weights to lower the", "start": 505.979, "duration": 4.741}, {"text": "loss function so at every iteration you", "start": 508.379, "duration": 4.681}, {"text": "know you want to the loss function value", "start": 510.72, "duration": 4.999}, {"text": "to be lower than it was the last time", "start": 513.06, "duration": 4.8}, {"text": "and this is what happens during model", "start": 515.719, "duration": 4.62}, {"text": "training", "start": 517.86, "duration": 2.479}, {"text": "uh one way to perform this iterative", "start": 521.459, "duration": 4.981}, {"text": "approach to optimization is with a", "start": 524.399, "duration": 4.5}, {"text": "gradient descent method and here is an", "start": 526.44, "duration": 5.22}, {"text": "illustration of uh using the gradient", "start": 528.899, "duration": 4.741}, {"text": "descent method to optimize the model's", "start": 531.66, "duration": 3.0}, {"text": "parameters", "start": 533.64, "duration": 3.18}, {"text": "so given a set of Weights your initial", "start": 534.66, "duration": 3.799}, {"text": "weights or your current set of Weights", "start": 536.82, "duration": 4.56}, {"text": "you want to compute the gradient", "start": 538.459, "duration": 4.601}, {"text": "of the loss function so the loss", "start": 541.38, "duration": 5.16}, {"text": "function is shown in the uh as the the", "start": 543.06, "duration": 7.56}, {"text": "orange here the gradient is tangent to", "start": 546.54, "duration": 6.799}, {"text": "that so you want to find the gradient", "start": 550.62, "duration": 6.96}, {"text": "and that is the the direction of", "start": 553.339, "duration": 6.161}, {"text": "sequence decline that you want to change", "start": 557.58, "duration": 3.24}, {"text": "your weights you want to move the", "start": 559.5, "duration": 3.24}, {"text": "weights down in that direction", "start": 560.82, "duration": 4.38}, {"text": "and the amount of changes is determined", "start": 562.74, "duration": 5.159}, {"text": "by a learning rate so that determines", "start": 565.2, "duration": 4.68}, {"text": "how big of a step we want to move the", "start": 567.899, "duration": 5.181}, {"text": "weights in the gradient Direction", "start": 569.88, "duration": 3.2}, {"text": "so this is looking at it if you have", "start": 573.12, "duration": 6.24}, {"text": "just two parameters just two weights", "start": 575.76, "duration": 6.54}, {"text": "um and if you can imagine that you have", "start": 579.36, "duration": 4.979}, {"text": "a bowl right your air surface or the", "start": 582.3, "duration": 3.599}, {"text": "what the loss function looks like is", "start": 584.339, "duration": 3.781}, {"text": "like a bull and if you start out you", "start": 585.899, "duration": 4.081}, {"text": "might be on the side of the bowl and", "start": 588.12, "duration": 3.6}, {"text": "what you want to do is you want to end", "start": 589.98, "duration": 3.24}, {"text": "up at the end of training you want to", "start": 591.72, "duration": 3.299}, {"text": "end up at the bottom of that bowl", "start": 593.22, "duration": 4.38}, {"text": "so if you take the gradient then that is", "start": 595.019, "duration": 4.38}, {"text": "the direction of steepest Decline and", "start": 597.6, "duration": 3.06}, {"text": "that's the direction that you want to", "start": 599.399, "duration": 3.781}, {"text": "move your weights in now this is a very", "start": 600.66, "duration": 5.88}, {"text": "well behaved and very small error", "start": 603.18, "duration": 5.64}, {"text": "function air surface but if you can", "start": 606.54, "duration": 3.6}, {"text": "imagine", "start": 608.82, "duration": 2.88}, {"text": "you know in a neural network you can", "start": 610.14, "duration": 4.98}, {"text": "have tens or hundreds of parameters of", "start": 611.7, "duration": 5.94}, {"text": "dimensions and then in a deep neural", "start": 615.12, "duration": 4.2}, {"text": "network you can have you know up to", "start": 617.64, "duration": 4.259}, {"text": "millions of parameters so this surface", "start": 619.32, "duration": 5.639}, {"text": "can look extremely rough and you can", "start": 621.899, "duration": 5.401}, {"text": "have a lot of local Minima so the", "start": 624.959, "duration": 4.801}, {"text": "process of finding the minimum is", "start": 627.3, "duration": 4.26}, {"text": "extremely difficult it's an extremely", "start": 629.76, "duration": 4.34}, {"text": "difficult problem", "start": 631.56, "duration": 2.54}, {"text": "okay so there are different ways to do", "start": 634.8, "duration": 4.44}, {"text": "gradient descent there's batch gradient", "start": 636.959, "duration": 3.661}, {"text": "descent", "start": 639.24, "duration": 3.0}, {"text": "um in which you compute the gradient", "start": 640.62, "duration": 4.32}, {"text": "over the entire training data set but if", "start": 642.24, "duration": 5.159}, {"text": "you have a really really large training", "start": 644.94, "duration": 3.54}, {"text": "data set", "start": 647.399, "duration": 3.06}, {"text": "um that's a lot of processing time to", "start": 648.48, "duration": 5.28}, {"text": "move just you know in one step and so", "start": 650.459, "duration": 5.641}, {"text": "you can also do stochastic gradient", "start": 653.76, "duration": 5.22}, {"text": "descent and with stochastic gradient", "start": 656.1, "duration": 4.979}, {"text": "descent you compute the gradient over a", "start": 658.98, "duration": 4.08}, {"text": "single chaining sample so you move you", "start": 661.079, "duration": 3.601}, {"text": "adjust the weights after just a single", "start": 663.06, "duration": 5.64}, {"text": "chain example so those are two extremes", "start": 664.68, "duration": 6.48}, {"text": "um you can also do mini batch gradient", "start": 668.7, "duration": 4.62}, {"text": "descent and this is Computing the", "start": 671.16, "duration": 5.04}, {"text": "gradient over batches or subsets of the", "start": 673.32, "duration": 6.66}, {"text": "training data and the batch size in", "start": 676.2, "duration": 5.639}, {"text": "practice is based on memory constraints", "start": 679.98, "duration": 3.96}, {"text": "and it can also be the value of the", "start": 681.839, "duration": 3.601}, {"text": "batch size can be determined through", "start": 683.94, "duration": 3.139}, {"text": "experimental", "start": 685.44, "duration": 4.44}, {"text": "experimental runs", "start": 687.079, "duration": 3.76}, {"text": "um", "start": 689.88, "duration": 3.6}, {"text": "so note though that so usually in", "start": 690.839, "duration": 4.74}, {"text": "practice people do midi batch grading", "start": 693.48, "duration": 5.22}, {"text": "descent but you'll see that people refer", "start": 695.579, "duration": 5.101}, {"text": "to this as the cast ingredient descent", "start": 698.7, "duration": 6.36}, {"text": "or SGD for short but just know that SGD", "start": 700.68, "duration": 6.36}, {"text": "often is used to mean mini batch", "start": 705.06, "duration": 3.36}, {"text": "screening descent so if you have a batch", "start": 707.04, "duration": 3.419}, {"text": "size greater than one then you're doing", "start": 708.42, "duration": 5.96}, {"text": "Mini battery in descent", "start": 710.459, "duration": 3.921}, {"text": "okay so now that we know some basics of", "start": 718.2, "duration": 4.079}, {"text": "neural networks let's talk about some", "start": 720.06, "duration": 5.399}, {"text": "basics of of date models we'll start out", "start": 722.279, "duration": 5.041}, {"text": "by talking about the different types of", "start": 725.459, "duration": 4.021}, {"text": "layers that you can find in a deep model", "start": 727.32, "duration": 4.74}, {"text": "so as I mentioned before conventional", "start": 729.48, "duration": 4.919}, {"text": "neural networks so-called shallow neural", "start": 732.06, "duration": 5.1}, {"text": "networks have only a few layers in", "start": 734.399, "duration": 4.62}, {"text": "addition the layers of fully connected", "start": 737.16, "duration": 4.98}, {"text": "meaning that each processing unit is", "start": 739.019, "duration": 5.461}, {"text": "connected to all processing units in the", "start": 742.14, "duration": 3.36}, {"text": "next layer", "start": 744.48, "duration": 3.24}, {"text": "so deep neural networks on the other", "start": 745.5, "duration": 4.56}, {"text": "hand can have many many layers and", "start": 747.72, "duration": 3.96}, {"text": "typically they have several types", "start": 750.06, "duration": 4.86}, {"text": "several different types of layers", "start": 751.68, "duration": 5.94}, {"text": "okay so here's the general", "start": 754.92, "duration": 4.8}, {"text": "structure of a deep Network", "start": 757.62, "duration": 4.86}, {"text": "so it has a sequence of layers not just", "start": 759.72, "duration": 6.66}, {"text": "uh it has a sequence of layers and it", "start": 762.48, "duration": 5.58}, {"text": "has many different types of layers not", "start": 766.38, "duration": 3.78}, {"text": "just the fully connected ones that we", "start": 768.06, "duration": 4.459}, {"text": "saw with the neural network", "start": 770.16, "duration": 5.34}, {"text": "and each layer transforms its input to", "start": 772.519, "duration": 5.081}, {"text": "generate an output through a non-linear", "start": 775.5, "duration": 4.139}, {"text": "function just like with the conventional", "start": 777.6, "duration": 4.58}, {"text": "neural network", "start": 779.639, "duration": 2.541}, {"text": "okay so one type of layer that is", "start": 783.899, "duration": 5.701}, {"text": "commonly used is a convolutional layer", "start": 787.2, "duration": 4.68}, {"text": "and this forms the core building block", "start": 789.6, "duration": 4.58}, {"text": "of many deep models", "start": 791.88, "duration": 4.68}, {"text": "a convolutional layer performs", "start": 794.18, "duration": 5.74}, {"text": "operations on the input using what's", "start": 796.56, "duration": 5.579}, {"text": "called convolutional filters", "start": 799.92, "duration": 4.14}, {"text": "so these filters have parameters that", "start": 802.139, "duration": 4.32}, {"text": "are adjusted during training and they", "start": 804.06, "duration": 4.32}, {"text": "learn to detect features in the in the", "start": 806.459, "duration": 4.261}, {"text": "input that are important for the task at", "start": 808.38, "duration": 4.8}, {"text": "hand so so far that sounds like your", "start": 810.72, "duration": 4.859}, {"text": "conventional neural networks so one of", "start": 813.18, "duration": 4.26}, {"text": "the key differences between a deep", "start": 815.579, "duration": 3.181}, {"text": "neural network", "start": 817.44, "duration": 4.079}, {"text": "uh between a fully connected layer in a", "start": 818.76, "duration": 6.12}, {"text": "deep neural network is that the filter", "start": 821.519, "duration": 6.06}, {"text": "operates on only a local region of the", "start": 824.88, "duration": 4.74}, {"text": "input right as shown in this picture", "start": 827.579, "duration": 4.2}, {"text": "here so you have we have here the filter", "start": 829.62, "duration": 3.719}, {"text": "shown in blue and that's a three by", "start": 831.779, "duration": 4.8}, {"text": "three filter so it only operates on a", "start": 833.339, "duration": 5.641}, {"text": "three by three region in the input", "start": 836.579, "duration": 5.341}, {"text": "and then the convolution operation is", "start": 838.98, "duration": 5.52}, {"text": "performed by sliding the filter over the", "start": 841.92, "duration": 3.419}, {"text": "input", "start": 844.5, "duration": 3.139}, {"text": "and the output of the convolution filter", "start": 845.339, "duration": 4.921}, {"text": "is a feature map that's shown in the", "start": 847.639, "duration": 5.2}, {"text": "kind of pink purple color", "start": 850.26, "duration": 4.68}, {"text": "and that specifies", "start": 852.839, "duration": 6.06}, {"text": "the feature map specifies the feature", "start": 854.94, "duration": 6.36}, {"text": "that this filter is detecting and how it", "start": 858.899, "duration": 7.041}, {"text": "detects that feature across the input", "start": 861.3, "duration": 4.64}, {"text": "all right so let's take a look at the", "start": 866.7, "duration": 4.759}, {"text": "convolution operation because that's key", "start": 869.519, "duration": 5.701}, {"text": "to the convolutional layer and key to a", "start": 871.459, "duration": 6.581}, {"text": "lot of deep learning models", "start": 875.22, "duration": 4.919}, {"text": "so each filter performs a convolution", "start": 878.04, "duration": 4.44}, {"text": "operation over the input what we see", "start": 880.139, "duration": 4.621}, {"text": "here is a three by three filter the", "start": 882.48, "duration": 4.859}, {"text": "numbers in the filter are its parameters", "start": 884.76, "duration": 4.22}, {"text": "or its weights", "start": 887.339, "duration": 5.341}, {"text": "so on the left here you see a an", "start": 888.98, "duration": 6.52}, {"text": "animation of how the convolution filter", "start": 892.68, "duration": 6.12}, {"text": "kind of processes each patch of input", "start": 895.5, "duration": 6.6}, {"text": "and then it slides over the input", "start": 898.8, "duration": 5.099}, {"text": "um", "start": 902.1, "duration": 5.46}, {"text": "so what happens on the right is what", "start": 903.899, "duration": 5.341}, {"text": "happens in the first step", "start": 907.56, "duration": 3.779}, {"text": "so what we see here is that the", "start": 909.24, "duration": 4.76}, {"text": "convolution operation processes on", "start": 911.339, "duration": 5.281}, {"text": "processes that patch in the upper left", "start": 914.0, "duration": 6.459}, {"text": "of the input and it just uh computes the", "start": 916.62, "duration": 5.94}, {"text": "dot product of its weights and the", "start": 920.459, "duration": 3.5}, {"text": "values in the input", "start": 922.56, "duration": 5.279}, {"text": "so when it processes that first patch of", "start": 923.959, "duration": 6.101}, {"text": "input the output is a four right because", "start": 927.839, "duration": 5.101}, {"text": "you're only summing up what's in the", "start": 930.06, "duration": 4.74}, {"text": "four corners and in the center so for", "start": 932.94, "duration": 4.38}, {"text": "this patch the output is a four and that", "start": 934.8, "duration": 4.38}, {"text": "4 goes into the feature map in the upper", "start": 937.32, "duration": 3.8}, {"text": "left hand corner", "start": 939.18, "duration": 4.88}, {"text": "then the second step", "start": 941.12, "duration": 6.459}, {"text": "the filter will slide over and processes", "start": 944.06, "duration": 6.519}, {"text": "that next batch that next patch in the", "start": 947.579, "duration": 7.26}, {"text": "input and when we when the filter", "start": 950.579, "duration": 7.081}, {"text": "processes that next patch of input then", "start": 954.839, "duration": 4.5}, {"text": "the output is a three and that goes into", "start": 957.66, "duration": 2.94}, {"text": "the future map", "start": 959.339, "duration": 4.261}, {"text": "right and so it repeats this process", "start": 960.6, "duration": 5.58}, {"text": "until it gets until it covers the entire", "start": 963.6, "duration": 6.599}, {"text": "input and then the feature map is filled", "start": 966.18, "duration": 6.719}, {"text": "so you can see in the left there how the", "start": 970.199, "duration": 5.76}, {"text": "filter slides over the input in order to", "start": 972.899, "duration": 5.761}, {"text": "create an output in the feature map okay", "start": 975.959, "duration": 4.921}, {"text": "so that's how a convolution operation", "start": 978.66, "duration": 4.7}, {"text": "works", "start": 980.88, "duration": 2.48}, {"text": "um a convolutional layer", "start": 987.36, "duration": 3.719}, {"text": "then", "start": 989.279, "duration": 3.661}, {"text": "is um", "start": 991.079, "duration": 4.801}, {"text": "made up of many of these filters and the", "start": 992.94, "duration": 4.319}, {"text": "convolutional layer performs the", "start": 995.88, "duration": 3.3}, {"text": "convolution on the input", "start": 997.259, "duration": 7.38}, {"text": "and oftentimes convolutional layers are", "start": 999.18, "duration": 7.019}, {"text": "um", "start": 1004.639, "duration": 4.94}, {"text": "applied to multi-dimensional input", "start": 1006.199, "duration": 5.58}, {"text": "such as an image", "start": 1009.579, "duration": 5.081}, {"text": "so an image so the input would be a", "start": 1011.779, "duration": 5.101}, {"text": "volume of size height times weight times", "start": 1014.66, "duration": 4.34}, {"text": "channels and for an image that would be", "start": 1016.88, "duration": 5.28}, {"text": "three for red green and blue channels", "start": 1019.0, "duration": 5.199}, {"text": "so the convolutional layer performs a", "start": 1022.16, "duration": 3.6}, {"text": "convolution on the input volume with", "start": 1024.199, "duration": 2.701}, {"text": "these filters", "start": 1025.76, "duration": 3.78}, {"text": "and each filter again is connected to", "start": 1026.9, "duration": 5.039}, {"text": "just a local region in an input as shown", "start": 1029.54, "duration": 4.2}, {"text": "in this diagram", "start": 1031.939, "duration": 6.061}, {"text": "so each filter is a circle and it only", "start": 1033.74, "duration": 7.52}, {"text": "operates on a local region at a time", "start": 1038.0, "duration": 6.66}, {"text": "and the a convolutional layer can have", "start": 1041.26, "duration": 5.98}, {"text": "more than one filters and it often has", "start": 1044.66, "duration": 5.1}, {"text": "many filters and the number of filters", "start": 1047.24, "duration": 4.5}, {"text": "in the convolution is referred to as the", "start": 1049.76, "duration": 4.62}, {"text": "depth of the convolution layer and then", "start": 1051.74, "duration": 4.2}, {"text": "the result of the convolution is passed", "start": 1054.38, "duration": 3.24}, {"text": "through a non-linear activation function", "start": 1055.94, "duration": 4.58}, {"text": "just as a processing unit in the", "start": 1057.62, "duration": 4.919}, {"text": "conventional neural network that we saw", "start": 1060.52, "duration": 4.2}, {"text": "before", "start": 1062.539, "duration": 2.181}, {"text": "okay so then", "start": 1067.039, "duration": 2.701}, {"text": "um", "start": 1068.78, "duration": 3.06}, {"text": "these are some parameters of a", "start": 1069.74, "duration": 3.6}, {"text": "convolutional", "start": 1071.84, "duration": 4.32}, {"text": "filter you have the filter size and that", "start": 1073.34, "duration": 5.0}, {"text": "that's the receptive field of the filter", "start": 1076.16, "duration": 4.74}, {"text": "so that's how many pixels or how many", "start": 1078.34, "duration": 4.719}, {"text": "values it operates on and so for this", "start": 1080.9, "duration": 4.08}, {"text": "diagram here the filters are three by", "start": 1083.059, "duration": 3.661}, {"text": "three so it operates on a three by three", "start": 1084.98, "duration": 4.5}, {"text": "patch of the input The Stride is the", "start": 1086.72, "duration": 5.04}, {"text": "sliding amount that is the number of", "start": 1089.48, "duration": 4.26}, {"text": "pixels by which the filter is moved over", "start": 1091.76, "duration": 5.1}, {"text": "the image as it processes the input", "start": 1093.74, "duration": 6.36}, {"text": "the padding you can have padding around", "start": 1096.86, "duration": 5.22}, {"text": "the input volume so that your input in", "start": 1100.1, "duration": 3.84}, {"text": "the in the output feature map has the", "start": 1102.08, "duration": 3.06}, {"text": "same size", "start": 1103.94, "duration": 3.239}, {"text": "and again the number of filters is", "start": 1105.14, "duration": 5.6}, {"text": "referred to as the depth of the filter", "start": 1107.179, "duration": 3.561}, {"text": "okay so that's a convolutional layer", "start": 1111.679, "duration": 4.521}, {"text": "another", "start": 1113.48, "duration": 2.72}, {"text": "layer that is commonly used is the", "start": 1116.96, "duration": 4.94}, {"text": "pooling layer", "start": 1119.9, "duration": 4.56}, {"text": "the pulling layer performs a down", "start": 1121.9, "duration": 5.38}, {"text": "sampling operation that reduces the", "start": 1124.46, "duration": 6.3}, {"text": "spatial size of the input so and this", "start": 1127.28, "duration": 7.62}, {"text": "example we have a two by two filter in", "start": 1130.76, "duration": 7.56}, {"text": "the in the pooling layer and the", "start": 1134.9, "duration": 5.1}, {"text": "operation in this pulling layer is the", "start": 1138.32, "duration": 4.56}, {"text": "max operation so if you apply this", "start": 1140.0, "duration": 6.38}, {"text": "filter to the pink patch in the input", "start": 1142.88, "duration": 5.76}, {"text": "then the output would be six because", "start": 1146.38, "duration": 4.48}, {"text": "that's the max number", "start": 1148.64, "duration": 4.2}, {"text": "um and then if you slide it over with a", "start": 1150.86, "duration": 3.72}, {"text": "stride of two so you're sliding over two", "start": 1152.84, "duration": 5.579}, {"text": "values or two pixels the max operation", "start": 1154.58, "duration": 6.479}, {"text": "there where we turn an eight and then", "start": 1158.419, "duration": 5.101}, {"text": "processing the yellow patch would give a", "start": 1161.059, "duration": 4.5}, {"text": "three and then processing the blue patch", "start": 1163.52, "duration": 4.62}, {"text": "would give a four and so the output of", "start": 1165.559, "duration": 6.061}, {"text": "this pulling layer would be the results", "start": 1168.14, "duration": 7.039}, {"text": "here shown on the on the right", "start": 1171.62, "duration": 3.559}, {"text": "so for", "start": 1176.059, "duration": 4.681}, {"text": "um an input volume pulling is performed", "start": 1177.74, "duration": 4.74}, {"text": "independently on every slice of the", "start": 1180.74, "duration": 2.939}, {"text": "input volume", "start": 1182.48, "duration": 3.96}, {"text": "now the max operator is most often we", "start": 1183.679, "duration": 5.221}, {"text": "used in the pulling layer but you can", "start": 1186.44, "duration": 6.02}, {"text": "also use average pooling as well", "start": 1188.9, "duration": 3.56}, {"text": "okay and deep models can also have fully", "start": 1194.539, "duration": 3.961}, {"text": "connected layers just like in the", "start": 1196.94, "duration": 3.32}, {"text": "conventional neural network", "start": 1198.5, "duration": 5.34}, {"text": "so this means that every input is", "start": 1200.26, "duration": 5.44}, {"text": "connected to", "start": 1203.84, "duration": 4.38}, {"text": "um all the processing units", "start": 1205.7, "duration": 5.099}, {"text": "and a fully connected layer in a deep", "start": 1208.22, "duration": 5.94}, {"text": "model is usually only found at the end", "start": 1210.799, "duration": 5.581}, {"text": "of the of the model", "start": 1214.16, "duration": 5.04}, {"text": "um close to where it is uh outputting", "start": 1216.38, "duration": 4.74}, {"text": "closer to the output layer where it", "start": 1219.2, "duration": 4.32}, {"text": "outputs its predictions and the output", "start": 1221.12, "duration": 4.32}, {"text": "of the fully connected layer can be a", "start": 1223.52, "duration": 4.98}, {"text": "vector of probabilities for categories", "start": 1225.44, "duration": 5.099}, {"text": "for a classification problem as shown", "start": 1228.5, "duration": 4.28}, {"text": "here", "start": 1230.539, "duration": 2.241}, {"text": "another", "start": 1238.1, "duration": 4.14}, {"text": "type of layer that is used in a deep", "start": 1239.679, "duration": 5.201}, {"text": "network is a Dropout layer", "start": 1242.24, "duration": 4.02}, {"text": "and", "start": 1244.88, "duration": 4.56}, {"text": "in a Dropout layer processing units are", "start": 1246.26, "duration": 4.98}, {"text": "randomly randomly dropped during", "start": 1249.44, "duration": 4.68}, {"text": "training and this is to prevent the", "start": 1251.24, "duration": 5.88}, {"text": "units for co-adacting which means that", "start": 1254.12, "duration": 4.62}, {"text": "you're forcing each unit to learn", "start": 1257.12, "duration": 3.78}, {"text": "something unique", "start": 1258.74, "duration": 5.1}, {"text": "um uh unique meaning that it's it's an", "start": 1260.9, "duration": 5.82}, {"text": "important feature for the task that the", "start": 1263.84, "duration": 5.64}, {"text": "model is being trained on and this helps", "start": 1266.72, "duration": 5.88}, {"text": "to address overfitting overfitting is a", "start": 1269.48, "duration": 5.4}, {"text": "phenomenon in machine learning and deep", "start": 1272.6, "duration": 4.92}, {"text": "learning that occurs when a model learns", "start": 1274.88, "duration": 5.039}, {"text": "the noise in the training data instead", "start": 1277.52, "duration": 3.899}, {"text": "of learning the structure or the", "start": 1279.919, "duration": 3.721}, {"text": "distribution of the data", "start": 1281.419, "duration": 3.661}, {"text": "um and overfitting prevents the model", "start": 1283.64, "duration": 4.08}, {"text": "from generalizing to new data so that", "start": 1285.08, "duration": 4.8}, {"text": "means it doesn't do well on new data so", "start": 1287.72, "duration": 4.14}, {"text": "you want to avoid overfitting so a", "start": 1289.88, "duration": 4.2}, {"text": "Dropout layer helps to address this", "start": 1291.86, "duration": 5.299}, {"text": "problem of overfitting", "start": 1294.08, "duration": 3.079}, {"text": "um then you can also use a batch", "start": 1298.7, "duration": 4.58}, {"text": "normalization layer in a deep Network", "start": 1300.62, "duration": 6.299}, {"text": "and this layer in this layer the input", "start": 1303.28, "duration": 6.879}, {"text": "is normalized before it gets processed", "start": 1306.919, "duration": 5.76}, {"text": "and normalization is just you know your", "start": 1310.159, "duration": 4.14}, {"text": "the normal way of doing normalization", "start": 1312.679, "duration": 3.12}, {"text": "you subtract the mean and divide by the", "start": 1314.299, "duration": 5.341}, {"text": "same deviation and the benefits of a", "start": 1315.799, "duration": 6.421}, {"text": "batch normalization layer is that it", "start": 1319.64, "duration": 6.84}, {"text": "increases stability because each layer", "start": 1322.22, "duration": 5.22}, {"text": "um", "start": 1326.48, "duration": 3.179}, {"text": "can learn more a little bit more", "start": 1327.44, "duration": 4.46}, {"text": "independently of the other layers", "start": 1329.659, "duration": 6.26}, {"text": "and batch normalization also", "start": 1331.9, "duration": 7.06}, {"text": "usually leads to faster conversions that", "start": 1335.919, "duration": 4.981}, {"text": "means the training", "start": 1338.96, "duration": 3.86}, {"text": "converges faster", "start": 1340.9, "duration": 5.92}, {"text": "it is uh it can also make the model less", "start": 1342.82, "duration": 6.94}, {"text": "sensitive to weight in initialization so", "start": 1346.82, "duration": 4.56}, {"text": "in a neural network or a deep neural", "start": 1349.76, "duration": 2.94}, {"text": "network the weights are initialized", "start": 1351.38, "duration": 2.34}, {"text": "randomly", "start": 1352.7, "duration": 4.2}, {"text": "uh so batch normalization can in", "start": 1353.72, "duration": 5.64}, {"text": "practice help the network be less", "start": 1356.9, "duration": 4.2}, {"text": "sensitive to the initial weights of the", "start": 1359.36, "duration": 4.319}, {"text": "initial values of the weights", "start": 1361.1, "duration": 5.1}, {"text": "um and it can also address uh", "start": 1363.679, "duration": 5.521}, {"text": "overfitting", "start": 1366.2, "duration": 3.0}, {"text": "okay so then the Deep model", "start": 1371.36, "duration": 4.5}, {"text": "um a deep model is constructed out of", "start": 1373.94, "duration": 4.68}, {"text": "these different types of layers", "start": 1375.86, "duration": 5.16}, {"text": "um and as you can see here so you can", "start": 1378.62, "duration": 4.74}, {"text": "have you can have convolution followed", "start": 1381.02, "duration": 5.34}, {"text": "by value activation followed by a", "start": 1383.36, "duration": 5.1}, {"text": "cooling and then at the very end you can", "start": 1386.36, "duration": 4.62}, {"text": "have a fully connected layer and the", "start": 1388.46, "duration": 4.38}, {"text": "layers collectively transform the input", "start": 1390.98, "duration": 4.579}, {"text": "to the output", "start": 1392.84, "duration": 2.719}, {"text": "Okay so", "start": 1396.34, "duration": 4.0}, {"text": "the layers that we just talked about", "start": 1398.419, "duration": 3.481}, {"text": "form the building blocks of a deep", "start": 1400.34, "duration": 3.6}, {"text": "learning model so now let's look at some", "start": 1401.9, "duration": 4.019}, {"text": "deep learning architectures that are", "start": 1403.94, "duration": 4.7}, {"text": "commonly used", "start": 1405.919, "duration": 2.721}, {"text": "so probably the most popular deep", "start": 1408.919, "duration": 3.661}, {"text": "Network architecture is the", "start": 1410.96, "duration": 4.28}, {"text": "convolutional neural network for CNN", "start": 1412.58, "duration": 5.7}, {"text": "so CNN's", "start": 1415.24, "duration": 4.0}, {"text": "um", "start": 1418.28, "duration": 6.06}, {"text": "this is a a typical CNN architecture", "start": 1419.24, "duration": 6.6}, {"text": "um the model consists of several", "start": 1424.34, "duration": 3.18}, {"text": "repeating sets of layers called blocks", "start": 1425.84, "duration": 4.38}, {"text": "and actually this is uh the case for all", "start": 1427.52, "duration": 5.1}, {"text": "deep learning models", "start": 1430.22, "duration": 4.86}, {"text": "um and cnns have been mostly used for", "start": 1432.62, "duration": 5.52}, {"text": "image analysis tasks so the input is", "start": 1435.08, "duration": 5.4}, {"text": "often an input volume right an image of", "start": 1438.14, "duration": 4.44}, {"text": "a size width times height times the", "start": 1440.48, "duration": 5.22}, {"text": "number of channels or three for RGB and", "start": 1442.58, "duration": 5.459}, {"text": "the output for classification task is a", "start": 1445.7, "duration": 3.78}, {"text": "vector of numbers that represent the", "start": 1448.039, "duration": 3.601}, {"text": "cross the class probabilities", "start": 1449.48, "duration": 5.579}, {"text": "so what we see here is a", "start": 1451.64, "duration": 8.279}, {"text": "CNN model called a vgg 16 Network", "start": 1455.059, "duration": 7.62}, {"text": "so we have the input image here so", "start": 1459.919, "duration": 5.041}, {"text": "that's 224 by 224 so that's the number", "start": 1462.679, "duration": 4.5}, {"text": "of pixels for the height and the width", "start": 1464.96, "duration": 4.079}, {"text": "and then times three which is the number", "start": 1467.179, "duration": 4.5}, {"text": "of channels right RGB", "start": 1469.039, "duration": 5.581}, {"text": "and then you can have two convolutional", "start": 1471.679, "duration": 5.041}, {"text": "layer followed by a Max pooling layer so", "start": 1474.62, "duration": 3.84}, {"text": "the max pooling layer is shown in the", "start": 1476.72, "duration": 3.78}, {"text": "red and remember that the max pooling", "start": 1478.46, "duration": 4.199}, {"text": "layer reduces the spatial size of the", "start": 1480.5, "duration": 3.24}, {"text": "input", "start": 1482.659, "duration": 4.921}, {"text": "so you go from 224 by 224 to 112 by 112", "start": 1483.74, "duration": 6.059}, {"text": "after the max bowling layer and then you", "start": 1487.58, "duration": 3.479}, {"text": "have these repeating blocks", "start": 1489.799, "duration": 3.24}, {"text": "convolutional layers followed by Max", "start": 1491.059, "duration": 3.061}, {"text": "pooling", "start": 1493.039, "duration": 3.361}, {"text": "uh and that repeats convolutional layer", "start": 1494.12, "duration": 4.5}, {"text": "by maxwelling and then at the very end", "start": 1496.4, "duration": 4.139}, {"text": "here what you have is a fully connected", "start": 1498.62, "duration": 4.26}, {"text": "layer and then that's followed by a soft", "start": 1500.539, "duration": 5.52}, {"text": "soft Max activation and that gives you", "start": 1502.88, "duration": 6.419}, {"text": "the probabilities for the classes that", "start": 1506.059, "duration": 6.0}, {"text": "you want the model to to learn and in", "start": 1509.299, "duration": 4.321}, {"text": "this case the number of classes is a", "start": 1512.059, "duration": 3.86}, {"text": "thousand", "start": 1513.62, "duration": 2.299}, {"text": "and here are some", "start": 1516.74, "duration": 4.38}, {"text": "um", "start": 1520.159, "duration": 4.441}, {"text": "popular CNN models and the ones that are", "start": 1521.12, "duration": 5.64}, {"text": "listed here are all winners or top", "start": 1524.6, "duration": 5.9}, {"text": "contenders of the imagenet challenge", "start": 1526.76, "duration": 6.96}, {"text": "and this is a competition with several", "start": 1530.5, "duration": 6.159}, {"text": "image analysis tasks and every time that", "start": 1533.72, "duration": 4.74}, {"text": "you talk about CNN you really should", "start": 1536.659, "duration": 3.841}, {"text": "understand what imagenet is because it's", "start": 1538.46, "duration": 4.079}, {"text": "such an important", "start": 1540.5, "duration": 2.82}, {"text": "um", "start": 1542.539, "duration": 5.461}, {"text": "factor in the popularity and success of", "start": 1543.32, "duration": 7.52}, {"text": "CNN so let's talk about what imagenet is", "start": 1548.0, "duration": 6.659}, {"text": "so imagenet is a database that was", "start": 1550.84, "duration": 6.579}, {"text": "developed for computer vision research", "start": 1554.659, "duration": 3.961}, {"text": "um", "start": 1557.419, "duration": 4.021}, {"text": "it has lots and lots of images that are", "start": 1558.62, "duration": 7.32}, {"text": "hand annotated and labeled for many many", "start": 1561.44, "duration": 7.099}, {"text": "categories", "start": 1565.94, "duration": 2.599}, {"text": "um", "start": 1568.58, "duration": 2.76}, {"text": "and like I said imagestat has been", "start": 1569.419, "duration": 4.081}, {"text": "really instrumental to Vision research", "start": 1571.34, "duration": 4.439}, {"text": "as well as deep learning research and if", "start": 1573.5, "duration": 3.9}, {"text": "we look at the history we can understand", "start": 1575.779, "duration": 4.921}, {"text": "why so imagenet has been used for this", "start": 1577.4, "duration": 7.62}, {"text": "challenge called the ILS VRC challenge", "start": 1580.7, "duration": 5.82}, {"text": "um and that stands for imagenet", "start": 1585.02, "duration": 3.779}, {"text": "large-scale the visual recognition", "start": 1586.52, "duration": 5.24}, {"text": "Challenge and this was started in", "start": 1588.799, "duration": 6.841}, {"text": "2010. there are many image analysis", "start": 1591.76, "duration": 6.22}, {"text": "tasks but the most important or most", "start": 1595.64, "duration": 4.2}, {"text": "popular one is the image classification", "start": 1597.98, "duration": 6.36}, {"text": "task where the model is given an input", "start": 1599.84, "duration": 6.54}, {"text": "image and it has to classify that image", "start": 1604.34, "duration": 4.5}, {"text": "into one of a thousand categories and", "start": 1606.38, "duration": 4.26}, {"text": "these are you know your basic categories", "start": 1608.84, "duration": 3.18}, {"text": "these are your you know regular images", "start": 1610.64, "duration": 3.18}, {"text": "that you see on the internet and the", "start": 1612.02, "duration": 3.48}, {"text": "categories or things like cats and dogs", "start": 1613.82, "duration": 5.28}, {"text": "and cars and boats and things like that", "start": 1615.5, "duration": 6.62}, {"text": "um so", "start": 1619.1, "duration": 3.02}, {"text": "um in it started in 2010 and in 2011", "start": 1622.76, "duration": 7.68}, {"text": "the error rate on the imagenet data for", "start": 1626.9, "duration": 6.0}, {"text": "this image classification task was", "start": 1630.44, "duration": 4.619}, {"text": "around 25 percent", "start": 1632.9, "duration": 4.74}, {"text": "um so this is using conventional image", "start": 1635.059, "duration": 3.72}, {"text": "processing techniques like Edge", "start": 1637.64, "duration": 3.24}, {"text": "detection and health transform and", "start": 1638.779, "duration": 3.9}, {"text": "things like that", "start": 1640.88, "duration": 4.2}, {"text": "um in 2012 this was the first time that", "start": 1642.679, "duration": 5.821}, {"text": "we saw an entry that used a deep", "start": 1645.08, "duration": 6.24}, {"text": "learning Model A CNN called an alexnet", "start": 1648.5, "duration": 5.94}, {"text": "and with the Alex net the error rate on", "start": 1651.32, "duration": 6.359}, {"text": "the imagenet data uh dropped way down it", "start": 1654.44, "duration": 6.18}, {"text": "went from 25 to about 15 percent", "start": 1657.679, "duration": 5.341}, {"text": "and ever since then the error rate has", "start": 1660.62, "duration": 6.9}, {"text": "dropped dramatically in 2015 the error", "start": 1663.02, "duration": 7.62}, {"text": "rate was about three and a half percent", "start": 1667.52, "duration": 5.279}, {"text": "and this was achieved by another really", "start": 1670.64, "duration": 5.7}, {"text": "popular CNN called a resnet and this was", "start": 1672.799, "duration": 5.101}, {"text": "actually better than human performance", "start": 1676.34, "duration": 4.5}, {"text": "so they changed a human labeler and", "start": 1677.9, "duration": 5.58}, {"text": "resnet did better than a human labeler", "start": 1680.84, "duration": 6.78}, {"text": "so as you can see imagenet and this", "start": 1683.48, "duration": 5.4}, {"text": "image challenge", "start": 1687.62, "duration": 4.32}, {"text": "has really been instrumental to how", "start": 1688.88, "duration": 4.86}, {"text": "people perceive deep learning models and", "start": 1691.94, "duration": 5.06}, {"text": "cnns in particular", "start": 1693.74, "duration": 3.26}, {"text": "okay so CNN applications lots and lots", "start": 1697.82, "duration": 4.8}, {"text": "of different types of image analysis", "start": 1700.7, "duration": 5.04}, {"text": "tasks but they've also been used for", "start": 1702.62, "duration": 5.82}, {"text": "natural languishing natural language", "start": 1705.74, "duration": 5.16}, {"text": "processing tasks as well as well as", "start": 1708.44, "duration": 5.88}, {"text": "other types of pattern recognition type", "start": 1710.9, "duration": 6.08}, {"text": "tasks", "start": 1714.32, "duration": 2.66}, {"text": "okay another", "start": 1718.22, "duration": 4.02}, {"text": "um popular", "start": 1720.559, "duration": 3.961}, {"text": "deep learning architecture is called a", "start": 1722.24, "duration": 4.559}, {"text": "unit and looking at the structure I", "start": 1724.52, "duration": 4.259}, {"text": "think you can understand how this model", "start": 1726.799, "duration": 4.5}, {"text": "got its name because because it has a", "start": 1728.779, "duration": 5.341}, {"text": "U-shaped structure", "start": 1731.299, "duration": 7.021}, {"text": "um so the unit is often used for image", "start": 1734.12, "duration": 6.179}, {"text": "segmentation for an image segmentation", "start": 1738.32, "duration": 3.38}, {"text": "task and", "start": 1740.299, "duration": 4.38}, {"text": "the goal of image segmentation is to", "start": 1741.7, "duration": 5.74}, {"text": "assign a class to each pixel in an image", "start": 1744.679, "duration": 6.061}, {"text": "so for this image you know you want to", "start": 1747.44, "duration": 6.119}, {"text": "be able to say which pixels belong to", "start": 1750.74, "duration": 4.819}, {"text": "people", "start": 1753.559, "duration": 5.401}, {"text": "which pixels belong to cars which pixels", "start": 1755.559, "duration": 6.041}, {"text": "belong to trees and buildings and the", "start": 1758.96, "duration": 5.099}, {"text": "roads so", "start": 1761.6, "duration": 5.04}, {"text": "um the model partitions the image into", "start": 1764.059, "duration": 5.461}, {"text": "different segments each one representing", "start": 1766.64, "duration": 5.1}, {"text": "a different entity right so that's image", "start": 1769.52, "duration": 5.22}, {"text": "segmentation so the unit is often used", "start": 1771.74, "duration": 4.86}, {"text": "for image segmentation", "start": 1774.74, "duration": 5.039}, {"text": "and the unet architecture has what is", "start": 1776.6, "duration": 5.699}, {"text": "referred to as an encoder decoding type", "start": 1779.779, "duration": 3.541}, {"text": "of network", "start": 1782.299, "duration": 5.821}, {"text": "and that is because on one side it has a", "start": 1783.32, "duration": 8.3}, {"text": "Contracting path that encodes the input", "start": 1788.12, "duration": 8.279}, {"text": "such as an image into a vector", "start": 1791.62, "duration": 7.299}, {"text": "um and this is uh the encoding path", "start": 1796.399, "duration": 5.221}, {"text": "usually consists of", "start": 1798.919, "duration": 4.921}, {"text": "um convolutional layers followed by Max", "start": 1801.62, "duration": 5.52}, {"text": "pooling and several repeating blocks of", "start": 1803.84, "duration": 6.48}, {"text": "uh convolutional layers and Max blowing", "start": 1807.14, "duration": 4.5}, {"text": "so", "start": 1810.32, "duration": 4.02}, {"text": "um this encoding path is very similar to", "start": 1811.64, "duration": 5.34}, {"text": "a CNN that we've seen before right so it", "start": 1814.34, "duration": 6.559}, {"text": "takes an input image can applies", "start": 1816.98, "duration": 7.74}, {"text": "convolutional layer and pooling so that", "start": 1820.899, "duration": 7.481}, {"text": "it encodes the input into a vector", "start": 1824.72, "duration": 5.36}, {"text": "in a unit", "start": 1828.38, "duration": 5.64}, {"text": "you also have a decoding path so that's", "start": 1830.08, "duration": 6.4}, {"text": "where the model", "start": 1834.02, "duration": 3.18}, {"text": "um", "start": 1836.48, "duration": 4.62}, {"text": "decodes that Vector into a slightly", "start": 1837.2, "duration": 5.42}, {"text": "different", "start": 1841.1, "duration": 6.62}, {"text": "version of the image", "start": 1842.62, "duration": 5.1}, {"text": "um", "start": 1854.0, "duration": 5.36}, {"text": "and so here are some applications", "start": 1855.02, "duration": 4.34}, {"text": "of the unit so it was originally created", "start": 1859.76, "duration": 6.72}, {"text": "for segmenting biomedical", "start": 1863.48, "duration": 6.72}, {"text": "images but it has been applied to lots", "start": 1866.48, "duration": 6.179}, {"text": "of other different applications as well", "start": 1870.2, "duration": 5.88}, {"text": "um the lower left there shows it being", "start": 1872.659, "duration": 5.341}, {"text": "used for object detection to understand", "start": 1876.08, "duration": 4.92}, {"text": "a scheme but it's also been used a lot", "start": 1878.0, "duration": 6.0}, {"text": "for medical image analysis and also for", "start": 1881.0, "duration": 7.1}, {"text": "satellite image processing as well", "start": 1884.0, "duration": 4.1}, {"text": "okay another", "start": 1890.179, "duration": 2.901}, {"text": "type of deep learning model is an lscm", "start": 1893.179, "duration": 5.1}, {"text": "and this stands for a long short-term", "start": 1896.779, "duration": 4.201}, {"text": "memory and it is a type of recurrent", "start": 1898.279, "duration": 5.221}, {"text": "neural network that is used for sequence", "start": 1900.98, "duration": 3.74}, {"text": "learning", "start": 1903.5, "duration": 4.799}, {"text": "so sequence learning is the task of", "start": 1904.72, "duration": 6.04}, {"text": "learning a signal with an ordering or a", "start": 1908.299, "duration": 5.401}, {"text": "Time component so for an example a stock", "start": 1910.76, "duration": 4.44}, {"text": "price right a stock price changes", "start": 1913.7, "duration": 5.219}, {"text": "changes over time speech signals is also", "start": 1915.2, "duration": 6.06}, {"text": "has a temporal component and also video", "start": 1918.919, "duration": 5.88}, {"text": "analysis tracking where an object how an", "start": 1921.26, "duration": 6.06}, {"text": "object changes in a video these are all", "start": 1924.799, "duration": 5.061}, {"text": "sequences", "start": 1927.32, "duration": 2.54}, {"text": "um so a type of neural network called a", "start": 1930.919, "duration": 4.38}, {"text": "recurrent Network can be used to model", "start": 1933.86, "duration": 3.559}, {"text": "sequences and time dependent signals", "start": 1935.299, "duration": 4.141}, {"text": "they're called recurrent neural network", "start": 1937.419, "duration": 4.86}, {"text": "because they have a cyclic connection", "start": 1939.44, "duration": 6.0}, {"text": "that feed the previous activation back", "start": 1942.279, "duration": 5.26}, {"text": "to the network as part of the input", "start": 1945.44, "duration": 6.479}, {"text": "so this allows for temple context to be", "start": 1947.539, "duration": 5.88}, {"text": "learned and stored", "start": 1951.919, "duration": 3.48}, {"text": "and this means that the predictions at", "start": 1953.419, "duration": 3.841}, {"text": "the current time step depending depend", "start": 1955.399, "duration": 4.861}, {"text": "on not only the current input but also", "start": 1957.26, "duration": 4.74}, {"text": "on the previous predictions", "start": 1960.26, "duration": 4.86}, {"text": "and how much of that context needs to be", "start": 1962.0, "duration": 5.159}, {"text": "remembered has to be it is a learned", "start": 1965.12, "duration": 3.6}, {"text": "parameter", "start": 1967.159, "duration": 6.301}, {"text": "so here is a overview of what the lstm", "start": 1968.72, "duration": 6.72}, {"text": "architecture looks like", "start": 1973.46, "duration": 3.12}, {"text": "um", "start": 1975.44, "duration": 3.479}, {"text": "so information flows through these", "start": 1976.58, "duration": 4.56}, {"text": "memory blocks called cells so these", "start": 1978.919, "duration": 4.38}, {"text": "cells are the the green rectangle shown", "start": 1981.14, "duration": 5.1}, {"text": "in this figure here and each cell has", "start": 1983.299, "duration": 5.941}, {"text": "these structures called Gates that", "start": 1986.24, "duration": 4.319}, {"text": "manipulate", "start": 1989.24, "duration": 4.26}, {"text": "how much information the the cell", "start": 1990.559, "duration": 6.24}, {"text": "remembers and how in other words it", "start": 1993.5, "duration": 6.419}, {"text": "learns what's important for the task and", "start": 1996.799, "duration": 5.281}, {"text": "what they can discard and so the", "start": 1999.919, "duration": 5.88}, {"text": "structure of the cell allows the lstm to", "start": 2002.08, "duration": 6.959}, {"text": "selectively remember or forget pieces of", "start": 2005.799, "duration": 5.1}, {"text": "information that is important for the", "start": 2009.039, "duration": 5.0}, {"text": "tasks that it has to learn", "start": 2010.899, "duration": 3.14}, {"text": "lstm applications as in", "start": 2014.22, "duration": 5.86}, {"text": "many applications having to do with", "start": 2017.62, "duration": 4.82}, {"text": "speech and language", "start": 2020.08, "duration": 6.78}, {"text": "and also video analysis people use lstm", "start": 2022.44, "duration": 6.88}, {"text": "for video analysis and also protein", "start": 2026.86, "duration": 3.84}, {"text": "structure prediction as well as stock", "start": 2029.32, "duration": 2.88}, {"text": "price prediction", "start": 2030.7, "duration": 6.3}, {"text": "so these are all sequence type problems", "start": 2032.2, "duration": 8.099}, {"text": "okay and then another popular", "start": 2037.0, "duration": 7.2}, {"text": "deep learning uh type model is called a", "start": 2040.299, "duration": 6.061}, {"text": "again or a generative adversarial", "start": 2044.2, "duration": 4.5}, {"text": "Network and this is a deep learning", "start": 2046.36, "duration": 5.22}, {"text": "approach to generative modeling", "start": 2048.7, "duration": 5.34}, {"text": "um that means that the model can", "start": 2051.58, "duration": 4.98}, {"text": "generate data it not only processes data", "start": 2054.04, "duration": 5.52}, {"text": "but it also generates data so the model", "start": 2056.56, "duration": 5.779}, {"text": "learns the structure of the input data", "start": 2059.56, "duration": 5.46}, {"text": "and so it can generate new data with", "start": 2062.339, "duration": 5.621}, {"text": "similar characteristics as we input data", "start": 2065.02, "duration": 6.359}, {"text": "so it again you actually have two models", "start": 2067.96, "duration": 5.219}, {"text": "there's a generator that generates new", "start": 2071.379, "duration": 3.48}, {"text": "samples and then there's a discriminator", "start": 2073.179, "duration": 4.021}, {"text": "that determines whether whether the", "start": 2074.859, "duration": 4.8}, {"text": "sample is generated or from the input", "start": 2077.2, "duration": 4.979}, {"text": "data and the discriminator and the", "start": 2079.659, "duration": 4.68}, {"text": "generator are trained in an adversarial", "start": 2082.179, "duration": 6.301}, {"text": "way and what we mean by that is", "start": 2084.339, "duration": 6.241}, {"text": "um well so here here's the structure of", "start": 2088.48, "duration": 6.359}, {"text": "the Gan so you have real images so", "start": 2090.58, "duration": 6.32}, {"text": "that's true ground truth", "start": 2094.839, "duration": 4.861}, {"text": "data and then you have this generator", "start": 2096.9, "duration": 7.18}, {"text": "here so the generator is trained to", "start": 2099.7, "duration": 7.8}, {"text": "learn the distribution of the real data", "start": 2104.08, "duration": 5.999}, {"text": "so that it can generate data that comes", "start": 2107.5, "duration": 4.8}, {"text": "from the same distribution as the input", "start": 2110.079, "duration": 5.401}, {"text": "data right so the generator learns to", "start": 2112.3, "duration": 5.88}, {"text": "generate images", "start": 2115.48, "duration": 4.26}, {"text": "and then", "start": 2118.18, "duration": 5.22}, {"text": "both the true images and the generated", "start": 2119.74, "duration": 6.54}, {"text": "images are fed to the discriminator and", "start": 2123.4, "duration": 6.179}, {"text": "the discriminator is chained to", "start": 2126.28, "duration": 5.42}, {"text": "distinguish between", "start": 2129.579, "duration": 5.821}, {"text": "generated data and true data so it", "start": 2131.7, "duration": 5.74}, {"text": "determines the discriminator determines", "start": 2135.4, "duration": 4.679}, {"text": "if the input comes from the generator or", "start": 2137.44, "duration": 5.04}, {"text": "the true training data right and the", "start": 2140.079, "duration": 4.201}, {"text": "error from the discriminator also flows", "start": 2142.48, "duration": 3.66}, {"text": "back to the generator so that the", "start": 2144.28, "duration": 3.66}, {"text": "generator gets better and better at", "start": 2146.14, "duration": 3.38}, {"text": "generating", "start": 2147.94, "duration": 6.6}, {"text": "data and with the goal of being able to", "start": 2149.52, "duration": 7.54}, {"text": "fake being able to fool the", "start": 2154.54, "duration": 4.02}, {"text": "discriminator so that the discriminator", "start": 2157.06, "duration": 3.18}, {"text": "can't tell the difference between a", "start": 2158.56, "duration": 4.44}, {"text": "generated image and a True Image okay so", "start": 2160.24, "duration": 5.099}, {"text": "that's how it's these two", "start": 2163.0, "duration": 4.38}, {"text": "components of the Gan are trained in an", "start": 2165.339, "duration": 4.74}, {"text": "adversarial way", "start": 2167.38, "duration": 5.1}, {"text": "okay there have been many very", "start": 2170.079, "duration": 5.54}, {"text": "interesting applications of Gans", "start": 2172.48, "duration": 7.139}, {"text": "probably the most uh probably the in the", "start": 2175.619, "duration": 7.661}, {"text": "the first one is training again to", "start": 2179.619, "duration": 5.701}, {"text": "generate faces", "start": 2183.28, "duration": 4.98}, {"text": "um and these are the faces that were", "start": 2185.32, "duration": 6.06}, {"text": "actually generated by again", "start": 2188.26, "duration": 6.599}, {"text": "um and this uh in this experiment they", "start": 2191.38, "duration": 4.62}, {"text": "generate", "start": 2194.859, "duration": 2.641}, {"text": "um they train again", "start": 2196.0, "duration": 4.079}, {"text": "to generate realistic photographs of", "start": 2197.5, "duration": 4.26}, {"text": "human faces and the training data was of", "start": 2200.079, "duration": 3.901}, {"text": "celebrity faces that's why you see such", "start": 2201.76, "duration": 7.22}, {"text": "beautiful faces on this data set", "start": 2203.98, "duration": 5.0}, {"text": "um another really cool application of", "start": 2209.92, "duration": 5.28}, {"text": "Gans is image to image translation so", "start": 2212.38, "duration": 6.12}, {"text": "this means that the model can transform", "start": 2215.2, "duration": 6.419}, {"text": "images from one domain to another domain", "start": 2218.5, "duration": 5.82}, {"text": "so what you see here on the left here is", "start": 2221.619, "duration": 4.921}, {"text": "if you give it a Monet painting an image", "start": 2224.32, "duration": 6.259}, {"text": "of a Monet painting it can generate", "start": 2226.54, "duration": 8.36}, {"text": "a kind of a natural photo", "start": 2230.579, "duration": 6.52}, {"text": "of that painting", "start": 2234.9, "duration": 4.959}, {"text": "and vice versa if you give it a photo it", "start": 2237.099, "duration": 5.041}, {"text": "can generate an image that has this", "start": 2239.859, "duration": 4.401}, {"text": "monet-like quality to it", "start": 2242.14, "duration": 4.68}, {"text": "and similarly with zebras and horses", "start": 2244.26, "duration": 5.74}, {"text": "given a picture of a zebra it can", "start": 2246.82, "duration": 4.68}, {"text": "generate a horse so it takes away the", "start": 2250.0, "duration": 4.14}, {"text": "stripes and it can also do it can go", "start": 2251.5, "duration": 5.82}, {"text": "from horses to zebras and also summer to", "start": 2254.14, "duration": 7.38}, {"text": "winter winter to summer so it transforms", "start": 2257.32, "duration": 8.1}, {"text": "an image and adds certain qualities from", "start": 2261.52, "duration": 8.24}, {"text": "another domain to the resulting image", "start": 2265.42, "duration": 4.34}, {"text": "um another application of Gans is super", "start": 2271.9, "duration": 7.38}, {"text": "resolution and this task is to create a", "start": 2275.079, "duration": 5.821}, {"text": "high resolution image given a low", "start": 2279.28, "duration": 3.839}, {"text": "resolution image", "start": 2280.9, "duration": 5.16}, {"text": "um so what you see here are four", "start": 2283.119, "duration": 6.061}, {"text": "different or three different ways of", "start": 2286.06, "duration": 5.16}, {"text": "um doing performing this super", "start": 2289.18, "duration": 4.74}, {"text": "resolution task so given the high a low", "start": 2291.22, "duration": 5.22}, {"text": "resolution image they apply different", "start": 2293.92, "duration": 4.26}, {"text": "methods to come up with a high", "start": 2296.44, "duration": 4.32}, {"text": "resolution image so the leftmost One is", "start": 2298.18, "duration": 5.52}, {"text": "using by cubic interpolation as you can", "start": 2300.76, "duration": 6.0}, {"text": "see that's pretty fuzzy still the one", "start": 2303.7, "duration": 5.76}, {"text": "next to that is using a deep residual", "start": 2306.76, "duration": 5.42}, {"text": "Network so that's a a", "start": 2309.46, "duration": 6.24}, {"text": "just a CNN just a normal CNN and then", "start": 2312.18, "duration": 6.28}, {"text": "the one next to that is using again and", "start": 2315.7, "duration": 3.96}, {"text": "if you compare that to the original", "start": 2318.46, "duration": 3.36}, {"text": "image you can see that the the quality", "start": 2319.66, "duration": 5.88}, {"text": "is really superb so Gans have been used", "start": 2321.82, "duration": 6.539}, {"text": "for this type of application where you", "start": 2325.54, "duration": 5.28}, {"text": "can create high resolution images from", "start": 2328.359, "duration": 5.041}, {"text": "lower resolution images", "start": 2330.82, "duration": 4.259}, {"text": "okay and there are lots of other really", "start": 2333.4, "duration": 4.62}, {"text": "fascinating application with Gans uh", "start": 2335.079, "duration": 5.941}, {"text": "text to image post generation you can go", "start": 2338.02, "duration": 5.28}, {"text": "from photos to emojis and also face", "start": 2341.02, "duration": 4.559}, {"text": "aging so it's really interesting what", "start": 2343.3, "duration": 5.24}, {"text": "people have done with games", "start": 2345.579, "duration": 2.961}, {"text": "okay so now that you know about all", "start": 2349.66, "duration": 3.6}, {"text": "these", "start": 2352.24, "duration": 2.879}, {"text": "you know really interesting applications", "start": 2353.26, "duration": 5.04}, {"text": "and really cool architectures uh where", "start": 2355.119, "duration": 6.661}, {"text": "can you go to to create your own", "start": 2358.3, "duration": 4.86}, {"text": "um so there are many deep learning", "start": 2361.78, "duration": 4.44}, {"text": "libraries especially python-based ones", "start": 2363.16, "duration": 4.38}, {"text": "um tensorflow", "start": 2366.22, "duration": 2.46}, {"text": "um I think everybody knows about", "start": 2367.54, "duration": 4.26}, {"text": "tensorflow this is a framework developed", "start": 2368.68, "duration": 4.56}, {"text": "by Google", "start": 2371.8, "duration": 2.22}, {"text": "um", "start": 2373.24, "duration": 3.599}, {"text": "and there's also Keras and that's a high", "start": 2374.02, "duration": 4.98}, {"text": "level API that sits on top of tensorflow", "start": 2376.839, "duration": 5.24}, {"text": "as well as other lower level Frameworks", "start": 2379.0, "duration": 6.54}, {"text": "so Keras is pretty high level and it's", "start": 2382.079, "duration": 6.04}, {"text": "really easy to understand the cares", "start": 2385.54, "duration": 3.9}, {"text": "command so", "start": 2388.119, "duration": 4.021}, {"text": "um Keras is kind of you know the uh the", "start": 2389.44, "duration": 4.74}, {"text": "go-to for beginners so it's um it's a", "start": 2392.14, "duration": 4.92}, {"text": "really good package Pi torch is another", "start": 2394.18, "duration": 4.679}, {"text": "machine learning deep learning framework", "start": 2397.06, "duration": 4.98}, {"text": "and that was developed by Facebook", "start": 2398.859, "duration": 5.76}, {"text": "um Cafe and Cafe two", "start": 2402.04, "duration": 4.559}, {"text": "um Cafe two actually now belongs to", "start": 2404.619, "duration": 3.361}, {"text": "Facebook and it has been merged into", "start": 2406.599, "duration": 4.941}, {"text": "pytorch and then there's also Apache", "start": 2407.98, "duration": 6.24}, {"text": "mxnet and this is a framework that is", "start": 2411.54, "duration": 4.48}, {"text": "used by Amazon", "start": 2414.22, "duration": 3.54}, {"text": "and there are many other ones but these", "start": 2416.02, "duration": 3.78}, {"text": "are the most popular ones", "start": 2417.76, "duration": 3.66}, {"text": "uh there are other deep learning", "start": 2419.8, "duration": 4.14}, {"text": "libraries as well for Java there's deep", "start": 2421.42, "duration": 4.4}, {"text": "learning 4J", "start": 2423.94, "duration": 6.48}, {"text": "and then for R uh most of the popular", "start": 2425.82, "duration": 7.48}, {"text": "deep learning Frameworks also work with", "start": 2430.42, "duration": 5.64}, {"text": "r so tensorflow Keras msnet all work", "start": 2433.3, "duration": 3.9}, {"text": "with r", "start": 2436.06, "duration": 3.6}, {"text": "and then there's lots of cloud options", "start": 2437.2, "duration": 6.44}, {"text": "as well Google Cloud Amazon sagemaker", "start": 2439.66, "duration": 7.08}, {"text": "Microsoft Azure and IBM Watson are all", "start": 2443.64, "duration": 5.219}, {"text": "the big names in the", "start": 2446.74, "duration": 4.44}, {"text": "cloud-based machine learning deep", "start": 2448.859, "duration": 5.641}, {"text": "learning offerings", "start": 2451.18, "duration": 3.32}, {"text": "okay so now let's talk about transfer", "start": 2456.4, "duration": 3.84}, {"text": "learning", "start": 2459.04, "duration": 2.94}, {"text": "um because I said this is this is a", "start": 2460.24, "duration": 3.48}, {"text": "technique that is used to speed up", "start": 2461.98, "duration": 4.32}, {"text": "training and has been really key to the", "start": 2463.72, "duration": 5.66}, {"text": "popularity of deep models okay so", "start": 2466.3, "duration": 6.66}, {"text": "transfer learning is a technique that is", "start": 2469.38, "duration": 6.82}, {"text": "used to overcome the challenges of uh", "start": 2472.96, "duration": 5.28}, {"text": "training deep learning models from", "start": 2476.2, "duration": 4.379}, {"text": "scratch so remember that in a deep", "start": 2478.24, "duration": 3.96}, {"text": "learning model you can have you know", "start": 2480.579, "duration": 4.381}, {"text": "easily millions of parameters and in", "start": 2482.2, "duration": 5.82}, {"text": "order to adjust those parameters you", "start": 2484.96, "duration": 6.659}, {"text": "need a lot lots and lots of data", "start": 2488.02, "duration": 6.66}, {"text": "um and getting that data for any domain", "start": 2491.619, "duration": 6.121}, {"text": "is really expensive and time consuming", "start": 2494.68, "duration": 5.399}, {"text": "so oftentimes you end up with not having", "start": 2497.74, "duration": 3.66}, {"text": "enough data", "start": 2500.079, "duration": 3.981}, {"text": "and even if you do have enough data", "start": 2501.4, "duration": 5.16}, {"text": "training a deep learning model from", "start": 2504.06, "duration": 5.14}, {"text": "scratch can take a very very long time", "start": 2506.56, "duration": 6.059}, {"text": "and so the idea is that you want to use", "start": 2509.2, "duration": 6.18}, {"text": "a pre-trained model instead of starting", "start": 2512.619, "duration": 3.921}, {"text": "from scratch", "start": 2515.38, "duration": 4.68}, {"text": "and this means that you use a model that", "start": 2516.54, "duration": 6.66}, {"text": "was trained on another data set", "start": 2520.06, "duration": 6.12}, {"text": "and this can serve as your starting", "start": 2523.2, "duration": 3.879}, {"text": "point", "start": 2526.18, "duration": 3.12}, {"text": "for your model and then you can train", "start": 2527.079, "duration": 4.981}, {"text": "that model on your data set and on your", "start": 2529.3, "duration": 4.92}, {"text": "task", "start": 2532.06, "duration": 3.48}, {"text": "and", "start": 2534.22, "duration": 3.379}, {"text": "um", "start": 2535.54, "duration": 2.059}, {"text": "the the pre-chain model that most people", "start": 2538.54, "duration": 4.44}, {"text": "use are models that were trained on the", "start": 2540.76, "duration": 3.96}, {"text": "imagenet database and so that's why", "start": 2542.98, "duration": 3.48}, {"text": "imagenet is such an important part of", "start": 2544.72, "duration": 5.24}, {"text": "deep learning models", "start": 2546.46, "duration": 3.5}, {"text": "okay and there are two different types", "start": 2550.0, "duration": 4.32}, {"text": "of different approaches to transfer", "start": 2551.98, "duration": 5.7}, {"text": "learning one is feature extraction and", "start": 2554.32, "duration": 5.519}, {"text": "here you just remove the last fully", "start": 2557.68, "duration": 3.659}, {"text": "connected layer that's the", "start": 2559.839, "duration": 3.121}, {"text": "classification layer from your tree", "start": 2561.339, "duration": 3.601}, {"text": "chain model and then you treat the rest", "start": 2562.96, "duration": 3.78}, {"text": "of the network as you're as a feature", "start": 2564.94, "duration": 3.659}, {"text": "extractor right so you just run your", "start": 2566.74, "duration": 3.96}, {"text": "images through the network and then you", "start": 2568.599, "duration": 6.061}, {"text": "extract the outputs of that last layer", "start": 2570.7, "duration": 7.08}, {"text": "and then he used those as features to or", "start": 2574.66, "duration": 6.06}, {"text": "to train a new classifier on your task", "start": 2577.78, "duration": 6.0}, {"text": "and on your data and that new classifier", "start": 2580.72, "duration": 4.68}, {"text": "is often referred as the Top Model", "start": 2583.78, "duration": 3.96}, {"text": "because it sits on top of the of the uh", "start": 2585.4, "duration": 3.959}, {"text": "the entire model", "start": 2587.74, "duration": 4.74}, {"text": "another approach is fine tuning and here", "start": 2589.359, "duration": 5.941}, {"text": "you tune the weights in some layers of", "start": 2592.48, "duration": 4.5}, {"text": "the original model along with the", "start": 2595.3, "duration": 3.559}, {"text": "weights of the top models", "start": 2596.98, "duration": 5.879}, {"text": "so you train a model using your data set", "start": 2598.859, "duration": 7.72}, {"text": "and train it on your task but it saves", "start": 2602.859, "duration": 5.76}, {"text": "you time because you're not training", "start": 2606.579, "duration": 3.361}, {"text": "everything and you're not training", "start": 2608.619, "duration": 4.161}, {"text": "things from scratch", "start": 2609.94, "duration": 2.84}, {"text": "okay so like I said", "start": 2613.119, "duration": 5.281}, {"text": "um CNN's uh people use transfer learning", "start": 2615.46, "duration": 4.8}, {"text": "a lot with cnns and some popular", "start": 2618.4, "duration": 4.08}, {"text": "architectures are listed here and again", "start": 2620.26, "duration": 4.2}, {"text": "these are all winners of The imagenet", "start": 2622.48, "duration": 4.92}, {"text": "Challenge so these are models that were", "start": 2624.46, "duration": 5.04}, {"text": "trained on on the imagenet data and", "start": 2627.4, "duration": 4.199}, {"text": "people use these models a lot as the", "start": 2629.5, "duration": 3.24}, {"text": "pre-trained model", "start": 2631.599, "duration": 4.081}, {"text": "for transfer learning", "start": 2632.74, "duration": 5.28}, {"text": "Okay so", "start": 2635.68, "duration": 5.06}, {"text": "why does transfer learning work", "start": 2638.02, "duration": 5.599}, {"text": "so remember that", "start": 2640.74, "duration": 7.06}, {"text": "a deep model has many many layers of", "start": 2643.619, "duration": 7.661}, {"text": "many many layers and they learn and and", "start": 2647.8, "duration": 5.519}, {"text": "these layers allow the model to learn", "start": 2651.28, "duration": 4.38}, {"text": "different abstraction different levels", "start": 2653.319, "duration": 5.04}, {"text": "of abstraction of the data and so the", "start": 2655.66, "duration": 5.459}, {"text": "idea is that the lower levels the lower", "start": 2658.359, "duration": 4.681}, {"text": "layers of the model", "start": 2661.119, "duration": 5.581}, {"text": "learn to detect generic features that", "start": 2663.04, "duration": 6.48}, {"text": "are common in images and then the higher", "start": 2666.7, "duration": 4.98}, {"text": "layers detect more specialized features", "start": 2669.52, "duration": 5.46}, {"text": "so in this schematic what we see is that", "start": 2671.68, "duration": 6.6}, {"text": "the lower layers which are up here can", "start": 2674.98, "duration": 4.74}, {"text": "detect can learn to detect things like", "start": 2678.28, "duration": 4.92}, {"text": "edges and Corners um you know at", "start": 2679.72, "duration": 5.639}, {"text": "different orientation and then as you", "start": 2683.2, "duration": 4.1}, {"text": "get higher", "start": 2685.359, "duration": 5.401}, {"text": "you get more specific to the task at", "start": 2687.3, "duration": 7.6}, {"text": "hand so in these middle layers the model", "start": 2690.76, "duration": 6.12}, {"text": "can learn things like what an eye looks", "start": 2694.9, "duration": 4.26}, {"text": "like or what a nose looks like and then", "start": 2696.88, "duration": 5.52}, {"text": "at the highest levels then get to be", "start": 2699.16, "duration": 5.159}, {"text": "very specific to the task at hand so", "start": 2702.4, "duration": 4.199}, {"text": "these layers will learn you know what a", "start": 2704.319, "duration": 4.381}, {"text": "specific face looks like", "start": 2706.599, "duration": 4.321}, {"text": "and so the idea is that you can keep the", "start": 2708.7, "duration": 5.639}, {"text": "lower layers that learn just you know", "start": 2710.92, "duration": 5.399}, {"text": "the generic features that you see in all", "start": 2714.339, "duration": 4.621}, {"text": "images and then you can fine tune the", "start": 2716.319, "duration": 5.52}, {"text": "higher layers to be more specific to", "start": 2718.96, "duration": 5.04}, {"text": "your task", "start": 2721.839, "duration": 4.441}, {"text": "Okay so", "start": 2724.0, "duration": 5.76}, {"text": "a vgg is can be used as a pre-chain", "start": 2726.28, "duration": 5.88}, {"text": "network and this is a BGG that we talked", "start": 2729.76, "duration": 4.26}, {"text": "about before this is the structure of", "start": 2732.16, "duration": 3.24}, {"text": "the qvg", "start": 2734.02, "duration": 3.54}, {"text": "so if you were to do feature extraction", "start": 2735.4, "duration": 4.919}, {"text": "using the vgg then you would just", "start": 2737.56, "duration": 5.88}, {"text": "extract the features here right that the", "start": 2740.319, "duration": 5.641}, {"text": "layer before the fully connected layer", "start": 2743.44, "duration": 4.98}, {"text": "and then you feed those features into a", "start": 2745.96, "duration": 3.96}, {"text": "separate classifier for your particular", "start": 2748.42, "duration": 5.28}, {"text": "class for your particular task", "start": 2749.92, "duration": 6.36}, {"text": "uh if you were to do fine tuning then", "start": 2753.7, "duration": 5.159}, {"text": "you would adjust the weights in the top", "start": 2756.28, "duration": 4.799}, {"text": "layers using your new data set right so", "start": 2758.859, "duration": 3.72}, {"text": "you would adjust the weights of these", "start": 2761.079, "duration": 3.54}, {"text": "layers of the top layers as well as your", "start": 2762.579, "duration": 4.881}, {"text": "top model", "start": 2764.619, "duration": 2.841}, {"text": "okay so then", "start": 2768.579, "duration": 5.161}, {"text": "uh what we're going to see next is a", "start": 2771.46, "duration": 5.46}, {"text": "transfer learning demo and this is", "start": 2773.74, "duration": 6.26}, {"text": "learning this is training the model to", "start": 2776.92, "duration": 5.46}, {"text": "distinguish between cats and dog images", "start": 2780.0, "duration": 5.079}, {"text": "from a kaggle data set and we're going", "start": 2782.38, "duration": 5.219}, {"text": "to be using fine tuning so we'll use the", "start": 2785.079, "duration": 6.901}, {"text": "vgg16 network like we just uh showed", "start": 2787.599, "duration": 6.661}, {"text": "um and so this vgg was chained on", "start": 2791.98, "duration": 3.72}, {"text": "imagenet data and that's going to be our", "start": 2794.26, "duration": 3.72}, {"text": "pre-trained model and we will replace", "start": 2795.7, "duration": 3.96}, {"text": "the last fully connected layer with a", "start": 2797.98, "duration": 3.24}, {"text": "neural network that was previously", "start": 2799.66, "duration": 4.439}, {"text": "trained and then we'll fine-tune the", "start": 2801.22, "duration": 4.8}, {"text": "last convolutional block as well as a", "start": 2804.099, "duration": 5.48}, {"text": "fully connected layer on this data", "start": 2806.02, "duration": 3.559}, {"text": "okay so I'm gonna switch now over to the", "start": 2811.3, "duration": 6.019}, {"text": "demo", "start": 2814.96, "duration": 2.359}, {"text": "okay", "start": 2823.78, "duration": 2.66}, {"text": "let's see", "start": 2829.9, "duration": 2.54}, {"text": "okay so you want usually", "start": 2843.0, "duration": 4.54}, {"text": "Okay so", "start": 2872.8, "duration": 6.059}, {"text": "all right so you should all be seeing a", "start": 2875.98, "duration": 5.599}, {"text": "Jupiter notebook", "start": 2878.859, "duration": 2.72}, {"text": "um", "start": 2881.98, "duration": 2.0}, {"text": "okay so this is fine-tuning a", "start": 2888.7, "duration": 3.78}, {"text": "pre-trained", "start": 2891.339, "duration": 4.74}, {"text": "uh network uh namely the vgg on the cats", "start": 2892.48, "duration": 5.76}, {"text": "and dogs classification", "start": 2896.079, "duration": 4.681}, {"text": "okay so let's go through here we're just", "start": 2898.24, "duration": 4.859}, {"text": "going to import Keras here and note that", "start": 2900.76, "duration": 3.96}, {"text": "this is using the tensorflow backend", "start": 2903.099, "duration": 3.781}, {"text": "because Keras can run on top of", "start": 2904.72, "duration": 3.359}, {"text": "tensorflow", "start": 2906.88, "duration": 3.12}, {"text": "and then we'll import the libraries that", "start": 2908.079, "duration": 4.28}, {"text": "we need", "start": 2910.0, "duration": 2.359}, {"text": "um we're just printing out here", "start": 2913.119, "duration": 5.841}, {"text": "the different versions", "start": 2915.16, "duration": 3.8}, {"text": "um setting the log level so that it", "start": 2921.46, "duration": 5.94}, {"text": "doesn't spit out too many messages", "start": 2924.28, "duration": 7.339}, {"text": "set the random generator so that we can", "start": 2927.4, "duration": 6.86}, {"text": "have some control over", "start": 2931.619, "duration": 4.361}, {"text": "reproducibility", "start": 2934.26, "duration": 4.3}, {"text": "Okay so", "start": 2935.98, "duration": 6.42}, {"text": "here we are setting where we're telling", "start": 2938.56, "duration": 4.5}, {"text": "um", "start": 2942.4, "duration": 3.0}, {"text": "the model where the training and", "start": 2943.06, "duration": 5.4}, {"text": "validation data are so it's in our uh", "start": 2945.4, "duration": 6.48}, {"text": "it's in a data subdirectory and then the", "start": 2948.46, "duration": 5.28}, {"text": "training images are in train and then", "start": 2951.88, "duration": 4.08}, {"text": "the validation images are invalidation", "start": 2953.74, "duration": 5.7}, {"text": "and the number of samples we have 2 000", "start": 2955.96, "duration": 5.7}, {"text": "training samples and 800 validation", "start": 2959.44, "duration": 4.86}, {"text": "samples and image width and height or we", "start": 2961.66, "duration": 5.04}, {"text": "have one 150 by 150.", "start": 2964.3, "duration": 3.12}, {"text": "um", "start": 2966.7, "duration": 2.22}, {"text": "pixels", "start": 2967.42, "duration": 5.64}, {"text": "okay so this is we're gonna load", "start": 2968.92, "duration": 7.86}, {"text": "the vgg and then if you do a summary", "start": 2973.06, "duration": 5.279}, {"text": "you'll see", "start": 2976.78, "duration": 6.72}, {"text": "uh the layers that make up the vgg16", "start": 2978.339, "duration": 7.74}, {"text": "so there are convolutional layers", "start": 2983.5, "duration": 5.339}, {"text": "followed by Max bowling so that block", "start": 2986.079, "duration": 4.201}, {"text": "gets repeated right you have", "start": 2988.839, "duration": 3.121}, {"text": "convolutional layers followed by Max", "start": 2990.28, "duration": 3.6}, {"text": "bowling", "start": 2991.96, "duration": 3.48}, {"text": "um", "start": 2993.88, "duration": 4.4}, {"text": "and", "start": 2995.44, "duration": 2.84}, {"text": "so you can see that with this network", "start": 3000.56, "duration": 7.18}, {"text": "we have about uh you know 15 million", "start": 3004.319, "duration": 4.8}, {"text": "parameters", "start": 3007.74, "duration": 2.7}, {"text": "well that's a lot of that's a lot of", "start": 3009.119, "duration": 2.7}, {"text": "parameters right", "start": 3010.44, "duration": 6.84}, {"text": "okay so here we are going to add the Top", "start": 3011.819, "duration": 6.721}, {"text": "Model that we had trained before", "start": 3017.28, "duration": 2.52}, {"text": "previously", "start": 3018.54, "duration": 4.02}, {"text": "so I'll do that", "start": 3019.8, "duration": 6.6}, {"text": "all right and so what this says is we're", "start": 3022.56, "duration": 6.259}, {"text": "adding", "start": 3026.4, "duration": 2.419}, {"text": "the top level here and this is this", "start": 3029.099, "duration": 4.921}, {"text": "means that we're adding a fully", "start": 3032.819, "duration": 3.181}, {"text": "connected layer dance is fully connected", "start": 3034.02, "duration": 5.7}, {"text": "we're also adding drop out and the", "start": 3036.0, "duration": 6.599}, {"text": "percentage of Dropout is 50 here and", "start": 3039.72, "duration": 4.32}, {"text": "then the activation we're using is", "start": 3042.599, "duration": 3.621}, {"text": "sigmoid", "start": 3044.04, "duration": 6.84}, {"text": "uh so we are here loading the weights", "start": 3046.22, "duration": 7.54}, {"text": "that we had trained previously", "start": 3050.88, "duration": 6.54}, {"text": "uh then we add the top model", "start": 3053.76, "duration": 6.0}, {"text": "and then since we're doing fine tuning", "start": 3057.42, "duration": 4.199}, {"text": "we're only going to be adjusting the", "start": 3059.76, "duration": 3.66}, {"text": "weights in the top convolutional block", "start": 3061.619, "duration": 4.381}, {"text": "so we're freezing everything before that", "start": 3063.42, "duration": 5.22}, {"text": "right so we're saying that everything up", "start": 3066.0, "duration": 5.46}, {"text": "to layer 15 freeze those weights we", "start": 3068.64, "duration": 4.5}, {"text": "don't want those weights to change", "start": 3071.46, "duration": 5.399}, {"text": "and then this is compiling it", "start": 3073.14, "duration": 5.52}, {"text": "compiling that means you know you're", "start": 3076.859, "duration": 3.421}, {"text": "putting together the model and then", "start": 3078.66, "duration": 5.399}, {"text": "we're looking at the model summary here", "start": 3080.28, "duration": 4.799}, {"text": "um", "start": 3084.059, "duration": 3.54}, {"text": "and so everything is the same except", "start": 3085.079, "duration": 6.181}, {"text": "here we see that these models we have", "start": 3087.599, "duration": 5.701}, {"text": "frozen so those models are I'm sorry", "start": 3091.26, "duration": 3.799}, {"text": "those parameters are not going to be", "start": 3093.3, "duration": 4.259}, {"text": "adjusted during training and we're only", "start": 3095.059, "duration": 5.201}, {"text": "going to be adjusting these models in", "start": 3097.559, "duration": 4.5}, {"text": "the top convolutional block as well as", "start": 3100.26, "duration": 3.839}, {"text": "the top model", "start": 3102.059, "duration": 5.121}, {"text": "Okay so", "start": 3104.099, "duration": 3.081}, {"text": "uh sure", "start": 3108.38, "duration": 3.179}, {"text": "fight and with the training stuff", "start": 3117.54, "duration": 6.059}, {"text": "um they should all be the same", "start": 3121.5, "duration": 4.2}, {"text": "so you can either crop for the training", "start": 3123.599, "duration": 3.661}, {"text": "set all the images should be the same so", "start": 3125.7, "duration": 5.1}, {"text": "you can either crop them or um", "start": 3127.26, "duration": 6.24}, {"text": "uh you know had", "start": 3130.8, "duration": 4.74}, {"text": "all right second question history what", "start": 3133.5, "duration": 4.4}, {"text": "would be the reason", "start": 3135.54, "duration": 2.36}, {"text": "non-trainable parameters we're freezing", "start": 3142.38, "duration": 4.679}, {"text": "so that means that we're freezing the", "start": 3144.839, "duration": 4.621}, {"text": "weights uh remember because we're all", "start": 3147.059, "duration": 6.381}, {"text": "we're doing fine tuning so we are only", "start": 3149.46, "duration": 3.98}, {"text": "um adjusting the weights of the top", "start": 3153.54, "duration": 4.44}, {"text": "convolutional block as well as the top", "start": 3156.18, "duration": 3.659}, {"text": "model and so", "start": 3157.98, "duration": 3.96}, {"text": "we are freezing these waves we don't", "start": 3159.839, "duration": 4.441}, {"text": "want those to change", "start": 3161.94, "duration": 4.34}, {"text": "because we're using a pre-trained model", "start": 3164.28, "duration": 4.559}, {"text": "third question which is your criteria", "start": 3166.28, "duration": 5.559}, {"text": "for using that number of", "start": 3168.839, "duration": 5.52}, {"text": "conversion layers contribution layers", "start": 3171.839, "duration": 5.161}, {"text": "and sigmoid activation", "start": 3174.359, "duration": 6.421}, {"text": "so the sigmoid you can you can also use", "start": 3177.0, "duration": 5.579}, {"text": "um", "start": 3180.78, "duration": 4.2}, {"text": "because this is a binary classification", "start": 3182.579, "duration": 5.04}, {"text": "you only have one input so you can use a", "start": 3184.98, "duration": 4.74}, {"text": "signaloid because that gives you zero", "start": 3187.619, "duration": 4.321}, {"text": "and one straight away", "start": 3189.72, "duration": 3.54}, {"text": "um", "start": 3191.94, "duration": 3.179}, {"text": "and then what was the second part of the", "start": 3193.26, "duration": 3.059}, {"text": "question", "start": 3195.119, "duration": 4.021}, {"text": "um this year um criteria produces that", "start": 3196.319, "duration": 5.341}, {"text": "number of convolution layers", "start": 3199.14, "duration": 4.679}, {"text": "oh so the number so this is the number", "start": 3201.66, "duration": 4.02}, {"text": "of convolutional layers that this came", "start": 3203.819, "duration": 5.101}, {"text": "with the vgg 16 this was not uh this you", "start": 3205.68, "duration": 4.5}, {"text": "know this was not something I came up", "start": 3208.92, "duration": 4.56}, {"text": "with the bgc16 was uh is a pre-trained", "start": 3210.18, "duration": 5.159}, {"text": "model and they did lots of experiments", "start": 3213.48, "duration": 3.54}, {"text": "to figure out you know the best number", "start": 3215.339, "duration": 2.581}, {"text": "of", "start": 3217.02, "duration": 3.12}, {"text": "convolutional layers but usually what", "start": 3217.92, "duration": 3.899}, {"text": "you see is two or three convolutional", "start": 3220.14, "duration": 4.14}, {"text": "layers followed by a Max pooling and", "start": 3221.819, "duration": 3.841}, {"text": "then that gets repeated several times", "start": 3224.28, "duration": 4.68}, {"text": "and oftentimes you have to do you know", "start": 3225.66, "duration": 4.919}, {"text": "kind of experimental results to", "start": 3228.96, "duration": 4.8}, {"text": "determine how many blocks you need for", "start": 3230.579, "duration": 5.04}, {"text": "your application", "start": 3233.76, "duration": 4.859}, {"text": "but this the structure of the vgg just", "start": 3235.619, "duration": 6.801}, {"text": "came with the BGG so", "start": 3238.619, "duration": 3.801}, {"text": "okay", "start": 3242.579, "duration": 4.381}, {"text": "um so I'm going to", "start": 3244.02, "duration": 5.039}, {"text": "start this because it might take a long", "start": 3246.96, "duration": 4.5}, {"text": "time to actually do the", "start": 3249.059, "duration": 5.3}, {"text": "training", "start": 3251.46, "duration": 2.899}, {"text": "so I'm going to start that and then we", "start": 3256.2, "duration": 3.119}, {"text": "can go back and talk to talk about it", "start": 3257.76, "duration": 3.24}, {"text": "okay so again", "start": 3259.319, "duration": 2.341}, {"text": "um", "start": 3261.0, "duration": 2.52}, {"text": "this is the total number of parameters", "start": 3261.66, "duration": 4.919}, {"text": "we're freezing everything up into the", "start": 3263.52, "duration": 5.16}, {"text": "last convolutional block and then we're", "start": 3266.579, "duration": 3.54}, {"text": "just training", "start": 3268.68, "duration": 4.62}, {"text": "we're just adjusting the", "start": 3270.119, "duration": 5.101}, {"text": "um parameters in the convolutional block", "start": 3273.3, "duration": 4.08}, {"text": "as well as the top model", "start": 3275.22, "duration": 6.48}, {"text": "okay so here is training preparing the", "start": 3277.38, "duration": 5.88}, {"text": "data", "start": 3281.7, "duration": 3.68}, {"text": "um", "start": 3283.26, "duration": 2.12}, {"text": "this says", "start": 3286.02, "duration": 3.18}, {"text": "um", "start": 3288.0, "duration": 4.2}, {"text": "that we are rescaling the pixel values", "start": 3289.2, "duration": 4.919}, {"text": "to be between zero and one that just", "start": 3292.2, "duration": 3.84}, {"text": "speeds up training because if you have", "start": 3294.119, "duration": 5.521}, {"text": "two big uh of values then the gradient", "start": 3296.04, "duration": 5.519}, {"text": "can get saturated and that slows", "start": 3299.64, "duration": 2.9}, {"text": "training", "start": 3301.559, "duration": 4.861}, {"text": "and what we see here is data", "start": 3302.54, "duration": 5.74}, {"text": "augmentation so data augmentation is", "start": 3306.42, "duration": 4.86}, {"text": "technique to add volume as well as", "start": 3308.28, "duration": 6.299}, {"text": "variability to your data set so this", "start": 3311.28, "duration": 6.18}, {"text": "means that for instead of just using", "start": 3314.579, "duration": 4.76}, {"text": "what's in the training data we're gonna", "start": 3317.46, "duration": 5.159}, {"text": "add images that are slight variations of", "start": 3319.339, "duration": 6.041}, {"text": "the training data so we're going to add", "start": 3322.619, "duration": 6.061}, {"text": "images that have some Shear to them uh", "start": 3325.38, "duration": 5.1}, {"text": "different Zoom levels and also we're", "start": 3328.68, "duration": 4.139}, {"text": "going to do a flip a horizontal flip so", "start": 3330.48, "duration": 4.44}, {"text": "that increases the size of our training", "start": 3332.819, "duration": 4.441}, {"text": "data as well as the variability and", "start": 3334.92, "duration": 4.26}, {"text": "that's important for getting the model", "start": 3337.26, "duration": 5.22}, {"text": "to focus on what is important for", "start": 3339.18, "duration": 5.879}, {"text": "learning this task and it also increases", "start": 3342.48, "duration": 5.94}, {"text": "the the number of training images", "start": 3345.059, "duration": 3.961}, {"text": "um", "start": 3348.42, "duration": 3.06}, {"text": "so that's what we do so we do", "start": 3349.02, "duration": 3.0}, {"text": "um", "start": 3351.48, "duration": 3.599}, {"text": "augmentation for training we don't do it", "start": 3352.02, "duration": 4.68}, {"text": "for validation because we don't need to", "start": 3355.079, "duration": 6.061}, {"text": "we just need to to look at the image and", "start": 3356.7, "duration": 5.639}, {"text": "the only thing that we are doing with", "start": 3361.14, "duration": 4.08}, {"text": "the validation data is to rescale the", "start": 3362.339, "duration": 5.461}, {"text": "pixel values", "start": 3365.22, "duration": 6.06}, {"text": "okay so here we are setting up", "start": 3367.8, "duration": 5.58}, {"text": "the train generator this means that it", "start": 3371.28, "duration": 3.24}, {"text": "will read", "start": 3373.38, "duration": 3.959}, {"text": "the images from this directory that we", "start": 3374.52, "duration": 4.16}, {"text": "gave it before", "start": 3377.339, "duration": 4.561}, {"text": "and it will we will do this in batch and", "start": 3378.68, "duration": 5.26}, {"text": "we are looking at a batch size of 16", "start": 3381.9, "duration": 3.419}, {"text": "here", "start": 3383.94, "duration": 4.08}, {"text": "and the class mode here is binary so", "start": 3385.319, "duration": 5.461}, {"text": "there's only one output and it the", "start": 3388.02, "duration": 4.74}, {"text": "target is either zero or one zero for", "start": 3390.78, "duration": 3.86}, {"text": "cats and one for dogs", "start": 3392.76, "duration": 5.52}, {"text": "same thing here for the validation", "start": 3394.64, "duration": 5.919}, {"text": "and", "start": 3398.28, "duration": 6.9}, {"text": "the model uh so then we we see that we", "start": 3400.559, "duration": 6.901}, {"text": "have 2000 images for training and 800", "start": 3405.18, "duration": 5.159}, {"text": "images for validation and each one has", "start": 3407.46, "duration": 5.159}, {"text": "two classes cats and dogs", "start": 3410.339, "duration": 7.621}, {"text": "okay so this is before uh fine-tuning so", "start": 3412.619, "duration": 6.841}, {"text": "we're looking let's focus on the", "start": 3417.96, "duration": 4.92}, {"text": "accuracy we are getting 0.944 for the", "start": 3419.46, "duration": 6.56}, {"text": "accuracy on the training data and then", "start": 3422.88, "duration": 6.42}, {"text": "0.8975 for the validation okay so here's", "start": 3426.02, "duration": 5.2}, {"text": "where we're going to fine-tune the model", "start": 3429.3, "duration": 4.86}, {"text": "and we only did it for five epics um", "start": 3431.22, "duration": 4.98}, {"text": "because you know just for the sake of", "start": 3434.16, "duration": 3.24}, {"text": "time", "start": 3436.2, "duration": 5.0}, {"text": "um so we are training for five epics", "start": 3437.4, "duration": 3.8}, {"text": "um so", "start": 3441.24, "duration": 4.98}, {"text": "this history thing uh keeps track of you", "start": 3442.92, "duration": 5.22}, {"text": "know parameter values that happen during", "start": 3446.22, "duration": 3.899}, {"text": "training", "start": 3448.14, "duration": 2.939}, {"text": "um", "start": 3450.119, "duration": 4.681}, {"text": "and so we are this is the output it has", "start": 3451.079, "duration": 6.061}, {"text": "trained for five epics and it outputs", "start": 3454.8, "duration": 4.14}, {"text": "the loss the accuracy this is the", "start": 3457.14, "duration": 3.3}, {"text": "accuracy for", "start": 3458.94, "duration": 5.159}, {"text": "the training data so as you can see it", "start": 3460.44, "duration": 5.0}, {"text": "went from", "start": 3464.099, "duration": 4.681}, {"text": "0.83 up to about 0.96 for the training", "start": 3465.44, "duration": 5.58}, {"text": "data and for the validation it went from", "start": 3468.78, "duration": 6.839}, {"text": "0.911 to 0.9187", "start": 3471.02, "duration": 8.02}, {"text": "okay so this is after fine-tuning then", "start": 3475.619, "duration": 5.72}, {"text": "this is what we get", "start": 3479.04, "duration": 6.0}, {"text": "for the training validation and for the", "start": 3481.339, "duration": 6.76}, {"text": "testing for the uh validation for the", "start": 3485.04, "duration": 5.1}, {"text": "testing accuracy this is a training", "start": 3488.099, "duration": 3.901}, {"text": "accuracy and this is the", "start": 3490.14, "duration": 3.06}, {"text": "uh", "start": 3492.0, "duration": 3.96}, {"text": "accuracy on the validation side", "start": 3493.2, "duration": 6.359}, {"text": "okay so we went from", "start": 3495.96, "duration": 5.099}, {"text": "zero point", "start": 3499.559, "duration": 3.841}, {"text": "uh eight nine seven five with a", "start": 3501.059, "duration": 5.881}, {"text": "validation before fine-tuning to zero", "start": 3503.4, "duration": 6.3}, {"text": "point uh nine one eight seven five at", "start": 3506.94, "duration": 4.74}, {"text": "the end of fine tuning", "start": 3509.7, "duration": 4.98}, {"text": "okay so and then so we want to save the", "start": 3511.68, "duration": 5.52}, {"text": "model and the weights here", "start": 3514.68, "duration": 4.139}, {"text": "um we're going to call it fine tune and", "start": 3517.2, "duration": 4.379}, {"text": "the model file is going to be written", "start": 3518.819, "duration": 5.341}, {"text": "out here and the weights file will be", "start": 3521.579, "duration": 4.26}, {"text": "written out here", "start": 3524.16, "duration": 4.8}, {"text": "okay and then this just shows that we", "start": 3525.839, "duration": 4.801}, {"text": "can load the model", "start": 3528.96, "duration": 4.5}, {"text": "and then test again oh we haven't gone", "start": 3530.64, "duration": 4.439}, {"text": "through there okay so let's save the", "start": 3533.46, "duration": 3.599}, {"text": "model", "start": 3535.079, "duration": 3.601}, {"text": "now that we're trained we want to save", "start": 3537.059, "duration": 4.981}, {"text": "those model the model weights", "start": 3538.68, "duration": 5.46}, {"text": "and then let's see if we load it again", "start": 3542.04, "duration": 4.079}, {"text": "we should get exactly the same", "start": 3544.14, "duration": 4.199}, {"text": "results", "start": 3546.119, "duration": 4.281}, {"text": "okay", "start": 3548.339, "duration": 5.22}, {"text": "and then we can print the training", "start": 3550.4, "duration": 4.24}, {"text": "history", "start": 3553.559, "duration": 4.141}, {"text": "so this prints the validation loss and", "start": 3554.64, "duration": 4.199}, {"text": "accuracy", "start": 3557.7, "duration": 5.099}, {"text": "and then we can also plot the training", "start": 3558.839, "duration": 6.061}, {"text": "accuracy in the blue versus the", "start": 3562.799, "duration": 4.441}, {"text": "validation accuracy which is in the", "start": 3564.9, "duration": 4.52}, {"text": "orange", "start": 3567.24, "duration": 2.18}, {"text": "okay so now let's use our fine-tuned", "start": 3569.46, "duration": 4.98}, {"text": "bottle to predict", "start": 3572.579, "duration": 4.201}, {"text": "uh the class of an image", "start": 3574.44, "duration": 5.1}, {"text": "okay so here's a cat", "start": 3576.78, "duration": 6.539}, {"text": "number zero is cat and dog is one so", "start": 3579.54, "duration": 5.759}, {"text": "this it's pretty you know it did a", "start": 3583.319, "duration": 4.141}, {"text": "really good job here it has a high", "start": 3585.299, "duration": 4.56}, {"text": "confidence that this is a cat and that's", "start": 3587.46, "duration": 5.28}, {"text": "good because it is a cat image of a cat", "start": 3589.859, "duration": 6.24}, {"text": "okay so how about if we look at a dog", "start": 3592.74, "duration": 5.16}, {"text": "this is a picture of a dog and it does", "start": 3596.099, "duration": 4.74}, {"text": "have a very good prediction score for a", "start": 3597.9, "duration": 4.26}, {"text": "dog as well", "start": 3600.839, "duration": 3.78}, {"text": "okay so let's look at something that may", "start": 3602.16, "duration": 3.6}, {"text": "not be", "start": 3604.619, "duration": 2.581}, {"text": "as clear", "start": 3605.76, "duration": 3.66}, {"text": "so this is a picture of a dog", "start": 3607.2, "duration": 4.859}, {"text": "but it thinks that it's a cat but if you", "start": 3609.42, "duration": 5.28}, {"text": "look at the picture right if you look at", "start": 3612.059, "duration": 5.04}, {"text": "it really quickly you might think that", "start": 3614.7, "duration": 4.26}, {"text": "that's a cat too because of the pose and", "start": 3617.099, "duration": 4.621}, {"text": "the furriness of of the animal and it's", "start": 3618.96, "duration": 5.28}, {"text": "also kind of a dark image", "start": 3621.72, "duration": 5.22}, {"text": "so that's probably why it didn't do as", "start": 3624.24, "duration": 5.819}, {"text": "well on this image as the other images", "start": 3626.94, "duration": 6.48}, {"text": "okay so that's a demo of uh fine tuning", "start": 3630.059, "duration": 5.821}, {"text": "which is a transfer learning approach", "start": 3633.42, "duration": 4.32}, {"text": "Okay so", "start": 3635.88, "duration": 4.88}, {"text": "I think we are", "start": 3637.74, "duration": 3.02}, {"text": "[Music]", "start": 3641.83, "duration": 1.989}, {"text": "um", "start": 3642.299, "duration": 4.5}, {"text": "close to where over time actually but", "start": 3643.819, "duration": 6.661}, {"text": "let me just kind of go back to", "start": 3646.799, "duration": 3.681}, {"text": "um the PowerPoint here and kind of", "start": 3651.96, "duration": 4.639}, {"text": "finish this out", "start": 3653.819, "duration": 2.78}, {"text": "okay so we just did that okay so here", "start": 3661.559, "duration": 4.441}, {"text": "are the topics that uh we covered we", "start": 3663.299, "duration": 4.8}, {"text": "talked about some uh basics of neural", "start": 3666.0, "duration": 3.839}, {"text": "networks and then talk about deep", "start": 3668.099, "duration": 4.2}, {"text": "Network layers the architectures and the", "start": 3669.839, "duration": 4.98}, {"text": "libraries that you can use to build your", "start": 3672.299, "duration": 4.02}, {"text": "own deep learning model and then we", "start": 3674.819, "duration": 2.881}, {"text": "talked about transfer learning and we", "start": 3676.319, "duration": 3.121}, {"text": "looked at any chance for learning demo", "start": 3677.7, "duration": 4.919}, {"text": "and I have some resources here", "start": 3679.44, "duration": 6.06}, {"text": "um this this is a Stanford class on", "start": 3682.619, "duration": 4.861}, {"text": "neural networks and cnns in particular", "start": 3685.5, "duration": 3.359}, {"text": "and it's excellent it's an excellent", "start": 3687.48, "duration": 4.2}, {"text": "source the Keras documentation is great", "start": 3688.859, "duration": 4.521}, {"text": "if you want to look at", "start": 3691.68, "duration": 4.679}, {"text": "how to use Keras to create your own deep", "start": 3693.38, "duration": 5.979}, {"text": "learning model tensorflow also has a", "start": 3696.359, "duration": 6.24}, {"text": "couple of really good sites for you to", "start": 3699.359, "duration": 5.581}, {"text": "look at and then there are references to", "start": 3702.599, "duration": 4.621}, {"text": "unit lstm and Gans", "start": 3704.94, "duration": 4.5}, {"text": "and then this is", "start": 3707.22, "duration": 3.42}, {"text": "um these are the references for the", "start": 3709.44, "duration": 4.139}, {"text": "transfer learning demo so I think we are", "start": 3710.64, "duration": 4.02}, {"text": "at the end", "start": 3713.579, "duration": 1.681}, {"text": "um", "start": 3714.66, "duration": 4.08}, {"text": "we probably have a couple of minutes for", "start": 3715.26, "duration": 7.039}, {"text": "questions so people want to stick around", "start": 3718.74, "duration": 3.559}, {"text": "there were a couple of questions", "start": 3726.38, "duration": 4.199}, {"text": "and then how many convolution layers you", "start": 3737.54, "duration": 6.42}, {"text": "need to be there before", "start": 3740.46, "duration": 3.5}, {"text": "um so there's no", "start": 3745.2, "duration": 5.399}, {"text": "uh the problem with", "start": 3747.599, "duration": 4.801}, {"text": "deep learning models is that there's no", "start": 3750.599, "duration": 3.121}, {"text": "Theory so a lot of it is just", "start": 3752.4, "duration": 4.38}, {"text": "experimental a lot of people use two or", "start": 3753.72, "duration": 4.619}, {"text": "three convolutional layers and Then", "start": 3756.78, "duration": 4.26}, {"text": "followed by a Max pooling layer", "start": 3758.339, "duration": 4.681}, {"text": "um", "start": 3761.04, "duration": 6.18}, {"text": "so uh it's really you know", "start": 3763.02, "duration": 6.24}, {"text": "kind of a trial and error process to", "start": 3767.22, "duration": 4.16}, {"text": "building the right", "start": 3769.26, "duration": 4.799}, {"text": "architecture but", "start": 3771.38, "duration": 5.02}, {"text": "if you can use a pre-trained model I", "start": 3774.059, "duration": 4.081}, {"text": "would start out with that and then you", "start": 3776.4, "duration": 2.419}, {"text": "know", "start": 3778.14, "duration": 3.78}, {"text": "just fine tune or just use it for a", "start": 3778.819, "duration": 5.141}, {"text": "chance for learning for your particular", "start": 3781.92, "duration": 3.84}, {"text": "task", "start": 3783.96, "duration": 3.8}, {"text": "um", "start": 3785.76, "duration": 2.0}, {"text": "okay", "start": 3788.7, "duration": 2.72}, {"text": "there's so many things there are so many", "start": 3793.4, "duration": 5.98}, {"text": "parameters why isn't overfitting always", "start": 3796.859, "duration": 5.061}, {"text": "a problem", "start": 3799.38, "duration": 2.54}, {"text": "yeah so that's kind of um", "start": 3802.38, "duration": 5.52}, {"text": "kind of the magic that comes behind us I", "start": 3805.799, "duration": 4.081}, {"text": "think there's there's a lot of you know", "start": 3807.9, "duration": 4.399}, {"text": "other", "start": 3809.88, "duration": 2.419}, {"text": "um", "start": 3812.94, "duration": 3.54}, {"text": "things that that come into play here", "start": 3814.5, "duration": 3.42}, {"text": "that I don't think people know yet", "start": 3816.48, "duration": 3.18}, {"text": "because there's just no Theory", "start": 3817.92, "duration": 2.879}, {"text": "um there are things like you know the", "start": 3819.66, "duration": 2.459}, {"text": "residual Network", "start": 3820.799, "duration": 3.06}, {"text": "um that has residual connections that", "start": 3822.119, "duration": 3.0}, {"text": "kind of", "start": 3823.859, "duration": 3.72}, {"text": "um help with this overfitting because it", "start": 3825.119, "duration": 4.2}, {"text": "can you know you can", "start": 3827.579, "duration": 3.841}, {"text": "the residual connections allow you to", "start": 3829.319, "duration": 3.901}, {"text": "kind of skip over connections that you", "start": 3831.42, "duration": 5.22}, {"text": "don't need and it allows the model to", "start": 3833.22, "duration": 5.94}, {"text": "um just use what it needs and also", "start": 3836.64, "duration": 4.5}, {"text": "people have looked at it and it turns", "start": 3839.16, "duration": 4.02}, {"text": "out that", "start": 3841.14, "duration": 3.179}, {"text": "um", "start": 3843.18, "duration": 3.3}, {"text": "what people think is happening with a", "start": 3844.319, "duration": 5.28}, {"text": "deep model is that uh really what's", "start": 3846.48, "duration": 6.599}, {"text": "being created is a bunch of models so an", "start": 3849.599, "duration": 6.48}, {"text": "ensemble of models and so what comes out", "start": 3853.079, "duration": 4.801}, {"text": "at the end is really you know kind of", "start": 3856.079, "duration": 4.821}, {"text": "the conglomerate uh", "start": 3857.88, "duration": 6.0}, {"text": "processing of all these uh Ensemble of", "start": 3860.9, "duration": 5.32}, {"text": "models and so that might be the reason", "start": 3863.88, "duration": 5.219}, {"text": "why you know they work so well", "start": 3866.22, "duration": 5.28}, {"text": "um but there's a lot of", "start": 3869.099, "duration": 4.321}, {"text": "people trying to figure out you know why", "start": 3871.5, "duration": 4.2}, {"text": "these neural networks work so well but", "start": 3873.42, "duration": 2.82}, {"text": "um", "start": 3875.7, "duration": 4.2}, {"text": "it's uh yeah I think the theory is", "start": 3876.24, "duration": 7.5}, {"text": "really needed in this field", "start": 3879.9, "duration": 5.58}, {"text": "all right somebody would like you to", "start": 3883.74, "duration": 5.22}, {"text": "show 80 slide 81 again the num just the", "start": 3885.48, "duration": 5.7}, {"text": "references and then we have a question", "start": 3888.96, "duration": 4.619}, {"text": "are the number of parameters higher than", "start": 3891.18, "duration": 5.34}, {"text": "the number of data points here and I", "start": 3893.579, "duration": 4.321}, {"text": "think they're talking about the Jupiter", "start": 3896.52, "duration": 4.26}, {"text": "notebook example you showed yeah so", "start": 3897.9, "duration": 4.919}, {"text": "definitely the number of training", "start": 3900.78, "duration": 5.519}, {"text": "parameters is higher than the number of", "start": 3902.819, "duration": 6.061}, {"text": "training examples uh so let's see let me", "start": 3906.299, "duration": 5.181}, {"text": "go back to", "start": 3908.88, "duration": 2.6}, {"text": "PowerPoint", "start": 3916.98, "duration": 2.96}, {"text": "okay so what I", "start": 3934.92, "duration": 3.439}, {"text": "mean", "start": 3940.559, "duration": 5.28}, {"text": "I don't see my PowerPoint", "start": 3942.079, "duration": 6.641}, {"text": "that's interesting just do desktop you", "start": 3945.839, "duration": 4.561}, {"text": "find an um", "start": 3948.72, "duration": 3.06}, {"text": "the Powerpoints right there that's what", "start": 3950.4, "duration": 3.3}, {"text": "we're looking at well because I have it", "start": 3951.78, "duration": 4.339}, {"text": "on yeah", "start": 3953.7, "duration": 2.419}, {"text": "I see okay uh", "start": 3956.339, "duration": 8.301}, {"text": "which slide is it it's um", "start": 3959.64, "duration": 5.0}, {"text": "it's at the end after resources I think", "start": 3965.9, "duration": 5.08}, {"text": "they're even okay", "start": 3968.7, "duration": 4.98}, {"text": "okay there was a question about that oh", "start": 3970.98, "duration": 4.079}, {"text": "yeah you will make the slides available", "start": 3973.68, "duration": 5.54}, {"text": "so you don't have to write it down", "start": 3975.059, "duration": 4.161}, {"text": "there models for transfer learning for", "start": 3981.72, "duration": 5.52}, {"text": "sequential time series data", "start": 3984.0, "duration": 5.579}, {"text": "are there models for transfer learning", "start": 3987.24, "duration": 7.099}, {"text": "for sequential time series data", "start": 3989.579, "duration": 4.76}, {"text": "um it's an interesting question", "start": 3995.7, "duration": 4.139}, {"text": "um I am not sure about that I know that", "start": 3997.5, "duration": 4.26}, {"text": "there are databases that you know you", "start": 3999.839, "duration": 4.861}, {"text": "can use to train to to pre-train your", "start": 4001.76, "duration": 6.839}, {"text": "model on I'm not sure that uh I've seen", "start": 4004.7, "duration": 6.24}, {"text": "work where they actually do you know", "start": 4008.599, "duration": 4.02}, {"text": "transfer learning the same way but I", "start": 4010.94, "duration": 3.899}, {"text": "assume that you can", "start": 4012.619, "duration": 3.661}, {"text": "um because it's the same concept that", "start": 4014.839, "duration": 3.0}, {"text": "the problem though is that you know", "start": 4016.28, "duration": 2.88}, {"text": "images", "start": 4017.839, "duration": 3.181}, {"text": "there are lots of features that are", "start": 4019.16, "duration": 4.08}, {"text": "common in all sorts of images but if", "start": 4021.02, "duration": 4.14}, {"text": "you're talking about language especially", "start": 4023.24, "duration": 3.299}, {"text": "if you're", "start": 4025.16, "duration": 2.879}, {"text": "you're talking about different languages", "start": 4026.539, "duration": 3.901}, {"text": "I'm not sure what the commonality there", "start": 4028.039, "duration": 6.3}, {"text": "would be so I think that's the challenge", "start": 4030.44, "duration": 6.419}, {"text": "we still have a few questions so is", "start": 4034.339, "duration": 4.561}, {"text": "there no fundamental mathematical reason", "start": 4036.859, "duration": 5.601}, {"text": "this all works that we know of", "start": 4038.9, "duration": 7.26}, {"text": "well we what we know is about is has to", "start": 4042.46, "duration": 5.68}, {"text": "do with neural networks so you know", "start": 4046.16, "duration": 3.6}, {"text": "people have proven that a neural network", "start": 4048.14, "duration": 4.56}, {"text": "with just one hidden layer can learn any", "start": 4049.76, "duration": 4.079}, {"text": "function", "start": 4052.7, "duration": 2.879}, {"text": "so we know that from a from a neural", "start": 4053.839, "duration": 3.601}, {"text": "network but when you talk about deep", "start": 4055.579, "duration": 3.361}, {"text": "networks I think it gets a little bit", "start": 4057.44, "duration": 3.899}, {"text": "more complicated because uh you know", "start": 4058.94, "duration": 4.02}, {"text": "there's so many factors that come into", "start": 4061.339, "duration": 3.24}, {"text": "play", "start": 4062.96, "duration": 4.079}, {"text": "um so many parameters", "start": 4064.579, "duration": 4.74}, {"text": "um I mean I they're probably some Theory", "start": 4067.039, "duration": 3.421}, {"text": "out there but I don't think there's", "start": 4069.319, "duration": 3.121}, {"text": "anything that tells you you know if you", "start": 4070.46, "duration": 4.079}, {"text": "use this many convolutional layer you're", "start": 4072.44, "duration": 4.56}, {"text": "gonna be able to approximate this", "start": 4074.539, "duration": 4.921}, {"text": "function so I think", "start": 4077.0, "duration": 4.079}, {"text": "I think a lot of that is still being", "start": 4079.46, "duration": 3.72}, {"text": "worked out", "start": 4081.079, "duration": 4.381}, {"text": "all right great presentation is there", "start": 4083.18, "duration": 4.8}, {"text": "any advantage to using the particular", "start": 4085.46, "duration": 5.159}, {"text": "version of tensorflow that you are using", "start": 4087.98, "duration": 5.52}, {"text": "in the example or is there a reason not", "start": 4090.619, "duration": 5.881}, {"text": "to move to tensorflow 2.0 at this time", "start": 4093.5, "duration": 4.38}, {"text": "yeah", "start": 4096.5, "duration": 4.319}, {"text": "oh 2.0 this was just an example you know", "start": 4097.88, "duration": 4.859}, {"text": "a demo that I've used before", "start": 4100.819, "duration": 3.841}, {"text": "um I know that tensorflow 2.0 actually", "start": 4102.739, "duration": 4.681}, {"text": "has carers as part of it so", "start": 4104.66, "duration": 4.619}, {"text": "um you know I I want to be able to see", "start": 4107.42, "duration": 3.54}, {"text": "how that is different from care so that", "start": 4109.279, "duration": 2.821}, {"text": "would be something interesting but yeah", "start": 4110.96, "duration": 3.12}, {"text": "definitely tensorflow is supposed to be", "start": 4112.1, "duration": 4.02}, {"text": "a lot better so I would encourage you if", "start": 4114.08, "duration": 3.119}, {"text": "you're starting out to start out with", "start": 4116.12, "duration": 4.139}, {"text": "tensor to tensorflow 2.0", "start": 4117.199, "duration": 5.64}, {"text": "okay question do numerical prediction", "start": 4120.259, "duration": 4.621}, {"text": "models use a different activation", "start": 4122.839, "duration": 4.641}, {"text": "function", "start": 4124.88, "duration": 2.6}, {"text": "no you don't have to the activation", "start": 4128.14, "duration": 5.32}, {"text": "functions in the on the hidden layer uh", "start": 4130.279, "duration": 6.301}, {"text": "you know you can use anything most", "start": 4133.46, "duration": 5.7}, {"text": "people use value for deep learning", "start": 4136.58, "duration": 4.86}, {"text": "models so that could work for", "start": 4139.16, "duration": 4.32}, {"text": "classification as well as regression now", "start": 4141.44, "duration": 3.72}, {"text": "on the output right you don't want to", "start": 4143.48, "duration": 3.719}, {"text": "use cross entropy for regression you", "start": 4145.16, "duration": 3.42}, {"text": "want to use something like NSE mean", "start": 4147.199, "duration": 2.761}, {"text": "squared error", "start": 4148.58, "duration": 4.32}, {"text": "so the output is what is important", "start": 4149.96, "duration": 4.92}, {"text": "the activation on the applicator for", "start": 4152.9, "duration": 3.359}, {"text": "regression", "start": 4154.88, "duration": 4.26}, {"text": "all right here's a tube three-part", "start": 4156.259, "duration": 5.761}, {"text": "question great job mine lots of people", "start": 4159.14, "duration": 5.099}, {"text": "saying great job and thanks can you", "start": 4162.02, "duration": 4.86}, {"text": "explain the dimensions used in layers of", "start": 4164.239, "duration": 5.1}, {"text": "the model for example in a demo you used", "start": 4166.88, "duration": 5.52}, {"text": "images of Dimension 150 by 150 by three", "start": 4169.339, "duration": 5.821}, {"text": "but one of the convolutional layers was", "start": 4172.4, "duration": 7.14}, {"text": "a size 18 by 18 by 512 what is the 512", "start": 4175.16, "duration": 6.86}, {"text": "for", "start": 4179.54, "duration": 2.48}, {"text": "um so the input Dimension the the", "start": 4183.319, "duration": 3.781}, {"text": "dimension of the input image just", "start": 4185.48, "duration": 3.54}, {"text": "depends on your training data right how", "start": 4187.1, "duration": 4.86}, {"text": "big uh of the images do you have and how", "start": 4189.02, "duration": 4.62}, {"text": "much do you want to focus what do you", "start": 4191.96, "duration": 4.14}, {"text": "want to focus in on uh the dimensions of", "start": 4193.64, "duration": 3.9}, {"text": "the convolutional layers have to do with", "start": 4196.1, "duration": 4.4}, {"text": "the number of filters", "start": 4197.54, "duration": 2.96}, {"text": "um so and each filter is essentially you", "start": 4200.719, "duration": 5.281}, {"text": "know looking at a different feature so", "start": 4203.84, "duration": 5.16}, {"text": "you could have you know uh 32 filters", "start": 4206.0, "duration": 6.12}, {"text": "looking at 32 different features", "start": 4209.0, "duration": 7.38}, {"text": "so so the the input dimension", "start": 4212.12, "duration": 7.14}, {"text": "is determined by your chaining data and", "start": 4216.38, "duration": 5.58}, {"text": "the dimension of the the hidden layers", "start": 4219.26, "duration": 4.919}, {"text": "the convolutional layers have to do with", "start": 4221.96, "duration": 5.54}, {"text": "the number of filters that you want", "start": 4224.179, "duration": 3.321}, {"text": "a couple more questions one more", "start": 4228.199, "duration": 5.101}, {"text": "question can Gans be used to do data", "start": 4230.42, "duration": 5.6}, {"text": "augmentation", "start": 4233.3, "duration": 2.72}, {"text": "I mean data augmentation is pretty", "start": 4236.32, "duration": 4.18}, {"text": "simple right I mean it's just these are", "start": 4238.46, "duration": 4.86}, {"text": "operations like you rotate you add Zoom", "start": 4240.5, "duration": 5.82}, {"text": "you add flips uh you got translation so", "start": 4243.32, "duration": 5.1}, {"text": "those are not things that you need to", "start": 4246.32, "duration": 5.879}, {"text": "learn uh so data augmentation I think", "start": 4248.42, "duration": 6.299}, {"text": "the operations are pretty", "start": 4252.199, "duration": 4.741}, {"text": "um basic so I don't think you need to", "start": 4254.719, "duration": 4.44}, {"text": "use again for that I think again would", "start": 4256.94, "duration": 5.46}, {"text": "be more appropriate if you wanted to", "start": 4259.159, "duration": 6.121}, {"text": "um you know given an image you want", "start": 4262.4, "duration": 5.16}, {"text": "a slightly different version of that", "start": 4265.28, "duration": 3.419}, {"text": "image", "start": 4267.56, "duration": 3.72}, {"text": "or uh you know to generate", "start": 4268.699, "duration": 2.941}, {"text": "[Music]", "start": 4271.28, "duration": 1.919}, {"text": "um", "start": 4271.64, "duration": 3.48}, {"text": "you know things like if you see a", "start": 4273.199, "duration": 3.601}, {"text": "photograph you want to be able to create", "start": 4275.12, "duration": 4.14}, {"text": "an emoji from it so that that's the kind", "start": 4276.8, "duration": 3.359}, {"text": "of thing that you want to do with the", "start": 4279.26, "duration": 3.08}, {"text": "game", "start": 4280.159, "duration": 2.181}, {"text": "question what are the characteristics of", "start": 4283.159, "duration": 4.801}, {"text": "a good training data set and is there a", "start": 4285.32, "duration": 6.0}, {"text": "Best Practices guide and can I run deep", "start": 4287.96, "duration": 5.64}, {"text": "learning models and exceed out using", "start": 4291.32, "duration": 5.22}, {"text": "exceed allocations like on using jet", "start": 4293.6, "duration": 5.76}, {"text": "stream are there tutorials for that", "start": 4296.54, "duration": 6.0}, {"text": "tutorials for using um", "start": 4299.36, "duration": 5.819}, {"text": "for running deep learning models using", "start": 4302.54, "duration": 5.28}, {"text": "exceed allocations well you can use an", "start": 4305.179, "duration": 5.04}, {"text": "exceed allocation for anything in the", "start": 4307.82, "duration": 3.96}, {"text": "GitHub repo", "start": 4310.219, "duration": 4.98}, {"text": "yeah lots of GitHub repos that you can", "start": 4311.78, "duration": 5.16}, {"text": "look at", "start": 4315.199, "duration": 3.361}, {"text": "um the other part of the question has to", "start": 4316.94, "duration": 3.92}, {"text": "do with how what is", "start": 4318.56, "duration": 5.58}, {"text": "characteristics of a glitching data so", "start": 4320.86, "duration": 5.14}, {"text": "you want", "start": 4324.14, "duration": 4.88}, {"text": "you know more data is always better", "start": 4326.0, "duration": 8.4}, {"text": "and you want to be able to um", "start": 4329.02, "duration": 8.62}, {"text": "the the data has to", "start": 4334.4, "duration": 6.24}, {"text": "capture the problem well so a lot of", "start": 4337.64, "duration": 6.0}, {"text": "times uh with real world data it's", "start": 4340.64, "duration": 5.099}, {"text": "coming up with label data that's really", "start": 4343.64, "duration": 4.2}, {"text": "the challenge", "start": 4345.739, "duration": 4.381}, {"text": "um so you know if you if it's a", "start": 4347.84, "duration": 3.899}, {"text": "classification problem then you want to", "start": 4350.12, "duration": 3.3}, {"text": "make sure that you have the correct", "start": 4351.739, "duration": 4.881}, {"text": "labels and that you have sufficient data", "start": 4353.42, "duration": 5.64}, {"text": "and then the other important thing is", "start": 4356.62, "duration": 4.9}, {"text": "when you train of course you want a", "start": 4359.06, "duration": 4.679}, {"text": "training and a validation and you want", "start": 4361.52, "duration": 4.62}, {"text": "to be able to make sure that when you", "start": 4363.739, "duration": 4.381}, {"text": "partition your available data set into", "start": 4366.14, "duration": 4.4}, {"text": "train and validation that", "start": 4368.12, "duration": 4.8}, {"text": "you know the the chain of validation", "start": 4370.54, "duration": 3.88}, {"text": "data sets have the same characteristics", "start": 4372.92, "duration": 4.56}, {"text": "right so you don't want uh for example", "start": 4374.42, "duration": 5.16}, {"text": "all negative examples should be in your", "start": 4377.48, "duration": 3.78}, {"text": "training data and no negative examples", "start": 4379.58, "duration": 3.36}, {"text": "at all in your validation so they have", "start": 4381.26, "duration": 3.959}, {"text": "to have the same distribution of", "start": 4382.94, "duration": 4.5}, {"text": "positive and negative examples for", "start": 4385.219, "duration": 4.581}, {"text": "example", "start": 4387.44, "duration": 2.36}, {"text": "okay yeah thanks very much thanks very", "start": 4397.34, "duration": 4.14}, {"text": "much everyone we'll wrap it up um if you", "start": 4399.56, "duration": 3.36}, {"text": "have questions we can follow up via", "start": 4401.48, "duration": 4.08}, {"text": "email I posted a link to the slides and", "start": 4402.92, "duration": 5.58}, {"text": "we appreciate your interest", "start": 4405.56, "duration": 6.38}, {"text": "all right thanks everybody for joining", "start": 4408.5, "duration": 3.44}]