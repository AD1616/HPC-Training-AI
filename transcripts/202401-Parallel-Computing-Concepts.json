[{"text": "okay so well welcome to the um to to our", "start": 2.12, "duration": 3.96}, {"text": "first webinar of the year I'm going to", "start": 4.72, "duration": 3.4}, {"text": "be covering a topic that I call parallel", "start": 6.08, "duration": 4.92}, {"text": "Computing Concepts um Cindy is going to", "start": 8.12, "duration": 4.96}, {"text": "put a couple of links into the into the", "start": 11.0, "duration": 5.56}, {"text": "chat the the first one um is location", "start": 13.08, "duration": 5.16}, {"text": "where we're going to have the PDFs for", "start": 16.56, "duration": 3.479}, {"text": "for all of the present for all the", "start": 18.24, "duration": 4.119}, {"text": "presentations in this series um", "start": 20.039, "duration": 4.56}, {"text": "including including today's talk and", "start": 22.359, "duration": 6.041}, {"text": "then I also have a link to a GitHub repo", "start": 24.599, "duration": 5.481}, {"text": "that that covers some of the examples", "start": 28.4, "duration": 3.56}, {"text": "going to be going over so today is going", "start": 30.08, "duration": 3.639}, {"text": "to be mostly lectured just a little bit", "start": 31.96, "duration": 3.48}, {"text": "of Hands-On at the end but I think", "start": 33.719, "duration": 4.441}, {"text": "you're going to find the um find the", "start": 35.44, "duration": 5.439}, {"text": "notebook very very um very", "start": 38.16, "duration": 4.68}, {"text": "helpful all right so just a little bit", "start": 40.879, "duration": 4.881}, {"text": "about complex so this is a new program", "start": 42.84, "duration": 5.399}, {"text": "um started late last year funded by the", "start": 45.76, "duration": 6.2}, {"text": "National Science Foundation so the idea", "start": 48.239, "duration": 6.0}, {"text": "behind complex or comprehensive learning", "start": 51.96, "duration": 4.72}, {"text": "for end users to effectively utilize", "start": 54.239, "duration": 5.041}, {"text": "cyber infrastructure is that we want to", "start": 56.68, "duration": 4.719}, {"text": "cover the train that non-programmers", "start": 59.28, "duration": 4.48}, {"text": "will need in order to effectively use", "start": 61.399, "duration": 6.001}, {"text": "supercomputers so we know that um o over", "start": 63.76, "duration": 6.12}, {"text": "the years more and more of our user base", "start": 67.4, "duration": 4.48}, {"text": "is somebody who's using somebody else's", "start": 69.88, "duration": 4.0}, {"text": "application but there's still a lot that", "start": 71.88, "duration": 3.36}, {"text": "you need to know about parallel", "start": 73.88, "duration": 3.36}, {"text": "Computing So today we're going to be", "start": 75.24, "duration": 3.44}, {"text": "starting off with parallel Computing", "start": 77.24, "duration": 3.839}, {"text": "Concepts other topics in the series are", "start": 78.68, "duration": 4.24}, {"text": "going to include Linux tools and Bash", "start": 81.079, "duration": 4.641}, {"text": "scripting security batch Computing how", "start": 82.92, "duration": 4.839}, {"text": "to get help data management and", "start": 85.72, "duration": 4.88}, {"text": "interactive computing", "start": 87.759, "duration": 4.36}, {"text": "so with that I'm going to jump I'm going", "start": 90.6, "duration": 3.839}, {"text": "to jump into my presentation start off", "start": 92.119, "duration": 4.081}, {"text": "with a little bit of an introduction", "start": 94.439, "duration": 3.441}, {"text": "we're there we're then going to go over", "start": 96.2, "duration": 5.36}, {"text": "processes threads MPI and openmp hybrid", "start": 97.88, "duration": 6.72}, {"text": "applications om's law other limits on", "start": 101.56, "duration": 4.64}, {"text": "the scalability of your", "start": 104.6, "duration": 3.96}, {"text": "applications um running parallel", "start": 106.2, "duration": 4.879}, {"text": "applications and doing a what we call a", "start": 108.56, "duration": 4.8}, {"text": "scaling study and where to go next for", "start": 111.079, "duration": 5.201}, {"text": "for um for more", "start": 113.36, "duration": 2.92}, {"text": "information so I'm going to say that the", "start": 116.719, "duration": 4.281}, {"text": "reason most you here is that your", "start": 119.24, "duration": 3.72}, {"text": "workloads have grown to the point where", "start": 121.0, "duration": 4.119}, {"text": "you can no longer run them on your local", "start": 122.96, "duration": 4.199}, {"text": "resources so most of you have probably", "start": 125.119, "duration": 4.28}, {"text": "been doing Computing on on laptops and", "start": 127.159, "duration": 5.961}, {"text": "desktop systems up till now um but now", "start": 129.399, "duration": 5.681}, {"text": "Ju Just the amount of data the length", "start": 133.12, "duration": 3.8}, {"text": "the calculations the size of the", "start": 135.08, "duration": 4.4}, {"text": "simulation that grown so you may need to", "start": 136.92, "duration": 4.64}, {"text": "start using um parallel Computing and", "start": 139.48, "duration": 4.16}, {"text": "the types of resources that we provide", "start": 141.56, "duration": 4.2}, {"text": "here at San Diego supercomputer", "start": 143.64, "duration": 5.599}, {"text": "supercomputer Center and other um netn", "start": 145.76, "duration": 5.4}, {"text": "net National", "start": 149.239, "duration": 6.121}, {"text": "centers so this session is intended", "start": 151.16, "duration": 7.24}, {"text": "for I say anyone who currently runs", "start": 155.36, "duration": 4.879}, {"text": "plans Runners thinking about running", "start": 158.4, "duration": 4.28}, {"text": "applications on Parallel", "start": 160.239, "duration": 4.92}, {"text": "computers um who writes proposals for", "start": 162.68, "duration": 4.559}, {"text": "computer time on campus clusters", "start": 165.159, "duration": 4.281}, {"text": "nationally nationally allocated systems", "start": 167.239, "duration": 4.28}, {"text": "or other", "start": 169.44, "duration": 4.439}, {"text": "resources if you purchase time on", "start": 171.519, "duration": 4.561}, {"text": "compute resources say in the cloud and", "start": 173.879, "duration": 4.881}, {"text": "you want to maximize the return on your", "start": 176.08, "duration": 4.519}, {"text": "investment if you're considering", "start": 178.76, "duration": 4.759}, {"text": "purchasing hardware for your lab or", "start": 180.599, "duration": 4.321}, {"text": "you're just simply curious about", "start": 183.519, "duration": 3.64}, {"text": "parallel Computing so all good reasons", "start": 184.92, "duration": 3.52}, {"text": "to be", "start": 187.159, "duration": 3.321}, {"text": "here so we're going to talk a little bit", "start": 188.44, "duration": 3.519}, {"text": "about the motivation for for doing", "start": 190.48, "duration": 4.72}, {"text": "parallel Computing and and and this this", "start": 191.959, "duration": 5.881}, {"text": "session in particular so much of the", "start": 195.2, "duration": 4.56}, {"text": "training in parallel Computing is", "start": 197.84, "duration": 4.039}, {"text": "targeted people who write their own", "start": 199.76, "duration": 4.32}, {"text": "parallel application and we've done this", "start": 201.879, "duration": 4.601}, {"text": "for many many years we focus on things", "start": 204.08, "duration": 6.68}, {"text": "like MPI openmp Cuda profil applications", "start": 206.48, "duration": 6.479}, {"text": "Performance Tuning all very very", "start": 210.76, "duration": 4.679}, {"text": "important topics but if you're not", "start": 212.959, "duration": 4.521}, {"text": "actually writing your own code if you're", "start": 215.439, "duration": 4.281}, {"text": "primarily using somebody else's", "start": 217.48, "duration": 3.959}, {"text": "application you really don't need to", "start": 219.72, "duration": 4.079}, {"text": "know these topics I mentioned them", "start": 221.439, "duration": 3.88}, {"text": "earlier", "start": 223.799, "duration": 3.881}, {"text": "um we are not going to go into any of", "start": 225.319, "duration": 5.56}, {"text": "the details of MPI openmp or Cuda but", "start": 227.68, "duration": 5.199}, {"text": "just maybe touching them briefly so that", "start": 230.879, "duration": 3.801}, {"text": "you know what they", "start": 232.879, "duration": 6.321}, {"text": "are um now because of the way Compu", "start": 234.68, "duration": 6.399}, {"text": "training has has been done for parallel", "start": 239.2, "duration": 4.319}, {"text": "Computing often times the end users who", "start": 241.079, "duration": 4.88}, {"text": "are not developers rarely or never get a", "start": 243.519, "duration": 4.201}, {"text": "proper introduction to parallel", "start": 245.959, "duration": 3.12}, {"text": "Computing", "start": 247.72, "duration": 3.92}, {"text": "Concepts so even if you don't write code", "start": 249.079, "duration": 4.201}, {"text": "it's still important that you understand", "start": 251.64, "duration": 3.56}, {"text": "some of the basic principles so you can", "start": 253.28, "duration": 3.639}, {"text": "make the most effective use of the", "start": 255.2, "duration": 4.999}, {"text": "valuable Advanced cyber", "start": 256.919, "duration": 5.521}, {"text": "infrastructure so just talking about a", "start": 260.199, "duration": 4.321}, {"text": "few of the myths um so surrounding", "start": 262.44, "duration": 4.36}, {"text": "parallel Computing um and I've come", "start": 264.52, "duration": 4.8}, {"text": "across these many times um so first of", "start": 266.8, "duration": 5.0}, {"text": "all this idea that parallel Computing is", "start": 269.32, "duration": 5.599}, {"text": "for astrophysicists Engineers climate", "start": 271.8, "duration": 5.72}, {"text": "modelers others working in traditionally", "start": 274.919, "duration": 4.241}, {"text": "math intensive", "start": 277.52, "duration": 3.72}, {"text": "fields and I'm going to say that this", "start": 279.16, "duration": 4.28}, {"text": "might have been partially true decades", "start": 281.24, "duration": 4.56}, {"text": "ago today nearly every field of research", "start": 283.44, "duration": 4.479}, {"text": "makes use parallel Computing this", "start": 285.8, "duration": 4.08}, {"text": "includes the social sciences Life", "start": 287.919, "duration": 4.56}, {"text": "Sciences Arts and Humanities in in", "start": 289.88, "duration": 5.0}, {"text": "addition to our um traditional user base", "start": 292.479, "duration": 4.72}, {"text": "of physics chemistry engineering", "start": 294.88, "duration": 3.68}, {"text": "Material", "start": 297.199, "duration": 3.0}, {"text": "Science", "start": 298.56, "duration": 3.68}, {"text": "um another meth parallel Computing is", "start": 300.199, "duration": 4.361}, {"text": "that if we throw more Hardware at a", "start": 302.24, "duration": 4.12}, {"text": "problem it will automatically reduce the", "start": 304.56, "duration": 3.359}, {"text": "time to", "start": 306.36, "duration": 3.679}, {"text": "solution and I think most of you", "start": 307.919, "duration": 3.681}, {"text": "probably know this already but parallel", "start": 310.039, "duration": 3.16}, {"text": "Computing is only going to help if you", "start": 311.6, "duration": 4.0}, {"text": "have an application that has been", "start": 313.199, "duration": 4.081}, {"text": "specifically written to take advantage", "start": 315.6, "duration": 4.08}, {"text": "of parallel hardware and even if you do", "start": 317.28, "duration": 4.24}, {"text": "have a parallel code there's an inherent", "start": 319.68, "duration": 4.4}, {"text": "limit on the", "start": 321.52, "duration": 2.56}, {"text": "scalability um one caveat though is that", "start": 324.28, "duration": 4.359}, {"text": "a high througha Computing workload can", "start": 326.84, "duration": 4.6}, {"text": "use parallel Compu to run many single", "start": 328.639, "duration": 4.84}, {"text": "core or single GPU instances of an", "start": 331.44, "duration": 4.599}, {"text": "application and this would give you um", "start": 333.479, "duration": 5.081}, {"text": "essentially perfect", "start": 336.039, "duration": 5.481}, {"text": "scaling and then finally another myth is", "start": 338.56, "duration": 4.52}, {"text": "that you need to be a programmer or a", "start": 341.52, "duration": 3.679}, {"text": "software developer to make use parallel", "start": 343.08, "duration": 4.08}, {"text": "Computing and as I mentioned in the", "start": 345.199, "duration": 4.041}, {"text": "beginning most users of parallel", "start": 347.16, "duration": 4.52}, {"text": "computers nowadays are not programmers", "start": 349.24, "duration": 4.64}, {"text": "instead they're using mature thirdparty", "start": 351.68, "duration": 3.6}, {"text": "software that had been developed", "start": 353.88, "duration": 3.36}, {"text": "elsewhere and made available to the", "start": 355.28, "duration": 5.0}, {"text": "community so in a nutshell many of you", "start": 357.24, "duration": 4.64}, {"text": "are going to be using somebody else's", "start": 360.28, "duration": 4.56}, {"text": "code if you're doing climate and weather", "start": 361.88, "duration": 5.12}, {"text": "um simulations you're going to use codes", "start": 364.84, "duration": 4.16}, {"text": "like Warf that is show over on the left", "start": 367.0, "duration": 5.16}, {"text": "left side of the um of the slide if", "start": 369.0, "duration": 4.759}, {"text": "you're doing molecular Dynamic", "start": 372.16, "duration": 3.84}, {"text": "simulations biological systems you're", "start": 373.759, "duration": 4.88}, {"text": "going to be using Amber and gromax and", "start": 376.0, "duration": 5.56}, {"text": "namdi and other programs if you're doing", "start": 378.639, "duration": 5.4}, {"text": "phog gentics you're going to use codes", "start": 381.56, "duration": 5.359}, {"text": "like like raxml um and Mr Bays to", "start": 384.039, "duration": 4.56}, {"text": "construct those phog gentic trees if", "start": 386.919, "duration": 3.641}, {"text": "you're doing electronic structure you", "start": 388.599, "duration": 4.6}, {"text": "might be using cp2k or vasp or other", "start": 390.56, "duration": 4.44}, {"text": "applications", "start": 393.199, "duration": 5.0}, {"text": "so there's a lot of mature sof mature", "start": 395.0, "duration": 5.16}, {"text": "software out there and I have a feeling", "start": 398.199, "duration": 3.241}, {"text": "that the majority of you are going to be", "start": 400.16, "duration": 3.479}, {"text": "using somebody else's", "start": 401.44, "duration": 5.159}, {"text": "code um just a very very quick", "start": 403.639, "duration": 4.801}, {"text": "introduction to parallel computers", "start": 406.599, "duration": 3.801}, {"text": "multiple clusters and parallel computers", "start": 408.44, "duration": 3.72}, {"text": "consist of multiple compute nodes", "start": 410.4, "duration": 4.4}, {"text": "connected by a fast Network each of the", "start": 412.16, "duration": 5.8}, {"text": "GP s each of the CPU nodes on most of", "start": 414.8, "duration": 4.6}, {"text": "these nationally allocated systems", "start": 417.96, "duration": 4.16}, {"text": "systems are going to contain two um to", "start": 419.4, "duration": 5.919}, {"text": "two processors each with multiple cores", "start": 422.12, "duration": 5.72}, {"text": "um the GPU nodes will um typically", "start": 425.319, "duration": 6.6}, {"text": "contain four gpus to node so even if you", "start": 427.84, "duration": 7.12}, {"text": "are running within a single node you can", "start": 431.919, "duration": 4.96}, {"text": "still take advantage of parallel", "start": 434.96, "duration": 4.48}, {"text": "Computing so to effectively use this", "start": 436.879, "duration": 4.32}, {"text": "Hardware we need applications that have", "start": 439.44, "duration": 3.24}, {"text": "been paralyzed so they can run on", "start": 441.199, "duration": 4.44}, {"text": "multiple cores or multiple gpus within a", "start": 442.68, "duration": 5.639}, {"text": "node or across multiple", "start": 445.639, "duration": 4.24}, {"text": "nodes", "start": 448.319, "duration": 4.56}, {"text": "so Cindy do we have any questions so far", "start": 449.879, "duration": 5.561}, {"text": "none at this time all right I didn't", "start": 452.879, "duration": 4.44}, {"text": "think we would at this point okay so now", "start": 455.44, "duration": 3.12}, {"text": "I'm going to talk about a few of the", "start": 457.319, "duration": 3.681}, {"text": "important Concepts in parallel Computing", "start": 458.56, "duration": 4.28}, {"text": "we're going to talk about processes and", "start": 461.0, "duration": 5.639}, {"text": "threads along with MPI and open", "start": 462.84, "duration": 7.0}, {"text": "MP so you might have come across the um", "start": 466.639, "duration": 6.161}, {"text": "across the terms processes and threads", "start": 469.84, "duration": 5.96}, {"text": "these are both um independent sequences", "start": 472.8, "duration": 3.799}, {"text": "of", "start": 475.8, "duration": 3.16}, {"text": "execution you could think of a process", "start": 476.599, "duration": 4.801}, {"text": "as being an instance of program with", "start": 478.96, "duration": 4.959}, {"text": "access to its own memory State and file", "start": 481.4, "duration": 4.32}, {"text": "descriptors basically means that a", "start": 483.919, "duration": 4.84}, {"text": "process can open and close files threads", "start": 485.72, "duration": 5.36}, {"text": "are lightweight entities that execute", "start": 488.759, "duration": 5.44}, {"text": "inside a process um every process is", "start": 491.08, "duration": 5.079}, {"text": "going to have at least one thread and", "start": 494.199, "duration": 4.0}, {"text": "threads within a process can access", "start": 496.159, "duration": 3.48}, {"text": "shared", "start": 498.199, "duration": 4.72}, {"text": "memory now if you um Hunt online for for", "start": 499.639, "duration": 4.84}, {"text": "resources describing the difference", "start": 502.919, "duration": 3.921}, {"text": "between threads and processes most of", "start": 504.479, "duration": 4.201}, {"text": "them tend to be geared toward computer", "start": 506.84, "duration": 3.359}, {"text": "scientists they get really deep in the", "start": 508.68, "duration": 3.44}, {"text": "weeds but I found a couple of links here", "start": 510.199, "duration": 4.32}, {"text": "that I think do a reasonably good job of", "start": 512.12, "duration": 5.599}, {"text": "addressing the topic um there's one um", "start": 514.519, "duration": 6.0}, {"text": "discussion here on stack Overflow which", "start": 517.719, "duration": 4.76}, {"text": "isn't too technical but maybe a little", "start": 520.519, "duration": 5.44}, {"text": "little on the nerdier side and then um", "start": 522.479, "duration": 6.36}, {"text": "at the bottom of the link to a article", "start": 525.959, "duration": 7.481}, {"text": "at educ cba.com which is a little more", "start": 528.839, "duration": 7.761}, {"text": "informal so the reason that we um care", "start": 533.44, "duration": 5.519}, {"text": "about processes and threads is it's", "start": 536.6, "duration": 3.96}, {"text": "going to have an impact on how we run", "start": 538.959, "duration": 5.361}, {"text": "our applications so a process it's going", "start": 540.56, "duration": 6.44}, {"text": "to incur more overhead but it's also", "start": 544.32, "duration": 5.04}, {"text": "more flexible multiple processes can be", "start": 547.0, "duration": 4.88}, {"text": "run within a compute node or across", "start": 549.36, "duration": 4.28}, {"text": "multiple compute nodes what we call", "start": 551.88, "duration": 4.12}, {"text": "distributed memory parallel Computing", "start": 553.64, "duration": 4.36}, {"text": "threads on the other hand they incur", "start": 556.0, "duration": 2.8}, {"text": "less", "start": 558.0, "duration": 3.44}, {"text": "overhead um threaded codes can use less", "start": 558.8, "duration": 4.92}, {"text": "memory since threads within a process", "start": 561.44, "duration": 4.12}, {"text": "can have access to the same data", "start": 563.72, "duration": 3.92}, {"text": "structure the downside though is that", "start": 565.56, "duration": 4.519}, {"text": "they're less flexible multiple threads", "start": 567.64, "duration": 4.639}, {"text": "associated with process can only be run", "start": 570.079, "duration": 4.681}, {"text": "within a single compute node which um", "start": 572.279, "duration": 4.24}, {"text": "where all the where all the processors", "start": 574.76, "duration": 5.16}, {"text": "have access to um shared pool of", "start": 576.519, "duration": 7.281}, {"text": "memory all right so why do you care the", "start": 579.92, "duration": 5.68}, {"text": "type of parallelization the application", "start": 583.8, "duration": 4.36}, {"text": "is going to determine where and how you", "start": 585.6, "duration": 5.28}, {"text": "can run your code so distributed memory", "start": 588.16, "duration": 5.04}, {"text": "applications multiple processes or", "start": 590.88, "duration": 4.44}, {"text": "multiple instances of program these can", "start": 593.2, "duration": 5.199}, {"text": "be run on one or more nodes shared", "start": 595.32, "duration": 5.32}, {"text": "memory application threaded applications", "start": 598.399, "duration": 5.201}, {"text": "should be run on a single node now we", "start": 600.64, "duration": 4.6}, {"text": "also have what we call hybrid", "start": 603.6, "duration": 4.08}, {"text": "applications which which are a mix of", "start": 605.24, "duration": 4.24}, {"text": "distributed memory and shared memory", "start": 607.68, "duration": 4.0}, {"text": "paralyzation they can be run on one or", "start": 609.48, "duration": 4.359}, {"text": "more nodes but you need to consider the", "start": 611.68, "duration": 4.48}, {"text": "balance between threads and processes", "start": 613.839, "duration": 3.68}, {"text": "and these hybrid applications are", "start": 616.16, "duration": 3.32}, {"text": "becoming much much more common", "start": 617.519, "duration": 5.76}, {"text": "especially as the number of CPUs on a on", "start": 619.48, "duration": 7.28}, {"text": "a processor has grown and in all cases", "start": 623.279, "duration": 4.961}, {"text": "you may need to think about how", "start": 626.76, "duration": 3.28}, {"text": "processes and thre are mapped and bound", "start": 628.24, "duration": 3.0}, {"text": "to", "start": 630.04, "duration": 3.479}, {"text": "cores so in addition being aware of", "start": 631.24, "duration": 4.2}, {"text": "threads and processes will help you to", "start": 633.519, "duration": 3.56}, {"text": "understand how your code is utilized in", "start": 635.44, "duration": 5.56}, {"text": "the hardware and identify some um common", "start": 637.079, "duration": 5.88}, {"text": "problems so I'm going to talk just a", "start": 641.0, "duration": 4.32}, {"text": "little bit about MPI I know I said", "start": 642.959, "duration": 4.0}, {"text": "earlier that we're not going to cover", "start": 645.32, "duration": 3.12}, {"text": "programming topics and we're not going", "start": 646.959, "duration": 5.161}, {"text": "to get into the details of um of MPI but", "start": 648.44, "duration": 6.399}, {"text": "you should be aware of what it is um MPI", "start": 652.12, "duration": 5.959}, {"text": "is the standard for paralyzing C C++ and", "start": 654.839, "duration": 6.081}, {"text": "for Trend codes so they can run on dist", "start": 658.079, "duration": 5.281}, {"text": "on distributed memory multiple compute", "start": 660.92, "duration": 4.919}, {"text": "node systems so while it's not", "start": 663.36, "duration": 5.279}, {"text": "officially um adopted by any major", "start": 665.839, "duration": 4.881}, {"text": "standards bodies it's really become the", "start": 668.639, "duration": 6.961}, {"text": "deao standard I.E it's available um um", "start": 670.72, "duration": 6.559}, {"text": "almost everywhere and almost everyone", "start": 675.6, "duration": 3.84}, {"text": "who does large scale parallel Computing", "start": 677.279, "duration": 4.921}, {"text": "uses it now there are multiple", "start": 679.44, "duration": 5.28}, {"text": "open-source implementations including", "start": 682.2, "duration": 5.48}, {"text": "open MPI mvapich and M pitch along with", "start": 684.72, "duration": 5.0}, {"text": "vendor supported versions", "start": 687.68, "duration": 4.2}, {"text": "MPI applications can be run within a", "start": 689.72, "duration": 4.72}, {"text": "single node and all widely used MPI", "start": 691.88, "duration": 4.32}, {"text": "implementations are optimized to take", "start": 694.44, "duration": 3.76}, {"text": "advantage of the fast inode", "start": 696.2, "duration": 4.079}, {"text": "communication so basically this means", "start": 698.2, "duration": 4.56}, {"text": "you can use MPI you can have multiple", "start": 700.279, "duration": 5.481}, {"text": "processes running within a single node", "start": 702.76, "duration": 5.24}, {"text": "but um you don't need you won't", "start": 705.76, "duration": 3.48}, {"text": "personally need to worry about the", "start": 708.0, "duration": 3.56}, {"text": "details of how that communication within", "start": 709.24, "duration": 3.839}, {"text": "a node is is", "start": 711.56, "duration": 4.12}, {"text": "implemented MPI is portable it could be", "start": 713.079, "duration": 5.0}, {"text": "used anywhere where an MPI library has", "start": 715.68, "duration": 4.159}, {"text": "been installed", "start": 718.079, "duration": 3.961}, {"text": "and finally though although MPI is often", "start": 719.839, "duration": 4.12}, {"text": "synonymous with Distributing memory", "start": 722.04, "duration": 3.68}, {"text": "paralyzation there are some other", "start": 723.959, "duration": 3.921}, {"text": "options that are gaining again gain", "start": 725.72, "duration": 7.679}, {"text": "adoption such as charm Plus+ UPC and", "start": 727.88, "duration": 9.24}, {"text": "X10 so although we're discussing MPI the", "start": 733.399, "duration": 5.24}, {"text": "same principles are going to apply when", "start": 737.12, "duration": 3.48}, {"text": "paralyzing deep learning applications", "start": 738.639, "duration": 5.121}, {"text": "say using horovod which is a distributed", "start": 740.6, "duration": 5.08}, {"text": "deep learning framework for tensorflow", "start": 743.76, "duration": 5.68}, {"text": "Caris P torch and Apache mxnet and", "start": 745.68, "duration": 5.56}, {"text": "this is the Nvidia Collective", "start": 749.44, "duration": 4.519}, {"text": "communication", "start": 751.24, "duration": 2.719}, {"text": "Library so a brief example of a um cut", "start": 754.72, "duration": 5.72}, {"text": "code that's paralyzes an", "start": 758.639, "duration": 4.401}, {"text": "MPI um if you if you don't have any", "start": 760.44, "duration": 3.959}, {"text": "programming experience you don't really", "start": 763.04, "duration": 2.88}, {"text": "need to worry about these slides if", "start": 764.399, "duration": 2.8}, {"text": "you've done a little bit of programming", "start": 765.92, "duration": 5.32}, {"text": "you might want to follow along so and um", "start": 767.199, "duration": 6.64}, {"text": "so so we're going to start off with the", "start": 771.24, "duration": 4.159}, {"text": "first program that you learn in just", "start": 773.839, "duration": 3.201}, {"text": "about any programming language hello", "start": 775.399, "duration": 5.56}, {"text": "world so we show here on the left left a", "start": 777.04, "duration": 8.08}, {"text": "um a a a c program implementing hello", "start": 780.959, "duration": 6.721}, {"text": "world now MPI applications are pretty", "start": 785.12, "duration": 4.6}, {"text": "dense they're written at a low level if", "start": 787.68, "duration": 4.599}, {"text": "I wanted to paralyze this I'm going from", "start": 789.72, "duration": 6.64}, {"text": "the simple code to a parallel version", "start": 792.279, "duration": 5.761}, {"text": "you're going to see that there's a lot", "start": 796.36, "duration": 3.919}, {"text": "more code in there um I've highlighted", "start": 798.04, "duration": 5.2}, {"text": "the new code in purple um this includes", "start": 800.279, "duration": 6.201}, {"text": "the MPI header um a bunch of calls to", "start": 803.24, "duration": 5.32}, {"text": "initialize the MPI environment get the", "start": 806.48, "duration": 4.919}, {"text": "number number of processes get a rank or", "start": 808.56, "duration": 5.0}, {"text": "a number associated with each process", "start": 811.399, "duration": 4.44}, {"text": "get the names of the processors and then", "start": 813.56, "duration": 5.48}, {"text": "finally I'm going to have each um each", "start": 815.839, "duration": 6.921}, {"text": "process write out its its its", "start": 819.04, "duration": 6.2}, {"text": "ID so if you're not a programmer", "start": 822.76, "duration": 4.199}, {"text": "application developer you do not need to", "start": 825.24, "duration": 4.24}, {"text": "know MPI you just need to be aware that", "start": 826.959, "duration": 4.44}, {"text": "it exists and that if you're building", "start": 829.48, "duration": 4.159}, {"text": "your own executable you'll need to use", "start": 831.399, "duration": 4.88}, {"text": "the appropriate", "start": 833.639, "duration": 2.64}, {"text": "wrappers okay openmp is an application", "start": 836.36, "duration": 5.719}, {"text": "programming interface API for shared", "start": 839.92, "duration": 5.96}, {"text": "memory programming also in cc++ and", "start": 842.079, "duration": 6.241}, {"text": "fortrend um openm P takes a little bit", "start": 845.88, "duration": 5.399}, {"text": "different approach it's um it gives a", "start": 848.32, "duration": 4.639}, {"text": "collection of compiler directives", "start": 851.279, "duration": 3.8}, {"text": "Library retains an environment", "start": 852.959, "duration": 4.641}, {"text": "variables it's actually implemented by", "start": 855.079, "duration": 5.361}, {"text": "the compiler rather than as a separate", "start": 857.6, "duration": 5.56}, {"text": "library and to the best of my knowledge", "start": 860.44, "duration": 4.88}, {"text": "openmp is supported by all of the major", "start": 863.16, "duration": 6.919}, {"text": "compilers the IBM Intel GCC p gii the", "start": 865.32, "duration": 7.28}, {"text": "AMD um optimizing compilers that we have", "start": 870.079, "duration": 4.56}, {"text": "installed in expans", "start": 872.6, "duration": 4.84}, {"text": "aocc like MPI it's portable and can be", "start": 874.639, "duration": 5.361}, {"text": "used anywhere and although openmp is", "start": 877.44, "duration": 4.199}, {"text": "often synonymous to Shared memory", "start": 880.0, "duration": 4.16}, {"text": "paralyzation there are other options um", "start": 881.639, "duration": 5.12}, {"text": "like Silk posx threads and specialized", "start": 884.16, "duration": 7.239}, {"text": "libraries for python R and other", "start": 886.759, "duration": 8.08}, {"text": "languages and then to show an example um", "start": 891.399, "duration": 4.56}, {"text": "again if you're not if you're not a", "start": 894.839, "duration": 4.601}, {"text": "programmer um feel free to um Daydream", "start": 895.959, "duration": 5.8}, {"text": "for the next minute but if you've done a", "start": 899.44, "duration": 3.839}, {"text": "little bit of", "start": 901.759, "duration": 3.44}, {"text": "programming um you know we'll show an", "start": 903.279, "duration": 4.161}, {"text": "example here of how code is paralyzed us", "start": 905.199, "duration": 5.64}, {"text": "an openmp again you do not need to if", "start": 907.44, "duration": 5.399}, {"text": "you're not a programmer or developer you", "start": 910.839, "duration": 4.36}, {"text": "don't need to know openmp you just need", "start": 912.839, "duration": 5.201}, {"text": "to be aware that it exists um and that", "start": 915.199, "duration": 4.521}, {"text": "if you're building your own exe", "start": 918.04, "duration": 3.32}, {"text": "executable if you're getting the code", "start": 919.72, "duration": 3.0}, {"text": "from somebody else and you're actually", "start": 921.36, "duration": 3.08}, {"text": "building on your system that you're", "start": 922.72, "duration": 3.08}, {"text": "going to have to use the appropriate", "start": 924.44, "duration": 2.519}, {"text": "compiler", "start": 925.8, "duration": 4.08}, {"text": "Flags so", "start": 926.959, "duration": 5.641}, {"text": "what code um that has been paralyzed", "start": 929.88, "duration": 5.6}, {"text": "using open MP will look like is we might", "start": 932.6, "duration": 6.28}, {"text": "have a few definitions at the top um an", "start": 935.48, "duration": 7.359}, {"text": "an open MP header um we might def Define", "start": 938.88, "duration": 5.92}, {"text": "the chunk size the number of iterations", "start": 942.839, "duration": 3.521}, {"text": "of loop that we're going to be assign to", "start": 944.8, "duration": 3.92}, {"text": "each thread and then the individual", "start": 946.36, "duration": 5.36}, {"text": "Loops are going to be paralyzed using um", "start": 948.72, "duration": 6.4}, {"text": "using pragmas um we can do more complex", "start": 951.72, "duration": 5.359}, {"text": "paralyzation we can do task based", "start": 955.12, "duration": 4.719}, {"text": "paralyzation we can um organize the code", "start": 957.079, "duration": 5.0}, {"text": "into chunks but mostly we're going to be", "start": 959.839, "duration": 3.8}, {"text": "starting with our original code and", "start": 962.079, "duration": 2.56}, {"text": "we're going to be adding these", "start": 963.639, "duration": 4.401}, {"text": "directives to do the", "start": 964.639, "duration": 5.68}, {"text": "paralyzation so let me just go over one", "start": 968.04, "duration": 3.799}, {"text": "more slide and then we'll pause and see", "start": 970.319, "duration": 3.481}, {"text": "if there are any questions so MPI and", "start": 971.839, "duration": 5.841}, {"text": "openmp the big picture on the left we", "start": 973.8, "duration": 7.0}, {"text": "have processes again multiple multiple", "start": 977.68, "duration": 4.599}, {"text": "instances of a program that are", "start": 980.8, "duration": 3.399}, {"text": "communicating with each other these", "start": 982.279, "duration": 4.881}, {"text": "multiple processes are managed using MPI", "start": 984.199, "duration": 5.76}, {"text": "and MPI is implemented libraries such as", "start": 987.16, "duration": 6.119}, {"text": "M pitch mvapich open MP and vendor", "start": 989.959, "duration": 6.24}, {"text": "implementations and on the right we we", "start": 993.279, "duration": 5.521}, {"text": "have threads and multiple threads are", "start": 996.199, "duration": 5.76}, {"text": "managed using openmp and openmp is", "start": 998.8, "duration": 5.44}, {"text": "implemented by the compilers including", "start": 1001.959, "duration": 5.56}, {"text": "GCC PGI Intel and", "start": 1004.24, "duration": 5.92}, {"text": "aocc and with that I'm GNA pause and", "start": 1007.519, "duration": 5.32}, {"text": "Cindy do we have any questions yes we do", "start": 1010.16, "duration": 5.32}, {"text": "have a couple okay um so one at the very", "start": 1012.839, "duration": 5.44}, {"text": "beginning of this session um was I feel", "start": 1015.48, "duration": 4.64}, {"text": "very very tough to use parallel", "start": 1018.279, "duration": 3.36}, {"text": "Computing when it comes to", "start": 1020.12, "duration": 5.52}, {"text": "postprocessing and visualization any", "start": 1021.639, "duration": 5.56}, {"text": "guidance", "start": 1025.64, "duration": 4.559}, {"text": "oh", "start": 1027.199, "duration": 3.0}, {"text": "um you know so so for", "start": 1030.319, "duration": 5.6}, {"text": "um yeah so I see there's a little bit of", "start": 1033.48, "duration": 5.0}, {"text": "discussion here", "start": 1035.919, "duration": 6.321}, {"text": "um parallel visualization is tough um I", "start": 1038.48, "duration": 6.16}, {"text": "know that there are some tools that have", "start": 1042.24, "duration": 4.439}, {"text": "come out of the come out of the National", "start": 1044.64, "duration": 4.6}, {"text": "Labs that allow you to do visual ization", "start": 1046.679, "duration": 5.521}, {"text": "in parallel um I'm not personally", "start": 1049.24, "duration": 5.96}, {"text": "familiar with them with postprocess", "start": 1052.2, "duration": 5.12}, {"text": "again that's going to depend", "start": 1055.2, "duration": 4.96}, {"text": "on on what exactly it is that you're", "start": 1057.32, "duration": 5.8}, {"text": "doing um you know you you might want to", "start": 1060.16, "duration": 5.84}, {"text": "see if there are any um parallel", "start": 1063.12, "duration": 4.64}, {"text": "implementations of what you're trying to", "start": 1066.0, "duration": 6.12}, {"text": "do or um you may want to consider taking", "start": 1067.76, "duration": 7.32}, {"text": "sort of a high throughput approach where", "start": 1072.12, "duration": 4.72}, {"text": "if if at all possible if you could take", "start": 1075.08, "duration": 5.599}, {"text": "the output and output could be", "start": 1076.84, "duration": 7.12}, {"text": "um could be processed independently you", "start": 1080.679, "duration": 7.201}, {"text": "could just do multiple instances of the", "start": 1083.96, "duration": 3.92}, {"text": "postprocessing and then s yeah oh s I've", "start": 1088.44, "duration": 5.8}, {"text": "got the chat here and I have it listed", "start": 1091.48, "duration": 4.4}, {"text": "so I'll just kind of um read through", "start": 1094.24, "duration": 4.28}, {"text": "them okay um the next one is going to be", "start": 1095.88, "duration": 4.679}, {"text": "does distributed memory necessarily", "start": 1098.52, "duration": 5.399}, {"text": "entail use of MPI yes that that's a good", "start": 1100.559, "duration": 7.441}, {"text": "question I'm going to say um 98% of the", "start": 1103.919, "duration": 6.961}, {"text": "time if you're doing distributed memory", "start": 1108.0, "duration": 5.0}, {"text": "the distributed memory parallel", "start": 1110.88, "duration": 5.12}, {"text": "Computing it's probably going to be MPI", "start": 1113.0, "duration": 5.84}, {"text": "I mention a few of the other um options", "start": 1116.0, "duration": 6.76}, {"text": "out there charm Plus+ which is used in", "start": 1118.84, "duration": 6.0}, {"text": "um Nat namdi a popular molecular", "start": 1122.76, "duration": 5.32}, {"text": "Dynamics code um but it's it's generally", "start": 1124.84, "duration": 6.04}, {"text": "going it's generally going to be MPI", "start": 1128.08, "duration": 6.479}, {"text": "yep so next is going to be do CPU cores", "start": 1130.88, "duration": 6.08}, {"text": "have any relation to threads SL", "start": 1134.559, "duration": 5.161}, {"text": "processes an example example if a CPU", "start": 1136.96, "duration": 5.4}, {"text": "has 16 cores does that mean it can", "start": 1139.72, "duration": 4.6}, {"text": "support 16", "start": 1142.36, "duration": 4.199}, {"text": "processes yeah you know so there there", "start": 1144.32, "duration": 4.76}, {"text": "there is a relation normally you would", "start": 1146.559, "duration": 4.801}, {"text": "do um what one", "start": 1149.08, "duration": 5.4}, {"text": "thread um you would do one thread or one", "start": 1151.36, "duration": 4.679}, {"text": "process per", "start": 1154.48, "duration": 5.48}, {"text": "core um now if if you experiment with", "start": 1156.039, "duration": 5.481}, {"text": "with some of the parallel codes let's", "start": 1159.96, "duration": 4.28}, {"text": "say that we have a CPU with with 16", "start": 1161.52, "duration": 6.279}, {"text": "cores you could actually run 32 threads", "start": 1164.24, "duration": 6.24}, {"text": "AC cross those 16 cors but but there's", "start": 1167.799, "duration": 4.641}, {"text": "no free lunch you're going to have", "start": 1170.48, "duration": 4.199}, {"text": "threads or processes competing with each", "start": 1172.44, "duration": 6.68}, {"text": "other so um I ideally you you'll find", "start": 1174.679, "duration": 7.081}, {"text": "that if you if you use more processes", "start": 1179.12, "duration": 5.2}, {"text": "say twice as many processes or or", "start": 1181.76, "duration": 4.799}, {"text": "threads as you do have cores that they", "start": 1184.32, "duration": 7.68}, {"text": "are going to um only get 50% of of that", "start": 1186.559, "duration": 7.521}, {"text": "core all right now we have four more", "start": 1192.0, "duration": 4.96}, {"text": "questions um another one is is MPI only", "start": 1194.08, "duration": 5.4}, {"text": "for multiple processes on a single node", "start": 1196.96, "duration": 5.8}, {"text": "or can we use MPI for multiple nodes if", "start": 1199.48, "duration": 5.64}, {"text": "not what can we use for using multiple", "start": 1202.76, "duration": 6.24}, {"text": "processes on multiple noes yes so um MPI", "start": 1205.12, "duration": 7.52}, {"text": "can be used you used for both um the the", "start": 1209.0, "duration": 5.679}, {"text": "the real power of MPI is that you can", "start": 1212.64, "duration": 4.399}, {"text": "run across multiple nodes and unless", "start": 1214.679, "duration": 4.36}, {"text": "you're using charm Plus+ or some of the", "start": 1217.039, "duration": 4.0}, {"text": "other options I described earlier you", "start": 1219.039, "duration": 5.961}, {"text": "will you will need to use um", "start": 1221.039, "duration": 7.361}, {"text": "MPI next is is parallel Computing best", "start": 1225.0, "duration": 7.96}, {"text": "with C C+ and portran can open MP and", "start": 1228.4, "duration": 8.44}, {"text": "MPI still be used with python okay so um", "start": 1232.96, "duration": 4.88}, {"text": "the first", "start": 1236.84, "duration": 4.36}, {"text": "question is oh sorry oh yes yeah Cindy I", "start": 1237.84, "duration": 5.52}, {"text": "got it thanks thanks um", "start": 1241.2, "duration": 4.52}, {"text": "so I'm I'm gonna say I'm gonna say it's", "start": 1243.36, "duration": 5.88}, {"text": "a three-way tie um we see a lot of", "start": 1245.72, "duration": 7.28}, {"text": "parallel applications NC n C++ and and", "start": 1249.24, "duration": 6.799}, {"text": "and in fortrend and ideally it should it", "start": 1253.0, "duration": 6.96}, {"text": "should work equally well now can openmp", "start": 1256.039, "duration": 6.801}, {"text": "and MPI still be used with python that", "start": 1259.96, "duration": 5.719}, {"text": "that's a little more complicated", "start": 1262.84, "duration": 6.16}, {"text": "um because of the way python was", "start": 1265.679, "duration": 5.841}, {"text": "designed I believe that it that it", "start": 1269.0, "duration": 4.799}, {"text": "cannot currently hand handle multiple", "start": 1271.52, "duration": 5.519}, {"text": "threads now regarding MPI you you can't", "start": 1273.799, "duration": 7.161}, {"text": "use the MPI libraries as is but there is", "start": 1277.039, "duration": 9.841}, {"text": "a um PI MPI implementation which is very", "start": 1280.96, "duration": 10.24}, {"text": "um wa which clo which closely follows um", "start": 1286.88, "duration": 6.52}, {"text": "the the MPI", "start": 1291.2, "duration": 4.64}, {"text": "standard a followup to qu that question", "start": 1293.4, "duration": 6.56}, {"text": "is what about rust rust you know I've", "start": 1295.84, "duration": 6.959}, {"text": "I'm completely completely unfamiliar", "start": 1299.96, "duration": 5.56}, {"text": "with rust um I did come across an", "start": 1302.799, "duration": 5.12}, {"text": "article though where I think rust does", "start": 1305.52, "duration": 3.92}, {"text": "have some parallel Computing", "start": 1307.919, "duration": 3.24}, {"text": "capabilities", "start": 1309.44, "duration": 4.56}, {"text": "um built into the language from the", "start": 1311.159, "duration": 5.52}, {"text": "start but I'm you know I I don't know", "start": 1314.0, "duration": 4.72}, {"text": "enough about that all right just a", "start": 1316.679, "duration": 5.641}, {"text": "couple more Bob okay um can we use MPI", "start": 1318.72, "duration": 6.959}, {"text": "with r okay again we um you won't be", "start": 1322.32, "duration": 5.8}, {"text": "able to use the the ex you won't be able", "start": 1325.679, "duration": 5.201}, {"text": "to use the MPI implementations that I", "start": 1328.12, "duration": 7.679}, {"text": "mentioned earlier um but there is a", "start": 1330.88, "duration": 9.08}, {"text": "um yeah I see I see Mar Marty um already", "start": 1335.799, "duration": 8.561}, {"text": "addressed the using rmpi um but but", "start": 1339.96, "duration": 5.68}, {"text": "there", "start": 1344.36, "duration": 3.76}, {"text": "are there are different libraries though", "start": 1345.64, "duration": 6.12}, {"text": "before for paralyzing our", "start": 1348.12, "duration": 3.64}, {"text": "code all right and then two more can you", "start": 1352.76, "duration": 5.96}, {"text": "suggest profiling tools for analyzing", "start": 1356.279, "duration": 5.76}, {"text": "performances of MPI openmp and Cuda", "start": 1358.72, "duration": 6.64}, {"text": "based programs", "start": 1362.039, "duration": 3.321}, {"text": "applications oh okay you know I didn't I", "start": 1365.84, "duration": 4.319}, {"text": "didn't really want to get into profiling", "start": 1368.559, "duration": 3.441}, {"text": "in this code since we're in in this", "start": 1370.159, "duration": 3.12}, {"text": "presentation since we're not really", "start": 1372.0, "duration": 4.799}, {"text": "covering programmer topics um I'm going", "start": 1373.279, "duration": 7.081}, {"text": "to say if for for serial codes my my", "start": 1376.799, "duration": 6.441}, {"text": "favorite starting point is is is always", "start": 1380.36, "duration": 6.96}, {"text": "GPR um for parallel codes I'm gonna say", "start": 1383.24, "duration": 5.559}, {"text": "probably the state-ofthe-art tool is", "start": 1387.32, "duration": 4.2}, {"text": "something called to which allows you to", "start": 1388.799, "duration": 6.801}, {"text": "do a very detailed profiling of um open", "start": 1391.52, "duration": 6.72}, {"text": "MP and MPI", "start": 1395.6, "duration": 6.04}, {"text": "applications lastly do we have any rule", "start": 1398.24, "duration": 4.84}, {"text": "to", "start": 1401.64, "duration": 5.039}, {"text": "decompose NS or NOS of grids per", "start": 1403.08, "duration": 5.32}, {"text": "processors for", "start": 1406.679, "duration": 4.24}, {"text": "to for optimize resource", "start": 1408.4, "duration": 4.96}, {"text": "allocations okay do we have any rule to", "start": 1410.919, "duration": 4.561}, {"text": "decompose number of grids", "start": 1413.36, "duration": 4.199}, {"text": "per you know I'm going to say that", "start": 1415.48, "duration": 4.04}, {"text": "there's not a not not a hard and fast", "start": 1417.559, "duration": 3.321}, {"text": "rle I'm going to get into that a little", "start": 1419.52, "duration": 3.48}, {"text": "bit later I'm going to say if I", "start": 1420.88, "duration": 3.279}, {"text": "understand the question", "start": 1423.0, "duration": 4.159}, {"text": "correctly", "start": 1424.159, "duration": 3.0}, {"text": "um I I would I would recommend that you", "start": 1428.279, "duration": 3.841}, {"text": "actually want to you want to Benchmark", "start": 1430.64, "duration": 3.919}, {"text": "your code and and do a scaling study so", "start": 1432.12, "duration": 5.48}, {"text": "if your problem is too small um you're", "start": 1434.559, "duration": 6.081}, {"text": "you're going to um you're going to you", "start": 1437.6, "duration": 4.88}, {"text": "won't be able to spread it across enough", "start": 1440.64, "duration": 4.48}, {"text": "cores or nodes so and that this is going", "start": 1442.48, "duration": 4.28}, {"text": "to be the later part of the talk I'm", "start": 1445.12, "duration": 5.84}, {"text": "basically GNA say do a do a benchmarking", "start": 1446.76, "duration": 6.799}, {"text": "study and that is it for now Bob thank", "start": 1450.96, "duration": 4.199}, {"text": "you very okay great thank you and that", "start": 1453.559, "duration": 4.201}, {"text": "thank you Marty and Cindy okay so now", "start": 1455.159, "duration": 3.841}, {"text": "we're g to move on we're GNA talk a", "start": 1457.76, "duration": 3.399}, {"text": "little bit about hybrid", "start": 1459.0, "duration": 5.159}, {"text": "applications so many modern parallel", "start": 1461.159, "duration": 4.561}, {"text": "applications they're built using a", "start": 1464.159, "duration": 3.161}, {"text": "hybrid approach you take advantage of", "start": 1465.72, "duration": 4.28}, {"text": "both distributed and shared memory so", "start": 1467.32, "duration": 4.88}, {"text": "this will typically involve MPI and open", "start": 1470.0, "duration": 4.24}, {"text": "MP although as I mentioned some other", "start": 1472.2, "duration": 4.64}, {"text": "combinations are possible now hybrid", "start": 1474.24, "duration": 4.28}, {"text": "codes have the advantage over purely", "start": 1476.84, "duration": 4.319}, {"text": "shared distributed memory apps and that", "start": 1478.52, "duration": 4.879}, {"text": "shared memory apps okay can have a", "start": 1481.159, "duration": 4.801}, {"text": "limited scalability within the node and", "start": 1483.399, "duration": 4.4}, {"text": "can definitely not be run across", "start": 1485.96, "duration": 5.12}, {"text": "multiple nodes um distributed memory", "start": 1487.799, "duration": 5.0}, {"text": "apps may have a higher higher memory", "start": 1491.08, "duration": 3.28}, {"text": "requirements and more overhead when", "start": 1492.799, "duration": 2.921}, {"text": "running within a", "start": 1494.36, "duration": 3.88}, {"text": "Noe so we're going to take a look at a", "start": 1495.72, "duration": 4.839}, {"text": "very very simplified version of parallel", "start": 1498.24, "duration": 4.6}, {"text": "computer um it just consists of two", "start": 1500.559, "duration": 6.041}, {"text": "nodes um each of them with um 16 compute", "start": 1502.84, "duration": 5.839}, {"text": "cores so the big gray box is going to be", "start": 1506.6, "duration": 4.28}, {"text": "our node each of those little blue boxes", "start": 1508.679, "duration": 5.961}, {"text": "will be a core and we have a", "start": 1510.88, "duration": 6.44}, {"text": "interconnect so if we're running purely", "start": 1514.64, "duration": 5.0}, {"text": "message pass an", "start": 1517.32, "duration": 4.52}, {"text": "application um they're typically going", "start": 1519.64, "duration": 5.72}, {"text": "to be executed with one process per core", "start": 1521.84, "duration": 6.04}, {"text": "again we dis we um answer the question", "start": 1525.36, "duration": 5.0}, {"text": "earlier there's no there's no hard and", "start": 1527.88, "duration": 5.32}, {"text": "fast rule that says you can only one run", "start": 1530.36, "duration": 5.12}, {"text": "one process per core but if you run", "start": 1533.2, "duration": 4.479}, {"text": "multiple processes per core they're", "start": 1535.48, "duration": 3.679}, {"text": "going to be competing with each other", "start": 1537.679, "duration": 3.441}, {"text": "and your codee's not going to run any", "start": 1539.159, "duration": 5.961}, {"text": "faster so here we we see an example of", "start": 1541.12, "duration": 6.76}, {"text": "an application running 32 processes", "start": 1545.12, "duration": 5.48}, {"text": "across two 16 core nodes and we have one", "start": 1547.88, "duration": 5.919}, {"text": "process associated with each core now if", "start": 1550.6, "duration": 5.799}, {"text": "we have a threaded application here", "start": 1553.799, "duration": 4.641}, {"text": "we're just going to have a single where", "start": 1556.399, "duration": 4.801}, {"text": "we're going to have a single process and", "start": 1558.44, "duration": 4.76}, {"text": "within that process we're going to have", "start": 1561.2, "duration": 4.12}, {"text": "multiple threads so here we're sh an", "start": 1563.2, "duration": 4.88}, {"text": "example of um threaded application", "start": 1565.32, "duration": 6.0}, {"text": "single process running 16 threads across", "start": 1568.08, "duration": 6.24}, {"text": "16 cores um I said here that", "start": 1571.32, "duration": 5.04}, {"text": "applications that paralyzed using", "start": 1574.32, "duration": 4.12}, {"text": "threading only are restricted to running", "start": 1576.36, "duration": 4.679}, {"text": "within a single node um for for those of", "start": 1578.44, "duration": 4.08}, {"text": "you who are who are sticklers for", "start": 1581.039, "duration": 3.601}, {"text": "parallel Computing technically any", "start": 1582.52, "duration": 4.08}, {"text": "programming model can be mapped to any", "start": 1584.64, "duration": 5.399}, {"text": "hardware but in practice if you try to", "start": 1586.6, "duration": 5.72}, {"text": "do if you try to run an application with", "start": 1590.039, "duration": 4.441}, {"text": "multiple threads across multiple nodes", "start": 1592.32, "duration": 5.16}, {"text": "you're going to get terrible", "start": 1594.48, "duration": 5.88}, {"text": "performance now hybrid applications is", "start": 1597.48, "duration": 6.319}, {"text": "where we mix processes and threads and", "start": 1600.36, "duration": 4.88}, {"text": "this is going to be really really", "start": 1603.799, "duration": 3.48}, {"text": "important um for a lot of you who are", "start": 1605.24, "duration": 4.319}, {"text": "using existing third-party parallel", "start": 1607.279, "duration": 4.961}, {"text": "applications so any combination of", "start": 1609.559, "duration": 4.881}, {"text": "threads and processes within a node is", "start": 1612.24, "duration": 5.159}, {"text": "allow so here we see an example where we", "start": 1614.44, "duration": 5.64}, {"text": "have two processes per node though those", "start": 1617.399, "duration": 4.76}, {"text": "larger um orange", "start": 1620.08, "duration": 4.12}, {"text": "rectangles", "start": 1622.159, "duration": 5.561}, {"text": "um but we have 16 threads sorry and we", "start": 1624.2, "duration": 5.839}, {"text": "have eight threads within each process", "start": 1627.72, "duration": 4.839}, {"text": "so again we're still", "start": 1630.039, "duration": 5.161}, {"text": "um we're we're still executing one", "start": 1632.559, "duration": 4.801}, {"text": "thread per core but we divide it up a", "start": 1635.2, "duration": 4.079}, {"text": "little bit differently so here we have", "start": 1637.36, "duration": 5.319}, {"text": "four processes two processes per node", "start": 1639.279, "duration": 5.561}, {"text": "and eight processes sorry and eight", "start": 1642.679, "duration": 3.841}, {"text": "threads per", "start": 1644.84, "duration": 4.68}, {"text": "process", "start": 1646.52, "duration": 3.0}, {"text": "um a typical scenario and I'm going to", "start": 1650.0, "duration": 5.039}, {"text": "say this is getting a little out of date", "start": 1653.399, "duration": 4.081}, {"text": "is run one process per node and use", "start": 1655.039, "duration": 4.481}, {"text": "threads within with within a node but", "start": 1657.48, "duration": 4.079}, {"text": "this is becoming a little less common as", "start": 1659.52, "duration": 3.92}, {"text": "the number of cores on a node continues", "start": 1661.559, "duration": 3.72}, {"text": "to", "start": 1663.44, "duration": 4.64}, {"text": "grow um for hybrid apps we need to try", "start": 1665.279, "duration": 4.721}, {"text": "to choose the the the balance between", "start": 1668.08, "duration": 3.68}, {"text": "processes and threads giving the range", "start": 1670.0, "duration": 4.159}, {"text": "of options so we have show here on the", "start": 1671.76, "duration": 6.0}, {"text": "left one process per node 16 threads per", "start": 1674.159, "duration": 6.4}, {"text": "process all the way down to on the right", "start": 1677.76, "duration": 5.759}, {"text": "to um one process um what one process", "start": 1680.559, "duration": 6.72}, {"text": "per core and you know the common", "start": 1683.519, "duration": 5.601}, {"text": "question would be you know how how do I", "start": 1687.279, "duration": 4.28}, {"text": "choose and I say the only way to do this", "start": 1689.12, "duration": 3.96}, {"text": "is that you'll need to Benchmark your", "start": 1691.559, "duration": 3.48}, {"text": "application try to run this with", "start": 1693.08, "duration": 4.0}, {"text": "different combinations of of processes", "start": 1695.039, "duration": 3.601}, {"text": "and cores and see what gives you the", "start": 1697.08, "duration": 3.079}, {"text": "best", "start": 1698.64, "duration": 4.159}, {"text": "performance so any questions before I", "start": 1700.159, "duration": 5.12}, {"text": "move on to amell's", "start": 1702.799, "duration": 6.521}, {"text": "law yes um does a task and slurm", "start": 1705.279, "duration": 7.081}, {"text": "correspond to a", "start": 1709.32, "duration": 3.04}, {"text": "process", "start": 1712.72, "duration": 4.52}, {"text": "o", "start": 1714.24, "duration": 3.0}, {"text": "um yes yes typically a task will be um", "start": 1717.96, "duration": 8.36}, {"text": "will be associated with a", "start": 1722.88, "duration": 3.44}, {"text": "process that is all Bob okay all right", "start": 1727.0, "duration": 5.48}, {"text": "so now we're gonna get on to um om", "start": 1730.48, "duration": 3.919}, {"text": "doll's law we're going to get into a", "start": 1732.48, "duration": 5.4}, {"text": "little more um practical content so if", "start": 1734.399, "duration": 5.801}, {"text": "you know only", "start": 1737.88, "duration": 5.039}, {"text": "one um what one bit of theoretical", "start": 1740.2, "duration": 4.76}, {"text": "parallel Computing it should be amd's", "start": 1742.919, "duration": 5.201}, {"text": "law and this is a description of the", "start": 1744.96, "duration": 5.439}, {"text": "absolute limit on the speed up of a code", "start": 1748.12, "duration": 5.12}, {"text": "as a function of the um proportion of", "start": 1750.399, "duration": 4.841}, {"text": "the code that can be paralyzed and the", "start": 1753.24, "duration": 5.039}, {"text": "number of processes so this is truly the", "start": 1755.24, "duration": 5.279}, {"text": "most fundamental law of parallel", "start": 1758.279, "duration": 4.161}, {"text": "Computing um right now this is going to", "start": 1760.519, "duration": 3.241}, {"text": "be a little", "start": 1762.44, "duration": 4.2}, {"text": "abstract um but we're going to we're", "start": 1763.76, "duration": 5.399}, {"text": "going to say that p is the fraction of", "start": 1766.64, "duration": 4.08}, {"text": "the codee that can be", "start": 1769.159, "duration": 3.52}, {"text": "paralyzed um and it's a little hard to", "start": 1770.72, "duration": 4.76}, {"text": "tell sometimes you know what what part", "start": 1772.679, "duration": 4.561}, {"text": "of the code can be paralyzed but let's", "start": 1775.48, "duration": 3.88}, {"text": "just assume for now that we can look at", "start": 1777.24, "duration": 3.919}, {"text": "the entire workload for the application", "start": 1779.36, "duration": 3.4}, {"text": "and say that P is the fraction that we", "start": 1781.159, "duration": 4.4}, {"text": "can then paralized s is going to be the", "start": 1782.76, "duration": 4.96}, {"text": "fraction the code that must be run", "start": 1785.559, "duration": 4.801}, {"text": "sequentially so S Plus p is going to", "start": 1787.72, "duration": 5.48}, {"text": "equal to one and N is the number of", "start": 1790.36, "duration": 5.36}, {"text": "processes so so the speed up that you're", "start": 1793.2, "duration": 4.56}, {"text": "going to get as a fun function of the", "start": 1795.72, "duration": 5.24}, {"text": "number of processors is going to be 1/ 1", "start": 1797.76, "duration": 6.919}, {"text": "- P plus p/ n so you can see that the", "start": 1800.96, "duration": 5.68}, {"text": "parallel part of the code it's going to", "start": 1804.679, "duration": 4.921}, {"text": "continue to um the time that we spend", "start": 1806.64, "duration": 6.08}, {"text": "executing that is going to decrease with", "start": 1809.6, "duration": 6.36}, {"text": "with the number of processors but the um", "start": 1812.72, "duration": 5.64}, {"text": "serial content the time that we spend on", "start": 1815.96, "duration": 4.079}, {"text": "that is going to be", "start": 1818.36, "duration": 4.4}, {"text": "fixed so in the limit as the number of", "start": 1820.039, "duration": 5.281}, {"text": "processes go to Infinity the theoretical", "start": 1822.76, "duration": 4.72}, {"text": "speed up is going to depend only on the", "start": 1825.32, "duration": 4.64}, {"text": "proportion of the parallel content so we", "start": 1827.48, "duration": 6.28}, {"text": "let N Go to Infinity um our expression", "start": 1829.96, "duration": 6.319}, {"text": "simplifies the 1 over 1 minus P or one /", "start": 1833.76, "duration": 4.799}, {"text": "s so I'm going to say this doesn't look", "start": 1836.279, "duration": 3.64}, {"text": "so bad but we're going to show in the", "start": 1838.559, "duration": 3.401}, {"text": "next slide that it doesn't take much", "start": 1839.919, "duration": 4.521}, {"text": "serial content in to impact the speed up", "start": 1841.96, "duration": 3.88}, {"text": "of your", "start": 1844.44, "duration": 5.56}, {"text": "code so what I'm showing here is is a PL", "start": 1845.84, "duration": 6.8}, {"text": "um on on the Y AIS is the speed up how", "start": 1850.0, "duration": 6.24}, {"text": "much fast the code is running um and on", "start": 1852.64, "duration": 7.2}, {"text": "the um x-axis I'm showing the number of", "start": 1856.24, "duration": 6.64}, {"text": "cores um and this is a log scale at the", "start": 1859.84, "duration": 6.839}, {"text": "bottom so you can see that if the", "start": 1862.88, "duration": 5.48}, {"text": "fraction of the code that could be", "start": 1866.679, "duration": 5.48}, {"text": "paralyzed is 50% the maximum speed up no", "start": 1868.36, "duration": 6.319}, {"text": "matter how many processes we use is only", "start": 1872.159, "duration": 5.321}, {"text": "going to be two when we go to 75% max", "start": 1874.679, "duration": 8.12}, {"text": "speed up will be four 90% 10 95%", "start": 1877.48, "duration": 7.799}, {"text": "20 so let's go a little bit further look", "start": 1882.799, "duration": 6.76}, {"text": "at a code that has 99 % parallel content", "start": 1885.279, "duration": 8.841}, {"text": "so even with 99% paralyzable content we", "start": 1889.559, "duration": 6.84}, {"text": "wouldn't even be able to make good use", "start": 1894.12, "duration": 5.919}, {"text": "of a single modern 128 core node so in", "start": 1896.399, "duration": 6.041}, {"text": "this case if we were running this code", "start": 1900.039, "duration": 4.36}, {"text": "with with P equals", "start": 1902.44, "duration": 5.44}, {"text": "99% if we were here um indicated by the", "start": 1904.399, "duration": 7.28}, {"text": "by the arrow running on on 128 cores we", "start": 1907.88, "duration": 6.32}, {"text": "would have only gotten a speed up of 60", "start": 1911.679, "duration": 4.761}, {"text": "instead of the um instead of a speed up", "start": 1914.2, "duration": 3.56}, {"text": "of 128", "start": 1916.44, "duration": 3.839}, {"text": "if you were running at this parall", "start": 1917.76, "duration": 4.159}, {"text": "efficiency on our", "start": 1920.279, "duration": 3.681}, {"text": "supercomputers um we would be upset", "start": 1921.919, "duration": 3.76}, {"text": "because you would be making bad use of", "start": 1923.96, "duration": 3.839}, {"text": "the supercomputer and you would be", "start": 1925.679, "duration": 4.12}, {"text": "burning through your allocation much", "start": 1927.799, "duration": 5.081}, {"text": "more quickly than you need", "start": 1929.799, "duration": 3.081}, {"text": "to so in addition to omd's law there are", "start": 1933.36, "duration": 5.159}, {"text": "other limits on", "start": 1936.679, "duration": 4.761}, {"text": "scalability so as I mentioned omd's law", "start": 1938.519, "duration": 4.601}, {"text": "know what I'm going to pause here um", "start": 1941.44, "duration": 3.76}, {"text": "Cindy do we have more", "start": 1943.12, "duration": 4.88}, {"text": "questions uh yes but Marty's kind of", "start": 1945.2, "duration": 4.4}, {"text": "having the conversation through the chat", "start": 1948.0, "duration": 3.519}, {"text": "so okay I'll let him complete that and", "start": 1949.6, "duration": 3.24}, {"text": "we'll come back if they're still", "start": 1951.519, "duration": 5.04}, {"text": "standing okay s sounds", "start": 1952.84, "duration": 3.719}, {"text": "good all right so um Aldo's law this", "start": 1956.639, "duration": 5.321}, {"text": "sets a theoretical upper limit on the", "start": 1959.88, "duration": 3.84}, {"text": "speed up but there are other things that", "start": 1961.96, "duration": 3.319}, {"text": "are going to affect your", "start": 1963.72, "duration": 3.36}, {"text": "scalability um there's going to be", "start": 1965.279, "duration": 4.24}, {"text": "limits to due to the problem size", "start": 1967.08, "duration": 4.599}, {"text": "there's Communications overhead there's", "start": 1969.519, "duration": 5.04}, {"text": "going to be uneven load balancing so in", "start": 1971.679, "duration": 4.48}, {"text": "real life applications involve", "start": 1974.559, "duration": 4.12}, {"text": "Communications and synchronization for", "start": 1976.159, "duration": 4.321}, {"text": "example all the threads or processes", "start": 1978.679, "duration": 3.201}, {"text": "must complete their work before", "start": 1980.48, "duration": 4.24}, {"text": "proceeding or irregular problems on", "start": 1981.88, "duration": 5.44}, {"text": "non-cartesian brids the speed up can be", "start": 1984.72, "duration": 5.88}, {"text": "much less than predicted by om doll's", "start": 1987.32, "duration": 6.359}, {"text": "law so I'm starting off with Pro with", "start": 1990.6, "duration": 5.4}, {"text": "problem siiz limitations and the only", "start": 1993.679, "duration": 4.401}, {"text": "reason that I'm using um Hello Kitty", "start": 1996.0, "duration": 4.44}, {"text": "with a paintbrush in this example is", "start": 1998.08, "duration": 4.36}, {"text": "that it was the only public domain image", "start": 2000.44, "duration": 4.28}, {"text": "with a transparent background of a", "start": 2002.44, "duration": 3.76}, {"text": "character with a paintbrush that I could", "start": 2004.72, "duration": 2.88}, {"text": "find", "start": 2006.2, "duration": 3.719}, {"text": "um and and this is this is a really", "start": 2007.6, "duration": 5.6}, {"text": "common example in um in introduction to", "start": 2009.919, "duration": 5.681}, {"text": "parallel Computing so so let's say that", "start": 2013.2, "duration": 4.439}, {"text": "we have um one person or in this case", "start": 2015.6, "duration": 5.36}, {"text": "one kitty is pain in a fence we assume", "start": 2017.639, "duration": 6.321}, {"text": "that it takes um 48 minutes for for the", "start": 2020.96, "duration": 5.28}, {"text": "for for the one kitty to paint the to", "start": 2023.96, "duration": 4.839}, {"text": "paint the entire fence and you would you", "start": 2026.24, "duration": 4.76}, {"text": "would think intuitively that if we get", "start": 2028.799, "duration": 4.201}, {"text": "if we get two cats that they would be", "start": 2031.0, "duration": 3.919}, {"text": "able to paint the fence in half the time", "start": 2033.0, "duration": 5.96}, {"text": "or in 24 minutes", "start": 2034.919, "duration": 4.041}, {"text": "and then we might think if we use four", "start": 2039.159, "duration": 4.561}, {"text": "cats we could paint the fence in a", "start": 2041.279, "duration": 4.801}, {"text": "quarter of the time or in this case 12", "start": 2043.72, "duration": 4.84}, {"text": "minutes and maybe eight cats that we", "start": 2046.08, "duration": 4.519}, {"text": "could paint the fence in 16 minutes and", "start": 2048.56, "duration": 4.359}, {"text": "you can see now um the fence is getting", "start": 2050.599, "duration": 4.201}, {"text": "a little little crowded there's really", "start": 2052.919, "duration": 4.68}, {"text": "not enough work for um for for for each", "start": 2054.8, "duration": 4.96}, {"text": "of the um for each of the painters to", "start": 2057.599, "duration": 5.56}, {"text": "do and if we go to a ridiculous", "start": 2059.76, "duration": 5.639}, {"text": "example um you know let let's say that", "start": 2063.159, "duration": 4.561}, {"text": "we had 32 painters trying to paint this", "start": 2065.399, "duration": 3.96}, {"text": "fence we see that there's just not", "start": 2067.72, "duration": 4.399}, {"text": "enough work So eventually you're going", "start": 2069.359, "duration": 4.201}, {"text": "to get to the point where there's not", "start": 2072.119, "duration": 2.8}, {"text": "enough work to keep all of your", "start": 2073.56, "duration": 4.119}, {"text": "processor the GPU is busy and in fact", "start": 2074.919, "duration": 5.72}, {"text": "not only will the um run runtime level", "start": 2077.679, "duration": 4.601}, {"text": "off but you may see that your", "start": 2080.639, "duration": 4.121}, {"text": "application run slower since the", "start": 2082.28, "duration": 6.079}, {"text": "parallel overhead um associated with", "start": 2084.76, "duration": 5.8}, {"text": "running on that many um that that many", "start": 2088.359, "duration": 6.201}, {"text": "cores overruns the overwhelms the", "start": 2090.56, "duration": 6.4}, {"text": "runtime another thing that can impact is", "start": 2094.56, "duration": 5.36}, {"text": "we'll call uneven load balancing so", "start": 2096.96, "duration": 4.76}, {"text": "paralyzing code requires dividing the", "start": 2099.92, "duration": 3.88}, {"text": "computational work into chunks that can", "start": 2101.72, "duration": 4.76}, {"text": "be executed independently now if the", "start": 2103.8, "duration": 5.559}, {"text": "work cannot be distributed evenly then", "start": 2106.48, "duration": 4.92}, {"text": "processor are going to sit idle waiting", "start": 2109.359, "duration": 4.561}, {"text": "for that longest chunk to finish so", "start": 2111.4, "duration": 5.4}, {"text": "here's an example where let's say we", "start": 2113.92, "duration": 5.52}, {"text": "have we have four chunks of work um", "start": 2116.8, "duration": 5.36}, {"text": "indicated by the solid blue", "start": 2119.44, "duration": 5.2}, {"text": "boxes these need to be completed before", "start": 2122.16, "duration": 4.88}, {"text": "the next four chunks of work are done so", "start": 2124.64, "duration": 4.64}, {"text": "let's say that we assume we we assign", "start": 2127.04, "duration": 5.76}, {"text": "these to CPUs 0 1 2 and", "start": 2129.28, "duration": 6.28}, {"text": "3 and the vertical bars are", "start": 2132.8, "duration": 4.72}, {"text": "synchronization points where all the", "start": 2135.56, "duration": 4.32}, {"text": "tasks or the threads have to complete", "start": 2137.52, "duration": 5.76}, {"text": "before proceeding um so for example", "start": 2139.88, "duration": 5.32}, {"text": "maybe the processes need to exchange", "start": 2143.28, "duration": 3.92}, {"text": "newly computed results or threads need", "start": 2145.2, "duration": 4.32}, {"text": "to finish updating a shared array so", "start": 2147.2, "duration": 5.76}, {"text": "what's going to happen is um cpu1 is", "start": 2149.52, "duration": 5.559}, {"text": "going to finish first and then it's", "start": 2152.96, "duration": 4.68}, {"text": "going to sit idle follow followed by CPU", "start": 2155.079, "duration": 4.841}, {"text": "3 and CPU 0 so they're going to be", "start": 2157.64, "duration": 3.56}, {"text": "sitting idle they're not going to be", "start": 2159.92, "duration": 3.12}, {"text": "doing any useful work while they're", "start": 2161.2, "duration": 3.919}, {"text": "waiting for CPU 2 to", "start": 2163.04, "duration": 4.72}, {"text": "finish um then we assigned the next four", "start": 2165.119, "duration": 5.48}, {"text": "chunks work that this time cpu1 got the", "start": 2167.76, "duration": 6.559}, {"text": "longest chunk um CPUs Z one and three", "start": 2170.599, "duration": 5.52}, {"text": "are going to be waiting until cpu1", "start": 2174.319, "duration": 4.081}, {"text": "finishes so this is another Factor", "start": 2176.119, "duration": 4.641}, {"text": "that's going to um that that's going to", "start": 2178.4, "duration": 5.28}, {"text": "affect our um affect the speed up that", "start": 2180.76, "duration": 4.96}, {"text": "we can get as we run in", "start": 2183.68, "duration": 3.96}, {"text": "parallel", "start": 2185.72, "duration": 3.48}, {"text": "and then finally there's Communications", "start": 2187.64, "duration": 4.04}, {"text": "overhead um this is really common if", "start": 2189.2, "duration": 4.48}, {"text": "you're doing computational flu Dynamics", "start": 2191.68, "duration": 5.28}, {"text": "or Magneto hydr Dynamics climate weather", "start": 2193.68, "duration": 5.48}, {"text": "any kind of simulation that involves um", "start": 2196.96, "duration": 4.359}, {"text": "solving systems of par partial", "start": 2199.16, "duration": 4.76}, {"text": "differential equations on a grid um I", "start": 2201.319, "duration": 4.241}, {"text": "saw earlier in the chat somebody", "start": 2203.92, "duration": 6.0}, {"text": "mentioned um mentioned open foam um yes", "start": 2205.56, "duration": 7.44}, {"text": "you can you can run open foam on our on", "start": 2209.92, "duration": 5.56}, {"text": "our supercomputers but when you run it", "start": 2213.0, "duration": 3.64}, {"text": "you're going to run into the situ", "start": 2215.48, "duration": 4.119}, {"text": "situation that we're seeing here so", "start": 2216.64, "duration": 4.64}, {"text": "let's say we take our we take our", "start": 2219.599, "duration": 3.681}, {"text": "computational grid of course this is a", "start": 2221.28, "duration": 5.839}, {"text": "very very small example a 16x 16 grid", "start": 2223.28, "duration": 5.76}, {"text": "we're going to divide this into four 8", "start": 2227.119, "duration": 4.281}, {"text": "by8 chunks and we're going to distribute", "start": 2229.04, "duration": 5.44}, {"text": "this across the four", "start": 2231.4, "duration": 3.08}, {"text": "processes now we're going to assume that", "start": 2235.16, "duration": 4.04}, {"text": "each cell is updated using the values of", "start": 2237.119, "duration": 4.081}, {"text": "the four neighboring cells so for the", "start": 2239.2, "duration": 3.76}, {"text": "cell that are within the interior of", "start": 2241.2, "duration": 4.0}, {"text": "each chunk calculations can be done", "start": 2242.96, "duration": 4.56}, {"text": "within each process", "start": 2245.2, "duration": 3.32}, {"text": "but you see it's going to get a little", "start": 2247.52, "duration": 2.52}, {"text": "more complicated for those cells that", "start": 2248.52, "duration": 3.12}, {"text": "are at the boundaries they're going to", "start": 2250.04, "duration": 3.559}, {"text": "need data belonging to a neighboring", "start": 2251.64, "duration": 5.0}, {"text": "process so to accommodate this typically", "start": 2253.599, "duration": 5.52}, {"text": "the solutions of systems of partial", "start": 2256.64, "duration": 4.959}, {"text": "differential equations will set up a", "start": 2259.119, "duration": 4.121}, {"text": "Halo of ghost", "start": 2261.599, "duration": 5.24}, {"text": "cells around um around the", "start": 2263.24, "duration": 8.079}, {"text": "grid so we're going to um periodically", "start": 2266.839, "duration": 6.721}, {"text": "could communicate data be between these", "start": 2271.319, "duration": 6.401}, {"text": "processes um to fill these G cells so", "start": 2273.56, "duration": 7.4}, {"text": "that um each cell along the boundary", "start": 2277.72, "duration": 4.84}, {"text": "will have access to the data that it", "start": 2280.96, "duration": 4.2}, {"text": "needs so this data movement this is", "start": 2282.56, "duration": 6.4}, {"text": "going to um introduce overhead we're", "start": 2285.16, "duration": 6.32}, {"text": "going to be limited both by the latency", "start": 2288.96, "duration": 5.32}, {"text": "how long it takes to communicate that", "start": 2291.48, "duration": 5.04}, {"text": "first how long it takes to communicate", "start": 2294.28, "duration": 4.319}, {"text": "that first by of data and by the", "start": 2296.52, "duration": 3.76}, {"text": "bandwidth the the rate at which we can", "start": 2298.599, "duration": 3.881}, {"text": "move the data so this is going to", "start": 2300.28, "duration": 4.72}, {"text": "further impact the um performance", "start": 2302.48, "duration": 5.359}, {"text": "through a parallel application", "start": 2305.0, "duration": 4.839}, {"text": "so at this point you might be thinking", "start": 2307.839, "duration": 2.801}, {"text": "you", "start": 2309.839, "duration": 3.881}, {"text": "know how does anybody how does anybody", "start": 2310.64, "duration": 5.6}, {"text": "do parallel Computing we're limited by", "start": 2313.72, "duration": 4.72}, {"text": "omd's law and then we have all of these", "start": 2316.24, "duration": 3.96}, {"text": "other factors you know related to", "start": 2318.44, "duration": 3.84}, {"text": "problem size and Communications overhead", "start": 2320.2, "duration": 4.24}, {"text": "and load balancing no how does anybody", "start": 2322.28, "duration": 4.28}, {"text": "ever use all the cores on a single", "start": 2324.44, "duration": 4.399}, {"text": "modern compute node let alone the full", "start": 2326.56, "duration": 3.64}, {"text": "power of a large", "start": 2328.839, "duration": 3.76}, {"text": "supercomputer so first of all the", "start": 2330.2, "duration": 4.24}, {"text": "reality is that most parallel", "start": 2332.599, "duration": 4.201}, {"text": "applications do not scale to thousand or", "start": 2334.44, "duration": 4.72}, {"text": "even hundreds of cores in fact if you", "start": 2336.8, "duration": 4.92}, {"text": "would look at the workload on expense", "start": 2339.16, "duration": 5.36}, {"text": "many of the jobs run within many of them", "start": 2341.72, "duration": 5.599}, {"text": "run within a single node so just because", "start": 2344.52, "duration": 7.12}, {"text": "we have um you know a a supercomputer", "start": 2347.319, "duration": 6.441}, {"text": "with um T of thousands or hundreds of", "start": 2351.64, "duration": 3.8}, {"text": "thousands of cores doesn't mean that you", "start": 2353.76, "duration": 5.44}, {"text": "need to use all of them also um there", "start": 2355.44, "duration": 6.36}, {"text": "there strategies to um to achieve High", "start": 2359.2, "duration": 5.32}, {"text": "scalability We Grow the problem size", "start": 2361.8, "duration": 4.279}, {"text": "with the number of cores or nodes that", "start": 2364.52, "duration": 2.52}, {"text": "we're using", "start": 2366.079, "duration": 2.721}, {"text": "we overlap the communication with the", "start": 2367.04, "duration": 3.96}, {"text": "computation we use Dynamic load", "start": 2368.8, "duration": 4.0}, {"text": "balancing to assign worked cores that", "start": 2371.0, "duration": 4.0}, {"text": "they become idle or we increase the", "start": 2372.8, "duration": 4.2}, {"text": "ratio of the comp of the computation and", "start": 2375.0, "duration": 4.4}, {"text": "the communications again this is all", "start": 2377.0, "duration": 4.24}, {"text": "going to be done by the software", "start": 2379.4, "duration": 4.0}, {"text": "developer um So", "start": 2381.24, "duration": 4.64}, {"text": "within um chemistry and molecular", "start": 2383.4, "duration": 5.48}, {"text": "Dynamics and flu Dynamics codes the", "start": 2385.88, "duration": 5.04}, {"text": "developers will have thought of these", "start": 2388.88, "duration": 5.32}, {"text": "things so that they could get better", "start": 2390.92, "duration": 3.28}, {"text": "performance so I'm going to pause here", "start": 2394.44, "duration": 4.56}, {"text": "and see if there are", "start": 2397.28, "duration": 5.039}, {"text": "any um any", "start": 2399.0, "duration": 6.96}, {"text": "questions yes we do have one or two um", "start": 2402.319, "duration": 6.961}, {"text": "one is a true or false in General slurm", "start": 2405.96, "duration": 5.6}, {"text": "job array makes more efficient use of", "start": 2409.28, "duration": 4.72}, {"text": "resources open parentheses capable of", "start": 2411.56, "duration": 4.32}, {"text": "freeing them up sooner closed", "start": 2414.0, "duration": 4.48}, {"text": "parentheses than the same computational", "start": 2415.88, "duration": 7.4}, {"text": "SLP parallel workload using gnu parallel", "start": 2418.48, "duration": 7.32}, {"text": "open parentheses forces all processes", "start": 2423.28, "duration": 3.68}, {"text": "and therefore", "start": 2425.8, "duration": 4.84}, {"text": "resources to wait or idle until the last", "start": 2426.96, "duration": 5.96}, {"text": "processes finish", "start": 2430.64, "duration": 5.08}, {"text": "finishes okay so I am not I am not", "start": 2432.92, "duration": 5.28}, {"text": "familiar with with with G parallel but", "start": 2435.72, "duration": 4.56}, {"text": "based on the description I would say", "start": 2438.2, "duration": 4.399}, {"text": "that slurm Java Rays would would make", "start": 2440.28, "duration": 5.76}, {"text": "more um more efficient use because once", "start": 2442.599, "duration": 6.081}, {"text": "one of those um jobs within the ray", "start": 2446.04, "duration": 4.96}, {"text": "finishes the hardware is then freed up", "start": 2448.68, "duration": 6.84}, {"text": "and available for a um available for for", "start": 2451.0, "duration": 7.319}, {"text": "the next job", "start": 2455.52, "duration": 2.799}, {"text": "thank you Bob and then the last one is", "start": 2459.0, "duration": 4.0}, {"text": "going to be can you please clarify", "start": 2461.0, "duration": 4.839}, {"text": "intraband and interpool communication", "start": 2463.0, "duration": 5.16}, {"text": "which one is better do we need to use", "start": 2465.839, "duration": 4.681}, {"text": "both intra and", "start": 2468.16, "duration": 5.679}, {"text": "interpool oo", "start": 2470.52, "duration": 5.76}, {"text": "um you know I'm not sure what you mean", "start": 2473.839, "duration": 5.201}, {"text": "by intraand and intra poool I'm going to", "start": 2476.28, "duration": 6.12}, {"text": "assume that um but by intraand intraband", "start": 2479.04, "duration": 8.44}, {"text": "you mean within within a compute node", "start": 2482.4, "duration": 7.719}, {"text": "um as opposed to um", "start": 2487.48, "duration": 4.92}, {"text": "across okay yes I see a note here from", "start": 2490.119, "duration": 4.521}, {"text": "Marty yes so if you if you mean", "start": 2492.4, "duration": 3.8}, {"text": "intranode and", "start": 2494.64, "duration": 4.16}, {"text": "internode um I'm going to say that that", "start": 2496.2, "duration": 5.36}, {"text": "the communications is going to be faster", "start": 2498.8, "duration": 6.4}, {"text": "within a nodee um because we we we don't", "start": 2501.56, "duration": 5.72}, {"text": "we don't need to go out to the network", "start": 2505.2, "duration": 6.04}, {"text": "um and the MPI libraries are designed in", "start": 2507.28, "duration": 6.52}, {"text": "such a way that they can implement the", "start": 2511.24, "duration": 6.4}, {"text": "communications faster within a node", "start": 2513.8, "duration": 7.799}, {"text": "um but do we need to use both um intra", "start": 2517.64, "duration": 7.52}, {"text": "and interpo or intra and Inter node if", "start": 2521.599, "duration": 5.881}, {"text": "you want AC if you want to run across", "start": 2525.16, "duration": 3.88}, {"text": "multiple compute nodes if you really", "start": 2527.48, "duration": 3.48}, {"text": "want to do parallel Computing at Large", "start": 2529.04, "duration": 4.68}, {"text": "Scale you'll need to do both um now the", "start": 2530.96, "duration": 5.639}, {"text": "good thing is that you don't need to", "start": 2533.72, "duration": 5.76}, {"text": "worry about the details of oh this is", "start": 2536.599, "duration": 4.72}, {"text": "the communication being done between", "start": 2539.48, "duration": 4.44}, {"text": "processes within a single node or across", "start": 2541.319, "duration": 4.881}, {"text": "nodes the um appropriate protocol", "start": 2543.92, "duration": 4.8}, {"text": "forward be", "start": 2546.2, "duration": 2.52}, {"text": "USS and one two just came in what it's", "start": 2548.92, "duration": 5.679}, {"text": "uh what do you mean by Communications in", "start": 2552.4, "duration": 4.48}, {"text": "the previous slide example given", "start": 2554.599, "duration": 5.0}, {"text": "increased ratio of computation to", "start": 2556.88, "duration": 7.0}, {"text": "Communications yeah let me go back um", "start": 2559.599, "duration": 6.321}, {"text": "sorry send could you repeat that", "start": 2563.88, "duration": 4.92}, {"text": "again yes it's um what do you mean by", "start": 2565.92, "duration": 5.12}, {"text": "Communications in the previous", "start": 2568.8, "duration": 5.519}, {"text": "slide okay yes yes by but by by", "start": 2571.04, "duration": 7.12}, {"text": "Communications we mean um but moving", "start": 2574.319, "duration": 6.881}, {"text": "data from from one process to the next I", "start": 2578.16, "duration": 5.64}, {"text": "mentioned earlier that that processes", "start": 2581.2, "duration": 5.08}, {"text": "you could think of a process as being an", "start": 2583.8, "duration": 5.08}, {"text": "instance of a program and it has its own", "start": 2586.28, "duration": 6.36}, {"text": "memory space um so when it needs data", "start": 2588.88, "duration": 5.8}, {"text": "belonging to another process that has to", "start": 2592.64, "duration": 5.12}, {"text": "be explicitly communicated using a", "start": 2594.68, "duration": 6.96}, {"text": "Communications Library like like", "start": 2597.76, "duration": 3.88}, {"text": "MPI", "start": 2605.079, "duration": 3.0}, {"text": "okay so I'm going to move on", "start": 2608.44, "duration": 3.84}, {"text": "um okay", "start": 2612.8, "duration": 5.16}, {"text": "so know I I left off here I said all is", "start": 2614.96, "duration": 5.52}, {"text": "not lost um we we we can still do", "start": 2617.96, "duration": 4.159}, {"text": "parallel Computing and sometimes still", "start": 2620.48, "duration": 4.76}, {"text": "scale to a very large number of nodes of", "start": 2622.119, "duration": 6.641}, {"text": "course so first of all", "start": 2625.24, "duration": 6.28}, {"text": "um regarding problem size", "start": 2628.76, "duration": 5.28}, {"text": "limitations sure you if you start with a", "start": 2631.52, "duration": 4.799}, {"text": "very small problem you may not be able", "start": 2634.04, "duration": 4.96}, {"text": "to run it across um a large number of", "start": 2636.319, "duration": 5.28}, {"text": "cores but but often we Sol we solve a", "start": 2639.0, "duration": 4.559}, {"text": "larger problem so again getting back to", "start": 2641.599, "duration": 5.081}, {"text": "the F fence painting example um we're", "start": 2643.559, "duration": 5.52}, {"text": "not we're not going to use 32 cats to", "start": 2646.68, "duration": 5.04}, {"text": "paint um to paint a small fence but we", "start": 2649.079, "duration": 4.961}, {"text": "might use them to paint a paint a much", "start": 2651.72, "duration": 5.28}, {"text": "larger fence um this is really really", "start": 2654.04, "duration": 5.0}, {"text": "common in parallel Computing where let's", "start": 2657.0, "duration": 4.52}, {"text": "say we're doing a climate or a weather", "start": 2659.04, "duration": 5.2}, {"text": "simulation is well we we use a larger", "start": 2661.52, "duration": 5.0}, {"text": "grid with a um", "start": 2664.24, "duration": 4.2}, {"text": "with with a finer grid space with with", "start": 2666.52, "duration": 4.0}, {"text": "finer grid size and now we have more", "start": 2668.44, "duration": 4.04}, {"text": "work that can be distributed across", "start": 2670.52, "duration": 4.92}, {"text": "multiple nodes or nodes or", "start": 2672.48, "duration": 6.2}, {"text": "cores um we can also overlap the", "start": 2675.44, "duration": 5.48}, {"text": "communications with with the", "start": 2678.68, "duration": 4.159}, {"text": "computation again this is something", "start": 2680.92, "duration": 3.88}, {"text": "that's going to be implemented by the", "start": 2682.839, "duration": 3.441}, {"text": "developer of the code but I'm going to", "start": 2684.8, "duration": 6.759}, {"text": "say that most um well-written modern um", "start": 2686.28, "duration": 7.839}, {"text": "applications will will will do this", "start": 2691.559, "duration": 5.0}, {"text": "especially system part", "start": 2694.119, "duration": 3.801}, {"text": "especially those that are solving", "start": 2696.559, "duration": 3.04}, {"text": "systems of partial differential", "start": 2697.92, "duration": 5.0}, {"text": "equations so what we could do is we", "start": 2699.599, "duration": 5.201}, {"text": "could be doing calculations and what we", "start": 2702.92, "duration": 6.24}, {"text": "consider the interior cells while while", "start": 2704.8, "duration": 5.4}, {"text": "while we're waiting for the", "start": 2709.16, "duration": 3.52}, {"text": "communications to finish so now there's", "start": 2710.2, "duration": 5.6}, {"text": "going to be um what while that data is", "start": 2712.68, "duration": 5.28}, {"text": "being communicated we're doing these", "start": 2715.8, "duration": 4.039}, {"text": "other other", "start": 2717.96, "duration": 4.359}, {"text": "computations we can do Dynamic load", "start": 2719.839, "duration": 4.52}, {"text": "balancing again this is something that's", "start": 2722.319, "duration": 4.201}, {"text": "going to be done by the developer the", "start": 2724.359, "duration": 4.281}, {"text": "code so imagine that we have a set of", "start": 2726.52, "duration": 3.559}, {"text": "tasks that could all be performed", "start": 2728.64, "duration": 3.0}, {"text": "independently and they take varying", "start": 2730.079, "duration": 4.28}, {"text": "amounts of time so if we do a static", "start": 2731.64, "duration": 5.0}, {"text": "assignment of those tasks to processes", "start": 2734.359, "duration": 5.0}, {"text": "that can lead to a large load imbalance", "start": 2736.64, "duration": 4.28}, {"text": "um we can get better performance if we", "start": 2739.359, "duration": 3.881}, {"text": "dynamically assigned worked processes to", "start": 2740.92, "duration": 5.159}, {"text": "become vital so let's assume that we had", "start": 2743.24, "duration": 5.119}, {"text": "um we had eight tasks that we need to", "start": 2746.079, "duration": 5.801}, {"text": "complete um ABC d e", "start": 2748.359, "duration": 6.361}, {"text": "fgh um if we just took those first four", "start": 2751.88, "duration": 5.32}, {"text": "tasks and then assign them to processor", "start": 2754.72, "duration": 5.119}, {"text": "zero and the last four tests processor", "start": 2757.2, "duration": 4.879}, {"text": "one you can see that processor one is", "start": 2759.839, "duration": 4.161}, {"text": "going to be finishing very early and", "start": 2762.079, "duration": 5.401}, {"text": "waiting um and waiting for a process of", "start": 2764.0, "duration": 5.92}, {"text": "zero to finish its work um what we could", "start": 2767.48, "duration": 4.839}, {"text": "do instead what we call Dynamic load", "start": 2769.92, "duration": 5.12}, {"text": "balancing is we can assign these tasks", "start": 2772.319, "duration": 5.04}, {"text": "to processes as they become as they", "start": 2775.04, "duration": 5.2}, {"text": "become idle we might assign task a to", "start": 2777.359, "duration": 6.521}, {"text": "processor zero task B to processor", "start": 2780.24, "duration": 7.16}, {"text": "one um processor Z finishes um finishes", "start": 2783.88, "duration": 6.84}, {"text": "its task and then it gets c and d and e", "start": 2787.4, "duration": 7.84}, {"text": "before P1 finishes B then we assign um f", "start": 2790.72, "duration": 5.68}, {"text": "to", "start": 2795.24, "duration": 6.92}, {"text": "P1 um g g to g g to g to to P0 and then", "start": 2796.4, "duration": 8.4}, {"text": "finally H top1 so if we do this if we", "start": 2802.16, "duration": 4.76}, {"text": "assign the work as those processes", "start": 2804.8, "duration": 5.4}, {"text": "become become idle we can um greatly", "start": 2806.92, "duration": 5.76}, {"text": "reduce the amount of amount of wasted", "start": 2810.2, "duration": 4.6}, {"text": "compute", "start": 2812.68, "duration": 4.639}, {"text": "Cycles and then finally let let me stop", "start": 2814.8, "duration": 3.88}, {"text": "here and I'm going to get into what is", "start": 2817.319, "duration": 3.76}, {"text": "probably the most practical part of this", "start": 2818.68, "duration": 5.0}, {"text": "um of this", "start": 2821.079, "duration": 2.601}, {"text": "webinar", "start": 2823.8, "duration": 3.2}, {"text": "[Music]", "start": 2824.81, "duration": 4.91}, {"text": "so yeah I from what I could tell it", "start": 2827.0, "duration": 5.119}, {"text": "looks", "start": 2829.72, "duration": 2.399}, {"text": "like okay yeah it looks like those been", "start": 2835.16, "duration": 3.919}, {"text": "answered already so so now I'm going to", "start": 2837.52, "duration": 4.039}, {"text": "get into what I think is the", "start": 2839.079, "duration": 5.801}, {"text": "um you know the probably most important", "start": 2841.559, "duration": 5.361}, {"text": "practical thing that you need to", "start": 2844.88, "duration": 4.84}, {"text": "know so and this is running parallel", "start": 2846.92, "duration": 4.639}, {"text": "applications so we talked about the", "start": 2849.72, "duration": 3.8}, {"text": "basics parallel Computing hardware and", "start": 2851.559, "duration": 3.681}, {"text": "threads and processes and hybrid", "start": 2853.52, "duration": 3.799}, {"text": "applications and omd's law and other", "start": 2855.24, "duration": 3.4}, {"text": "factors that affect", "start": 2857.319, "duration": 5.121}, {"text": "scalability so this is all you know a", "start": 2858.64, "duration": 5.959}, {"text": "lot of this is very theoretical a lot of", "start": 2862.44, "duration": 4.08}, {"text": "background but how do we actually know", "start": 2864.599, "duration": 4.601}, {"text": "how many CPUs or gpus to use when", "start": 2866.52, "duration": 5.559}, {"text": "running our parallel application and the", "start": 2869.2, "duration": 4.8}, {"text": "only way to definitively answer this", "start": 2872.079, "duration": 3.28}, {"text": "question is to perform what we call a", "start": 2874.0, "duration": 4.04}, {"text": "scale study where a representative", "start": 2875.359, "duration": 6.601}, {"text": "problem is run on a different number of", "start": 2878.04, "duration": 6.48}, {"text": "processors and when I say representative", "start": 2881.96, "duration": 5.04}, {"text": "I mean one with the same size say the", "start": 2884.52, "duration": 4.799}, {"text": "grid grid dimensions number of particles", "start": 2887.0, "duration": 4.96}, {"text": "number of images number of genomes and", "start": 2889.319, "duration": 5.441}, {"text": "complexity the level of of theory the", "start": 2891.96, "duration": 5.28}, {"text": "type of analysis the physics as the", "start": 2894.76, "duration": 5.44}, {"text": "research problem that you want to", "start": 2897.24, "duration": 6.359}, {"text": "solve so we see this a lot in requests", "start": 2900.2, "duration": 7.159}, {"text": "for compute time where the um runtime as", "start": 2903.599, "duration": 7.24}, {"text": "a function the number of cores is shown", "start": 2907.359, "duration": 6.641}, {"text": "um shown using using linear axes so", "start": 2910.839, "duration": 5.76}, {"text": "believe it or not these two these two", "start": 2914.0, "duration": 5.4}, {"text": "codes on the one left and the one right", "start": 2916.599, "duration": 4.561}, {"text": "have very very different scaling", "start": 2919.4, "duration": 4.64}, {"text": "Behavior but it's really hard to tell", "start": 2921.16, "duration": 4.439}, {"text": "because when I look at this plot the", "start": 2924.04, "duration": 4.24}, {"text": "timings at large core counts are", "start": 2925.599, "duration": 4.601}, {"text": "indistinguishable and by large core", "start": 2928.28, "duration": 4.12}, {"text": "counts this is even that large I just", "start": 2930.2, "duration": 5.04}, {"text": "went up to 128 cores the number of cores", "start": 2932.4, "duration": 7.12}, {"text": "that we have on a single compute note of", "start": 2935.24, "duration": 4.28}, {"text": "expense so the right way to plot the", "start": 2940.119, "duration": 6.641}, {"text": "scaling data is to use log axes so so", "start": 2942.52, "duration": 6.68}, {"text": "notice the these are um I'm plotting the", "start": 2946.76, "duration": 4.799}, {"text": "same data that I showed in the previous", "start": 2949.2, "duration": 5.2}, {"text": "slide exact same data except now we're", "start": 2951.559, "duration": 5.24}, {"text": "using log we're using log", "start": 2954.4, "duration": 6.719}, {"text": "axes so we can see the code on the left", "start": 2956.799, "duration": 6.401}, {"text": "this is an example of a code that has", "start": 2961.119, "duration": 4.96}, {"text": "perfect scalability as we increase the", "start": 2963.2, "duration": 5.32}, {"text": "number of cores the um parallel", "start": 2966.079, "duration": 5.641}, {"text": "efficiency stays at one and the the", "start": 2968.52, "duration": 5.92}, {"text": "runtime decreases proportionally to the", "start": 2971.72, "duration": 5.32}, {"text": "number of cores if we look on the right", "start": 2974.44, "duration": 5.0}, {"text": "we could see that um in the beginning we", "start": 2977.04, "duration": 3.84}, {"text": "we're doing pretty well we're getting", "start": 2979.44, "duration": 3.399}, {"text": "we're getting good performance but the", "start": 2980.88, "duration": 4.32}, {"text": "parallel efficiency the speed up divided", "start": 2982.839, "duration": 6.48}, {"text": "by the number of cores it really drops", "start": 2985.2, "duration": 4.119}, {"text": "off okay let me", "start": 2989.839, "duration": 6.841}, {"text": "um all right so where should you be on", "start": 2993.4, "duration": 5.439}, {"text": "the scaling curve so I'm going to say if", "start": 2996.68, "duration": 4.36}, {"text": "your work is not particularly sensitive", "start": 2998.839, "duration": 4.641}, {"text": "to the time to complete a single run", "start": 3001.04, "duration": 5.319}, {"text": "consider it using a CPU or a GPU count", "start": 3003.48, "duration": 7.4}, {"text": "at a at or very close to 100% efficiency", "start": 3006.359, "duration": 7.281}, {"text": "sometimes that may even mean um running", "start": 3010.88, "duration": 5.679}, {"text": "um many jobs each on a single core um", "start": 3013.64, "duration": 4.64}, {"text": "this makes sense for parameter sweep", "start": 3016.559, "duration": 3.76}, {"text": "workloads where the same calculations", "start": 3018.28, "duration": 4.2}, {"text": "run many times with different sets of", "start": 3020.319, "duration": 4.641}, {"text": "inputs so if you're doing say molecular", "start": 3022.48, "duration": 4.839}, {"text": "Dynamics computational chemistry and you", "start": 3024.96, "duration": 4.68}, {"text": "need to do a large large number of", "start": 3027.319, "duration": 5.841}, {"text": "simulations run each of those at a um at", "start": 3029.64, "duration": 6.479}, {"text": "a smaller CPU or GPU count where you're", "start": 3033.16, "duration": 5.0}, {"text": "getting where you're getting close the", "start": 3036.119, "duration": 4.121}, {"text": "linear speed", "start": 3038.16, "duration": 5.159}, {"text": "up now you can sometimes you can go a", "start": 3040.24, "duration": 4.96}, {"text": "little bit further out on the scaling", "start": 3043.319, "duration": 3.76}, {"text": "curve if the job would take an", "start": 3045.2, "duration": 3.919}, {"text": "unreasonably long time to finish at", "start": 3047.079, "duration": 5.161}, {"text": "lower core counts or if a shorter time", "start": 3049.119, "duration": 4.881}, {"text": "dissolution helps you make progress in", "start": 3052.24, "duration": 4.4}, {"text": "your research now if your code doesn't", "start": 3054.0, "duration": 5.359}, {"text": "have checkpoint restart capabilities and", "start": 3056.64, "duration": 5.08}, {"text": "the runtime would exceed Q limits you", "start": 3059.359, "duration": 3.96}, {"text": "might have no choice but to run at a", "start": 3061.72, "duration": 4.0}, {"text": "higher core", "start": 3063.319, "duration": 4.721}, {"text": "count um sometimes you're going to be", "start": 3065.72, "duration": 4.0}, {"text": "doing Computing where the time to", "start": 3068.04, "duration": 2.88}, {"text": "solution is", "start": 3069.72, "duration": 3.16}, {"text": "critical this might be a little more", "start": 3070.92, "duration": 4.439}, {"text": "common using Hardware that that you have", "start": 3072.88, "duration": 4.4}, {"text": "in your own lab so for example if you", "start": 3075.359, "duration": 3.76}, {"text": "have calculations that need to run on a", "start": 3077.28, "duration": 4.279}, {"text": "regular schedule data collected during", "start": 3079.119, "duration": 4.761}, {"text": "the day has to be processed overnight or", "start": 3081.559, "duration": 4.321}, {"text": "if you're doing severe weather", "start": 3083.88, "duration": 4.28}, {"text": "forecasting um I should mention though", "start": 3085.88, "duration": 3.84}, {"text": "that you shouldn't be doing severe", "start": 3088.16, "duration": 2.959}, {"text": "weather forecasting on our", "start": 3089.72, "duration": 3.52}, {"text": "supercomputers they're not designed for", "start": 3091.119, "duration": 7.521}, {"text": "operational work um so for example if we", "start": 3093.24, "duration": 7.599}, {"text": "um need need to use more cores in order", "start": 3098.64, "duration": 4.56}, {"text": "to finish processing the data overnight", "start": 3100.839, "duration": 5.28}, {"text": "or doing um again if you're doing", "start": 3103.2, "duration": 4.96}, {"text": "operational work not necessarily in our", "start": 3106.119, "duration": 4.2}, {"text": "supercomputers as long as the code is", "start": 3108.16, "duration": 4.919}, {"text": "running faster you'd probably", "start": 3110.319, "duration": 5.121}, {"text": "um you'd probably sacrifice some", "start": 3113.079, "duration": 4.561}, {"text": "parallel efficiency for the per for the", "start": 3115.44, "duration": 4.399}, {"text": "sake of getting that result", "start": 3117.64, "duration": 5.52}, {"text": "quickly I know I'm going to skip over", "start": 3119.839, "duration": 6.321}, {"text": "that", "start": 3123.16, "duration": 3.0}, {"text": "and so and I'm just going to go to the", "start": 3126.559, "duration": 6.881}, {"text": "um got go to the", "start": 3130.599, "duration": 5.361}, {"text": "questions oh okay um sorry I'm trying to", "start": 3133.44, "duration": 5.879}, {"text": "let's see where do we start okay um one", "start": 3135.96, "duration": 5.639}, {"text": "is is there any difference between using", "start": 3139.319, "duration": 5.721}, {"text": "tensors or matrices or vectors in Python", "start": 3141.599, "duration": 6.801}, {"text": "in order to better paralyze parallelize", "start": 3145.04, "duration": 7.0}, {"text": "the code how to find out S and P in the", "start": 3148.4, "duration": 8.0}, {"text": "mentioned law okay", "start": 3152.04, "duration": 4.36}, {"text": "um so is there any difference between", "start": 3156.92, "duration": 6.08}, {"text": "tensors or matrices or vectors um it", "start": 3159.28, "duration": 5.4}, {"text": "depends on the type of the calculation", "start": 3163.0, "duration": 3.96}, {"text": "that you're doing if you're say working", "start": 3164.68, "duration": 5.119}, {"text": "with um working with pi torch you you", "start": 3166.96, "duration": 5.24}, {"text": "you do want to be working working with", "start": 3169.799, "duration": 6.921}, {"text": "tensors um as far as matrices ver versus", "start": 3172.2, "duration": 8.52}, {"text": "vectors um I would say if", "start": 3176.72, "duration": 6.68}, {"text": "you're um if if you're using numpy", "start": 3180.72, "duration": 4.52}, {"text": "arrays that would probably be better", "start": 3183.4, "duration": 5.76}, {"text": "than than native python python vectors", "start": 3185.24, "duration": 5.96}, {"text": "okay and now how to find S&P and", "start": 3189.16, "duration": 5.52}, {"text": "dimension law there's really no easy way", "start": 3191.2, "duration": 7.159}, {"text": "to do that um you know in some simple", "start": 3194.68, "duration": 5.879}, {"text": "codes you might be able to look at the", "start": 3198.359, "duration": 4.801}, {"text": "look look at the calculations figure out", "start": 3200.559, "duration": 5.76}, {"text": "that there's um something like like", "start": 3203.16, "duration": 5.56}, {"text": "maybe IO or initialization that has to", "start": 3206.319, "duration": 4.881}, {"text": "be done serially and try timing that", "start": 3208.72, "duration": 4.839}, {"text": "portion of the code and then looking at", "start": 3211.2, "duration": 4.52}, {"text": "another portion of the code where it's", "start": 3213.559, "duration": 4.961}, {"text": "potentially paralyzable um generally we", "start": 3215.72, "duration": 7.119}, {"text": "don't know exactly what s andp are um", "start": 3218.52, "duration": 6.92}, {"text": "and your your best option is is just to", "start": 3222.839, "duration": 6.0}, {"text": "do a benchmark and study of your", "start": 3225.44, "duration": 3.399}, {"text": "code and then I see um Marty already", "start": 3229.4, "duration": 5.199}, {"text": "answered the co question about running", "start": 3232.64, "duration": 4.32}, {"text": "two or more processes per", "start": 3234.599, "duration": 5.881}, {"text": "node okay and then um from Shashi Mishra", "start": 3236.96, "duration": 6.68}, {"text": "when writing parallel code um Dynamic", "start": 3240.48, "duration": 5.92}, {"text": "allocation versus static yep so so Marty", "start": 3243.64, "duration": 6.6}, {"text": "covered that already um and again we", "start": 3246.4, "duration": 5.6}, {"text": "we're not covering programmer topics", "start": 3250.24, "duration": 5.52}, {"text": "here but it's going to depend on your um", "start": 3252.0, "duration": 5.92}, {"text": "it's going to depend on your application", "start": 3255.76, "duration": 5.44}, {"text": "if the say the amount of work being done", "start": 3257.92, "duration": 4.6}, {"text": "per", "start": 3261.2, "duration": 3.96}, {"text": "iteration is is is uniform you probably", "start": 3262.52, "duration": 4.279}, {"text": "want to do static but if you have an", "start": 3265.16, "duration": 4.48}, {"text": "application where say there could be a", "start": 3266.799, "duration": 5.241}, {"text": "large variation in work done per", "start": 3269.64, "duration": 4.28}, {"text": "iteration you might want to go Dynamic", "start": 3272.04, "duration": 6.36}, {"text": "and I've worked on codes where I've done", "start": 3273.92, "duration": 4.48}, {"text": "both okay so we are just here at the end", "start": 3278.599, "duration": 6.081}, {"text": "of the hour um where to go next in", "start": 3282.599, "duration": 4.921}, {"text": "conclusions so we just scratched surface", "start": 3284.68, "duration": 5.08}, {"text": "there's a link here um to to our", "start": 3287.52, "duration": 4.76}, {"text": "training catalog um and also if you're", "start": 3289.76, "duration": 4.039}, {"text": "familiar with the access program the", "start": 3292.28, "duration": 3.44}, {"text": "fall want to exceed they'll soon be", "start": 3293.799, "duration": 3.481}, {"text": "implementing a training", "start": 3295.72, "duration": 5.32}, {"text": "catalog um you can also find examples of", "start": 3297.28, "duration": 7.0}, {"text": "user guides for the NSF allocated", "start": 3301.04, "duration": 6.519}, {"text": "resources at these following links so", "start": 3304.28, "duration": 5.24}, {"text": "just to wrap up parallel Computing is", "start": 3307.559, "duration": 3.641}, {"text": "for everyone who wants accomplish more", "start": 3309.52, "duration": 3.319}, {"text": "research and solve more challenging", "start": 3311.2, "duration": 3.28}, {"text": "problems you don't need to be a", "start": 3312.839, "duration": 3.801}, {"text": "programmer but you do need to know some", "start": 3314.48, "duration": 4.359}, {"text": "of the fundamental fundamentals in order", "start": 3316.64, "duration": 4.32}, {"text": "to effectively use parallel", "start": 3318.839, "duration": 4.72}, {"text": "computers remember processes it could be", "start": 3320.96, "duration": 4.119}, {"text": "thought of as instances of approach", "start": 3323.559, "duration": 3.721}, {"text": "program threads with run within a", "start": 3325.079, "duration": 5.0}, {"text": "process and access shared data MPI and", "start": 3327.28, "duration": 5.64}, {"text": "open MP are both use paralyzed code Al", "start": 3330.079, "duration": 5.561}, {"text": "dolls law gives you a strict upper limit", "start": 3332.92, "duration": 4.639}, {"text": "in scalability but there are going to be", "start": 3335.64, "duration": 3.24}, {"text": "other factors that affect your", "start": 3337.559, "duration": 3.961}, {"text": "scalability and then finally know how to", "start": 3338.88, "duration": 4.439}, {"text": "display your scaling data and choose", "start": 3341.52, "duration": 2.68}, {"text": "core", "start": 3343.319, "duration": 5.8}, {"text": "counts and with that I'm going to um sub", "start": 3344.2, "duration": 7.08}, {"text": "sharing I'm going to go over to my", "start": 3349.119, "duration": 3.281}, {"text": "GitHub", "start": 3351.28, "duration": 5.72}, {"text": "repo um me just check", "start": 3352.4, "duration": 4.6}, {"text": "the yes so I see the question about", "start": 3360.119, "duration": 4.2}, {"text": "parallel efficiency has been been", "start": 3362.359, "duration": 5.0}, {"text": "addressed already yeah so basically it's", "start": 3364.319, "duration": 5.76}, {"text": "the it's the speed up divided by the", "start": 3367.359, "duration": 5.68}, {"text": "number of processes or threads and then", "start": 3370.079, "duration": 7.96}, {"text": "finally Carl asks can you comment on", "start": 3373.039, "duration": 7.52}, {"text": "which or some of the more frequent", "start": 3378.039, "duration": 6.161}, {"text": "applications used you use hybrid MPI and", "start": 3380.559, "duration": 6.361}, {"text": "open MP", "start": 3384.2, "duration": 2.72}, {"text": "um oh can you come", "start": 3391.52, "duration": 6.599}, {"text": "in you know I um Carl I'll have to get", "start": 3395.079, "duration": 5.0}, {"text": "back to you on on that on that question", "start": 3398.119, "duration": 4.601}, {"text": "I know though that most of the parallel", "start": 3400.079, "duration": 4.52}, {"text": "um but most of the widely used parallel", "start": 3402.72, "duration": 3.639}, {"text": "applications that run across multiple", "start": 3404.599, "duration": 4.641}, {"text": "nodes do use MPI Plus open", "start": 3406.359, "duration": 6.76}, {"text": "MP and um Marty addressed the question", "start": 3409.24, "duration": 6.4}, {"text": "about when you said tenses are better", "start": 3413.119, "duration": 5.641}, {"text": "yeah so tensors", "start": 3415.64, "duration": 7.32}, {"text": "um yeah so so I I I would use tensor", "start": 3418.76, "duration": 6.599}, {"text": "specifically if you have deep learning", "start": 3422.96, "duration": 5.28}, {"text": "um de deep learning", "start": 3425.359, "duration": 2.881}, {"text": "workloads and for that for those of you", "start": 3429.079, "duration": 3.401}, {"text": "going to stick around I'm going to give", "start": 3431.319, "duration": 5.121}, {"text": "a quick demo of the", "start": 3432.48, "duration": 6.4}, {"text": "um of the code that we have for for", "start": 3436.44, "duration": 5.0}, {"text": "generating the generating scaling plots", "start": 3438.88, "duration": 6.28}, {"text": "um so as as Cindy mentioned yes Elliott", "start": 3441.44, "duration": 5.52}, {"text": "um is it possible to get a copy of the", "start": 3445.16, "duration": 4.8}, {"text": "slides yes we had posted them earlier um", "start": 3446.96, "duration": 5.359}, {"text": "Cindy if you can go ahead", "start": 3449.96, "duration": 5.56}, {"text": "and um paste the link in again that", "start": 3452.319, "duration": 5.52}, {"text": "would be", "start": 3455.52, "duration": 5.279}, {"text": "great you got I'll take care of that all", "start": 3457.839, "duration": 5.881}, {"text": "right um question here from Reena oh", "start": 3460.799, "duration": 5.401}, {"text": "again as as s Cindy mentioned earlier we", "start": 3463.72, "duration": 4.52}, {"text": "are done with the um official", "start": 3466.2, "duration": 4.0}, {"text": "presentation but I will be sticking", "start": 3468.24, "duration": 3.48}, {"text": "around for the next half hour to do a", "start": 3470.2, "duration": 3.48}, {"text": "little demo and answer", "start": 3471.72, "duration": 4.399}, {"text": "questions um um from", "start": 3473.68, "duration": 5.28}, {"text": "Reena regarding the scaling curve why do", "start": 3476.119, "duration": 5.801}, {"text": "we care about parallel efficiency in", "start": 3478.96, "duration": 4.92}, {"text": "theory increase efficiency is nice but", "start": 3481.92, "duration": 3.28}, {"text": "that's a trade-off what are the", "start": 3483.88, "duration": 2.679}, {"text": "practical reasons we care about", "start": 3485.2, "duration": 4.72}, {"text": "efficiency yes that's a great question", "start": 3486.559, "duration": 4.961}, {"text": "um", "start": 3489.92, "duration": 4.48}, {"text": "particularly particularly with um with", "start": 3491.52, "duration": 5.279}, {"text": "with our supercomputers that are", "start": 3494.4, "duration": 4.56}, {"text": "allocated um you know allocated through", "start": 3496.799, "duration": 4.28}, {"text": "exceed sorry through access and", "start": 3498.96, "duration": 5.04}, {"text": "available to the entire re and available", "start": 3501.079, "duration": 6.04}, {"text": "to the entire research Community um we", "start": 3504.0, "duration": 4.2}, {"text": "want to make sure that they're being", "start": 3507.119, "duration": 3.801}, {"text": "used efficiently so for example if you", "start": 3508.2, "duration": 6.04}, {"text": "are if you were running an application", "start": 3510.92, "duration": 5.159}, {"text": "that's only getting", "start": 3514.24, "duration": 4.079}, {"text": "50%", "start": 3516.079, "duration": 4.96}, {"text": "um that that that's only getting 50%", "start": 3518.319, "duration": 4.841}, {"text": "parallel efficiency you're essentially", "start": 3521.039, "duration": 4.241}, {"text": "wasting half of those resources that", "start": 3523.16, "duration": 3.439}, {"text": "could be used for for another", "start": 3525.28, "duration": 4.96}, {"text": "calculation and I see that David um land", "start": 3526.599, "duration": 5.561}, {"text": "also weighed in you might care if you", "start": 3530.24, "duration": 4.0}, {"text": "had to pay money for the resources used", "start": 3532.16, "duration": 4.28}, {"text": "exactly if you were doing if you're", "start": 3534.24, "duration": 5.839}, {"text": "doing cloud computing and those um cloud", "start": 3536.44, "duration": 5.56}, {"text": "computing resources were coming out of", "start": 3540.079, "duration": 5.321}, {"text": "your research budget you would be um", "start": 3542.0, "duration": 4.76}, {"text": "You' be spending more money than you", "start": 3545.4, "duration": 4.639}, {"text": "need to so one way or other other either", "start": 3546.76, "duration": 5.839}, {"text": "you're wasting a shared resource like", "start": 3550.039, "duration": 5.28}, {"text": "the um like expans and bridges and stamp", "start": 3552.599, "duration": 8.081}, {"text": "p 3 or you're wasting money um on cloud", "start": 3555.319, "duration": 5.361}, {"text": "resources", "start": 3561.119, "duration": 2.281}, {"text": "[Music]", "start": 3562.21, "duration": 4.43}, {"text": "um all right I don't see any other", "start": 3563.4, "duration": 5.639}, {"text": "questions that that we haven't", "start": 3566.64, "duration": 5.159}, {"text": "addressed um although I do want to um", "start": 3569.039, "duration": 6.121}, {"text": "follow up on a comment here from ol", "start": 3571.799, "duration": 6.52}, {"text": "Schiller also requesting fewer resources", "start": 3575.16, "duration": 6.32}, {"text": "May reduce the weight time in the queue", "start": 3578.319, "duration": 5.8}, {"text": "yeah that that that that is correct it's", "start": 3581.48, "duration": 5.44}, {"text": "going to depend on the scheduling", "start": 3584.119, "duration": 6.0}, {"text": "policies um for for for that particular", "start": 3586.92, "duration": 4.72}, {"text": "machine", "start": 3590.119, "duration": 4.521}, {"text": "um if you're using if you're running a", "start": 3591.64, "duration": 4.719}, {"text": "at very large scale and say using", "start": 3594.64, "duration": 6.679}, {"text": "Stampede 3 and or the large systems at", "start": 3596.359, "duration": 7.321}, {"text": "the National Labs they may have", "start": 3601.319, "duration": 4.081}, {"text": "scheduling policies that make it a", "start": 3603.68, "duration": 3.56}, {"text": "little more favorable for the very large", "start": 3605.4, "duration": 4.08}, {"text": "jobs but generally though requesting", "start": 3607.24, "duration": 5.079}, {"text": "fewer resources is going okay can reduce", "start": 3609.48, "duration": 5.76}, {"text": "your your wait time in the", "start": 3612.319, "duration": 2.921}, {"text": "quey okay and then question here from", "start": 3617.44, "duration": 6.84}, {"text": "from Ava Spangler what sort of error", "start": 3620.76, "duration": 6.359}, {"text": "might see if you have too many workers", "start": 3624.28, "duration": 5.799}, {"text": "and not enough work um yeah and I think", "start": 3627.119, "duration": 5.601}, {"text": "D David D David David's answer there is", "start": 3630.079, "duration": 6.96}, {"text": "good um You probably won't see an error", "start": 3632.72, "duration": 6.399}, {"text": "you're just going to see a you're just", "start": 3637.039, "duration": 5.201}, {"text": "going to see a performance", "start": 3639.119, "duration": 3.121}, {"text": "hit so in fact I've", "start": 3642.96, "duration": 5.639}, {"text": "um I don't think I've ever come across", "start": 3645.68, "duration": 4.84}, {"text": "an example where I've run where I've run", "start": 3648.599, "duration": 4.72}, {"text": "on too many cores or too many nodes and", "start": 3650.52, "duration": 5.599}, {"text": "got incorrect Dan", "start": 3653.319, "duration": 3.921}, {"text": "now let", "start": 3656.119, "duration": 4.24}, {"text": "me go over to the go over to the GitHub", "start": 3657.24, "duration": 6.839}, {"text": "repo for those of you who are still", "start": 3660.359, "duration": 3.72}, {"text": "here okay so so earlier um Cindy had", "start": 3670.88, "duration": 6.6}, {"text": "posted the um had posted a link to the", "start": 3674.64, "duration": 5.479}, {"text": "to the GitHub repo that we use today so", "start": 3677.48, "duration": 4.76}, {"text": "if you go here to my parallel Computing", "start": 3680.119, "duration": 5.281}, {"text": "Concepts um repo yeah you'll see that I", "start": 3682.24, "duration": 4.64}, {"text": "have", "start": 3685.4, "duration": 6.6}, {"text": "um examples for for om doll's law and um", "start": 3686.88, "duration": 9.36}, {"text": "and strong scaling so the these were the", "start": 3692.0, "duration": 7.44}, {"text": "um the these were the um P python", "start": 3696.24, "duration": 4.879}, {"text": "notebooks that I used to generate the", "start": 3699.44, "duration": 4.359}, {"text": "figures for for this presentation and", "start": 3701.119, "duration": 4.761}, {"text": "down at the bottom you'll also see that", "start": 3703.799, "duration": 5.0}, {"text": "there is a um that you can launch these", "start": 3705.88, "duration": 6.28}, {"text": "using binder so I had done that earlier", "start": 3708.799, "duration": 6.161}, {"text": "and unless my code", "start": 3712.16, "duration": 5.959}, {"text": "on let me restart", "start": 3714.96, "duration": 3.159}, {"text": "here we'll go to the OM doll's law law", "start": 3721.76, "duration": 5.68}, {"text": "example", "start": 3724.839, "duration": 2.601}, {"text": "first no I think my um session must have", "start": 3729.92, "duration": 6.8}, {"text": "expired I started that too early let me", "start": 3733.559, "duration": 6.121}, {"text": "try launching that", "start": 3736.72, "duration": 2.96}, {"text": "again", "start": 3743.359, "duration": 3.0}, {"text": "let me go back", "start": 3768.96, "duration": 3.159}, {"text": "to Do's la", "start": 3772.88, "duration": 6.679}, {"text": "okay okay so if you want to if you want", "start": 3776.319, "duration": 5.401}, {"text": "to play around with om doll's law you", "start": 3779.559, "duration": 5.72}, {"text": "can you can open this notebook um I use", "start": 3781.72, "duration": 6.76}, {"text": "matplot lib to to generate the figures", "start": 3785.279, "duration": 6.441}, {"text": "um I have some data sets um showing", "start": 3788.48, "duration": 7.92}, {"text": "speed up with 50% 75 90 95 and 99% um", "start": 3791.72, "duration": 7.16}, {"text": "parallel", "start": 3796.4, "duration": 2.48}, {"text": "content sorry I need", "start": 3799.76, "duration": 6.519}, {"text": "to", "start": 3803.279, "duration": 3.0}, {"text": "and if you want you can um experiment", "start": 3811.319, "duration": 5.881}, {"text": "with this let me add", "start": 3813.839, "duration": 3.361}, {"text": "a would add", "start": 3819.799, "duration": 5.52}, {"text": "another line here for", "start": 3822.52, "duration": 6.36}, {"text": "99.9% parallel", "start": 3825.319, "duration": 3.561}, {"text": "content", "start": 3833.279, "duration": 3.0}, {"text": "and I think what you're going to find", "start": 3853.039, "duration": 4.121}, {"text": "even more useful though is the no", "start": 3854.16, "duration": 7.76}, {"text": "notebook for the strong scaling oh go", "start": 3857.16, "duration": 4.76}, {"text": "um looking at questions", "start": 3862.559, "duration": 6.361}, {"text": "here okay look looks like that's been", "start": 3866.44, "duration": 4.119}, {"text": "been addressed", "start": 3868.92, "duration": 4.639}, {"text": "already", "start": 3870.559, "duration": 3.0}, {"text": "um but in the strong", "start": 3874.599, "duration": 5.641}, {"text": "scaling example we can use this to", "start": 3877.119, "duration": 5.92}, {"text": "generate um scaling plots that you can", "start": 3880.24, "duration": 6.2}, {"text": "use in your allocation", "start": 3883.039, "duration": 8.841}, {"text": "requests so I have all the code here for", "start": 3886.44, "duration": 5.44}, {"text": "um for for doing the um but plotting the", "start": 3892.079, "duration": 9.52}, {"text": "scaling data on linear axes and on log", "start": 3895.64, "duration": 5.959}, {"text": "axes and you can actually use this to", "start": 3912.559, "duration": 4.681}, {"text": "generate", "start": 3915.16, "duration": 6.48}, {"text": "um soorry generate um scaling examples", "start": 3917.24, "duration": 7.76}, {"text": "that that are pretty much ready to go", "start": 3921.64, "duration": 6.439}, {"text": "now I realize that the that I didn't um", "start": 3925.0, "duration": 5.48}, {"text": "update the binder image I will do that", "start": 3928.079, "duration": 6.561}, {"text": "later but if we actually go to the um go", "start": 3930.48, "duration": 5.28}, {"text": "to the parallel", "start": 3934.64, "duration": 4.08}, {"text": "Computing Concepts", "start": 3935.76, "duration": 5.88}, {"text": "repo that I have um have an example down", "start": 3938.72, "duration": 4.68}, {"text": "at the bottom where you can enter your", "start": 3941.64, "duration": 4.959}, {"text": "own information your cores or gpus the", "start": 3943.4, "duration": 6.32}, {"text": "runtime the units whether using CPUs or", "start": 3946.599, "duration": 7.0}, {"text": "gpus time units and so on and generate", "start": 3949.72, "duration": 5.599}, {"text": "um scale plots that are pretty much", "start": 3953.599, "duration": 4.121}, {"text": "ready to", "start": 3955.319, "duration": 2.401}, {"text": "go okay so any any", "start": 3961.039, "duration": 4.201}, {"text": "questions Bob we did get a question in", "start": 3965.4, "duration": 4.36}, {"text": "the chat um is there a way to calculate", "start": 3967.52, "duration": 5.2}, {"text": "percentage paral paralyzable content or", "start": 3969.76, "duration": 5.799}, {"text": "is the visualization for ald law more of", "start": 3972.72, "duration": 5.359}, {"text": "a theoretical concept yeah I'm going to", "start": 3975.559, "duration": 4.401}, {"text": "say that it's more of a theoretical", "start": 3978.079, "duration": 4.76}, {"text": "theoretical concept Serena I talked", "start": 3979.96, "duration": 5.079}, {"text": "about that a little bit a little bit", "start": 3982.839, "duration": 5.24}, {"text": "earlier um unless you have a very very", "start": 3985.039, "duration": 4.681}, {"text": "simple", "start": 3988.079, "duration": 3.2}, {"text": "application", "start": 3989.72, "duration": 4.68}, {"text": "um and you and you manually put in", "start": 3991.279, "duration": 6.8}, {"text": "timers around the serial and the parel", "start": 3994.4, "duration": 7.24}, {"text": "well yeah and put timers around the Cal", "start": 3998.079, "duration": 6.72}, {"text": "and potentially paralyzable content to", "start": 4001.64, "duration": 5.32}, {"text": "determine how much time is being spent", "start": 4004.799, "duration": 5.0}, {"text": "in each chunk you probably won't be able", "start": 4006.96, "duration": 5.319}, {"text": "to calculate PRS um it's really more of", "start": 4009.799, "duration": 4.921}, {"text": "a theoretical concept and the main", "start": 4012.279, "duration": 4.28}, {"text": "purpose of om doll's law is to really", "start": 4014.72, "duration": 4.879}, {"text": "drive home how little serial content you", "start": 4016.559, "duration": 7.401}, {"text": "need to um to to", "start": 4019.599, "duration": 7.48}, {"text": "Really limit your ability to to run on", "start": 4023.96, "duration": 6.119}, {"text": "to to run a large number of cores and", "start": 4027.079, "duration": 5.841}, {"text": "nodes so again the I'm going to say the", "start": 4030.079, "duration": 6.321}, {"text": "most important thing is to just be aware", "start": 4032.92, "duration": 6.96}, {"text": "that we have um be be aware that omd's", "start": 4036.4, "duration": 6.04}, {"text": "law exists that we do have those limits", "start": 4039.88, "duration": 4.8}, {"text": "but practically you'll want to generate", "start": 4042.44, "duration": 4.08}, {"text": "scaling curves like I described in the", "start": 4044.68, "duration": 5.56}, {"text": "presentation and the GitHub", "start": 4046.52, "duration": 6.72}, {"text": "rebone right so we are now", "start": 4050.24, "duration": 5.599}, {"text": "um I guess we're kind of into the office", "start": 4053.24, "duration": 6.48}, {"text": "hours um but part of the presentation so", "start": 4055.839, "duration": 5.96}, {"text": "Cy I'm gonna say you could probably stop", "start": 4059.72, "duration": 5.079}, {"text": "recording", "start": 4061.799, "duration": 3.0}]