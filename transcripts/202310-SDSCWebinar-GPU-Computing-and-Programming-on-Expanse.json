[{"text": "hey hello everybody so let me try to", "start": 1.599, "duration": 5.081}, {"text": "share the", "start": 4.04, "duration": 2.64}, {"text": "screen okay so um welcome everybody uh", "start": 8.719, "duration": 4.721}, {"text": "thank you for joining for today's", "start": 12.04, "duration": 5.8}, {"text": "session um it's a onehour webinar and I", "start": 13.44, "duration": 6.88}, {"text": "will talk about GPU Computing and", "start": 17.84, "duration": 4.32}, {"text": "programming on expand we don't have much", "start": 20.32, "duration": 5.48}, {"text": "time so it's going to be a relatively", "start": 22.16, "duration": 5.76}, {"text": "quick overview I do have a lot of slides", "start": 25.8, "duration": 4.799}, {"text": "so I might skip over some rather quickly", "start": 27.92, "duration": 4.839}, {"text": "but if you have any questions in the", "start": 30.599, "duration": 6.561}, {"text": "meantime um please um add them to the to", "start": 32.759, "duration": 8.081}, {"text": "the chat and um Cindy will help me maybe", "start": 37.16, "duration": 6.04}, {"text": "moderate some of those questions and I", "start": 40.84, "duration": 3.64}, {"text": "hope there will be a little bit of time", "start": 43.2, "duration": 4.199}, {"text": "at the end um if you have specific", "start": 44.48, "duration": 6.399}, {"text": "questions um so basically what I plan to", "start": 47.399, "duration": 5.84}, {"text": "cover is give you a quick overview of", "start": 50.879, "duration": 6.441}, {"text": "GPU Hardware um Talk briefly about some", "start": 53.239, "duration": 7.401}, {"text": "examples of GPU enabled software", "start": 57.32, "duration": 5.879}, {"text": "um and then very briefly show you how", "start": 60.64, "duration": 5.799}, {"text": "gpus can be programmed bya libraries", "start": 63.199, "duration": 7.96}, {"text": "Cuda uh and a open ACC this very briefly", "start": 66.439, "duration": 7.36}, {"text": "and then we'll look at sdsc expans GPU", "start": 71.159, "duration": 5.761}, {"text": "noes how can we access those noes how do", "start": 73.799, "duration": 5.401}, {"text": "we run GPU jobs and you know developing", "start": 76.92, "duration": 4.0}, {"text": "softwares basically how do we use the", "start": 79.2, "duration": 4.44}, {"text": "compilers and uh", "start": 80.92, "duration": 6.12}, {"text": "profilers and so if you think about a", "start": 83.64, "duration": 6.479}, {"text": "GPU what what a GPU is is basically", "start": 87.04, "duration": 4.88}, {"text": "accelerators people talk about", "start": 90.119, "duration": 4.32}, {"text": "accelerated Computing that just means", "start": 91.92, "duration": 4.32}, {"text": "it's a specialized type of Hardware", "start": 94.439, "duration": 4.0}, {"text": "component uh to speed up a certain", "start": 96.24, "duration": 4.72}, {"text": "aspect of your computational workload", "start": 98.439, "duration": 4.121}, {"text": "and so in the past you know we had like", "start": 100.96, "duration": 3.32}, {"text": "floating Point code processors it's been", "start": 102.56, "duration": 3.879}, {"text": "merged into CPUs and you know there's", "start": 104.28, "duration": 4.56}, {"text": "many types of accelerators there's um", "start": 106.439, "duration": 5.801}, {"text": "fpgas there's specific MLA AI processors", "start": 108.84, "duration": 5.919}, {"text": "there's Quantum processing units now and", "start": 112.24, "duration": 3.96}, {"text": "so it depends on the workload that you", "start": 114.759, "duration": 5.601}, {"text": "have but gpus are very widely used um", "start": 116.2, "duration": 6.239}, {"text": "um you know all the development has been", "start": 120.36, "duration": 5.24}, {"text": "driven by $150 billion gaming industry", "start": 122.439, "duration": 4.841}, {"text": "in the past you had just these discrete", "start": 125.6, "duration": 4.12}, {"text": "GQ cards that you would stick in your", "start": 127.28, "duration": 4.92}, {"text": "you know desktop computer for rendering", "start": 129.72, "duration": 4.36}, {"text": "and now there are data center cards that", "start": 132.2, "duration": 4.88}, {"text": "are either on PCI Express Bus like these", "start": 134.08, "duration": 5.04}, {"text": "two here or you know they go directly", "start": 137.08, "duration": 4.92}, {"text": "into a a specific software like a", "start": 139.12, "duration": 6.32}, {"text": "CPU and what you need to know is um you", "start": 142.0, "duration": 5.08}, {"text": "can do general purpose Computing on", "start": 145.44, "duration": 4.079}, {"text": "these and you know they they they have a", "start": 147.08, "duration": 3.48}, {"text": "different", "start": 149.519, "duration": 2.681}, {"text": "um chip design so they need to be", "start": 150.56, "duration": 4.759}, {"text": "programmed in a different way um just", "start": 152.2, "duration": 5.24}, {"text": "very briefly why is there such an", "start": 155.319, "duration": 5.521}, {"text": "interesting gpus um you know if you look", "start": 157.44, "duration": 6.68}, {"text": "at this plot on the right um it shows", "start": 160.84, "duration": 5.679}, {"text": "microprocessor Trend data over the last", "start": 164.12, "duration": 5.68}, {"text": "decades and the important thing um to", "start": 166.519, "duration": 5.8}, {"text": "see here is that you know M's law", "start": 169.8, "duration": 4.799}, {"text": "basically an exponential growth in", "start": 172.319, "duration": 5.361}, {"text": "transistors is still true um however", "start": 174.599, "duration": 5.401}, {"text": "what happened since the mid 2000s of", "start": 177.68, "duration": 3.96}, {"text": "approximately you know the clock", "start": 180.0, "duration": 3.879}, {"text": "frequency couldn't be increased anymore", "start": 181.64, "duration": 4.48}, {"text": "and so instead you get more logical", "start": 183.879, "duration": 4.801}, {"text": "course so what you need to do is to um", "start": 186.12, "duration": 4.0}, {"text": "write parallel", "start": 188.68, "duration": 4.68}, {"text": "code and on top of that here's a", "start": 190.12, "duration": 7.92}, {"text": "comparison uh of CPU versus CPU um or", "start": 193.36, "duration": 7.239}, {"text": "GPU what we see here is the performance", "start": 198.04, "duration": 4.479}, {"text": "of floating Point operations on the left", "start": 200.599, "duration": 4.601}, {"text": "hand side it's um 32bit floats and the", "start": 202.519, "duration": 6.201}, {"text": "right hand side 64bit floats um so many", "start": 205.2, "duration": 6.399}, {"text": "science applications 64bit floats um you", "start": 208.72, "duration": 4.799}, {"text": "know other applications deep burning", "start": 211.599, "duration": 4.761}, {"text": "applications Ed of 32bit floats for", "start": 213.519, "duration": 4.72}, {"text": "inference or even you know other data", "start": 216.36, "duration": 4.079}, {"text": "types but the important piece to see", "start": 218.239, "duration": 4.241}, {"text": "here is that you know there's a", "start": 220.439, "duration": 4.041}, {"text": "significant higher floating Point", "start": 222.48, "duration": 4.319}, {"text": "performance of a single G GPU chip as", "start": 224.48, "duration": 5.36}, {"text": "compared to a a a CPU chip a multiple", "start": 226.799, "duration": 6.8}, {"text": "CPU chip and so this is older data but", "start": 229.84, "duration": 6.52}, {"text": "it still holds to date the second thing", "start": 233.599, "duration": 5.84}, {"text": "that's important is um memory bandwidth", "start": 236.36, "duration": 4.56}, {"text": "so what what you see here is the memory", "start": 239.439, "duration": 4.561}, {"text": "bandwidth in Gig byte per seconds um", "start": 240.92, "duration": 6.36}, {"text": "that just means how much data can the", "start": 244.0, "duration": 5.92}, {"text": "the the the processor or GPU transfer", "start": 247.28, "duration": 6.44}, {"text": "from Ram um to the compute unit in a", "start": 249.92, "duration": 6.599}, {"text": "given amount of time and so memory bandr", "start": 253.72, "duration": 4.239}, {"text": "is much higher for", "start": 256.519, "duration": 5.041}, {"text": "gpus than for CPUs um and that that is", "start": 257.959, "duration": 4.921}, {"text": "still the case you know there are some", "start": 261.56, "duration": 3.919}, {"text": "developments where um memory B been", "start": 262.88, "duration": 6.48}, {"text": "increased also for cpqs but that mostly", "start": 265.479, "duration": 7.121}, {"text": "um holds true this is very important for", "start": 269.36, "duration": 5.8}, {"text": "uh the re for for for one simple reason", "start": 272.6, "duration": 4.879}, {"text": "um floating Point performance is not", "start": 275.16, "duration": 5.759}, {"text": "everything um so many many algorithms", "start": 277.479, "duration": 4.921}, {"text": "and applications are actually memory", "start": 280.919, "duration": 3.161}, {"text": "bandwidth pH so basically what that", "start": 282.4, "duration": 4.079}, {"text": "means is the processor sits there the", "start": 284.08, "duration": 4.679}, {"text": "GPU sits there and waits for data to", "start": 286.479, "duration": 4.601}, {"text": "compute on and so if the memory band is", "start": 288.759, "duration": 5.361}, {"text": "high you know your Cod RS", "start": 291.08, "duration": 7.48}, {"text": "faster um and so this is about you know", "start": 294.12, "duration": 6.519}, {"text": "when when response was designed around", "start": 298.56, "duration": 4.079}, {"text": "that time you know you would be able to", "start": 300.639, "duration": 5.921}, {"text": "buy for in instance Intel zon uh", "start": 302.639, "duration": 6.161}, {"text": "processors data center processors and", "start": 306.56, "duration": 5.359}, {"text": "gpus at the time in Tesla V100 so v100s", "start": 308.8, "duration": 6.6}, {"text": "are also the CPU type um that we have in", "start": 311.919, "duration": 5.921}, {"text": "expon and you can see a", "start": 315.4, "duration": 6.16}, {"text": "comparison um of the floating Point", "start": 317.84, "duration": 6.199}, {"text": "performance um so double Precision is is", "start": 321.56, "duration": 5.6}, {"text": "64 pitch single Precision 32 pitch HP is", "start": 324.039, "duration": 5.561}, {"text": "like half Precision um so that would be", "start": 327.16, "duration": 4.8}, {"text": "16 bit um that's something you can't do", "start": 329.6, "duration": 4.599}, {"text": "on on at least not with a regular", "start": 331.96, "duration": 4.2}, {"text": "instruction set on on", "start": 334.199, "duration": 5.041}, {"text": "CPUs but um you have that on on the gpus", "start": 336.16, "duration": 4.84}, {"text": "and you can see that you get a very high", "start": 339.24, "duration": 3.84}, {"text": "floating Point performance in addition", "start": 341.0, "duration": 5.72}, {"text": "to that also the very high um memory BS", "start": 343.08, "duration": 5.52}, {"text": "the one thing that you need to know is a", "start": 346.72, "duration": 4.919}, {"text": "gpus sitting of the PCI Express bus so", "start": 348.6, "duration": 4.439}, {"text": "if you have data or a program that's", "start": 351.639, "duration": 3.081}, {"text": "running on the CPU and you need to load", "start": 353.039, "duration": 4.321}, {"text": "it onto the GPU you need to shuffle data", "start": 354.72, "duration": 5.12}, {"text": "through this very slow PCI Express plus", "start": 357.36, "duration": 4.6}, {"text": "these things might go away in the future", "start": 359.84, "duration": 4.0}, {"text": "and in some architectures Modern", "start": 361.96, "duration": 3.799}, {"text": "architectures but in general with these", "start": 363.84, "duration": 3.44}, {"text": "gpus that's something you need to keep", "start": 365.759, "duration": 2.321}, {"text": "in", "start": 367.28, "duration": 4.8}, {"text": "mind and yeah so you purchase cost and", "start": 368.08, "duration": 5.64}, {"text": "portability of code is of course", "start": 372.08, "duration": 3.76}, {"text": "important um in the past it wasn't so", "start": 373.72, "duration": 4.8}, {"text": "much the case where you only have Cuda", "start": 375.84, "duration": 4.919}, {"text": "which works only for NVIDIA gpus but now", "start": 378.52, "duration": 4.72}, {"text": "you know there's open ACC you can use", "start": 380.759, "duration": 5.201}, {"text": "open MP to program gpus opcl of course", "start": 383.24, "duration": 5.519}, {"text": "has been around there is hip um", "start": 385.96, "duration": 5.12}, {"text": "programming model and there's a new uh", "start": 388.759, "duration": 5.56}, {"text": "one CLE um that works across different", "start": 391.08, "duration": 7.04}, {"text": "types of both CPUs and gpus and um", "start": 394.319, "duration": 5.561}, {"text": "different types of gpus as well from", "start": 398.12, "duration": 4.24}, {"text": "different vendors so yes it is possible", "start": 399.88, "duration": 5.68}, {"text": "to write actually portable", "start": 402.36, "duration": 3.2}, {"text": "code um since we talked about", "start": 406.68, "duration": 5.56}, {"text": "performance just to give you a little", "start": 409.4, "duration": 4.56}, {"text": "bit of a perspective is you know if we", "start": 412.24, "duration": 5.16}, {"text": "go back to 2001 that's now over 20 years", "start": 413.96, "duration": 6.56}, {"text": "ago the fastest computer in the world at", "start": 417.4, "duration": 6.28}, {"text": "least publicly known at Lawrence", "start": 420.52, "duration": 5.6}, {"text": "Livermore lab had a total performance of", "start": 423.68, "duration": 4.799}, {"text": "12 teraflops", "start": 426.12, "duration": 5.84}, {"text": "approximately cost 110 million US dollar", "start": 428.479, "duration": 4.84}, {"text": "you look at", "start": 431.96, "duration": 5.12}, {"text": "expans it's got uh 3.4 ped flops just on", "start": 433.319, "duration": 7.0}, {"text": "the CPU noes and an additional 1.6", "start": 437.08, "duration": 6.519}, {"text": "pflops or 3.3 pflops in you know double", "start": 440.319, "duration": 6.201}, {"text": "or single Precision aggregate across the", "start": 443.599, "duration": 5.521}, {"text": "GPU nodes and that that machine and you", "start": 446.52, "duration": 4.32}, {"text": "know acquisition cost just cost 10", "start": 449.12, "duration": 5.12}, {"text": "million US dollars when it was prri so", "start": 450.84, "duration": 5.6}, {"text": "you know we get a much more um", "start": 454.24, "duration": 4.84}, {"text": "performance for uh you know much less", "start": 456.44, "duration": 4.8}, {"text": "money in some sense and you know if you", "start": 459.08, "duration": 4.679}, {"text": "go back 2020 and you would buy some top", "start": 461.24, "duration": 5.799}, {"text": "of the line Nvidia gaming gpus you know", "start": 463.759, "duration": 5.241}, {"text": "you can build your own machine that has", "start": 467.039, "duration": 4.72}, {"text": "you know 119 Tera flops is like 10 times", "start": 469.0, "duration": 6.4}, {"text": "as much as the fastest computer 20 years", "start": 471.759, "duration": 6.241}, {"text": "ago um at least in single Precision it", "start": 475.4, "duration": 4.68}, {"text": "cost you just a few thousand", "start": 478.0, "duration": 4.24}, {"text": "so you have a lot of um performance at", "start": 480.08, "duration": 4.72}, {"text": "your fingertips and you know there's a", "start": 482.24, "duration": 4.04}, {"text": "lot of potential to do great science", "start": 484.8, "duration": 2.76}, {"text": "with", "start": 486.28, "duration": 5.8}, {"text": "it and you know terms of gpus have been", "start": 487.56, "duration": 7.479}, {"text": "used virtually any field now this GPU", "start": 492.08, "duration": 4.679}, {"text": "accelerated software from", "start": 495.039, "duration": 3.401}, {"text": "bioinformatics through weather and", "start": 496.759, "duration": 4.601}, {"text": "climate modeling there's not much to say", "start": 498.44, "duration": 4.4}, {"text": "about you know there there's many Cod", "start": 501.36, "duration": 5.92}, {"text": "that still don't run on gpus but um um", "start": 502.84, "duration": 5.799}, {"text": "because there are certain complexities", "start": 507.28, "duration": 3.44}, {"text": "it's not is challenge challenging to R", "start": 508.639, "duration": 5.4}, {"text": "up code um for gpus if you have some say", "start": 510.72, "duration": 7.48}, {"text": "C++ or fortun code um but many codes", "start": 514.039, "duration": 5.041}, {"text": "have been", "start": 518.2, "duration": 3.959}, {"text": "parted and of course over the last years", "start": 519.08, "duration": 4.6}, {"text": "you know deep learning is one of the", "start": 522.159, "duration": 3.841}, {"text": "main consumers of of gpus it was also", "start": 523.68, "duration": 4.719}, {"text": "one of the drivers that has pushed some", "start": 526.0, "duration": 3.959}, {"text": "of the development and the demand for", "start": 528.399, "duration": 5.041}, {"text": "gpus over the last years and I don't", "start": 529.959, "duration": 6.0}, {"text": "want to talk much about that um not this", "start": 533.44, "duration": 4.88}, {"text": "not a presentation about deep burning", "start": 535.959, "duration": 3.921}, {"text": "but basically you know train a machine", "start": 538.32, "duration": 2.519}, {"text": "learning", "start": 539.88, "duration": 3.079}, {"text": "algorithm in this case with a deep", "start": 540.839, "duration": 5.24}, {"text": "learning um deep neural network that has", "start": 542.959, "duration": 4.88}, {"text": "you know an input layer then multiple", "start": 546.079, "duration": 4.041}, {"text": "layers here then an output layer and", "start": 547.839, "duration": 3.641}, {"text": "these you know these These are fully", "start": 550.12, "duration": 2.92}, {"text": "connected or they don't have to be fully", "start": 551.48, "duration": 3.68}, {"text": "connected but basically what happens", "start": 553.04, "duration": 4.84}, {"text": "these are tensor operations um in other", "start": 555.16, "duration": 5.239}, {"text": "words Matrix multiplications right so um", "start": 557.88, "duration": 4.76}, {"text": "for training the model and then for for", "start": 560.399, "duration": 3.921}, {"text": "evaluating the model once you've trained", "start": 562.64, "duration": 4.639}, {"text": "the data and these and and the weights", "start": 564.32, "duration": 6.24}, {"text": "associated with your uh um neuron", "start": 567.279, "duration": 5.161}, {"text": "Network you can use those to do", "start": 570.56, "duration": 6.08}, {"text": "predictions on um similar data sets and", "start": 572.44, "duration": 6.639}, {"text": "gpus are just very efficient at these", "start": 576.64, "duration": 5.48}, {"text": "operations historically 4x4 Matrix", "start": 579.079, "duration": 4.88}, {"text": "algebra is used in 3D Graphics so", "start": 582.12, "duration": 3.68}, {"text": "there's always been Hardware dedicated", "start": 583.959, "duration": 3.521}, {"text": "to do these types of operations very", "start": 585.8, "duration": 3.64}, {"text": "quickly and then over the last years", "start": 587.48, "duration": 2.96}, {"text": "there has", "start": 589.44, "duration": 3.519}, {"text": "been additions that are specifically", "start": 590.44, "duration": 4.36}, {"text": "designed for machine learning including", "start": 592.959, "duration": 4.0}, {"text": "half Precision arithmetics it can be", "start": 594.8, "duration": 4.08}, {"text": "used for inference and other data types", "start": 596.959, "duration": 5.56}, {"text": "on more modern gpus um so the Nvidia", "start": 598.88, "duration": 5.0}, {"text": "wter architecture for instance has", "start": 602.519, "duration": 4.0}, {"text": "so-called tensor course that does", "start": 603.88, "duration": 4.639}, {"text": "dedicated hardware for mixed Precision", "start": 606.519, "duration": 4.041}, {"text": "Matrix multiplications that are used in", "start": 608.519, "duration": 5.081}, {"text": "these um evaluation of neural networks", "start": 610.56, "duration": 4.8}, {"text": "and that all is supported directly", "start": 613.6, "duration": 4.08}, {"text": "easily um in Frameworks that are", "start": 615.36, "duration": 4.12}, {"text": "typically used for machine learning like", "start": 617.68, "duration": 2.92}, {"text": "py or", "start": 619.48, "duration": 4.2}, {"text": "t um so it's pretty straightforward to", "start": 620.6, "duration": 6.52}, {"text": "use gpus um if you have a different type", "start": 623.68, "duration": 6.2}, {"text": "of applications like um", "start": 627.12, "duration": 4.159}, {"text": "this is an example of a code that I've", "start": 629.88, "duration": 3.48}, {"text": "been working on it's called quick it's a", "start": 631.279, "duration": 4.8}, {"text": "quantum chemistry code um to complete", "start": 633.36, "duration": 4.12}, {"text": "electronic structure like electron", "start": 636.079, "duration": 3.76}, {"text": "densities and syic properties of", "start": 637.48, "duration": 4.599}, {"text": "molecules um with a range of different", "start": 639.839, "duration": 4.12}, {"text": "applications from computational TR", "start": 642.079, "duration": 3.721}, {"text": "design materials", "start": 643.959, "duration": 5.68}, {"text": "Etc and this is just um you know a", "start": 645.8, "duration": 6.479}, {"text": "comparison you know if you run a serial", "start": 649.639, "duration": 6.481}, {"text": "code we have a serial code um and an NPR", "start": 652.279, "duration": 5.921}, {"text": "parallel code but you know single GPU", "start": 656.12, "duration": 3.88}, {"text": "basically gives you significant speed up", "start": 658.2, "duration": 3.84}, {"text": "it's um you know equivalent to running", "start": 660.0, "duration": 6.76}, {"text": "on the order of 100 um CPU course and", "start": 662.04, "duration": 6.12}, {"text": "you can see this here on the right as", "start": 666.76, "duration": 3.879}, {"text": "well it's an older plot been performance", "start": 668.16, "duration": 4.04}, {"text": "improvements in the meantime but you see", "start": 670.639, "duration": 4.44}, {"text": "you know if you run the parallel code on", "start": 672.2, "duration": 5.879}, {"text": "many cores um you know you get the time", "start": 675.079, "duration": 4.88}, {"text": "down but here the dash line is basically", "start": 678.079, "duration": 4.121}, {"text": "much faster running a single GPU and it", "start": 679.959, "duration": 5.041}, {"text": "was comparable at that time you know", "start": 682.2, "duration": 7.28}, {"text": "State of-the-art CPUs versus um uh GP", "start": 685.0, "duration": 6.44}, {"text": "another example is molecular Dynamics", "start": 689.48, "duration": 4.84}, {"text": "you know I've been working on um where", "start": 691.44, "duration": 5.76}, {"text": "you know we have interactions between", "start": 694.32, "duration": 5.4}, {"text": "atom simulated with a relatively simple", "start": 697.2, "duration": 4.319}, {"text": "mathematical model but it's hundreds of", "start": 699.72, "duration": 3.44}, {"text": "thousands of atoms so there's there's", "start": 701.519, "duration": 2.921}, {"text": "many interactions that need to be", "start": 703.16, "duration": 3.52}, {"text": "computed and we need to do this for", "start": 704.44, "duration": 5.48}, {"text": "millions of time steps um so each time", "start": 706.68, "duration": 5.8}, {"text": "step has to be computed very quickly on", "start": 709.92, "duration": 4.359}, {"text": "the order of milliseconds otherwise you", "start": 712.48, "duration": 3.52}, {"text": "know we wouldn't be able to reach these", "start": 714.279, "duration": 4.081}, {"text": "um High time scales that are relevant of", "start": 716.0, "duration": 6.24}, {"text": "biology for instance and that that's", "start": 718.36, "duration": 6.76}, {"text": "just a performance um plot for the am", "start": 722.24, "duration": 5.52}, {"text": "molecular Dynamics code um you know that", "start": 725.12, "duration": 4.32}, {"text": "that can changes now it's it's it's", "start": 727.76, "duration": 3.44}, {"text": "there's a newer version available of", "start": 729.44, "duration": 3.959}, {"text": "course but you get the idea that you", "start": 731.2, "duration": 4.84}, {"text": "know you run on a single um node um in", "start": 733.399, "duration": 4.88}, {"text": "this case say for instance 36", "start": 736.04, "duration": 4.72}, {"text": "cores um and that's the performance that", "start": 738.279, "duration": 6.0}, {"text": "you get in terms of how much science you", "start": 740.76, "duration": 5.439}, {"text": "can do in 24", "start": 744.279, "duration": 5.56}, {"text": "hours um and on a single gtu you know", "start": 746.199, "duration": 5.76}, {"text": "completely outstrip that performance so", "start": 749.839, "duration": 3.24}, {"text": "the code is well written and", "start": 751.959, "duration": 3.32}, {"text": "parallelized you can get magnificent", "start": 753.079, "duration": 3.601}, {"text": "results in", "start": 755.279, "duration": 4.92}, {"text": "GPS um now the catches of course right", "start": 756.68, "duration": 6.04}, {"text": "um you know you have a", "start": 760.199, "duration": 5.401}, {"text": "code um you know you want to use gpus", "start": 762.72, "duration": 4.76}, {"text": "because they use less electricity they", "start": 765.6, "duration": 5.479}, {"text": "are very power efficient um and so the", "start": 767.48, "duration": 6.039}, {"text": "manager tells you or your boss or your", "start": 771.079, "duration": 5.921}, {"text": "Pi or you work to do this right um um", "start": 773.519, "duration": 5.041}, {"text": "program the software and at some point", "start": 777.0, "duration": 3.32}, {"text": "you know comes back and says why is your", "start": 778.56, "duration": 5.44}, {"text": "part so it can be easy if you just use a", "start": 780.32, "duration": 5.84}, {"text": "framework but in general it can be", "start": 784.0, "duration": 5.32}, {"text": "difficult because the architecture is", "start": 786.16, "duration": 6.239}, {"text": "different so this is a comparison of the", "start": 789.32, "duration": 5.079}, {"text": "real estate that youd have on a on on a", "start": 792.399, "duration": 5.201}, {"text": "chip on the Silicon right um a CPU", "start": 794.399, "duration": 5.56}, {"text": "versus a GPU and on a CPU you know we", "start": 797.6, "duration": 4.239}, {"text": "have just a handful of processing fors", "start": 799.959, "duration": 5.041}, {"text": "on the order of few dozens nowadays", "start": 801.839, "duration": 6.44}, {"text": "right um and the CPU does a lot of logic", "start": 805.0, "duration": 6.12}, {"text": "for you multi-level brand caching pre", "start": 808.279, "duration": 4.36}, {"text": "patching Branch prediction so it does a", "start": 811.12, "duration": 4.68}, {"text": "lot of things for you um the GPU has", "start": 812.639, "duration": 5.0}, {"text": "thousands of simplistic compute cores if", "start": 815.8, "duration": 3.32}, {"text": "you want so and they they are packaged", "start": 817.639, "duration": 3.041}, {"text": "into", "start": 819.12, "duration": 3.88}, {"text": "multiprocessors and so typically now you", "start": 820.68, "duration": 6.599}, {"text": "have 64 cores in one multiprocessor and", "start": 823.0, "duration": 6.199}, {"text": "they they operate in lock step so that", "start": 827.279, "duration": 4.441}, {"text": "means you need to write code that can do", "start": 829.199, "duration": 4.481}, {"text": "voriz loads and stores to memory you", "start": 831.72, "duration": 4.119}, {"text": "need to write code that operates data", "start": 833.68, "duration": 3.719}, {"text": "parallel so you need to be very careful", "start": 835.839, "duration": 4.841}, {"text": "on how you decom post the the the the", "start": 837.399, "duration": 5.0}, {"text": "science problem the implementation and", "start": 840.68, "duration": 5.519}, {"text": "the algorithms um in in in Li", "start": 842.399, "duration": 9.12}, {"text": "software and I have here a slide of um", "start": 846.199, "duration": 8.2}, {"text": "that is very old it's from 2009 but i'", "start": 851.519, "duration": 4.88}, {"text": "like to show it because in", "start": 854.399, "duration": 6.201}, {"text": "principle um not much has changed um so", "start": 856.399, "duration": 6.521}, {"text": "this is the Nvidia GPU architecture of a", "start": 860.6, "duration": 7.12}, {"text": "Tesla T10 in 2009 and um you see you", "start": 862.92, "duration": 6.68}, {"text": "have these multiprocessor there's one", "start": 867.72, "duration": 4.4}, {"text": "here another one here another one here", "start": 869.6, "duration": 4.32}, {"text": "and each of those", "start": 872.12, "duration": 4.48}, {"text": "multiprocessors um you could think of", "start": 873.92, "duration": 3.8}, {"text": "them as you", "start": 876.6, "duration": 6.0}, {"text": "know um a CPU core and then it contains", "start": 877.72, "duration": 8.799}, {"text": "additional CES right um and so it has", "start": 882.6, "duration": 5.76}, {"text": "single process and basically complete", "start": 886.519, "duration": 3.081}, {"text": "course and also double Precision", "start": 888.36, "duration": 3.44}, {"text": "complete course special function units", "start": 889.6, "duration": 3.84}, {"text": "and so on it's you know each of these", "start": 891.8, "duration": 5.44}, {"text": "processors has its own shared memory and", "start": 893.44, "duration": 6.319}, {"text": "um yeah so nowadays that typically 64", "start": 897.24, "duration": 5.48}, {"text": "cores in one of these um uh", "start": 899.759, "duration": 5.121}, {"text": "multiprocessors and and and they operate", "start": 902.72, "duration": 4.08}, {"text": "in lockstep they do the same operations", "start": 904.88, "duration": 4.04}, {"text": "at the same time if you write code that", "start": 906.8, "duration": 5.279}, {"text": "cannot do that then the code become", "start": 908.92, "duration": 5.359}, {"text": "serialized and you won't make use of of", "start": 912.079, "duration": 3.081}, {"text": "the", "start": 914.279, "duration": 2.881}, {"text": "GP the other thing that you see is", "start": 915.16, "duration": 3.479}, {"text": "there's many of these multiprocessors", "start": 917.16, "duration": 4.359}, {"text": "and each has many of these um processing", "start": 918.639, "duration": 7.281}, {"text": "units um that means you have effectively", "start": 921.519, "duration": 8.281}, {"text": "something like uh thousands of compute", "start": 925.92, "duration": 6.24}, {"text": "course um you need to write code that", "start": 929.8, "duration": 5.64}, {"text": "runs massively parallel we call that on", "start": 932.16, "duration": 6.039}, {"text": "tens of thousands of threads so that's", "start": 935.44, "duration": 6.28}, {"text": "the order of parallelism on a Sim", "start": 938.199, "duration": 3.521}, {"text": "GPU um yeah here's some brief", "start": 941.8, "duration": 6.08}, {"text": "information about tensor course um you", "start": 944.839, "duration": 4.48}, {"text": "know this is the specialized hardware", "start": 947.88, "duration": 3.0}, {"text": "for deep learning that helps accelerate", "start": 949.319, "duration": 3.481}, {"text": "Matrix multiply and accumulate", "start": 950.88, "duration": 3.36}, {"text": "operations", "start": 952.8, "duration": 4.839}, {"text": "MMA and so on the wter architecture we", "start": 954.24, "duration": 6.079}, {"text": "have tensor course with fp16 data types", "start": 957.639, "duration": 6.801}, {"text": "um later architectures um like the amp", "start": 960.319, "duration": 6.241}, {"text": "architecture media architecture has it", "start": 964.44, "duration": 7.199}, {"text": "also for additional data types um so", "start": 966.56, "duration": 6.92}, {"text": "there's like fully connected linear or", "start": 971.639, "duration": 3.841}, {"text": "dense layers and de networks", "start": 973.48, "duration": 3.56}, {"text": "convolutional layers and the current", "start": 975.48, "duration": 3.32}, {"text": "layers they all benefit from these", "start": 977.04, "duration": 4.32}, {"text": "operations because you effectively um", "start": 978.8, "duration": 5.159}, {"text": "you know what this Processing Unit does", "start": 981.36, "duration": 4.279}, {"text": "this you know this matrix multiplication", "start": 983.959, "duration": 3.521}, {"text": "of the subset of data and then", "start": 985.639, "duration": 4.601}, {"text": "accumulates it and there is", "start": 987.48, "duration": 4.2}, {"text": "um you can use that also for mixed", "start": 990.24, "duration": 2.68}, {"text": "Precision", "start": 991.68, "duration": 3.959}, {"text": "Matrix uh operations from the ql library", "start": 992.92, "duration": 5.479}, {"text": "if that's of interest", "start": 995.639, "duration": 2.76}, {"text": "to um you know the hardware", "start": 999.12, "duration": 6.639}, {"text": "characteristics change from GPU model to", "start": 1003.12, "duration": 4.92}, {"text": "GPU model different Generations but it's", "start": 1005.759, "duration": 4.56}, {"text": "not much different than you know having", "start": 1008.04, "duration": 6.719}, {"text": "different generations of uh CPUs right", "start": 1010.319, "duration": 6.76}, {"text": "um you see for instance this is the", "start": 1014.759, "duration": 4.961}, {"text": "Pascal architecture P1 100 um data", "start": 1017.079, "duration": 5.721}, {"text": "center GPU from Nvidia this is the V 100", "start": 1019.72, "duration": 4.719}, {"text": "so the vter architecture those are the", "start": 1022.8, "duration": 3.48}, {"text": "gpus that we have in", "start": 1024.439, "duration": 3.921}, {"text": "expon then there's newer one that's", "start": 1026.28, "duration": 4.639}, {"text": "called an a100 and there's now even a", "start": 1028.36, "duration": 5.959}, {"text": "newer one called h100 so the the hopper", "start": 1030.919, "duration": 6.241}, {"text": "architecture and so you see there's", "start": 1034.319, "duration": 5.081}, {"text": "always been an increase in the number of", "start": 1037.16, "duration": 4.72}, {"text": "multiprocessors each of those always had", "start": 1039.4, "duration": 4.639}, {"text": "over the last generation 64 single", "start": 1041.88, "duration": 5.0}, {"text": "Precision compute course and so you get", "start": 1044.039, "duration": 5.041}, {"text": "an idea about the parallelism right so", "start": 1046.88, "duration": 5.08}, {"text": "here you have about 5,000 fors here on", "start": 1049.08, "duration": 4.36}, {"text": "the", "start": 1051.96, "duration": 5.36}, {"text": "v100s and this is the actually the 64", "start": 1053.44, "duration": 6.32}, {"text": "fp64 floating Point performance that you", "start": 1057.32, "duration": 4.479}, {"text": "get so you get about seven ter flops out", "start": 1059.76, "duration": 4.0}, {"text": "of a single", "start": 1061.799, "duration": 5.201}, {"text": "GP um yeah and then you know there are", "start": 1063.76, "duration": 5.279}, {"text": "gpus also from AMD and Intel I'm not", "start": 1067.0, "duration": 6.039}, {"text": "going to talk about that um they work", "start": 1069.039, "duration": 5.721}, {"text": "similarly but in slightly different ways", "start": 1073.039, "duration": 5.081}, {"text": "and you know you have to program them", "start": 1074.76, "duration": 4.919}, {"text": "effect effectively in very similar ways", "start": 1078.12, "duration": 3.919}, {"text": "but you need to use different languages", "start": 1079.679, "duration": 4.681}, {"text": "or you need to use a portable language", "start": 1082.039, "duration": 5.281}, {"text": "um you know all these Frameworks like", "start": 1084.36, "duration": 4.88}, {"text": "hro tensor glob will obviously work on", "start": 1087.32, "duration": 3.359}, {"text": "all of those because somebody already", "start": 1089.24, "duration": 5.36}, {"text": "has put ad in into developing that for", "start": 1090.679, "duration": 7.12}, {"text": "you um so what that means for your", "start": 1094.6, "duration": 6.16}, {"text": "program is you know so you won't write", "start": 1097.799, "duration": 4.521}, {"text": "the code with an any assumption for how", "start": 1100.76, "duration": 3.96}, {"text": "many threads it will use you just use a", "start": 1102.32, "duration": 5.0}, {"text": "lot of threads um you have to match that", "start": 1104.72, "duration": 4.92}, {"text": "to the underlying Hardware", "start": 1107.32, "duration": 5.76}, {"text": "um and then you use functions basically", "start": 1109.64, "duration": 5.24}, {"text": "to query the hardware type and then you", "start": 1113.08, "duration": 4.64}, {"text": "can can do this at run time um codes", "start": 1114.88, "duration": 5.279}, {"text": "usually do this automatically for", "start": 1117.72, "duration": 4.439}, {"text": "you and you will launch many more", "start": 1120.159, "duration": 3.721}, {"text": "threads than processing course this", "start": 1122.159, "duration": 3.281}, {"text": "helps actually hi some of the memory", "start": 1123.88, "duration": 3.2}, {"text": "latency that's different from what you", "start": 1125.44, "duration": 6.719}, {"text": "would do on a um um with open p um on a", "start": 1127.08, "duration": 7.88}, {"text": "on a CPU usually over subscribing", "start": 1132.159, "duration": 5.361}, {"text": "doesn't help but here the GPU takes care", "start": 1134.96, "duration": 5.199}, {"text": "of these things for", "start": 1137.52, "duration": 4.279}, {"text": "you've seen differences in performance", "start": 1140.159, "duration": 5.321}, {"text": "in 64 and 32bit floating point so if you", "start": 1141.799, "duration": 5.76}, {"text": "can avoid using double Precision then", "start": 1145.48, "duration": 4.6}, {"text": "you get faster code but you have to be", "start": 1147.559, "duration": 4.841}, {"text": "very careful with numerical accuracy of", "start": 1150.08, "duration": 6.079}, {"text": "course um um one one thing is the data", "start": 1152.4, "duration": 6.279}, {"text": "center GPS you know they have a ratio of", "start": 1156.159, "duration": 5.88}, {"text": "2 to one typically of fp64 to fp32", "start": 1158.679, "duration": 6.0}, {"text": "Performance the cheap gaming gpus are", "start": 1162.039, "duration": 5.0}, {"text": "much worse in fp64 performance so if you", "start": 1164.679, "duration": 4.801}, {"text": "manage to avoid double you can run code", "start": 1167.039, "duration": 4.801}, {"text": "fast also on the D", "start": 1169.48, "duration": 5.88}, {"text": "gqs um now all of these things you don't", "start": 1171.84, "duration": 5.199}, {"text": "have to worry about if you use a", "start": 1175.36, "duration": 3.96}, {"text": "framework that's already been ported", "start": 1177.039, "duration": 3.921}, {"text": "like P tens of flow this will", "start": 1179.32, "duration": 3.239}, {"text": "automatically compile the machine", "start": 1180.96, "duration": 2.92}, {"text": "learning model for instance or the", "start": 1182.559, "duration": 4.521}, {"text": "targeted hardare right so um then we", "start": 1183.88, "duration": 6.08}, {"text": "don't worry about this", "start": 1187.08, "duration": 2.88}, {"text": "stuff good so what do we have as", "start": 1193.28, "duration": 5.759}, {"text": "programming languages available", "start": 1196.96, "duration": 5.56}, {"text": "um there's Cuda that's proprietary Works", "start": 1199.039, "duration": 5.561}, {"text": "only for NVIDIA gpus but it is the the", "start": 1202.52, "duration": 3.519}, {"text": "factory standard for most high", "start": 1204.6, "duration": 2.559}, {"text": "performance", "start": 1206.039, "duration": 4.201}, {"text": "codes um you know this is recommended of", "start": 1207.159, "duration": 5.561}, {"text": "course and is installed on on expans", "start": 1210.24, "duration": 4.28}, {"text": "because we have Nvidia", "start": 1212.72, "duration": 6.079}, {"text": "gpus um there's hip that's the um a", "start": 1214.52, "duration": 6.68}, {"text": "framework as a basically language that's", "start": 1218.799, "duration": 4.401}, {"text": "very similar to Cuda um it's developed", "start": 1221.2, "duration": 5.56}, {"text": "by AMD and it's an open source C++ Ram", "start": 1223.2, "duration": 6.04}, {"text": "time API and", "start": 1226.76, "duration": 6.279}, {"text": "language um it's very similar to Cuda um", "start": 1229.24, "duration": 6.919}, {"text": "and it works with Nvidia it uses Cuda in", "start": 1233.039, "duration": 7.88}, {"text": "the back end um and AMD vi vi amd's Rock", "start": 1236.159, "duration": 6.76}, {"text": "so if you have code that's written in", "start": 1240.919, "duration": 3.601}, {"text": "Hip you know you can compile it both for", "start": 1242.919, "duration": 3.64}, {"text": "NVIDIA and AMD", "start": 1244.52, "duration": 6.159}, {"text": "gqs um if you have C code it's very easy", "start": 1246.559, "duration": 6.401}, {"text": "to translate it into hip code because", "start": 1250.679, "duration": 5.24}, {"text": "the language is modeled after ke there's", "start": 1252.96, "duration": 4.68}, {"text": "even scripts that you automatically for", "start": 1255.919, "duration": 3.76}, {"text": "you and then you have to fix a few", "start": 1257.64, "duration": 5.36}, {"text": "things H you could use opencl some codes", "start": 1259.679, "duration": 4.921}, {"text": "are written in opencl that works for", "start": 1263.0, "duration": 5.679}, {"text": "NVIDIA AMD and int GPS um and there CLE", "start": 1264.6, "duration": 5.679}, {"text": "now it's a single Source high level", "start": 1268.679, "duration": 5.6}, {"text": "standard C++ programming model um Intel", "start": 1270.279, "duration": 6.441}, {"text": "is is putting a lot of effort into this", "start": 1274.279, "duration": 4.321}, {"text": "and um this kind of Target a range of", "start": 1276.72, "duration": 6.76}, {"text": "hus platforms so CPUs gpus fpgas um and", "start": 1278.6, "duration": 6.72}, {"text": "there are backhands now also for not", "start": 1283.48, "duration": 5.84}, {"text": "only Intel GPS but also um AMD and and", "start": 1285.32, "duration": 6.0}, {"text": "and and Nvidia", "start": 1289.32, "duration": 5.359}, {"text": "GPS so we will look only at Cuda briefly", "start": 1291.32, "duration": 6.4}, {"text": "here um there's also open ACC that's", "start": 1294.679, "duration": 7.48}, {"text": "accelerated directives uh for NVIDIA and", "start": 1297.72, "duration": 8.36}, {"text": "AMD um it works with C++ and", "start": 1302.159, "duration": 6.601}, {"text": "quatron and then there's open MP since", "start": 1306.08, "duration": 5.079}, {"text": "version 4 point", "start": 1308.76, "duration": 4.76}, {"text": "uh version four basically it includes", "start": 1311.159, "duration": 4.321}, {"text": "accelerator and vectorization directives", "start": 1313.52, "duration": 3.8}, {"text": "so you know can generate code that not", "start": 1315.48, "duration": 3.24}, {"text": "only parallel", "start": 1317.32, "duration": 3.839}, {"text": "for CPU but basically also", "start": 1318.72, "duration": 5.48}, {"text": "offloads um code sections in parallel", "start": 1321.159, "duration": 8.321}, {"text": "onto the GPU um it's becoming mature so", "start": 1324.2, "duration": 6.8}, {"text": "it depends on the compiler you have and", "start": 1329.48, "duration": 3.679}, {"text": "so on so we're not going to look at this", "start": 1331.0, "duration": 4.6}, {"text": "here", "start": 1333.159, "duration": 2.441}, {"text": "um because we have Nvidia gpus let's", "start": 1335.84, "duration": 5.0}, {"text": "have a brief look here so um Nvidia had", "start": 1338.32, "duration": 5.64}, {"text": "many years of developing Frameworks um", "start": 1340.84, "duration": 6.4}, {"text": "to help programming and these GPS um so", "start": 1343.96, "duration": 5.68}, {"text": "you know there some for practically any", "start": 1347.24, "duration": 5.28}, {"text": "any programming language um there's lots", "start": 1349.64, "duration": 5.84}, {"text": "of libraries and middlebar available", "start": 1352.52, "duration": 6.24}, {"text": "um and uh it works basically on any of", "start": 1355.48, "duration": 5.92}, {"text": "the gpus data center gpus different you", "start": 1358.76, "duration": 7.12}, {"text": "know generations and consumer gpus um", "start": 1361.4, "duration": 8.519}, {"text": "everything that you can think of um the", "start": 1365.88, "duration": 5.84}, {"text": "this this I took from the PC programming", "start": 1369.919, "duration": 3.521}, {"text": "guides so if you're interested in Puda", "start": 1371.72, "duration": 4.04}, {"text": "um take a look at the PDC programming", "start": 1373.44, "duration": 4.839}, {"text": "guide is actually easy Rel relatively", "start": 1375.76, "duration": 6.76}, {"text": "easy to digest if you already know C or", "start": 1378.279, "duration": 6.361}, {"text": "C++", "start": 1382.52, "duration": 4.399}, {"text": "um now if you want to use the Cuda", "start": 1384.64, "duration": 4.56}, {"text": "development", "start": 1386.919, "duration": 6.721}, {"text": "tools um there is uh the Cuda tool", "start": 1389.2, "duration": 7.839}, {"text": "kit which is free it contains the p c", "start": 1393.64, "duration": 5.44}, {"text": "compiler it contains all sorts of", "start": 1397.039, "duration": 4.361}, {"text": "libraries like linear algebra fast per", "start": 1399.08, "duration": 4.36}, {"text": "transforms deep neural network ROM", "start": 1401.4, "duration": 3.96}, {"text": "number generators etc", "start": 1403.44, "duration": 5.0}, {"text": "etc um it contains debugging tools to", "start": 1405.36, "duration": 5.72}, {"text": "like P GTB P", "start": 1408.44, "duration": 5.08}, {"text": "MJ and it contains profiling tools so", "start": 1411.08, "duration": 3.88}, {"text": "nvop and", "start": 1413.52, "duration": 4.36}, {"text": "nvvp um if we have time I'll briefly", "start": 1414.96, "duration": 4.599}, {"text": "talk about those at the end otherwise", "start": 1417.88, "duration": 3.159}, {"text": "we'll have to skti", "start": 1419.559, "duration": 5.801}, {"text": "that um so so just let me mention NVR", "start": 1421.039, "duration": 7.481}, {"text": "and nvvp so nvpr is the command line", "start": 1425.36, "duration": 6.88}, {"text": "profiler um and you can just use that um", "start": 1428.52, "duration": 5.92}, {"text": "and nvvp is you know you can load data", "start": 1432.24, "duration": 4.319}, {"text": "generated by NV Pro and so that's the", "start": 1434.44, "duration": 4.16}, {"text": "visual profiler and and and visualize", "start": 1436.559, "duration": 4.681}, {"text": "what's happening where your code spends", "start": 1438.6, "duration": 6.88}, {"text": "time and so on these are old they still", "start": 1441.24, "duration": 6.72}, {"text": "work um they're basically deprecated by", "start": 1445.48, "duration": 4.84}, {"text": "Nvidia but they still work and there's", "start": 1447.96, "duration": 4.839}, {"text": "the newer inside systems and inside", "start": 1450.32, "duration": 5.2}, {"text": "compute um um", "start": 1452.799, "duration": 6.321}, {"text": "profils um by n so they they are been", "start": 1455.52, "duration": 5.12}, {"text": "put putting a lot of work on these so", "start": 1459.12, "duration": 3.28}, {"text": "they work I ke very well to see a lot of", "start": 1460.64, "duration": 4.2}, {"text": "detailed information about your code and", "start": 1462.4, "duration": 7.08}, {"text": "where there might be some performance B", "start": 1464.84, "duration": 6.719}, {"text": "there are tter code samples available so", "start": 1469.48, "duration": 3.76}, {"text": "you can get them on", "start": 1471.559, "duration": 5.921}, {"text": "GitHub um you know look at the developer", "start": 1473.24, "duration": 5.319}, {"text": "documentation there's lots of", "start": 1477.48, "duration": 3.72}, {"text": "information and you can of course get", "start": 1478.559, "duration": 5.761}, {"text": "get the CU from here this all installed", "start": 1481.2, "duration": 5.479}, {"text": "on expans so if you are on expans and", "start": 1484.32, "duration": 4.4}, {"text": "you get onto", "start": 1486.679, "duration": 4.961}, {"text": "expans you want to use gpus and and you", "start": 1488.72, "duration": 5.36}, {"text": "know I'm going to show you how to do", "start": 1491.64, "duration": 4.96}, {"text": "that you need to get a GPU node and once", "start": 1494.08, "duration": 5.959}, {"text": "you're on a GPU node um you need to load", "start": 1496.6, "duration": 6.0}, {"text": "the corresponding um compter toolkit", "start": 1500.039, "duration": 6.081}, {"text": "that that we want to use", "start": 1502.6, "duration": 6.28}, {"text": "um you should do a module reset before", "start": 1506.12, "duration": 4.799}, {"text": "you do that because um when you", "start": 1508.88, "duration": 4.279}, {"text": "initially get onto the GPU nodes they", "start": 1510.919, "duration": 6.081}, {"text": "are set up with all the modules that are", "start": 1513.159, "duration": 6.64}, {"text": "appropriate for the CPU nodes so you", "start": 1517.0, "duration": 6.6}, {"text": "want to load the the the GPU environment", "start": 1519.799, "duration": 6.24}, {"text": "basically once that is done then you can", "start": 1523.6, "duration": 4.76}, {"text": "load the through TW it so the current", "start": 1526.039, "duration": 4.401}, {"text": "version that's that's installed is 11.7", "start": 1528.36, "duration": 3.6}, {"text": "there are also older versions available", "start": 1530.44, "duration": 5.56}, {"text": "if that is of use to you um if you have", "start": 1531.96, "duration": 6.28}, {"text": "any code that uses any of those", "start": 1536.0, "duration": 4.24}, {"text": "libraries or Cuda um then you also need", "start": 1538.24, "duration": 3.76}, {"text": "to load the", "start": 1540.24, "duration": 5.159}, {"text": "toolkit um typically unless the code has", "start": 1542.0, "duration": 5.559}, {"text": "been distributed in a way that is", "start": 1545.399, "duration": 3.921}, {"text": "basically hard link but that's typically", "start": 1547.559, "duration": 4.401}, {"text": "not the case um so so you load the to", "start": 1549.32, "duration": 6.68}, {"text": "and then you can run that c and a", "start": 1551.96, "duration": 4.04}, {"text": "code there is also the Nvidia HPC", "start": 1556.2, "duration": 7.0}, {"text": "software development kit SDK um is also", "start": 1559.6, "duration": 6.4}, {"text": "free it's meant to replace the cuter", "start": 1563.2, "duration": 6.599}, {"text": "toolkit at some point um it contains", "start": 1566.0, "duration": 5.44}, {"text": "most of the cuter toolkit including the", "start": 1569.799, "duration": 5.401}, {"text": "cuter compiler nvcc libraries DS", "start": 1571.44, "duration": 7.0}, {"text": "profiler and so on um but it also", "start": 1575.2, "duration": 6.839}, {"text": "contains a c and C++ compiler vron", "start": 1578.44, "duration": 7.08}, {"text": "compiler um and and and those support", "start": 1582.039, "duration": 6.921}, {"text": "CPU code um but Al open ACC so if you", "start": 1585.52, "duration": 6.039}, {"text": "want to use open ACC um you know you you", "start": 1588.96, "duration": 4.24}, {"text": "can load that HBC", "start": 1591.559, "duration": 4.761}, {"text": "SDK um these compilers used to be PGI", "start": 1593.2, "duration": 4.8}, {"text": "compilers in the past and at some point", "start": 1596.32, "duration": 4.239}, {"text": "Nvidia bought PGI and so it's it's it's", "start": 1598.0, "duration": 5.399}, {"text": "Nvidia compilers now and the way you use", "start": 1600.559, "duration": 5.0}, {"text": "those the same way you just load um the", "start": 1603.399, "duration": 4.841}, {"text": "corresponding module which is called", "start": 1605.559, "duration": 4.921}, {"text": "nbpc and the current version installed", "start": 1608.24, "duration": 3.08}, {"text": "is", "start": 1610.48, "duration": 3.079}, {"text": "21.9 uh there are other versions also", "start": 1611.32, "duration": 3.8}, {"text": "installed if you need", "start": 1613.559, "duration": 5.72}, {"text": "those so sly are there any any any", "start": 1615.12, "duration": 5.48}, {"text": "questions that I", "start": 1619.279, "duration": 5.961}, {"text": "should um answer right up to now Andy", "start": 1620.6, "duration": 7.48}, {"text": "there was one from an earlier slide um", "start": 1625.24, "duration": 6.36}, {"text": "and a few coming in um there was one", "start": 1628.08, "duration": 8.4}, {"text": "inquiring so that one is that DIY four", "start": 1631.6, "duration": 9.64}, {"text": "times means 4 RTX 380 question mark oh", "start": 1636.48, "duration": 9.079}, {"text": "yes yes yes okay um next one is Will", "start": 1641.24, "duration": 7.52}, {"text": "Will these two modules conflict are", "start": 1645.559, "duration": 6.081}, {"text": "there issues with the versions for Cuda", "start": 1648.76, "duration": 4.44}, {"text": "and N", "start": 1651.64, "duration": 5.36}, {"text": "vhpc um don't load them at the same time", "start": 1653.2, "duration": 7.44}, {"text": "I mean there's no reason to do that", "start": 1657.0, "duration": 7.64}, {"text": "um I I don't I yeah there there you know", "start": 1660.64, "duration": 6.2}, {"text": "probably the so probably what's going to", "start": 1664.64, "duration": 4.68}, {"text": "happen if you load this nbpc and if you", "start": 1666.84, "duration": 4.8}, {"text": "load the C to kit at the same", "start": 1669.32, "duration": 5.8}, {"text": "time um when you use the cter compilers", "start": 1671.64, "duration": 5.08}, {"text": "there's going to be errors because you", "start": 1675.12, "duration": 3.399}, {"text": "have um you know different versions of", "start": 1676.72, "duration": 3.88}, {"text": "the libraries that might be found in the", "start": 1678.519, "duration": 8.921}, {"text": "in the path the the um Nvidia um cc++", "start": 1680.6, "duration": 9.079}, {"text": "and forun compilers will work I think", "start": 1687.44, "duration": 4.28}, {"text": "but you know just don't do it I think", "start": 1689.679, "duration": 5.201}, {"text": "use one or the other at one", "start": 1691.72, "duration": 6.0}, {"text": "time so um basically there are three", "start": 1694.88, "duration": 5.039}, {"text": "ways to use gpus", "start": 1697.72, "duration": 5.319}, {"text": "um you know you have an application and", "start": 1699.919, "duration": 4.64}, {"text": "well if the application is already GPU", "start": 1703.039, "duration": 3.0}, {"text": "enabled you just need to run it right", "start": 1704.559, "duration": 2.921}, {"text": "and you need to load the corresponding", "start": 1706.039, "duration": 4.961}, {"text": "Inu tool if it relies on that", "start": 1707.48, "duration": 6.24}, {"text": "um then but if you develop your own", "start": 1711.0, "duration": 4.96}, {"text": "application you can do this with", "start": 1713.72, "duration": 5.48}, {"text": "libraries um basically drop an", "start": 1715.96, "duration": 6.0}, {"text": "acceleration um instead of using the", "start": 1719.2, "duration": 5.959}, {"text": "library the works for CPU you just use", "start": 1721.96, "duration": 6.04}, {"text": "the corresponding um GPU", "start": 1725.159, "duration": 6.561}, {"text": "and um you can use open ACC directives", "start": 1728.0, "duration": 5.64}, {"text": "that's relatively straightforward to", "start": 1731.72, "duration": 3.72}, {"text": "accelerate applications and your code", "start": 1733.64, "duration": 3.279}, {"text": "remains portable it still runs on the", "start": 1735.44, "duration": 2.76}, {"text": "CPU", "start": 1736.919, "duration": 3.081}, {"text": "or you use a programming language", "start": 1738.2, "duration": 3.68}, {"text": "specifically and and write both", "start": 1740.0, "duration": 3.679}, {"text": "dedicated for the", "start": 1741.88, "duration": 5.48}, {"text": "GPU and so let's look at GPU accelerated", "start": 1743.679, "duration": 5.921}, {"text": "libraries um you know it's of course", "start": 1747.36, "duration": 3.36}, {"text": "easy to", "start": 1749.6, "duration": 4.04}, {"text": "use um it's a drop in replacement", "start": 1750.72, "duration": 4.959}, {"text": "because most GPU accelerated libraries", "start": 1753.64, "duration": 4.2}, {"text": "follow standard apis um so there are", "start": 1755.679, "duration": 4.921}, {"text": "minimal code changes required there's", "start": 1757.84, "duration": 4.4}, {"text": "always some sort of code change or in", "start": 1760.6, "duration": 5.16}, {"text": "many cases um I'll show you an example", "start": 1762.24, "duration": 5.96}, {"text": "um these are high quality iies and", "start": 1765.76, "duration": 5.0}, {"text": "implementation so they are efficient use", "start": 1768.2, "duration": 3.839}, {"text": "those if you", "start": 1770.76, "duration": 4.08}, {"text": "can don't don't try to write your own", "start": 1772.039, "duration": 5.201}, {"text": "matrix multiplication except for learn", "start": 1774.84, "duration": 5.6}, {"text": "includer or something like that um yeah", "start": 1777.24, "duration": 5.679}, {"text": "so if you look at nvidia's website", "start": 1780.44, "duration": 5.04}, {"text": "there's a lot of information there um I", "start": 1782.919, "duration": 4.0}, {"text": "don't need to talk about details but", "start": 1785.48, "duration": 2.84}, {"text": "there's libraries for all sorts of", "start": 1786.919, "duration": 3.521}, {"text": "technicians", "start": 1788.32, "duration": 5.239}, {"text": "um and so here's an example what would", "start": 1790.44, "duration": 6.88}, {"text": "you do if you have um a a a blast", "start": 1793.559, "duration": 6.441}, {"text": "function in your code classes like basic", "start": 1797.32, "duration": 5.359}, {"text": "linear Al algebra systems so there's for", "start": 1800.0, "duration": 5.76}, {"text": "instance a simple example is the sapy", "start": 1802.679, "duration": 5.921}, {"text": "function um yeah for people who are", "start": 1805.76, "duration": 5.2}, {"text": "familiar with linear algebra you know", "start": 1808.6, "duration": 4.439}, {"text": "that that create straightforward it's", "start": 1810.96, "duration": 4.28}, {"text": "just it's just a standard function that", "start": 1813.039, "duration": 7.0}, {"text": "does a a basically um a * x and x is an", "start": 1815.24, "duration": 8.679}, {"text": "array a vector and and plus Y which is", "start": 1820.039, "duration": 6.401}, {"text": "also right so it scales one vector and", "start": 1823.919, "duration": 4.401}, {"text": "adds it to another", "start": 1826.44, "duration": 3.479}, {"text": "uh there's a corresponding function it's", "start": 1828.32, "duration": 3.359}, {"text": "just called Q Plus", "start": 1829.919, "duration": 4.76}, {"text": "SXP and then you have to manage just", "start": 1831.679, "duration": 6.081}, {"text": "replace the library call name um and you", "start": 1834.679, "duration": 4.401}, {"text": "have to manage data", "start": 1837.76, "duration": 3.44}, {"text": "locality u meaning you know you need to", "start": 1839.08, "duration": 5.0}, {"text": "allocate memory on the GPU you can do", "start": 1841.2, "duration": 4.839}, {"text": "this with CUA or with qbl there are", "start": 1844.08, "duration": 4.92}, {"text": "functions provided for you to doing that", "start": 1846.039, "duration": 4.76}, {"text": "um and then you rebuild and Link the CUA", "start": 1849.0, "duration": 4.6}, {"text": "accelerator libr right so you have", "start": 1850.799, "duration": 4.321}, {"text": "your", "start": 1853.6, "duration": 5.12}, {"text": "um you know you comp your code and then", "start": 1855.12, "duration": 6.08}, {"text": "compile your code with nbcc and then you", "start": 1858.72, "duration": 5.839}, {"text": "link it against the P plus", "start": 1861.2, "duration": 3.359}, {"text": "M and so here's this example right so", "start": 1864.679, "duration": 8.561}, {"text": "that's the API um for for the sax", "start": 1868.36, "duration": 6.799}, {"text": "function call you know you have to", "start": 1873.24, "duration": 3.88}, {"text": "define the size of the system this is", "start": 1875.159, "duration": 5.12}, {"text": "the value with which of a right so this", "start": 1877.12, "duration": 4.799}, {"text": "is the mathematical operation where", "start": 1880.279, "duration": 3.561}, {"text": "where Y and a are", "start": 1881.919, "duration": 5.321}, {"text": "arrays and single Precision data so fp3", "start": 1883.84, "duration": 7.12}, {"text": "true data um so you pass X and Y and", "start": 1887.24, "duration": 6.0}, {"text": "this is just a stride that is used in", "start": 1890.96, "duration": 3.599}, {"text": "this", "start": 1893.24, "duration": 3.799}, {"text": "equation um and yeah so all you do is", "start": 1894.559, "duration": 5.041}, {"text": "basically you replace that um you have", "start": 1897.039, "duration": 5.0}, {"text": "to pass a handle so that's basically to", "start": 1899.6, "duration": 3.84}, {"text": "initialize the", "start": 1902.039, "duration": 4.6}, {"text": "GPU um there's a function for that Q", "start": 1903.44, "duration": 4.28}, {"text": "Plus", "start": 1906.639, "duration": 3.28}, {"text": "create and then you destroy this once", "start": 1907.72, "duration": 6.079}, {"text": "you're done um you allocate memory on", "start": 1909.919, "duration": 5.321}, {"text": "the device and know this looks a little", "start": 1913.799, "duration": 4.281}, {"text": "bit um complicated but it's it's not um", "start": 1915.24, "duration": 4.919}, {"text": "literally just you know two arrays on", "start": 1918.08, "duration": 5.28}, {"text": "the device or on the GQ for array X and", "start": 1920.159, "duration": 6.281}, {"text": "Y you release the memory when you're", "start": 1923.36, "duration": 7.319}, {"text": "done you transfer the data from the CPU", "start": 1926.44, "duration": 6.359}, {"text": "so you have read it in from somewhere or", "start": 1930.679, "duration": 5.12}, {"text": "you know you have that available on your", "start": 1932.799, "duration": 4.76}, {"text": "in your program and now you want to do", "start": 1935.799, "duration": 3.6}, {"text": "this operation on the GPU so you", "start": 1937.559, "duration": 4.0}, {"text": "transfer data to the GPU with this", "start": 1939.399, "duration": 3.28}, {"text": "function", "start": 1941.559, "duration": 3.881}, {"text": "call perform this sax operation and then", "start": 1942.679, "duration": 6.801}, {"text": "you transfer the data back for now and", "start": 1945.44, "duration": 6.04}, {"text": "that's that's all that's required so", "start": 1949.48, "duration": 3.72}, {"text": "it's pretty", "start": 1951.48, "duration": 3.48}, {"text": "straightforward", "start": 1953.2, "duration": 4.8}, {"text": "um now you", "start": 1954.96, "duration": 5.04}, {"text": "know yeah so that's basically all I", "start": 1958.0, "duration": 6.12}, {"text": "wanted to say about lies right", "start": 1960.0, "duration": 7.159}, {"text": "um what's what's coming at the moment", "start": 1964.12, "duration": 6.519}, {"text": "and being developed um is acceleration", "start": 1967.159, "duration": 5.88}, {"text": "in standard langu so there's ISO C++ and", "start": 1970.639, "duration": 4.88}, {"text": "ISO foran developments where you", "start": 1973.039, "duration": 5.24}, {"text": "actually have um you know", "start": 1975.519, "duration": 8.721}, {"text": "parallel um um um functions you know you", "start": 1978.279, "duration": 7.841}, {"text": "have you know in the standard Library", "start": 1984.24, "duration": 5.08}, {"text": "C++ standard Library here is for the two", "start": 1986.12, "duration": 6.24}, {"text": "concurrent um so this would basically", "start": 1989.32, "duration": 7.359}, {"text": "parallelize automatically the um Loop", "start": 1992.36, "duration": 7.039}, {"text": "right that you have here or you see this", "start": 1996.679, "duration": 4.96}, {"text": "is basically the S speed operation right", "start": 1999.399, "duration": 5.041}, {"text": "implemented literally and you could just", "start": 2001.639, "duration": 4.52}, {"text": "compile that and and the compiler will", "start": 2004.44, "duration": 4.52}, {"text": "generate code", "start": 2006.159, "duration": 2.801}, {"text": "um there's Q numeric um you know where", "start": 2009.08, "duration": 6.52}, {"text": "you can do this in in", "start": 2013.0, "duration": 6.919}, {"text": "P um or if you have a c or C++ code um", "start": 2015.6, "duration": 6.959}, {"text": "you know you have the corresponding", "start": 2019.919, "duration": 6.081}, {"text": "function that you want to accelerate um", "start": 2022.559, "duration": 6.0}, {"text": "you can do this as open AC prnas or open", "start": 2026.0, "duration": 4.36}, {"text": "m prnas right so that's what I was", "start": 2028.559, "duration": 4.521}, {"text": "talking about open ACC and open m we", "start": 2030.36, "duration": 4.559}, {"text": "going look very briefly at at an open", "start": 2033.08, "duration": 5.4}, {"text": "ACC example um you know in a few minutes", "start": 2034.919, "duration": 7.041}, {"text": "um or you literally write functions", "start": 2038.48, "duration": 7.12}, {"text": "specifically using Fuda for the G you", "start": 2041.96, "duration": 5.24}, {"text": "know I will mention a few things on how", "start": 2045.6, "duration": 4.92}, {"text": "that looks like and how that maps to the", "start": 2047.2, "duration": 5.399}, {"text": "hardware U so these are the different", "start": 2050.52, "duration": 5.04}, {"text": "ways in which you actually program for a", "start": 2052.599, "duration": 6.681}, {"text": "GPU um from", "start": 2055.56, "duration": 8.2}, {"text": "basically um explicitly programming for", "start": 2059.28, "duration": 9.319}, {"text": "the GPU um to more simple approaches", "start": 2063.76, "duration": 8.96}, {"text": "that generate the code automatically for", "start": 2068.599, "duration": 7.08}, {"text": "you um here's an example that's actually", "start": 2072.72, "duration": 4.72}, {"text": "very simple so for numerical Computing", "start": 2075.679, "duration": 4.4}, {"text": "in Python you can use um qpy which is", "start": 2077.44, "duration": 4.639}, {"text": "the equivalent of nonp so if you're", "start": 2080.079, "duration": 4.721}, {"text": "familiar with python um numpy operates", "start": 2082.079, "duration": 6.121}, {"text": "on arrays of data um you know it's very", "start": 2084.8, "duration": 6.4}, {"text": "tuned for CPUs their implementations are", "start": 2088.2, "duration": 7.04}, {"text": "tuning for Intel CQ each vendor has like", "start": 2091.2, "duration": 6.48}, {"text": "optimizations um and there there's a qy", "start": 2095.24, "duration": 4.44}, {"text": "which is basically an UNP like interface", "start": 2097.68, "duration": 3.8}, {"text": "that makes it severly to run the", "start": 2099.68, "duration": 4.159}, {"text": "corresponding code code on the", "start": 2101.48, "duration": 3.92}, {"text": "GPU", "start": 2103.839, "duration": 4.121}, {"text": "so um you know you can copy data to the", "start": 2105.4, "duration": 5.56}, {"text": "GPU there's a qy ND array", "start": 2107.96, "duration": 7.119}, {"text": "um there is a data interoperability of", "start": 2110.96, "duration": 5.52}, {"text": "this actually with deep learning", "start": 2115.079, "duration": 5.321}, {"text": "Frameworks um um Rapids and", "start": 2116.48, "duration": 6.52}, {"text": "number and it uses highly tuned Nidia", "start": 2120.4, "duration": 4.56}, {"text": "libraries in the back backand right so", "start": 2123.0, "duration": 4.28}, {"text": "it can be some sort of like metric and", "start": 2124.96, "duration": 4.8}, {"text": "similar things um you can also write", "start": 2127.28, "duration": 6.319}, {"text": "custom LS but here's a simple example", "start": 2129.76, "duration": 6.24}, {"text": "for instance you know you have here um", "start": 2133.599, "duration": 5.321}, {"text": "you know initializing so you're using", "start": 2136.0, "duration": 7.599}, {"text": "numpy in Python and you initialize a a a", "start": 2138.92, "duration": 6.4}, {"text": "array with random", "start": 2143.599, "duration": 3.801}, {"text": "numbers and then you do a QR", "start": 2145.32, "duration": 3.759}, {"text": "factorization right that's a linear", "start": 2147.4, "duration": 4.64}, {"text": "algebra f a QR Factor it well if you", "start": 2149.079, "duration": 5.121}, {"text": "want to do this on the GPU with", "start": 2152.04, "duration": 5.16}, {"text": "QP um you just replace my Pi with q and", "start": 2154.2, "duration": 8.8}, {"text": "that's it um and well this this is sorry", "start": 2157.2, "duration": 7.72}, {"text": "this is a slide I actually got from", "start": 2163.0, "duration": 5.56}, {"text": "Nvidia um doesn't tell much what the f", "start": 2164.92, "duration": 5.399}, {"text": "xped up means here but you know you can", "start": 2168.56, "duration": 3.88}, {"text": "execute the code on the GQ and it will", "start": 2170.319, "duration": 5.201}, {"text": "run in parall on the", "start": 2172.44, "duration": 3.08}, {"text": "GP", "start": 2177.64, "duration": 5.719}, {"text": "um let me see it's 1140 so let me just", "start": 2179.2, "duration": 5.68}, {"text": "very briefly tell you something about", "start": 2183.359, "duration": 4.601}, {"text": "Cuda so Cuda um you know you get", "start": 2184.88, "duration": 5.959}, {"text": "information here on nvidia's website is", "start": 2187.96, "duration": 5.8}, {"text": "is the solution to run C or C++ simly on", "start": 2190.839, "duration": 8.961}, {"text": "GPS and Huda is NVIDIA only but um it is", "start": 2193.76, "duration": 9.28}, {"text": "widely used and it's modest extension so", "start": 2199.8, "duration": 4.92}, {"text": "there's not a lot to learn but it", "start": 2203.04, "duration": 3.68}, {"text": "requires major rewriting of code because", "start": 2204.72, "duration": 4.28}, {"text": "you need to specifically write functions", "start": 2206.72, "duration": 5.2}, {"text": "um in a way that they can execute using", "start": 2209.0, "duration": 5.52}, {"text": "cter uh nomenclature in parallel on the", "start": 2211.92, "duration": 4.199}, {"text": "dpu and you have to make sure that all", "start": 2214.52, "duration": 4.44}, {"text": "your data typ match Etc there's also", "start": 2216.119, "duration": 4.801}, {"text": "pter foron so if you're foron developer", "start": 2218.96, "duration": 3.48}, {"text": "you can do that it was originally", "start": 2220.92, "duration": 3.48}, {"text": "developed by PGI and it's available now", "start": 2222.44, "duration": 4.96}, {"text": "in the Nvidia foron", "start": 2224.4, "duration": 3.0}, {"text": "compiler um yeah here are some", "start": 2227.56, "duration": 5.279}, {"text": "informations about books these are older", "start": 2230.96, "duration": 3.48}, {"text": "books but you know you'll find a lot of", "start": 2232.839, "duration": 3.801}, {"text": "information on the web about Cuda and", "start": 2234.44, "duration": 5.24}, {"text": "there's many resources on this um GP", "start": 2236.64, "duration": 5.84}, {"text": "hackathon website um these are all the", "start": 2239.68, "duration": 4.52}, {"text": "books that are to get started but there", "start": 2242.48, "duration": 4.24}, {"text": "there are many others um out there so", "start": 2244.2, "duration": 3.879}, {"text": "very easy to find", "start": 2246.72, "duration": 4.28}, {"text": "information and just as an overview what", "start": 2248.079, "duration": 5.161}, {"text": "you typically have is you know you have", "start": 2251.0, "duration": 4.04}, {"text": "a serial code you have your own code in", "start": 2253.24, "duration": 5.56}, {"text": "C C++ forr um that's running on the host", "start": 2255.04, "duration": 6.96}, {"text": "we call host basically the CPUs and um", "start": 2258.8, "duration": 5.68}, {"text": "so you have a serial code or potentially", "start": 2262.0, "duration": 5.92}, {"text": "open it in parallel or an API code and", "start": 2264.48, "duration": 6.0}, {"text": "then you know you basically launch a", "start": 2267.92, "duration": 4.36}, {"text": "section of the code that executes on", "start": 2270.48, "duration": 4.639}, {"text": "what we call the device that is the GPU", "start": 2272.28, "duration": 4.64}, {"text": "and you see many threads that are", "start": 2275.119, "duration": 5.041}, {"text": "structured into a grid of blocks right", "start": 2276.92, "duration": 5.159}, {"text": "so you have a block and another block", "start": 2280.16, "duration": 3.52}, {"text": "and another block and the reason for", "start": 2282.079, "duration": 4.76}, {"text": "having these blocks is in a grid and and", "start": 2283.68, "duration": 5.24}, {"text": "thread structure because this maps onto", "start": 2286.839, "duration": 4.081}, {"text": "the multiprocessors that I talked to you", "start": 2288.92, "duration": 2.84}, {"text": "about", "start": 2290.92, "duration": 2.439}, {"text": "before", "start": 2291.76, "duration": 4.0}, {"text": "um so we have the serial code sections", "start": 2293.359, "duration": 4.801}, {"text": "and in between you you have calls to", "start": 2295.76, "duration": 4.319}, {"text": "transfer data to the GPU you have calls", "start": 2298.16, "duration": 4.12}, {"text": "to the launch the corresponding", "start": 2300.079, "duration": 3.961}, {"text": "functions on the GPU and execute those", "start": 2302.28, "duration": 4.44}, {"text": "in the GPU and at some point you know", "start": 2304.04, "duration": 5.76}, {"text": "back to this um serial codee and then", "start": 2306.72, "duration": 5.639}, {"text": "execute parallel code again what that", "start": 2309.8, "duration": 4.319}, {"text": "looks like is you know so you have the", "start": 2312.359, "duration": 4.72}, {"text": "host you have your CPU and its memory", "start": 2314.119, "duration": 5.321}, {"text": "and you have to device that's the GPU", "start": 2317.079, "duration": 4.441}, {"text": "with these multiprocessors you see one", "start": 2319.44, "duration": 4.679}, {"text": "here another multiprocessor in each and", "start": 2321.52, "duration": 4.599}, {"text": "so on and each one with many cores and", "start": 2324.119, "duration": 4.401}, {"text": "its own caches and so on there's", "start": 2326.119, "duration": 4.881}, {"text": "typically some some L2 caches well and", "start": 2328.52, "duration": 4.28}, {"text": "then interconnected interconnect", "start": 2331.0, "duration": 5.72}, {"text": "different um multiple GBS in in a node", "start": 2332.8, "duration": 6.96}, {"text": "and then and then it's on memory right", "start": 2336.72, "duration": 5.48}, {"text": "so we've seen that for the library as", "start": 2339.76, "duration": 4.079}, {"text": "well right so you copy input data from", "start": 2342.2, "duration": 3.08}, {"text": "the CPU to the", "start": 2343.839, "duration": 4.321}, {"text": "GPU you load the program you execute the", "start": 2345.28, "duration": 5.12}, {"text": "program right um on the GPU and then", "start": 2348.16, "duration": 3.64}, {"text": "when you're done you copy the results", "start": 2350.4, "duration": 4.08}, {"text": "back to the CPU and then do whatever", "start": 2351.8, "duration": 6.279}, {"text": "else need to do WR output um continue", "start": 2354.48, "duration": 4.92}, {"text": "processing on the", "start": 2358.079, "duration": 3.881}, {"text": "CPU um now we've seen these two", "start": 2359.4, "duration": 6.24}, {"text": "different memory spaces in practice you", "start": 2361.96, "duration": 5.92}, {"text": "know they split in separate and you need", "start": 2365.64, "duration": 5.12}, {"text": "to transfer data um there's a way to", "start": 2367.88, "duration": 5.56}, {"text": "program actually that's called unified", "start": 2370.76, "duration": 5.64}, {"text": "memory where um you don't you don't have", "start": 2373.44, "duration": 5.84}, {"text": "to do all this memory allocation and", "start": 2376.4, "duration": 5.88}, {"text": "moving the data to the GPU and back so", "start": 2379.28, "duration": 6.039}, {"text": "the the the computer can do that for you", "start": 2382.28, "duration": 4.559}, {"text": "um there's a pool of manag memory that", "start": 2385.319, "duration": 2.921}, {"text": "is shared between the host and the", "start": 2386.839, "duration": 3.921}, {"text": "device um but let me tell you this is", "start": 2388.24, "duration": 5.52}, {"text": "nice to program uh to get started with", "start": 2390.76, "duration": 5.04}, {"text": "um but it's primarily a productivity", "start": 2393.76, "duration": 3.68}, {"text": "feature on the memory copies of course", "start": 2395.8, "duration": 3.559}, {"text": "still happen under the hood so you know", "start": 2397.44, "duration": 4.28}, {"text": "it has performance impacts and you need", "start": 2399.359, "duration": 5.72}, {"text": "to be aware of those things in any", "start": 2401.72, "duration": 7.08}, {"text": "case um I wanted to introduce you very", "start": 2405.079, "duration": 8.0}, {"text": "quickly on some terms that are used um", "start": 2408.8, "duration": 6.0}, {"text": "in C as or", "start": 2413.079, "duration": 4.24}, {"text": "kernel is a code it's typically the", "start": 2414.8, "duration": 4.279}, {"text": "function that then can be executed on", "start": 2417.319, "duration": 4.961}, {"text": "the GPU and so this kernel will operate", "start": 2419.079, "duration": 6.801}, {"text": "in loog Step many threads and you know", "start": 2422.28, "duration": 6.319}, {"text": "and they will cing loep and So-Cal Works", "start": 2425.88, "duration": 5.88}, {"text": "um at this you know on on on these um", "start": 2428.599, "duration": 4.52}, {"text": "streaming", "start": 2431.76, "duration": 3.319}, {"text": "multiprocessors and currently so that's", "start": 2433.119, "duration": 5.521}, {"text": "302 threats on Nvidia Hardware um this", "start": 2435.079, "duration": 6.52}, {"text": "is also called a s IMT so simply single", "start": 2438.64, "duration": 6.32}, {"text": "instruction multiple thread um pilation", "start": 2441.599, "duration": 5.161}, {"text": "as opposed to simd which is sort of", "start": 2444.96, "duration": 5.639}, {"text": "similar but you know um in the CPU world", "start": 2446.76, "duration": 5.4}, {"text": "and then a thread is just an execution", "start": 2450.599, "duration": 5.881}, {"text": "of a kernel with a given index and um so", "start": 2452.16, "duration": 7.08}, {"text": "each thread uses its index to access a", "start": 2456.48, "duration": 5.28}, {"text": "subset of data so you can do this in in", "start": 2459.24, "duration": 5.599}, {"text": "Cuda there is um variables that you can", "start": 2461.76, "duration": 6.72}, {"text": "access to know which threat you know is", "start": 2464.839, "duration": 5.081}, {"text": "currently executing and which data", "start": 2468.48, "duration": 3.599}, {"text": "element it should operate on and then", "start": 2469.92, "duration": 3.48}, {"text": "these threats they are grouped into", "start": 2472.079, "duration": 2.961}, {"text": "blocks which are guaranteed to execute", "start": 2473.4, "duration": 3.56}, {"text": "on the same multiprocessor so that's", "start": 2475.04, "duration": 4.039}, {"text": "basically how the you know block maps to", "start": 2476.96, "duration": 4.48}, {"text": "the underlying Hardware the reason for", "start": 2479.079, "duration": 4.0}, {"text": "doing that is that threats within a", "start": 2481.44, "duration": 3.8}, {"text": "threat block because they are executing", "start": 2483.079, "duration": 4.52}, {"text": "on the same multiprocessor they can", "start": 2485.24, "duration": 4.839}, {"text": "synchronize and share data so you can do", "start": 2487.599, "duration": 4.281}, {"text": "some sort of like cash blocking and you", "start": 2490.079, "duration": 3.361}, {"text": "know exchanging data between different", "start": 2491.88, "duration": 4.04}, {"text": "threads and a block but not between", "start": 2493.44, "duration": 4.24}, {"text": "threads between other blocks then you", "start": 2495.92, "duration": 3.679}, {"text": "need to go through Global", "start": 2497.68, "duration": 4.36}, {"text": "memory and the grid is basically all the", "start": 2499.599, "duration": 4.161}, {"text": "blocks that are that are executing at", "start": 2502.04, "duration": 4.4}, {"text": "the moment on the GP and you know if you", "start": 2503.76, "duration": 5.28}, {"text": "know the grid size so how many blocks", "start": 2506.44, "duration": 4.76}, {"text": "you have and the block size so how many", "start": 2509.04, "duration": 3.76}, {"text": "threads block then you know how many", "start": 2511.2, "duration": 4.04}, {"text": "threads you run in", "start": 2512.8, "duration": 4.519}, {"text": "total and that's something that you know", "start": 2515.24, "duration": 3.96}, {"text": "you see here in this", "start": 2517.319, "duration": 4.081}, {"text": "visualization um you know here we have", "start": 2519.2, "duration": 6.399}, {"text": "six blocks uh running and each block has", "start": 2521.4, "duration": 6.48}, {"text": "12 threads that's not what you would", "start": 2525.599, "duration": 4.52}, {"text": "normally do I told you you need to have", "start": 2527.88, "duration": 5.0}, {"text": "on the order of 10,000 threads running", "start": 2530.119, "duration": 5.96}, {"text": "and because of this lock step operation", "start": 2532.88, "duration": 5.0}, {"text": "and the work size you should have a", "start": 2536.079, "duration": 4.601}, {"text": "multiple of 32 threads in each block but", "start": 2537.88, "duration": 4.12}, {"text": "this is just here for simple", "start": 2540.68, "duration": 2.919}, {"text": "visualization", "start": 2542.0, "duration": 6.2}, {"text": "um um yeah just to get get you this idea", "start": 2543.599, "duration": 6.72}, {"text": "there are these buil-in variables you", "start": 2548.2, "duration": 4.8}, {"text": "know grid Dimension block ID that you", "start": 2550.319, "duration": 6.641}, {"text": "can use in Cuda um and thread indices", "start": 2553.0, "duration": 6.96}, {"text": "like block Dimension um thread ID and", "start": 2556.96, "duration": 7.599}, {"text": "you know with that you can access data", "start": 2559.96, "duration": 7.44}, {"text": "um differently for each for each thre", "start": 2564.559, "duration": 4.401}, {"text": "the corresponding", "start": 2567.4, "duration": 4.08}, {"text": "data and you know and then you have", "start": 2568.96, "duration": 4.399}, {"text": "these sort of keywords", "start": 2571.48, "duration": 5.4}, {"text": "like declaration specifier um under _", "start": 2573.359, "duration": 5.601}, {"text": "Global uncore uncore is a function that", "start": 2576.88, "duration": 3.6}, {"text": "executes on the", "start": 2578.96, "duration": 5.28}, {"text": "GPU um but is called from the host code", "start": 2580.48, "duration": 6.119}, {"text": "right and you see then you would compute", "start": 2584.24, "duration": 4.599}, {"text": "something like the um this is a vector", "start": 2586.599, "duration": 3.321}, {"text": "addition", "start": 2588.839, "duration": 5.641}, {"text": "colel um you know the thread ID um and", "start": 2589.92, "duration": 7.639}, {"text": "the stride that you have to use", "start": 2594.48, "duration": 5.52}, {"text": "um and then and then you can have this", "start": 2597.559, "duration": 5.0}, {"text": "operation here so in each you have to", "start": 2600.0, "duration": 5.0}, {"text": "imagine that many of these kernels are", "start": 2602.559, "duration": 4.321}, {"text": "launched at the same time so and each", "start": 2605.0, "duration": 3.64}, {"text": "thread that is executed this pel is", "start": 2606.88, "duration": 5.199}, {"text": "basically operating on its own thread ID", "start": 2608.64, "duration": 5.76}, {"text": "for these arrays that are being pass to", "start": 2612.079, "duration": 4.0}, {"text": "the pel and that's how you parallelize", "start": 2614.4, "duration": 4.919}, {"text": "with with Cuda um and then you have all", "start": 2616.079, "duration": 6.04}, {"text": "these um other functions in the API that", "start": 2619.319, "duration": 5.0}, {"text": "have a device memory allocation", "start": 2622.119, "duration": 5.72}, {"text": "deallocation memory copies and you know", "start": 2624.319, "duration": 4.8}, {"text": "at some point you need to launch the", "start": 2627.839, "duration": 3.361}, {"text": "kernels and this is where this you know", "start": 2629.119, "duration": 4.041}, {"text": "thread and block size comes in so you", "start": 2631.2, "duration": 5.48}, {"text": "have this triple angle Chevron um um", "start": 2633.16, "duration": 6.0}, {"text": "syntax here when you actually call the", "start": 2636.68, "duration": 5.879}, {"text": "function that you define in with", "start": 2639.16, "duration": 5.84}, {"text": "Cuda um and it tells you know how many", "start": 2642.559, "duration": 5.361}, {"text": "threads you're going to have in a block", "start": 2645.0, "duration": 4.88}, {"text": "and how many blocks you're executing so", "start": 2647.92, "duration": 5.679}, {"text": "this is this dim three um data type", "start": 2649.88, "duration": 5.439}, {"text": "here", "start": 2653.599, "duration": 4.841}, {"text": "um yeah and and so so this is an", "start": 2655.319, "duration": 5.561}, {"text": "additional information about the threat", "start": 2658.44, "duration": 4.44}, {"text": "block size um", "start": 2660.88, "duration": 7.12}, {"text": "and the um the the the memory on the GP", "start": 2662.88, "duration": 5.12}, {"text": "um so the Cuda memory hierarchy so if", "start": 2669.48, "duration": 7.079}, {"text": "you look about the grid you know and", "start": 2674.599, "duration": 4.201}, {"text": "each block and the threads that are", "start": 2676.559, "duration": 4.081}, {"text": "operating in a block you know all of", "start": 2678.8, "duration": 4.64}, {"text": "those have access to the global memory", "start": 2680.64, "duration": 5.0}, {"text": "um there's shared memory on each block", "start": 2683.44, "duration": 4.24}, {"text": "so on each multiprocessor so all threads", "start": 2685.64, "duration": 3.56}, {"text": "have access to this and then of course", "start": 2687.68, "duration": 3.08}, {"text": "each thread has its own", "start": 2689.2, "duration": 3.96}, {"text": "registers and so those have different", "start": 2690.76, "duration": 4.44}, {"text": "speed and you know these are things that", "start": 2693.16, "duration": 4.439}, {"text": "you want to take into account when you", "start": 2695.2, "duration": 6.32}, {"text": "are um writing your", "start": 2697.599, "duration": 7.201}, {"text": "program um generally want to avoid data", "start": 2701.52, "duration": 5.319}, {"text": "transfers between the CPU and the GPU", "start": 2704.8, "duration": 5.12}, {"text": "because these are slow I talked about", "start": 2706.839, "duration": 5.28}, {"text": "this before you also want to minimize", "start": 2709.92, "duration": 4.12}, {"text": "access to global memory so what you do", "start": 2712.119, "duration": 4.681}, {"text": "basically is something that's similar to", "start": 2714.04, "duration": 5.44}, {"text": "on CPU you know cache blocking you take", "start": 2716.8, "duration": 4.72}, {"text": "advantage of this fast shared memory by", "start": 2719.48, "duration": 4.32}, {"text": "tying data so you have this shared", "start": 2721.52, "duration": 4.44}, {"text": "memory here and so all threats that are", "start": 2723.8, "duration": 4.36}, {"text": "running in a single block you know they", "start": 2725.96, "duration": 5.44}, {"text": "could fetch data put it into the chat", "start": 2728.16, "duration": 5.56}, {"text": "memory opgrade all on the data and then", "start": 2731.4, "duration": 3.719}, {"text": "write it back when they're done into", "start": 2733.72, "duration": 4.32}, {"text": "Global memory so that's a typical um um", "start": 2735.119, "duration": 5.881}, {"text": "way to accelerate code and and write it", "start": 2738.04, "duration": 4.96}, {"text": "efficiently so when you have a problem", "start": 2741.0, "duration": 3.599}, {"text": "like a matrix multiplication for", "start": 2743.0, "duration": 4.8}, {"text": "instance here a matrix a and a matrix B", "start": 2744.599, "duration": 6.881}, {"text": "times B gives you C you know in order to", "start": 2747.8, "duration": 5.519}, {"text": "compute a single one of these output", "start": 2751.48, "duration": 3.599}, {"text": "elements what you have to do is", "start": 2753.319, "duration": 3.28}, {"text": "basically a", "start": 2755.079, "duration": 5.48}, {"text": "a um dot product between a row and a and", "start": 2756.599, "duration": 7.801}, {"text": "and a column Vector in matrices A and B", "start": 2760.559, "duration": 6.201}, {"text": "and then you know you'd have to think", "start": 2764.4, "duration": 4.4}, {"text": "about you know about the width and the", "start": 2766.76, "duration": 4.599}, {"text": "height of the result Matrix is a", "start": 2768.8, "duration": 7.2}, {"text": "multiple the block size um you know what", "start": 2771.359, "duration": 6.161}, {"text": "how many threads should I launch you do", "start": 2776.0, "duration": 2.839}, {"text": "something I launch one thread per", "start": 2777.52, "duration": 4.12}, {"text": "element of CI um of course you know you", "start": 2778.839, "duration": 5.361}, {"text": "can't go an infinite number of threats", "start": 2781.64, "duration": 4.0}, {"text": "so you need have to think about these", "start": 2784.2, "duration": 2.24}, {"text": "things", "start": 2785.64, "duration": 3.36}, {"text": "um and then you do this cache blocking", "start": 2786.44, "duration": 4.24}, {"text": "for performance optimization when you", "start": 2789.0, "duration": 4.079}, {"text": "load sub blocks like is visualized here", "start": 2790.68, "duration": 4.76}, {"text": "you know you would basically load this", "start": 2793.079, "duration": 4.76}, {"text": "block uh and then in the next iteration", "start": 2795.44, "duration": 4.56}, {"text": "this block so in the first iteration", "start": 2797.839, "duration": 4.041}, {"text": "this block and the next one this block", "start": 2800.0, "duration": 6.839}, {"text": "you know to um to um you know compute", "start": 2801.88, "duration": 6.64}, {"text": "different output elements in your in", "start": 2806.839, "duration": 5.201}, {"text": "your Matrix C so there are examples um", "start": 2808.52, "duration": 4.88}, {"text": "in the cuter samples that you can look", "start": 2812.04, "duration": 3.559}, {"text": "at for this and it's also described in", "start": 2813.4, "duration": 5.48}, {"text": "the computer programming guide um but in", "start": 2815.599, "duration": 4.921}, {"text": "reality of course we would use a highly", "start": 2818.88, "duration": 6.6}, {"text": "optimized ql library for um a matrix", "start": 2820.52, "duration": 7.0}, {"text": "multiplication um but that's about as", "start": 2825.48, "duration": 6.04}, {"text": "much as I wanted to tell about Cuda um I", "start": 2827.52, "duration": 8.2}, {"text": "just wanted to very briefly touch open", "start": 2831.52, "duration": 7.16}, {"text": "ACC as I said so this is a open standard", "start": 2835.72, "duration": 5.56}, {"text": "and you can use it with a um pray", "start": 2838.68, "duration": 4.0}, {"text": "compilers in place and you can use it", "start": 2841.28, "duration": 3.559}, {"text": "with the Nvidia compilers so used to be", "start": 2842.68, "duration": 5.24}, {"text": "PGI compilers but now Nvidia compilers", "start": 2844.839, "duration": 5.601}, {"text": "and there's also open in P but", "start": 2847.92, "duration": 6.639}, {"text": "um the Nvidia HPC SDK has the Nvidia for", "start": 2850.44, "duration": 6.399}, {"text": "compilers and I mentioned before this is", "start": 2854.559, "duration": 3.641}, {"text": "how you load it and then you can use", "start": 2856.839, "duration": 2.401}, {"text": "open", "start": 2858.2, "duration": 4.159}, {"text": "ACC um now that you've seen how complex", "start": 2859.24, "duration": 5.16}, {"text": "and complicated it can be to write um", "start": 2862.359, "duration": 5.121}, {"text": "uter code if you already have something", "start": 2864.4, "duration": 6.04}, {"text": "like the SXP function right that does", "start": 2867.48, "duration": 5.0}, {"text": "nothing else than this you know for Loop", "start": 2870.44, "duration": 5.639}, {"text": "here well um you can just add a prog", "start": 2872.48, "duration": 5.8}, {"text": "here ACC kernels and that should", "start": 2876.079, "duration": 4.641}, {"text": "instruct this is open ACC language it", "start": 2878.28, "duration": 3.88}, {"text": "should instruct to automatically", "start": 2880.72, "duration": 4.96}, {"text": "generate code that um parallelizes this", "start": 2882.16, "duration": 6.199}, {"text": "operation this loop on a", "start": 2885.68, "duration": 5.52}, {"text": "GPU um there's an equivalent for forun", "start": 2888.359, "duration": 5.281}, {"text": "here so it would works both in C of C++", "start": 2891.2, "duration": 4.72}, {"text": "and forrun and you know there's always", "start": 2893.64, "duration": 5.56}, {"text": "some sort of like um directives um with", "start": 2895.92, "duration": 6.0}, {"text": "some Clauses um and it just looks a", "start": 2899.2, "duration": 4.44}, {"text": "little bit different than for and C but", "start": 2901.92, "duration": 3.399}, {"text": "it's it's pretty straightforward and if", "start": 2903.64, "duration": 3.6}, {"text": "you're from with open m it's not not", "start": 2905.319, "duration": 4.28}, {"text": "very different and the ideas are very", "start": 2907.24, "duration": 6.16}, {"text": "similar um you can have if Clauses and", "start": 2909.599, "duration": 4.96}, {"text": "uh data", "start": 2913.4, "duration": 4.6}, {"text": "Clauses and know the data Clauses they", "start": 2914.559, "duration": 5.56}, {"text": "have something like copy and copy in and", "start": 2918.0, "duration": 4.359}, {"text": "copy out and that or create in present", "start": 2920.119, "duration": 4.761}, {"text": "for instance and so that has to do with", "start": 2922.359, "duration": 5.081}, {"text": "the data movement to and from the GPU", "start": 2924.88, "duration": 4.76}, {"text": "right because also open ACC needs to", "start": 2927.44, "duration": 4.6}, {"text": "create code that allocates the memory", "start": 2929.64, "duration": 5.159}, {"text": "copies data from the CPU to the GPU and", "start": 2932.04, "duration": 5.6}, {"text": "copies it back when when when when um", "start": 2934.799, "duration": 7.121}, {"text": "you're done and so forth and I will very", "start": 2937.64, "duration": 7.4}, {"text": "briefly skip over an example that I like", "start": 2941.92, "duration": 6.679}, {"text": "to show is a Jacobi iteration um it's", "start": 2945.04, "duration": 5.92}, {"text": "basically on a grid and all you have to", "start": 2948.599, "duration": 5.841}, {"text": "know for the moment is that", "start": 2950.96, "duration": 6.72}, {"text": "um what the algorithm does at you know", "start": 2954.44, "duration": 5.72}, {"text": "at each time step or step of its", "start": 2957.68, "duration": 4.04}, {"text": "iteration is basically compute the", "start": 2960.16, "duration": 4.12}, {"text": "average of neighboring grip points and", "start": 2961.72, "duration": 4.56}, {"text": "updates the corresponding uh data", "start": 2964.28, "duration": 4.64}, {"text": "element of grid point until it", "start": 2966.28, "duration": 4.839}, {"text": "converges uh to a certain numerical", "start": 2968.92, "duration": 4.439}, {"text": "accuracy that's how the code looks like", "start": 2971.119, "duration": 3.761}, {"text": "it's basically goes through this", "start": 2973.359, "duration": 4.041}, {"text": "two-dimensional grid right Loops through", "start": 2974.88, "duration": 4.28}, {"text": "I and J through this two dimensional", "start": 2977.4, "duration": 4.159}, {"text": "grid don't need to know much about this", "start": 2979.16, "duration": 5.159}, {"text": "this is this is basically the you know", "start": 2981.559, "duration": 4.441}, {"text": "Computing the average of the neighboring", "start": 2984.319, "duration": 4.76}, {"text": "elements you see here I + one IUS one J", "start": 2986.0, "duration": 4.72}, {"text": "minus one J plus one right so these are", "start": 2989.079, "duration": 3.921}, {"text": "the neighboring elements and then", "start": 2990.72, "duration": 5.119}, {"text": "computes the difference in the current", "start": 2993.0, "duration": 5.319}, {"text": "iteration to the previous experation and", "start": 2995.839, "duration": 3.841}, {"text": "of course you know this can be done in", "start": 2998.319, "duration": 3.081}, {"text": "parallel because we have many grid", "start": 2999.68, "duration": 5.36}, {"text": "points and you know if you say", "start": 3001.4, "duration": 8.08}, {"text": "well open ACC go generate tels for that", "start": 3005.04, "duration": 6.4}, {"text": "and and parallelize it for", "start": 3009.48, "duration": 4.96}, {"text": "me that works you know you can use the", "start": 3011.44, "duration": 6.32}, {"text": "PGI compilers or the or the um Nvidia", "start": 3014.44, "duration": 5.359}, {"text": "compilers compile the code it will", "start": 3017.76, "duration": 3.839}, {"text": "automatically generate code and get you", "start": 3019.799, "duration": 3.401}, {"text": "information about you know what it's", "start": 3021.599, "duration": 5.161}, {"text": "actually doing um it generates Tesla", "start": 3023.2, "duration": 6.08}, {"text": "code that means for for the um data", "start": 3026.76, "duration": 6.96}, {"text": "center gpus um gang and Vector here is", "start": 3029.28, "duration": 7.16}, {"text": "you know you see it maps to block ID and", "start": 3033.72, "duration": 5.599}, {"text": "thread ID so that's generates basically", "start": 3036.44, "duration": 7.24}, {"text": "the corresponding cuter code for you um", "start": 3039.319, "duration": 7.8}, {"text": "and well if you do that and you execute", "start": 3043.68, "duration": 5.8}, {"text": "the code here's an example on an S", "start": 3047.119, "duration": 6.0}, {"text": "expans GPU node if I parallelize this", "start": 3049.48, "duration": 7.119}, {"text": "code with open MP you know I get a a", "start": 3053.119, "duration": 7.48}, {"text": "decent speed up of up about six with", "start": 3056.599, "duration": 7.321}, {"text": "this simple code on on hcpu course now", "start": 3060.599, "duration": 5.841}, {"text": "if I use open ACC it becomes incredibly", "start": 3063.92, "duration": 5.6}, {"text": "slow so something has been entirely", "start": 3066.44, "duration": 7.6}, {"text": "wrong right and you can profile the code", "start": 3069.52, "duration": 6.4}, {"text": "um that there's an environment variable", "start": 3074.04, "duration": 5.039}, {"text": "that you can use or there's a flag um", "start": 3075.92, "duration": 6.08}, {"text": "compilation that you can use", "start": 3079.079, "duration": 5.72}, {"text": "and this is profiling data that then has", "start": 3082.0, "duration": 5.0}, {"text": "been generated when you run the", "start": 3084.799, "duration": 5.201}, {"text": "code and you can see that a data region", "start": 3087.0, "duration": 6.079}, {"text": "is is encountered in the code with copy", "start": 3090.0, "duration": 5.839}, {"text": "in a copy out transfers and that takes", "start": 3093.079, "duration": 6.76}, {"text": "22.5 seconds um whereas the kernel", "start": 3095.839, "duration": 5.881}, {"text": "execution where it actually does this", "start": 3099.839, "duration": 4.641}, {"text": "computation takes only 1.5 seconds so we", "start": 3101.72, "duration": 4.44}, {"text": "spend all the time in data transfers", "start": 3104.48, "duration": 5.119}, {"text": "between host and device um", "start": 3106.16, "duration": 5.679}, {"text": "however um that is actually not", "start": 3109.599, "duration": 5.0}, {"text": "necessary right so we have these these", "start": 3111.839, "duration": 4.401}, {"text": "um two dimensional matrices with the", "start": 3114.599, "duration": 5.081}, {"text": "grid points um and every time this Loop", "start": 3116.24, "duration": 6.559}, {"text": "is is basically executed we copy to the", "start": 3119.68, "duration": 6.04}, {"text": "GPU and then back to the host and that's", "start": 3122.799, "duration": 4.681}, {"text": "actually necessary we just can keep", "start": 3125.72, "duration": 3.72}, {"text": "everything on GQ but the compiler", "start": 3127.48, "duration": 3.56}, {"text": "doesn't know without you telling it so", "start": 3129.44, "duration": 3.08}, {"text": "that's why it's important to understand", "start": 3131.04, "duration": 5.2}, {"text": "how the GP architecture Works um and if", "start": 3132.52, "duration": 5.92}, {"text": "you understand CA then this becomes", "start": 3136.24, "duration": 4.4}, {"text": "simple and straight forward as", "start": 3138.44, "duration": 6.08}, {"text": "well so we need to tell the compiler", "start": 3140.64, "duration": 7.52}, {"text": "that in the code that comes below um we", "start": 3144.52, "duration": 7.36}, {"text": "actually um copy the", "start": 3148.16, "duration": 6.959}, {"text": "um a so the input data this", "start": 3151.88, "duration": 6.719}, {"text": "Matrix and we create a new outside of", "start": 3155.119, "duration": 5.641}, {"text": "this Loop so then the compiler knows", "start": 3158.599, "duration": 4.561}, {"text": "well there's a data region that reaches", "start": 3160.76, "duration": 5.12}, {"text": "from here until the end and I don't need", "start": 3163.16, "duration": 4.76}, {"text": "to copy the data for them back every", "start": 3165.88, "duration": 6.0}, {"text": "time and if you do that now you get a", "start": 3167.92, "duration": 7.879}, {"text": "speed up that is um uh", "start": 3171.88, "duration": 8.239}, {"text": "6.7 times faster than eight open and P", "start": 3175.799, "duration": 6.121}, {"text": "threads so that's actually respectable", "start": 3180.119, "duration": 4.24}, {"text": "for very simple way of", "start": 3181.92, "duration": 4.6}, {"text": "writing um so let me look at the time", "start": 3184.359, "duration": 4.521}, {"text": "there's only five minutes left", "start": 3186.52, "duration": 4.799}, {"text": "um there's yeah some tips and tricks", "start": 3188.88, "duration": 4.919}, {"text": "about open ACC I will share those slides", "start": 3191.319, "duration": 4.401}, {"text": "what's really important is to tell you", "start": 3193.799, "duration": 3.161}, {"text": "how to use", "start": 3195.72, "duration": 5.8}, {"text": "expon um which um is why many of you are", "start": 3196.96, "duration": 8.28}, {"text": "here so this is our um main HPC resource", "start": 3201.52, "duration": 5.44}, {"text": "SCC that has been launched in the fall", "start": 3205.24, "duration": 4.879}, {"text": "of 2020 important piece is we have 52", "start": 3206.96, "duration": 5.56}, {"text": "GPU nodes with 208", "start": 3210.119, "duration": 7.601}, {"text": "gpus um so this is how the re look like", "start": 3212.52, "duration": 8.76}, {"text": "um so there's a total of 28100", "start": 3217.72, "duration": 7.119}, {"text": "gpus um it's connected to our storage", "start": 3221.28, "duration": 8.6}, {"text": "systems um same way as the CPU nodes um", "start": 3224.839, "duration": 8.041}, {"text": "uh each of those GPU has two 204 Inon", "start": 3229.88, "duration": 7.32}, {"text": "gold processors um 84 gab of RAM and", "start": 3232.88, "duration": 8.4}, {"text": "four of these Nvidia v00 sxm2 GPS um", "start": 3237.2, "duration": 6.359}, {"text": "that's more or less how how how one of", "start": 3241.28, "duration": 3.88}, {"text": "those nodes sort of looks like right and", "start": 3243.559, "duration": 3.121}, {"text": "then you can see the gpus that are", "start": 3245.16, "duration": 4.199}, {"text": "sitting here in one of those", "start": 3246.68, "duration": 5.36}, {"text": "nodes um they're all connected by fast", "start": 3249.359, "duration": 4.72}, {"text": "interconnect so if you have have a code", "start": 3252.04, "duration": 4.36}, {"text": "that actually um Can execute on multiple", "start": 3254.079, "duration": 6.121}, {"text": "gpus you can have um you can exploit", "start": 3256.4, "duration": 6.36}, {"text": "that um so you can see the note", "start": 3260.2, "duration": 5.159}, {"text": "technology um basically how the GP are", "start": 3262.76, "duration": 4.599}, {"text": "connected to each other by executing the", "start": 3265.359, "duration": 7.041}, {"text": "Nvidia SMI command with a topology", "start": 3267.359, "duration": 9.401}, {"text": "option and yeah so very slowly this you", "start": 3272.4, "duration": 6.0}, {"text": "probably have seen if you log into", "start": 3276.76, "duration": 4.28}, {"text": "expans um you're greeted by the expanse", "start": 3278.4, "duration": 4.56}, {"text": "um welcome", "start": 3281.04, "duration": 5.64}, {"text": "screen um One Way or", "start": 3282.96, "duration": 6.399}, {"text": "Another that keeps changing and updated", "start": 3286.68, "duration": 4.159}, {"text": "but the important piece is now how do", "start": 3289.359, "duration": 3.801}, {"text": "you access the GPU nodes right so you", "start": 3290.839, "duration": 4.321}, {"text": "can do this in your bash batch uh", "start": 3293.16, "duration": 4.199}, {"text": "submission script or you could do it", "start": 3295.16, "duration": 5.199}, {"text": "interactively with an srun command um or", "start": 3297.359, "duration": 4.401}, {"text": "an S allo", "start": 3300.359, "duration": 3.841}, {"text": "command important part is we have two", "start": 3301.76, "duration": 4.359}, {"text": "partitions one is called", "start": 3304.2, "duration": 5.32}, {"text": "GPU um that will give you an entire GPU", "start": 3306.119, "duration": 6.601}, {"text": "node with all four gpus right um and", "start": 3309.52, "duration": 5.079}, {"text": "then there's GPU shared which you should", "start": 3312.72, "duration": 4.52}, {"text": "use if you need only one or two or three", "start": 3314.599, "duration": 6.96}, {"text": "gpus right um so qualify this with a D-", "start": 3317.24, "duration": 6.76}, {"text": "gpus um", "start": 3321.559, "duration": 5.361}, {"text": "command uh where you specify how many", "start": 3324.0, "duration": 4.92}, {"text": "GPS you actually need so here's an", "start": 3326.92, "duration": 5.28}, {"text": "example with srun um where you obtain", "start": 3328.92, "duration": 6.52}, {"text": "access to a single GPU so I'm asking for", "start": 3332.2, "duration": 5.839}, {"text": "a single node on the GPU shared", "start": 3335.44, "duration": 5.52}, {"text": "partition and uh with one", "start": 3338.039, "duration": 6.8}, {"text": "GPU um I want to have a single task NPI", "start": 3340.96, "duration": 7.24}, {"text": "task um but 10 CPUs per task so then", "start": 3344.839, "duration": 5.121}, {"text": "then you could run say an open and P", "start": 3348.2, "duration": 4.28}, {"text": "code or you could compile um something", "start": 3349.96, "duration": 5.76}, {"text": "in parallel on that note so", "start": 3352.48, "duration": 5.839}, {"text": "if you do that um I'm asking here for", "start": 3355.72, "duration": 5.8}, {"text": "half an hour um and execute it you know", "start": 3358.319, "duration": 5.081}, {"text": "at some point you'll loog into that Noe", "start": 3361.52, "duration": 3.72}, {"text": "when you get access and then we can you", "start": 3363.4, "duration": 3.679}, {"text": "can use that", "start": 3365.24, "duration": 4.92}, {"text": "now important note is to make requests", "start": 3367.079, "duration": 4.881}, {"text": "proportional to the number of available", "start": 3370.16, "duration": 5.32}, {"text": "resources so if you have 4 v00 gpus and", "start": 3371.96, "duration": 6.399}, {"text": "40 CPU cores and 374 gab of RAM", "start": 3375.48, "duration": 5.559}, {"text": "available on each of those nodes don't", "start": 3378.359, "duration": 6.921}, {"text": "request more than 10 cores or 93 G by", "start": 3381.039, "duration": 6.681}, {"text": "Ram per GPU that you need otherwise you", "start": 3385.28, "duration": 4.559}, {"text": "will be charged proportionally more time", "start": 3387.72, "duration": 3.92}, {"text": "because you won't need the resources for", "start": 3389.839, "duration": 5.441}, {"text": "somebody else to use and once you're on", "start": 3391.64, "duration": 6.32}, {"text": "the noes you can use and load the GPU", "start": 3395.28, "duration": 4.4}, {"text": "related modules that I've been talking", "start": 3397.96, "duration": 4.159}, {"text": "about so do a module reset the module", "start": 3399.68, "duration": 4.119}, {"text": "perch is not necessary but if you are", "start": 3402.119, "duration": 3.841}, {"text": "paranoid do that perch basically removes", "start": 3403.799, "duration": 3.401}, {"text": "everything that's", "start": 3405.96, "duration": 3.48}, {"text": "loaded um and then you can load the", "start": 3407.2, "duration": 6.68}, {"text": "compter toolkit or the um Nvidia HPC", "start": 3409.44, "duration": 7.599}, {"text": "SDK um so here's an example you know", "start": 3413.88, "duration": 6.04}, {"text": "where I got interactive access to GPU", "start": 3417.039, "duration": 4.401}, {"text": "node", "start": 3419.92, "duration": 3.84}, {"text": "um and", "start": 3421.44, "duration": 5.919}, {"text": "then uh yeah I believe this should be", "start": 3423.76, "duration": 5.319}, {"text": "loaded automatically otherwise we have", "start": 3427.359, "duration": 4.44}, {"text": "to load the tool kit um and then if you", "start": 3429.079, "duration": 4.441}, {"text": "type Nvidia SMI you will see some", "start": 3431.799, "duration": 3.881}, {"text": "information about the GPU that's", "start": 3433.52, "duration": 3.92}, {"text": "accessible to you in this case it would", "start": 3435.68, "duration": 3.679}, {"text": "be a Tesla v00 right so you get", "start": 3437.44, "duration": 3.08}, {"text": "information", "start": 3439.359, "duration": 4.68}, {"text": "about the hardware with this Nvidia SMI", "start": 3440.52, "duration": 5.279}, {"text": "command the system management interface", "start": 3444.039, "duration": 3.161}, {"text": "and you get information about what's", "start": 3445.799, "duration": 3.24}, {"text": "running on the GPU in this case there's", "start": 3447.2, "duration": 3.04}, {"text": "nothing running and there should be", "start": 3449.039, "duration": 3.401}, {"text": "nothing running uh unless you start", "start": 3450.24, "duration": 4.52}, {"text": "something because um you got access to a", "start": 3452.44, "duration": 5.84}, {"text": "GPU um entirely for yourself so no no", "start": 3454.76, "duration": 4.96}, {"text": "running processes", "start": 3458.28, "duration": 4.88}, {"text": "found um there are multiple gpus but", "start": 3459.72, "duration": 5.16}, {"text": "they're shared but you get dedicated", "start": 3463.16, "duration": 4.0}, {"text": "access to a single GPU if you request a", "start": 3464.88, "duration": 3.56}, {"text": "single", "start": 3467.16, "duration": 6.32}, {"text": "GPU um if you load the um fter to kit or", "start": 3468.44, "duration": 7.24}, {"text": "nvbc you can check check for the version", "start": 3473.48, "duration": 5.639}, {"text": "of the NC compiler nvcc to see that it's", "start": 3475.68, "duration": 6.359}, {"text": "installed or you when you have nbpc", "start": 3479.119, "duration": 5.761}, {"text": "loaded you can check for the pgcc so the", "start": 3482.039, "duration": 5.881}, {"text": "PGI compiler which is just a symbolic", "start": 3484.88, "duration": 5.64}, {"text": "link to the Nvidia compiler so you", "start": 3487.92, "duration": 6.04}, {"text": "should get the same output um um so NVC", "start": 3490.52, "duration": 5.559}, {"text": "is the Nvidia", "start": 3493.96, "duration": 5.839}, {"text": "compil um what I recommend always to", "start": 3496.079, "duration": 6.801}, {"text": "look at is the um P kit samples you know", "start": 3499.799, "duration": 5.441}, {"text": "you can clone them from GitHub and", "start": 3502.88, "duration": 4.239}, {"text": "they're great resource so if you look", "start": 3505.24, "duration": 4.4}, {"text": "into that there is introductory examples", "start": 3507.119, "duration": 5.081}, {"text": "there is examples to use pter libraries", "start": 3509.64, "duration": 6.84}, {"text": "um there's Utilities in there etc etc", "start": 3512.2, "duration": 8.119}, {"text": "and um you can just compile those with a", "start": 3516.48, "duration": 6.079}, {"text": "make Command if you have asked for", "start": 3520.319, "duration": 4.24}, {"text": "single GP in 10 course you can compile", "start": 3522.559, "duration": 4.881}, {"text": "it in parallel on 10 course so it goes", "start": 3524.559, "duration": 5.881}, {"text": "faster um the executables will then", "start": 3527.44, "duration": 5.32}, {"text": "generated in the subd", "start": 3530.44, "duration": 6.679}, {"text": "directory um or this takes some time you", "start": 3532.76, "duration": 9.079}, {"text": "can also compile individual um examples", "start": 3537.119, "duration": 8.161}, {"text": "sorry there a typ here you know for", "start": 3541.839, "duration": 5.041}, {"text": "instance if you want to compile the", "start": 3545.28, "duration": 5.079}, {"text": "device query uh examples example from", "start": 3546.88, "duration": 6.04}, {"text": "the utilities example folder you go into", "start": 3550.359, "duration": 5.0}, {"text": "that folder and then you just have mic", "start": 3552.92, "duration": 3.679}, {"text": "you can also", "start": 3555.359, "duration": 5.161}, {"text": "tell uh that the the target um", "start": 3556.599, "duration": 7.081}, {"text": "architecture so SMS equal 70 is is is", "start": 3560.52, "duration": 5.36}, {"text": "for the voltage so you don't generate", "start": 3563.68, "duration": 4.119}, {"text": "code for other architectures that you", "start": 3565.88, "duration": 5.479}, {"text": "actually don't have on expon um it just", "start": 3567.799, "duration": 6.481}, {"text": "makes the compilation faster um and then", "start": 3571.359, "duration": 5.361}, {"text": "you can execute this code and device", "start": 3574.28, "duration": 4.4}, {"text": "query is exactly what I told you", "start": 3576.72, "duration": 3.76}, {"text": "somewhere earlier is that you know you", "start": 3578.68, "duration": 3.96}, {"text": "have cuter API function calls that can", "start": 3580.48, "duration": 4.839}, {"text": "Cary information about the device and so", "start": 3582.64, "duration": 5.8}, {"text": "you see information like oh I found one", "start": 3585.319, "duration": 6.681}, {"text": "CU capable device it's a test like 100", "start": 3588.44, "duration": 6.04}, {"text": "um you know it has so so many megabytes", "start": 3592.0, "duration": 5.799}, {"text": "and C of course Etc so you can get", "start": 3594.48, "duration": 5.28}, {"text": "information about the", "start": 3597.799, "duration": 4.601}, {"text": "hardware um and here are examples for", "start": 3599.76, "duration": 3.839}, {"text": "matrix", "start": 3602.4, "duration": 5.399}, {"text": "multiplication um the is an introductory", "start": 3603.599, "duration": 6.561}, {"text": "example so you compile that it will", "start": 3607.799, "duration": 4.081}, {"text": "execute the matrix multiplication you", "start": 3610.16, "duration": 3.439}, {"text": "can check the source code to see how it", "start": 3611.88, "duration": 3.84}, {"text": "was implemented it spits out the", "start": 3613.599, "duration": 3.76}, {"text": "performance that you get in this case", "start": 3615.72, "duration": 4.48}, {"text": "about 3.3 ter", "start": 3617.359, "duration": 5.081}, {"text": "teraflops and if you compare this for", "start": 3620.2, "duration": 4.08}, {"text": "instance to the matrix multiplication", "start": 3622.44, "duration": 4.56}, {"text": "example using the um UDA Library the", "start": 3624.28, "duration": 6.279}, {"text": "qast library uh you execute that that", "start": 3627.0, "duration": 6.16}, {"text": "gives you about 7.5 T flops so this is a", "start": 3630.559, "duration": 4.641}, {"text": "hand optimized version but this is the", "start": 3633.16, "duration": 5.0}, {"text": "real you know super tuned version so you", "start": 3635.2, "duration": 4.68}, {"text": "can see why you want to use libraries", "start": 3638.16, "duration": 3.399}, {"text": "when you can because you always get a", "start": 3639.88, "duration": 4.679}, {"text": "very very high performance and with that", "start": 3641.559, "duration": 7.321}, {"text": "you know I am at the top of the hour um", "start": 3644.559, "duration": 6.361}, {"text": "and I've gone a little bit over it I", "start": 3648.88, "duration": 3.719}, {"text": "will share these slides for you there's", "start": 3650.92, "duration": 5.199}, {"text": "a few additional information about um", "start": 3652.599, "duration": 6.601}, {"text": "Nvidia profiling tools NV Pro nvvp and", "start": 3656.119, "duration": 6.601}, {"text": "inside tools and also", "start": 3659.2, "duration": 7.0}, {"text": "um how to profile some of the machine", "start": 3662.72, "duration": 5.359}, {"text": "learning Frameworks with Nvidia", "start": 3666.2, "duration": 5.2}, {"text": "profiling tools um but we don't have", "start": 3668.079, "duration": 5.401}, {"text": "time to look at this now instead you", "start": 3671.4, "duration": 4.679}, {"text": "know if there's a question or two I'd be", "start": 3673.48, "duration": 5.96}, {"text": "happy to um still answer that so one", "start": 3676.079, "duration": 5.921}, {"text": "question is is there a CU like library", "start": 3679.44, "duration": 5.28}, {"text": "or GPU API for speeding up very large", "start": 3682.0, "duration": 6.799}, {"text": "database joints for say uh", "start": 3684.72, "duration": 6.56}, {"text": "postgress that is a question I don't", "start": 3688.799, "duration": 5.32}, {"text": "know how to answer", "start": 3691.28, "duration": 6.4}, {"text": "um I'm not familiar with database very", "start": 3694.119, "duration": 5.561}, {"text": "much but I'm sure", "start": 3697.68, "duration": 5.48}, {"text": "there you know I mean I I I just don't", "start": 3699.68, "duration": 5.32}, {"text": "know I mean it's something that I I", "start": 3703.16, "duration": 6.52}, {"text": "would recommend to look on on on", "start": 3705.0, "duration": 7.359}, {"text": "nvidia's website do some search or", "start": 3709.68, "duration": 4.8}, {"text": "perhaps contact our user Services Group", "start": 3712.359, "duration": 5.24}, {"text": "they might have um had some interactions", "start": 3714.48, "duration": 4.52}, {"text": "with database", "start": 3717.599, "duration": 3.561}, {"text": "developers thank you Andy I'll put the", "start": 3719.0, "duration": 5.16}, {"text": "user Services email in the chat as well", "start": 3721.16, "duration": 5.6}, {"text": "um next is is there a good way to run", "start": 3724.16, "duration": 5.879}, {"text": "multi Noe multi-gpu programs on", "start": 3726.76, "duration": 6.88}, {"text": "expans yes um you can you can do that um", "start": 3730.039, "duration": 4.721}, {"text": "there are different ways of course", "start": 3733.64, "duration": 2.8}, {"text": "depending on the code that you're using", "start": 3734.76, "duration": 3.24}, {"text": "or the code if you're writing your own", "start": 3736.44, "duration": 4.159}, {"text": "code different ways to do that first of", "start": 3738.0, "duration": 5.2}, {"text": "all on a single note you know you can", "start": 3740.599, "duration": 4.48}, {"text": "use multiple gpus", "start": 3743.2, "duration": 4.24}, {"text": "um using something that's called screams", "start": 3745.079, "duration": 5.52}, {"text": "or you you could have um an open andp", "start": 3747.44, "duration": 5.879}, {"text": "program and each CPU thread would", "start": 3750.599, "duration": 5.121}, {"text": "control a GPU or you can have an NPI", "start": 3753.319, "duration": 4.72}, {"text": "program and each task um uses a single", "start": 3755.72, "duration": 4.16}, {"text": "GPU that's what we do for our quantum", "start": 3758.039, "duration": 5.841}, {"text": "chemistry code and on expans um we've", "start": 3759.88, "duration": 6.439}, {"text": "tested it up to four nodes and 16 gpus", "start": 3763.88, "duration": 4.64}, {"text": "for instance then then you'd have to use", "start": 3766.319, "duration": 5.361}, {"text": "MPI you know for communication across", "start": 3768.52, "duration": 5.16}, {"text": "notes um but there are different ways to", "start": 3771.68, "duration": 3.8}, {"text": "implement that and and it's certainly", "start": 3773.68, "duration": 3.2}, {"text": "possible to", "start": 3775.48, "duration": 6.079}, {"text": "do thank you Andy and last one um is is", "start": 3776.88, "duration": 8.88}, {"text": "SDC using sentos if so what OS might the", "start": 3781.559, "duration": 7.0}, {"text": "center be migrating over", "start": 3785.76, "duration": 6.4}, {"text": "to oh um yeah you used to run sentos", "start": 3788.559, "duration": 5.641}, {"text": "it's Rocky lus at the", "start": 3792.16, "duration": 5.8}, {"text": "moment it's basically a a a continuation", "start": 3794.2, "duration": 5.879}, {"text": "of centers effort on some things it's", "start": 3797.96, "duration": 3.76}, {"text": "equivalent of centers but it's called", "start": 3800.079, "duration": 3.2}, {"text": "Rocky", "start": 3801.72, "duration": 4.0}, {"text": "l", "start": 3803.279, "duration": 4.601}, {"text": "and with that that is the last question", "start": 3805.72, "duration": 5.16}, {"text": "in the chat all right great um yeah so", "start": 3807.88, "duration": 6.159}, {"text": "thanks for um joining the webinar and uh", "start": 3810.88, "duration": 8.0}, {"text": "I hope uh I was able to um tell you", "start": 3814.039, "duration": 9.04}, {"text": "information that is useful for you", "start": 3818.88, "duration": 4.199}]