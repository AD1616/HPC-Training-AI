[{"text": "yep hi my name is Bob sis I'm the", "start": 1.0, "duration": 3.359}, {"text": "Director of Education and Training here", "start": 2.8, "duration": 4.639}, {"text": "at SSC a physicist by training and now a", "start": 4.359, "duration": 5.881}, {"text": "computational science generalist um so", "start": 7.439, "duration": 4.481}, {"text": "I'm going to be you know maybe changing", "start": 10.24, "duration": 4.92}, {"text": "gears a little bit compare relative to", "start": 11.92, "duration": 6.48}, {"text": "what I had originally promised um you're", "start": 15.16, "duration": 4.92}, {"text": "you're going to be hearing about CPU", "start": 18.4, "duration": 3.879}, {"text": "Computing and GPU Computing from our", "start": 20.08, "duration": 4.119}, {"text": "upcoming speakers um but I'm going to be", "start": 22.279, "duration": 4.041}, {"text": "going a little bit more into kind of", "start": 24.199, "duration": 4.721}, {"text": "General um concepts of parallel", "start": 26.32, "duration": 4.24}, {"text": "computing", "start": 28.92, "duration": 4.159}, {"text": "there's a link here I'll post this in", "start": 30.56, "duration": 5.96}, {"text": "the post this in the chat um just a few", "start": 33.079, "duration": 5.081}, {"text": "simple exercises that you might want to", "start": 36.52, "duration": 3.519}, {"text": "do when we're all done and that you", "start": 38.16, "duration": 4.84}, {"text": "might find um that that GitHub material", "start": 40.039, "duration": 4.761}, {"text": "useful if you're putting together", "start": 43.0, "duration": 4.92}, {"text": "allocation requests or creating scaling", "start": 44.8, "duration": 5.599}, {"text": "scaling benchmarking", "start": 47.92, "duration": 4.72}, {"text": "studies okay so break down what we're", "start": 50.399, "duration": 3.601}, {"text": "going to be doing start off with an", "start": 52.64, "duration": 3.36}, {"text": "introduction we're then going to go into", "start": 54.0, "duration": 5.48}, {"text": "processes threads MPI and openmp talk", "start": 56.0, "duration": 5.64}, {"text": "about how hardd applications that use", "start": 59.48, "duration": 5.56}, {"text": "both processes and threads om doll's law", "start": 61.64, "duration": 5.76}, {"text": "the other limits on scalability running", "start": 65.04, "duration": 4.88}, {"text": "parallel applications and then finally", "start": 67.4, "duration": 4.68}, {"text": "wrapping up with where to go next and a", "start": 69.92, "duration": 4.44}, {"text": "few", "start": 72.08, "duration": 2.28}, {"text": "conclusions so you know I'm going to say", "start": 74.479, "duration": 4.921}, {"text": "that the reason most of you are here is", "start": 77.479, "duration": 3.801}, {"text": "that you're is that you have machine", "start": 79.4, "duration": 4.28}, {"text": "learning workloads that have grown to", "start": 81.28, "duration": 3.92}, {"text": "the point where you can no longer run", "start": 83.68, "duration": 3.68}, {"text": "them on your local resources so on your", "start": 85.2, "duration": 6.279}, {"text": "laptops on your desktop systems so you", "start": 87.36, "duration": 6.36}, {"text": "know we're we're assuming that that", "start": 91.479, "duration": 4.841}, {"text": "everybody coming coming to the Sim on to", "start": 93.72, "duration": 4.56}, {"text": "already knows machine learning you're", "start": 96.32, "duration": 3.839}, {"text": "not here to learn the techniques you're", "start": 98.28, "duration": 3.479}, {"text": "learn you're learning how to do machine", "start": 100.159, "duration": 4.0}, {"text": "learning at scale and that you may need", "start": 101.759, "duration": 5.561}, {"text": "to use parallel Computing so this talk", "start": 104.159, "duration": 5.24}, {"text": "I'm going to say wasn't developed", "start": 107.32, "duration": 4.399}, {"text": "specifically for symol this is a more", "start": 109.399, "duration": 3.841}, {"text": "General presentation on Parallel", "start": 111.719, "duration": 3.801}, {"text": "Computing Concepts but the ideas that", "start": 113.24, "duration": 3.879}, {"text": "we're going to cover here are still", "start": 115.52, "duration": 3.559}, {"text": "going to apply in the context of machine", "start": 117.119, "duration": 5.161}, {"text": "learning so as you", "start": 119.079, "duration": 5.801}, {"text": "um you know as as you learn about", "start": 122.28, "duration": 4.479}, {"text": "techniques to to run in parallel use", "start": 124.88, "duration": 5.04}, {"text": "multiple CPUs multiple gpus kind of keep", "start": 126.759, "duration": 6.401}, {"text": "keep in mind what we're covering here", "start": 129.92, "duration": 6.16}, {"text": "today okay so I'm going to say this", "start": 133.16, "duration": 5.04}, {"text": "session is intended for anybody who", "start": 136.08, "duration": 4.2}, {"text": "currently runs plans to run or is", "start": 138.2, "duration": 4.24}, {"text": "thinking about running applications on", "start": 140.28, "duration": 4.599}, {"text": "Parallel", "start": 142.44, "duration": 4.079}, {"text": "computers um if you're going to be", "start": 144.879, "duration": 3.681}, {"text": "running a proposal for computer time and", "start": 146.519, "duration": 4.161}, {"text": "anything from a Campus Club cluster to a", "start": 148.56, "duration": 4.28}, {"text": "nationally allocated", "start": 150.68, "duration": 3.76}, {"text": "resource if you're going to be", "start": 152.84, "duration": 3.6}, {"text": "purchasing time on compute resources", "start": 154.44, "duration": 4.32}, {"text": "then you want to maximize your return on", "start": 156.44, "duration": 4.24}, {"text": "investment if you're considering", "start": 158.76, "duration": 4.44}, {"text": "purchasing hardware for your lab or if", "start": 160.68, "duration": 4.199}, {"text": "you're just simply curious about", "start": 163.2, "duration": 3.959}, {"text": "parallel Computing and I hope at the", "start": 164.879, "duration": 4.401}, {"text": "very least this last bullet applies to", "start": 167.159, "duration": 4.08}, {"text": "everybody here again because we are", "start": 169.28, "duration": 3.16}, {"text": "interested in", "start": 171.239, "duration": 6.241}, {"text": "doing in doing machine learning at", "start": 172.44, "duration": 7.96}, {"text": "scale so the motivation for this to", "start": 177.48, "duration": 5.039}, {"text": "is you know much of the training in", "start": 180.4, "duration": 4.0}, {"text": "parallel Computing is targeted people", "start": 182.519, "duration": 3.321}, {"text": "who write their own parallel", "start": 184.4, "duration": 3.08}, {"text": "applications and it focuses on", "start": 185.84, "duration": 3.84}, {"text": "programmer topics and I know that some", "start": 187.48, "duration": 4.08}, {"text": "of you here are programmers you are", "start": 189.68, "duration": 4.119}, {"text": "developers but a lot of you are also", "start": 191.56, "duration": 4.52}, {"text": "using um existing thirdparty", "start": 193.799, "duration": 4.72}, {"text": "applications maybe making minor tweaks", "start": 196.08, "duration": 5.04}, {"text": "to them and we find that outside of the", "start": 198.519, "duration": 4.8}, {"text": "developer and computer science community", "start": 201.12, "duration": 3.479}, {"text": "that a lot of you aren't getting", "start": 203.319, "duration": 5.441}, {"text": "exposure to the to the basics parallel", "start": 204.599, "duration": 6.36}, {"text": "computing I yeah covering the expo", "start": 208.76, "duration": 4.72}, {"text": "Little already um so even if you don't", "start": 210.959, "duration": 4.721}, {"text": "write code even if you're using somebody", "start": 213.48, "duration": 4.44}, {"text": "else's application if you're using", "start": 215.68, "duration": 4.88}, {"text": "existing deep learning workflows if", "start": 217.92, "duration": 5.44}, {"text": "you're scaling us scaling up you still", "start": 220.56, "duration": 5.239}, {"text": "need to understand some of the basics of", "start": 223.36, "duration": 4.4}, {"text": "parallel Computing so that you can make", "start": 225.799, "duration": 4.761}, {"text": "the most effective use of the advanced", "start": 227.76, "duration": 5.32}, {"text": "um cyber", "start": 230.56, "duration": 2.52}, {"text": "infrastructure so just couple um going", "start": 234.599, "duration": 6.041}, {"text": "to cover a couple of you know my myths", "start": 237.2, "duration": 6.8}, {"text": "that exist in parallel Computing um", "start": 240.64, "duration": 5.72}, {"text": "hopefully most of you realize that the", "start": 244.0, "duration": 4.48}, {"text": "that these are myths um but it's", "start": 246.36, "duration": 3.799}, {"text": "surprisingly number number of folks who", "start": 248.48, "duration": 3.959}, {"text": "could really benefit from for from", "start": 250.159, "duration": 5.041}, {"text": "high-end Computing from parallel systems", "start": 252.439, "duration": 5.32}, {"text": "who who who weren't aware of this", "start": 255.2, "duration": 5.279}, {"text": "so until the pandemic and then maybe", "start": 257.759, "duration": 5.361}, {"text": "starting a few months ago again um I've", "start": 260.479, "duration": 4.841}, {"text": "given a lot of tours at at", "start": 263.12, "duration": 5.68}, {"text": "SSC and one thing that seems to come up", "start": 265.32, "duration": 5.24}, {"text": "lot even with folks who have big", "start": 268.8, "duration": 4.2}, {"text": "Computing needs is that we exist for the", "start": 270.56, "duration": 4.4}, {"text": "astrophysicists and engineers and", "start": 273.0, "duration": 3.96}, {"text": "climate modelers and others who are", "start": 274.96, "duration": 4.519}, {"text": "working the traditionally math intensive", "start": 276.96, "duration": 4.72}, {"text": "Fields I'm going to say this might have", "start": 279.479, "duration": 5.28}, {"text": "been partially true decades ago but now", "start": 281.68, "duration": 5.239}, {"text": "nearly every field of research makes use", "start": 284.759, "duration": 4.561}, {"text": "of parallel Computing um this includes", "start": 286.919, "duration": 4.641}, {"text": "the social sciences Life Sciences Arts", "start": 289.32, "duration": 4.8}, {"text": "and Humanities and in addition to", "start": 291.56, "duration": 4.76}, {"text": "physics chemistry engineering Material", "start": 294.12, "duration": 4.32}, {"text": "Science and those other fields that we", "start": 296.32, "duration": 6.439}, {"text": "refer to as the as the usual suspect and", "start": 298.44, "duration": 5.759}, {"text": "again this is something that many many", "start": 302.759, "duration": 3.601}, {"text": "of you are aware of as we were screening", "start": 304.199, "duration": 4.161}, {"text": "the applicants for the foot for the", "start": 306.36, "duration": 5.279}, {"text": "summer Institute we saw that your that", "start": 308.36, "duration": 5.0}, {"text": "your research problems your domains your", "start": 311.639, "duration": 4.961}, {"text": "backgrounds really scan really um spend", "start": 313.36, "duration": 5.44}, {"text": "just about every field field of science", "start": 316.6, "duration": 4.4}, {"text": "and", "start": 318.8, "duration": 5.239}, {"text": "research okay another another problem", "start": 321.0, "duration": 5.919}, {"text": "that we see or another myth is that a", "start": 324.039, "duration": 4.521}, {"text": "lot of users think that throwing more", "start": 326.919, "duration": 3.681}, {"text": "hardwater problems will automatically", "start": 328.56, "duration": 4.16}, {"text": "reduce the time to", "start": 330.6, "duration": 4.2}, {"text": "solution and the parallel Computing", "start": 332.72, "duration": 3.28}, {"text": "though of course is only going to help", "start": 334.8, "duration": 3.239}, {"text": "you if you have an application that's", "start": 336.0, "duration": 3.639}, {"text": "been written to take advantage of", "start": 338.039, "duration": 3.921}, {"text": "parallel hardware and even if you do", "start": 339.639, "duration": 4.081}, {"text": "have a parallel code and this is", "start": 341.96, "duration": 2.959}, {"text": "something we're going to talk quite a", "start": 343.72, "duration": 3.599}, {"text": "bit about later in this talk is that", "start": 344.919, "duration": 5.081}, {"text": "there's always an inherent limit on", "start": 347.319, "duration": 4.88}, {"text": "scalability we'll talk about both OMD", "start": 350.0, "duration": 4.759}, {"text": "doll's law which gives you a theoretical", "start": 352.199, "duration": 7.0}, {"text": "upper limit on scalability and also um", "start": 354.759, "duration": 5.72}, {"text": "some of the other things that could", "start": 359.199, "duration": 5.361}, {"text": "could limit how far out your code", "start": 360.479, "duration": 4.081}, {"text": "scale um just one cave and this may this", "start": 365.639, "duration": 5.4}, {"text": "may apply to some of you is that if you", "start": 368.639, "duration": 4.12}, {"text": "have a high throughput Computing", "start": 371.039, "duration": 4.041}, {"text": "workload you can use parallel Computing", "start": 372.759, "duration": 4.72}, {"text": "essentially to run many single core or", "start": 375.08, "duration": 5.48}, {"text": "single GPU instances of your application", "start": 377.479, "duration": 5.641}, {"text": "and Achieve near perfect scaling so", "start": 380.56, "duration": 5.28}, {"text": "basically that the um that the speed up", "start": 383.12, "duration": 5.479}, {"text": "or the reduction run time to to", "start": 385.84, "duration": 4.919}, {"text": "completing your work is proportional to", "start": 388.599, "duration": 4.32}, {"text": "the to the number of cores or gpus that", "start": 390.759, "duration": 4.081}, {"text": "you're", "start": 392.919, "duration": 4.641}, {"text": "using and then finally um that you need", "start": 394.84, "duration": 4.52}, {"text": "to be a programmer or software developer", "start": 397.56, "duration": 3.44}, {"text": "to make use of parallel", "start": 399.36, "duration": 4.119}, {"text": "Computing so most users of parallel", "start": 401.0, "duration": 4.16}, {"text": "computers and I'm sure many of you here", "start": 403.479, "duration": 4.041}, {"text": "are also are not who are not programmers", "start": 405.16, "duration": 4.439}, {"text": "or programmers in the sense that you are", "start": 407.52, "duration": 5.36}, {"text": "developing um applications from scratch", "start": 409.599, "duration": 5.281}, {"text": "you're you're often using mature", "start": 412.88, "duration": 4.56}, {"text": "thirdparty software you may be using", "start": 414.88, "duration": 4.2}, {"text": "code that's been written by others and", "start": 417.44, "duration": 5.599}, {"text": "you'll be making minor um minor changes", "start": 419.08, "duration": 5.679}, {"text": "but but for the most part I'm going to", "start": 423.039, "duration": 4.72}, {"text": "say the majority of our users really do", "start": 424.759, "duration": 5.081}, {"text": "not write their own parallel parallel", "start": 427.759, "duration": 4.961}, {"text": "applications anymore they're using these", "start": 429.84, "duration": 4.88}, {"text": "apps that had been developed um", "start": 432.72, "duration": 4.319}, {"text": "developed by the", "start": 434.72, "duration": 5.199}, {"text": "community you so just to sum it up you", "start": 437.039, "duration": 4.801}, {"text": "know many of you are going to be using", "start": 439.919, "duration": 3.921}, {"text": "somebody else's code I'm just shown a", "start": 441.84, "duration": 4.52}, {"text": "few examples here that I think spand", "start": 443.84, "duration": 5.32}, {"text": "some some of the some of the work of the", "start": 446.36, "duration": 5.88}, {"text": "f here at S Summer Institute you know so", "start": 449.16, "duration": 5.0}, {"text": "in in weather modeling molecular", "start": 452.24, "duration": 4.16}, {"text": "Dynamics electronic structure", "start": 454.16, "duration": 5.28}, {"text": "calculations um construct the", "start": 456.4, "duration": 5.359}, {"text": "construction of philogenetic trees these", "start": 459.44, "duration": 4.92}, {"text": "these are all areas that we think of", "start": 461.759, "duration": 5.361}, {"text": "as you know to traditional hard science", "start": 464.36, "duration": 5.399}, {"text": "areas but in all of these the Deep", "start": 467.12, "duration": 4.079}, {"text": "learning is becoming more and more", "start": 469.759, "duration": 3.241}, {"text": "important you know in particular in", "start": 471.199, "duration": 4.241}, {"text": "weather modeling we're finding", "start": 473.0, "duration": 5.039}, {"text": "that um that the weather that the", "start": 475.44, "duration": 4.759}, {"text": "climate weather community is beginning", "start": 478.039, "duration": 4.84}, {"text": "to integrate machine learning models", "start": 480.199, "duration": 5.161}, {"text": "into the into their forecast you know it", "start": 482.879, "duration": 5.241}, {"text": "could be for the for the predictions um", "start": 485.36, "duration": 4.32}, {"text": "it could also be", "start": 488.12, "duration": 4.96}, {"text": "for um using simplified deep learning", "start": 489.68, "duration": 5.479}, {"text": "models for for some of the expensive", "start": 493.08, "duration": 4.839}, {"text": "physics that goes into weather modeling", "start": 495.159, "duration": 5.32}, {"text": "um in in in molecular Dynamics and", "start": 497.919, "duration": 4.4}, {"text": "computational chemistry we're seeing we", "start": 500.479, "duration": 4.601}, {"text": "begin to see the use of of of deep", "start": 502.319, "duration": 7.201}, {"text": "learning to um to estimate estimate", "start": 505.08, "duration": 7.12}, {"text": "intermolecular potentials right rather", "start": 509.52, "duration": 5.24}, {"text": "than doing they're doing very expensive", "start": 512.2, "duration": 4.8}, {"text": "calculations but but across across the", "start": 514.76, "duration": 3.839}, {"text": "board though it's very likely going to", "start": 517.0, "duration": 5.159}, {"text": "be using somebody else's mature", "start": 518.599, "duration": 5.92}, {"text": "code um so that brings us to parallel", "start": 522.159, "duration": 4.8}, {"text": "computers we won't go too much into this", "start": 524.519, "duration": 4.201}, {"text": "I think Mary is going to be touching on", "start": 526.959, "duration": 4.44}, {"text": "this in her talk so the kind of", "start": 528.72, "duration": 4.84}, {"text": "resources you're going to be using these", "start": 531.399, "duration": 4.0}, {"text": "you know modern clusters and parallel", "start": 533.56, "duration": 4.48}, {"text": "computers they consist of multiple", "start": 535.399, "duration": 4.721}, {"text": "compute nodes connected together by a", "start": 538.04, "duration": 3.039}, {"text": "fast", "start": 540.12, "duration": 4.76}, {"text": "Network each each CPU node typically is", "start": 541.079, "duration": 6.561}, {"text": "going to contain one or more most often", "start": 544.88, "duration": 6.28}, {"text": "two multicore processes while the GPU", "start": 547.64, "duration": 6.759}, {"text": "nodes will usually contain four gpus and", "start": 551.16, "duration": 5.119}, {"text": "I've seen very few systems out there", "start": 554.399, "duration": 5.56}, {"text": "that have anything but four gpus in them", "start": 556.279, "duration": 6.12}, {"text": "um reason I bring this up is that you", "start": 559.959, "duration": 5.12}, {"text": "want to be aware of the you want to be", "start": 562.399, "duration": 5.641}, {"text": "aware of the the hierarchy in in the in", "start": 565.079, "duration": 5.32}, {"text": "the architecture you know starting", "start": 568.04, "duration": 5.28}, {"text": "starting at the um processor you can", "start": 570.399, "duration": 5.12}, {"text": "have multiple cores you can have", "start": 573.32, "duration": 4.88}, {"text": "multiple processes within a node and", "start": 575.519, "duration": 4.56}, {"text": "those cores within that note are going", "start": 578.2, "duration": 3.84}, {"text": "to have access to the same pool of", "start": 580.079, "duration": 4.32}, {"text": "shared memory if you need to go if you", "start": 582.04, "duration": 3.88}, {"text": "need to go bigger if you need to scale", "start": 584.399, "duration": 3.641}, {"text": "out to to more to more Calles or more", "start": 585.92, "duration": 4.039}, {"text": "gpus you're then going to need to", "start": 588.04, "duration": 3.76}, {"text": "explicitly do some sort of message", "start": 589.959, "duration": 4.88}, {"text": "passing between those notes so to", "start": 591.8, "duration": 4.96}, {"text": "effectively use this Hardware we're", "start": 594.839, "duration": 3.361}, {"text": "going to need applications that have", "start": 596.76, "duration": 3.8}, {"text": "been paralyzed so they can run like I", "start": 598.2, "duration": 4.36}, {"text": "said on the multiple cores of gpus", "start": 600.56, "duration": 6.32}, {"text": "within a node or across the multiple", "start": 602.56, "duration": 4.32}, {"text": "nodes all right so we're going to go on", "start": 607.32, "duration": 3.4}, {"text": "to a little bit", "start": 609.279, "duration": 4.881}, {"text": "of parallel Computing Basics going to", "start": 610.72, "duration": 8.28}, {"text": "discuss processes threads MPI and", "start": 614.16, "duration": 4.84}, {"text": "openmp so so you may have heard about", "start": 619.32, "duration": 4.639}, {"text": "processes and threads um these are both", "start": 621.519, "duration": 4.801}, {"text": "independent sequences of", "start": 623.959, "duration": 4.601}, {"text": "execution um but but there but there's", "start": 626.32, "duration": 4.68}, {"text": "an important difference between them a", "start": 628.56, "duration": 5.12}, {"text": "process is an instance of a program with", "start": 631.0, "duration": 5.279}, {"text": "access to its own memory State and file", "start": 633.68, "duration": 4.44}, {"text": "descriptor so they can open and close", "start": 636.279, "duration": 4.36}, {"text": "files that the kind of parallelism that", "start": 638.12, "duration": 4.2}, {"text": "you're most likely going to be doing", "start": 640.639, "duration": 4.64}, {"text": "with deep learning is BAS is based on", "start": 642.32, "duration": 5.0}, {"text": "processes I'll me mention a little bit", "start": 645.279, "duration": 4.281}, {"text": "later something called called horovod", "start": 647.32, "duration": 4.319}, {"text": "which is used to paralyze deep learning", "start": 649.56, "duration": 5.2}, {"text": "Frameworks um horovod turn is built on", "start": 651.639, "duration": 6.041}, {"text": "top of something called MPI the message", "start": 654.76, "duration": 5.199}, {"text": "pass and interface which is the def", "start": 657.68, "duration": 5.8}, {"text": "facto standard for um", "start": 659.959, "duration": 7.201}, {"text": "paralyzing um C++ and for Trend codes um", "start": 663.48, "duration": 6.359}, {"text": "but there's also a possibility oh sorry", "start": 667.16, "duration": 5.76}, {"text": "yes so um horod is g to be built on MPI", "start": 669.839, "duration": 5.44}, {"text": "and something called nickel which is an", "start": 672.92, "duration": 4.68}, {"text": "Nvidia library for taking advantage of", "start": 675.279, "duration": 4.521}, {"text": "the fast Communications between be", "start": 677.6, "duration": 3.239}, {"text": "between", "start": 679.8, "duration": 3.159}, {"text": "gpus um there's also something called", "start": 680.839, "duration": 4.12}, {"text": "threads these are lighter weight", "start": 682.959, "duration": 4.601}, {"text": "entities that execute inside a", "start": 684.959, "duration": 5.041}, {"text": "process every process is going to have", "start": 687.56, "duration": 5.079}, {"text": "at least one thread and the threads", "start": 690.0, "duration": 5.279}, {"text": "within this process will all have access", "start": 692.639, "duration": 4.801}, {"text": "to the same pool of shared", "start": 695.279, "duration": 4.841}, {"text": "memory um there are some great online", "start": 697.44, "duration": 5.16}, {"text": "resources to describing the differences", "start": 700.12, "duration": 4.64}, {"text": "between threads and processes they tend", "start": 702.6, "duration": 4.4}, {"text": "to be geared towards computer scientists", "start": 704.76, "duration": 3.96}, {"text": "um but I think the following resources", "start": 707.0, "duration": 4.44}, {"text": "are pretty nice so depending on how deep", "start": 708.72, "duration": 6.08}, {"text": "you want to go there's a great thread on", "start": 711.44, "duration": 6.079}, {"text": "stack Overflow that really dives into", "start": 714.8, "duration": 4.08}, {"text": "the difference between threads and", "start": 717.519, "duration": 4.12}, {"text": "process processes and", "start": 718.88, "duration": 6.16}, {"text": "um there I found a a really nice more", "start": 721.639, "duration": 8.0}, {"text": "informal description um here at the educ", "start": 725.04, "duration": 8.4}, {"text": "cba.com um process ver versus ver versus", "start": 729.639, "duration": 6.721}, {"text": "thread blog post um so you know if you", "start": 733.44, "duration": 4.16}, {"text": "have any more questions if you want to", "start": 736.36, "duration": 3.24}, {"text": "go a little bit deeper I'd recommend", "start": 737.6, "duration": 5.239}, {"text": "that you take a look at", "start": 739.6, "duration": 3.239}, {"text": "those so some of the difference between", "start": 743.079, "duration": 5.641}, {"text": "processes and thread is that a first of", "start": 746.079, "duration": 4.801}, {"text": "all a process is going to incur more", "start": 748.72, "duration": 5.04}, {"text": "overhead it's what we consider a a a", "start": 750.88, "duration": 4.28}, {"text": "heavier weight", "start": 753.76, "duration": 4.319}, {"text": "object you're also more flexible though", "start": 755.16, "duration": 5.239}, {"text": "you can run multiple processes with it", "start": 758.079, "duration": 6.361}, {"text": "within a compute node or you can", "start": 760.399, "duration": 8.0}, {"text": "um run multiple R run multiple processes", "start": 764.44, "duration": 6.36}, {"text": "across multiple compute noes we're", "start": 768.399, "duration": 5.321}, {"text": "taking advantage of Distributing memory", "start": 770.8, "duration": 4.92}, {"text": "um processes are really much more", "start": 773.72, "duration": 3.6}, {"text": "General they're they're going to allow", "start": 775.72, "duration": 4.88}, {"text": "you to scale up further again for the", "start": 777.32, "duration": 5.4}, {"text": "Deep learning workloads it's much much", "start": 780.6, "duration": 3.2}, {"text": "more likely that you're going to be", "start": 782.72, "duration": 3.119}, {"text": "doing something that's process", "start": 783.8, "duration": 6.24}, {"text": "based threads they incur less overhead", "start": 785.839, "duration": 6.36}, {"text": "um threaded codes can use less memory", "start": 790.04, "duration": 4.28}, {"text": "since the threads within a process have", "start": 792.199, "duration": 4.32}, {"text": "access to that same data structure but", "start": 794.32, "duration": 3.959}, {"text": "they're also less flexible you're going", "start": 796.519, "duration": 3.76}, {"text": "to be limited in your scalability if", "start": 798.279, "duration": 5.601}, {"text": "you're using purely threads to just the", "start": 800.279, "duration": 5.521}, {"text": "hardware within within that compute mode", "start": 803.88, "duration": 4.0}, {"text": "it's basically a a shared memory", "start": 805.8, "duration": 5.08}, {"text": "application", "start": 807.88, "duration": 3.0}, {"text": "all right so you know we talked a little", "start": 811.839, "duration": 2.881}, {"text": "bit about process and threats I know", "start": 813.24, "duration": 5.159}, {"text": "that was a very very quick overview um", "start": 814.72, "duration": 5.6}, {"text": "why why do you care why why do you care", "start": 818.399, "duration": 4.161}, {"text": "about the difference between these so", "start": 820.32, "duration": 4.16}, {"text": "this type of paralyzation is going to", "start": 822.56, "duration": 4.16}, {"text": "determine how and where you're going to", "start": 824.48, "duration": 5.12}, {"text": "run your applications distributed memory", "start": 826.72, "duration": 5.52}, {"text": "apps multiple processes or instances of", "start": 829.6, "duration": 5.28}, {"text": "a program they can be run on one or more", "start": 832.24, "duration": 5.76}, {"text": "nodes shared memory threaded application", "start": 834.88, "duration": 5.199}, {"text": "should be run on a single", "start": 838.0, "duration": 4.72}, {"text": "node the hybrid applications which we're", "start": 840.079, "duration": 3.961}, {"text": "going to spend a little bit of time on", "start": 842.72, "duration": 3.919}, {"text": "can be run on one or more nodes but you", "start": 844.04, "duration": 4.159}, {"text": "should always consider the balance", "start": 846.639, "duration": 4.241}, {"text": "between threads and processes so again", "start": 848.199, "duration": 4.521}, {"text": "this may not matter quite as much for", "start": 850.88, "duration": 3.92}, {"text": "for the Deep learning applications that", "start": 852.72, "duration": 4.119}, {"text": "are going to be paralyzed using", "start": 854.8, "duration": 3.88}, {"text": "something that like horror VOD but if", "start": 856.839, "duration": 5.36}, {"text": "you're integrating deep learning into", "start": 858.68, "duration": 6.159}, {"text": "you know into a larger computational", "start": 862.199, "duration": 5.241}, {"text": "framework say including it", "start": 864.839, "duration": 6.281}, {"text": "in um in climate weather or molecular", "start": 867.44, "duration": 5.959}, {"text": "Dynamics Material Science simulations", "start": 871.12, "duration": 4.88}, {"text": "you will need to be aware of the these", "start": 873.399, "duration": 5.44}, {"text": "distinctions and in all cases you may", "start": 876.0, "duration": 4.92}, {"text": "need to consider how the processes and", "start": 878.839, "duration": 4.44}, {"text": "threads are bound to those cores within", "start": 880.92, "duration": 4.919}, {"text": "a compute", "start": 883.279, "duration": 2.56}, {"text": "node so talk a little bit about MPI um", "start": 887.16, "duration": 5.44}, {"text": "like I said this talk was was really", "start": 890.72, "duration": 4.119}, {"text": "developed for for for a different venue", "start": 892.6, "duration": 5.2}, {"text": "but everything that I say here about MPI", "start": 894.839, "duration": 5.321}, {"text": "and distributing memory um programming", "start": 897.8, "duration": 4.36}, {"text": "is going to apply to", "start": 900.16, "duration": 5.0}, {"text": "the to to deep learning applications", "start": 902.16, "duration": 5.56}, {"text": "that have been paralyzed using using", "start": 905.16, "duration": 6.84}, {"text": "horod or for example if you're doing the", "start": 907.72, "duration": 7.52}, {"text": "Deep learning you using spark so MPI is", "start": 912.0, "duration": 6.079}, {"text": "a standard for paralyzing C C++ and for", "start": 915.24, "duration": 6.599}, {"text": "Trend codes to run on distributed memory", "start": 918.079, "duration": 7.281}, {"text": "um system so multiple compute nodes now", "start": 921.839, "duration": 6.24}, {"text": "what's interesting is that MPI is not an", "start": 925.36, "duration": 4.36}, {"text": "official standard but it's kind of the", "start": 928.079, "duration": 4.081}, {"text": "def facto standard for for parallel", "start": 929.72, "duration": 5.64}, {"text": "Computing pretty much everybody uses it", "start": 932.16, "duration": 5.52}, {"text": "you may have heard of things like open", "start": 935.36, "duration": 6.039}, {"text": "MPI and mvapich and M pitch um these are", "start": 937.68, "duration": 6.68}, {"text": "just implementations of implementations", "start": 941.399, "duration": 3.961}, {"text": "of", "start": 944.36, "duration": 4.479}, {"text": "MPI NPI applications can be run within a", "start": 945.36, "duration": 6.399}, {"text": "shared memory node so for example if you", "start": 948.839, "duration": 6.841}, {"text": "have a deep learning um a deep learning", "start": 951.759, "duration": 5.841}, {"text": "application that you want to run within", "start": 955.68, "duration": 4.56}, {"text": "a single node but it's paralyzed using", "start": 957.6, "duration": 6.12}, {"text": "MPI or horovod or or nickel you you can", "start": 960.24, "duration": 6.039}, {"text": "still do that um even though MPI allows", "start": 963.72, "duration": 5.0}, {"text": "you to go across multiple nodes you can", "start": 966.279, "duration": 5.961}, {"text": "stay entirely within a node and MPI", "start": 968.72, "duration": 5.72}, {"text": "implementations are optimized to take", "start": 972.24, "duration": 5.399}, {"text": "advantage of that faster intran node", "start": 974.44, "duration": 7.48}, {"text": "communication um MPI and also nickel and", "start": 977.639, "duration": 8.921}, {"text": "um um porod that these are portable and", "start": 981.92, "duration": 6.32}, {"text": "although MPI synonymous with", "start": 986.56, "duration": 4.16}, {"text": "Distributing memory paralyzation keep in", "start": 988.24, "duration": 4.0}, {"text": "mind that there are other options out", "start": 990.72, "duration": 4.599}, {"text": "there that are gaining gaining", "start": 992.24, "duration": 3.079}, {"text": "adoption okay yeah so I kind of kind of", "start": 996.04, "duration": 4.159}, {"text": "touch in this already you know so", "start": 998.36, "duration": 3.919}, {"text": "although we're discussing MPI which is", "start": 1000.199, "duration": 4.08}, {"text": "more of the you know used more in the", "start": 1002.279, "duration": 5.201}, {"text": "traditional HPC um Computing world", "start": 1004.279, "duration": 5.56}, {"text": "everything that we say here also applies", "start": 1007.48, "duration": 4.32}, {"text": "to horror VOD a distributed deep", "start": 1009.839, "duration": 4.12}, {"text": "learning framework for tensorflow Caris", "start": 1011.8, "duration": 5.88}, {"text": "P torch and mxnet and to Nickel which is", "start": 1013.959, "duration": 6.041}, {"text": "the Nvidia collector Communications", "start": 1017.68, "duration": 5.2}, {"text": "Library which can be used for M", "start": 1020.0, "duration": 6.16}, {"text": "multi-gpu and multi multide", "start": 1022.88, "duration": 5.4}, {"text": "Communications and this is specifically", "start": 1026.16, "duration": 6.36}, {"text": "optimized for NVIDIA gpus and netw", "start": 1028.28, "duration": 4.24}, {"text": "working so just to show you what what an", "start": 1032.64, "duration": 6.52}, {"text": "MPI code looks like um MPI applications", "start": 1035.079, "duration": 5.48}, {"text": "are are pretty dense they're written at", "start": 1039.16, "duration": 3.879}, {"text": "a low level data is explicitly", "start": 1040.559, "duration": 4.801}, {"text": "communicated between trust this is using", "start": 1043.039, "duration": 6.481}, {"text": "calls the NPI um Library routines", "start": 1045.36, "duration": 6.52}, {"text": "um for most of you what in the in the", "start": 1049.52, "duration": 4.88}, {"text": "Deep learning world and especially if", "start": 1051.88, "duration": 4.72}, {"text": "you're not not not specifically a", "start": 1054.4, "duration": 4.36}, {"text": "program or an application developer you", "start": 1056.6, "duration": 4.72}, {"text": "don't really need to know MPI but you", "start": 1058.76, "duration": 5.24}, {"text": "just need to know that it exists so just", "start": 1061.32, "duration": 6.12}, {"text": "to show you um how quickly an MPI", "start": 1064.0, "duration": 8.72}, {"text": "application can um Okay can start", "start": 1067.44, "duration": 8.8}, {"text": "looking oh looking a little more complex", "start": 1072.72, "duration": 6.88}, {"text": "is I've shown here in purple all of the", "start": 1076.24, "duration": 5.36}, {"text": "additional code that we had to add in", "start": 1079.6, "duration": 5.319}, {"text": "order to create a parallel version of of", "start": 1081.6, "duration": 6.48}, {"text": "hello world again for many of your", "start": 1084.919, "duration": 5.561}, {"text": "applications um especially if you're", "start": 1088.08, "duration": 5.959}, {"text": "doing um doing something simple like", "start": 1090.48, "duration": 6.199}, {"text": "just Distributing data across across", "start": 1094.039, "duration": 5.081}, {"text": "processes and collecting results your", "start": 1096.679, "duration": 4.201}, {"text": "code is going to be a little bit simpler", "start": 1099.12, "duration": 3.6}, {"text": "but it's going to be based on these on", "start": 1100.88, "duration": 4.32}, {"text": "these same", "start": 1102.72, "duration": 4.8}, {"text": "principles um openm", "start": 1105.2, "duration": 5.959}, {"text": "P um Switching gears a little bit is an", "start": 1107.52, "duration": 6.08}, {"text": "API for shared memory parallel", "start": 1111.159, "duration": 5.76}, {"text": "programming um and it's used in cc++ and", "start": 1113.6, "duration": 5.48}, {"text": "fortrend it provides a collection of", "start": 1116.919, "duration": 4.361}, {"text": "compiler directives Library routines and", "start": 1119.08, "duration": 4.88}, {"text": "environment variables supported by all", "start": 1121.28, "duration": 3.92}, {"text": "of the major", "start": 1123.96, "duration": 6.199}, {"text": "compilers IBM Intel GCC PGI and the AMD", "start": 1125.2, "duration": 9.04}, {"text": "optimizing CN C++ compilers it it's it's", "start": 1130.159, "duration": 6.561}, {"text": "portable and it's often considered to be", "start": 1134.24, "duration": 4.24}, {"text": "synonymous with shared memory parallel", "start": 1136.72, "duration": 2.64}, {"text": "Compu", "start": 1138.48, "duration": 3.12}, {"text": "but there are other options out there", "start": 1139.36, "duration": 3.64}, {"text": "and again the reason I'm bringing up", "start": 1141.6, "duration": 4.4}, {"text": "openmp here is that if you are", "start": 1143.0, "duration": 5.12}, {"text": "developing a workload that", "start": 1146.0, "duration": 4.44}, {"text": "integrates that integrates deep learning", "start": 1148.12, "duration": 4.88}, {"text": "with traditional Computing you may need", "start": 1150.44, "duration": 5.64}, {"text": "to um parallel components of it", "start": 1153.0, "duration": 7.0}, {"text": "paralyzed components of it using", "start": 1156.08, "duration": 3.92}, {"text": "openmp so openmp paralyzation tends to", "start": 1160.48, "duration": 5.679}, {"text": "be um a little", "start": 1164.32, "duration": 4.68}, {"text": "bit a little bit I'm going to say easy a", "start": 1166.159, "duration": 3.921}, {"text": "little more", "start": 1169.0, "duration": 2.96}, {"text": "straightforward not quite as difficult", "start": 1170.08, "duration": 4.839}, {"text": "to read as NPI applications again if", "start": 1171.96, "duration": 5.199}, {"text": "you're not developing your own parallel", "start": 1174.919, "duration": 4.441}, {"text": "codes you don't need to be you don't", "start": 1177.159, "duration": 4.64}, {"text": "need to know open and P but this is one", "start": 1179.36, "duration": 3.799}, {"text": "of those things that you should be", "start": 1181.799, "duration": 3.601}, {"text": "available sorry one of those things that", "start": 1183.159, "duration": 4.481}, {"text": "you should be aware of so I'm going to", "start": 1185.4, "duration": 5.399}, {"text": "show here um simple Loop where we are", "start": 1187.64, "duration": 5.2}, {"text": "just", "start": 1190.799, "duration": 5.24}, {"text": "initializing um initializing a couple of", "start": 1192.84, "duration": 6.6}, {"text": "arrays and then adding them element by", "start": 1196.039, "duration": 7.281}, {"text": "element and if I want to paralyze that", "start": 1199.44, "duration": 6.08}, {"text": "all I had to do was add add a few", "start": 1203.32, "duration": 6.16}, {"text": "headers and a pragma down here in purple", "start": 1205.52, "duration": 7.84}, {"text": "to show how the how the work will be", "start": 1209.48, "duration": 3.88}, {"text": "distributed right so come the big", "start": 1216.48, "duration": 4.92}, {"text": "picture again everything that you see", "start": 1219.2, "duration": 4.839}, {"text": "here on the left where I say MPI think", "start": 1221.4, "duration": 5.48}, {"text": "MPI nickel and an", "start": 1224.039, "duration": 7.64}, {"text": "orod um MPI is used for used for", "start": 1226.88, "duration": 7.76}, {"text": "managing multiple processes and it's", "start": 1231.679, "duration": 5.36}, {"text": "implemented in libraries like M pit open", "start": 1234.64, "duration": 6.48}, {"text": "MPI mvapich and vendor implementations", "start": 1237.039, "duration": 6.361}, {"text": "threads multiple threads are managed", "start": 1241.12, "duration": 5.16}, {"text": "using something like open MP and this is", "start": 1243.4, "duration": 7.0}, {"text": "implemented directly by the by the", "start": 1246.28, "duration": 5.92}, {"text": "compilers so we're going to talk a", "start": 1250.4, "duration": 3.84}, {"text": "little bit about hybrid applications I'm", "start": 1252.2, "duration": 3.28}, {"text": "going to go through this part a little", "start": 1254.24, "duration": 2.96}, {"text": "bit quicker than usual see if we can", "start": 1255.48, "duration": 3.72}, {"text": "catch us up um", "start": 1257.2, "duration": 3.719}, {"text": "might make up a little bit of", "start": 1259.2, "duration": 3.959}, {"text": "time so a lot of the parallel", "start": 1260.919, "duration": 3.681}, {"text": "applications are going to be built using", "start": 1263.159, "duration": 3.281}, {"text": "a hybrid approach to take advantage of", "start": 1264.6, "duration": 5.4}, {"text": "both distributed and shared memory again", "start": 1266.44, "duration": 6.32}, {"text": "this often involves MPI and open MP", "start": 1270.0, "duration": 5.48}, {"text": "although other combinations are possible", "start": 1272.76, "duration": 4.519}, {"text": "um these hybrid codes have advantages", "start": 1275.48, "duration": 4.12}, {"text": "over the purely shared or distributed", "start": 1277.279, "duration": 4.76}, {"text": "memory apps so a shared memory", "start": 1279.6, "duration": 4.48}, {"text": "application can have limited scalability", "start": 1282.039, "duration": 3.52}, {"text": "within", "start": 1284.08, "duration": 4.56}, {"text": "node um and it cannot be run across B", "start": 1285.559, "duration": 5.881}, {"text": "multiple notes um with distributed", "start": 1288.64, "duration": 4.76}, {"text": "memory applications they may have higher", "start": 1291.44, "duration": 4.88}, {"text": "memory requirements and more overhead so", "start": 1293.4, "duration": 5.279}, {"text": "if we have a code that's been developed", "start": 1296.32, "duration": 5.16}, {"text": "to use both but both shared memory and", "start": 1298.679, "duration": 4.561}, {"text": "distributed memory parallelism we can", "start": 1301.48, "duration": 3.64}, {"text": "kind of get the best of both worlds we", "start": 1303.24, "duration": 4.039}, {"text": "can get better performance within that", "start": 1305.12, "duration": 4.76}, {"text": "node but we can also get scalability", "start": 1307.279, "duration": 4.64}, {"text": "across multiple", "start": 1309.88, "duration": 5.039}, {"text": "nodes so I'm going to show a very very", "start": 1311.919, "duration": 7.601}, {"text": "simplified parallel computer um", "start": 1314.919, "duration": 7.041}, {"text": "this you know this theoretical system", "start": 1319.52, "duration": 5.44}, {"text": "just consists of two nodes each with 16", "start": 1321.96, "duration": 6.92}, {"text": "cores joined by a network so the um the", "start": 1324.96, "duration": 7.4}, {"text": "larger box here is is a node it contains", "start": 1328.88, "duration": 6.039}, {"text": "16 cores and then the communications", "start": 1332.36, "duration": 4.52}, {"text": "between these nodes occurs over over", "start": 1334.919, "duration": 5.0}, {"text": "this network in the case of", "start": 1336.88, "duration": 5.76}, {"text": "expense that these nodes actually have", "start": 1339.919, "duration": 5.12}, {"text": "128 cores the interconnected is", "start": 1342.64, "duration": 4.56}, {"text": "something called infiniband and there's", "start": 1345.039, "duration": 5.12}, {"text": "actually close to I lost the track of", "start": 1347.2, "duration": 5.44}, {"text": "count but I think there is about 800", "start": 1350.159, "duration": 6.0}, {"text": "nodes on on", "start": 1352.64, "duration": 3.519}, {"text": "expans so if you have a pure message", "start": 1357.08, "duration": 6.719}, {"text": "pass an application again MPI nickel", "start": 1360.08, "duration": 6.199}, {"text": "horovod um typically you're going to", "start": 1363.799, "duration": 4.961}, {"text": "have one one process per core or one", "start": 1366.279, "duration": 5.4}, {"text": "process per per GPU so this is an", "start": 1368.76, "duration": 5.96}, {"text": "example showing running 32 processes", "start": 1371.679, "duration": 6.521}, {"text": "across these two 16 core notes where I", "start": 1374.72, "duration": 6.24}, {"text": "you use um a highlight and orange each", "start": 1378.2, "duration": 4.32}, {"text": "of", "start": 1380.96, "duration": 3.76}, {"text": "processes threaded", "start": 1382.52, "duration": 4.48}, {"text": "applications now that're in paralized", "start": 1384.72, "duration": 5.52}, {"text": "using only openmp or some other shared", "start": 1387.0, "duration": 5.08}, {"text": "memory Paradigm is going to be", "start": 1390.24, "duration": 4.6}, {"text": "restricted to a single node in this case", "start": 1392.08, "duration": 6.36}, {"text": "I show an application running 16 threads", "start": 1394.84, "duration": 7.0}, {"text": "across 16 cores within that", "start": 1398.44, "duration": 6.719}, {"text": "node and for these hybrid applications", "start": 1401.84, "duration": 5.8}, {"text": "said typically MPI Plus open MP but it", "start": 1405.159, "duration": 4.76}, {"text": "could be you know it it could it could", "start": 1407.64, "duration": 4.399}, {"text": "be um other implementations again", "start": 1409.919, "duration": 5.36}, {"text": "instead of MPI corod um any combination", "start": 1412.039, "duration": 5.361}, {"text": "of threads and processes within a node", "start": 1415.279, "duration": 5.681}, {"text": "is allowed in this case we're showing um", "start": 1417.4, "duration": 6.32}, {"text": "two processes per node and eight threads", "start": 1420.96, "duration": 5.079}, {"text": "per", "start": 1423.72, "duration": 2.319}, {"text": "process um we could also run um just", "start": 1426.4, "duration": 7.36}, {"text": "single process per node with um", "start": 1430.0, "duration": 9.72}, {"text": "16 with 16 threads per per node", "start": 1433.76, "duration": 5.96}, {"text": "but um we we could choose anything in", "start": 1440.0, "duration": 5.96}, {"text": "between and this is where things get get", "start": 1443.559, "duration": 4.6}, {"text": "get get a little tricky if you are doing", "start": 1445.96, "duration": 4.319}, {"text": "an application that implements both", "start": 1448.159, "duration": 4.161}, {"text": "shared memory and distributed memory", "start": 1450.279, "duration": 5.961}, {"text": "parallelism you you've got um you've got", "start": 1452.32, "duration": 6.92}, {"text": "a whole range from doing one process per", "start": 1456.24, "duration": 5.72}, {"text": "node to one process per core and", "start": 1459.24, "duration": 4.319}, {"text": "anything in", "start": 1461.96, "duration": 3.68}, {"text": "between so we're going to talk a little", "start": 1463.559, "duration": 6.161}, {"text": "bit about um soon you know where where", "start": 1465.64, "duration": 6.44}, {"text": "where you should be um how how to how to", "start": 1469.72, "duration": 4.679}, {"text": "choose between optimally between between", "start": 1472.08, "duration": 4.92}, {"text": "the threads and and processes but first", "start": 1474.399, "duration": 4.241}, {"text": "we need to discuss a little bit about", "start": 1477.0, "duration": 3.76}, {"text": "scalability and we're going to dive in", "start": 1478.64, "duration": 4.399}, {"text": "with something called omd's", "start": 1480.76, "duration": 8.159}, {"text": "law so if you know only one bit of", "start": 1483.039, "duration": 8.64}, {"text": "theoretical parallel Computing omd's law", "start": 1488.919, "duration": 5.0}, {"text": "is what you should know um really it's", "start": 1491.679, "duration": 4.36}, {"text": "really very simple it describes the", "start": 1493.919, "duration": 4.921}, {"text": "absolute limit on the speed up of a code", "start": 1496.039, "duration": 5.281}, {"text": "as a function of the proportion of the", "start": 1498.84, "duration": 4.68}, {"text": "code that can be paralyzed and the", "start": 1501.32, "duration": 4.52}, {"text": "number of processes so again for", "start": 1503.52, "duration": 4.279}, {"text": "processes you could think CPUs you could", "start": 1505.84, "duration": 3.36}, {"text": "think", "start": 1507.799, "duration": 4.6}, {"text": "gpus so let P be the fraction of the", "start": 1509.2, "duration": 6.479}, {"text": "code that can be paralyzed so some some", "start": 1512.399, "duration": 5.081}, {"text": "piece of work that can be that that can", "start": 1515.679, "duration": 4.48}, {"text": "be chopped up where each of the chunks", "start": 1517.48, "duration": 5.16}, {"text": "could be done in parallel you know a", "start": 1520.159, "duration": 6.081}, {"text": "good example this might be if we had", "start": 1522.64, "duration": 6.24}, {"text": "to you know if we took this group let's", "start": 1526.24, "duration": 5.64}, {"text": "say we had a we had 10,000 numbers that", "start": 1528.88, "duration": 5.96}, {"text": "had to be added up by hand um we could", "start": 1531.88, "duration": 5.44}, {"text": "take that divide it by we've got roughly", "start": 1534.84, "duration": 5.36}, {"text": "50 here you know divide that", "start": 1537.32, "duration": 7.079}, {"text": "by divide that by 50 um so that each of", "start": 1540.2, "duration": 6.76}, {"text": "you get I forgot what I started with but", "start": 1544.399, "duration": 4.441}, {"text": "let's say each of you had 2,000 numbers", "start": 1546.96, "duration": 5.92}, {"text": "to work on each of you can work on your", "start": 1548.84, "duration": 8.0}, {"text": "sum of of of summing up your subset of", "start": 1552.88, "duration": 5.6}, {"text": "the data that's something that could be", "start": 1556.84, "duration": 3.439}, {"text": "entirely in parallel but there's going", "start": 1558.48, "duration": 3.84}, {"text": "to be a certain amount of Serial work", "start": 1560.279, "duration": 4.12}, {"text": "that also needs to be at needs to be", "start": 1562.32, "duration": 4.32}, {"text": "done at the end say somebody collecting", "start": 1564.399, "duration": 4.041}, {"text": "all of those partial results and adding", "start": 1566.64, "duration": 4.88}, {"text": "them up so again let P be the fraction", "start": 1568.44, "duration": 4.88}, {"text": "of the work or the code that can be", "start": 1571.52, "duration": 4.56}, {"text": "paralyzed s is a fraction the code that", "start": 1573.32, "duration": 5.68}, {"text": "has to be run sequentially and N is the", "start": 1576.08, "duration": 5.64}, {"text": "number of processes so the absolute", "start": 1579.0, "duration": 4.36}, {"text": "theoretical speed up that you could get", "start": 1581.72, "duration": 3.48}, {"text": "for your application is going to be", "start": 1583.36, "duration": 6.039}, {"text": "given by this Formula 1 over 1 - p plus", "start": 1585.2, "duration": 6.68}, {"text": "p/", "start": 1589.399, "duration": 2.481}, {"text": "n and as the limit and the limit as the", "start": 1592.6, "duration": 6.76}, {"text": "number processes goes goes Infinity the", "start": 1595.76, "duration": 5.519}, {"text": "theoretical speed up is only going to", "start": 1599.36, "duration": 4.28}, {"text": "depend on the proportion of the parallel", "start": 1601.279, "duration": 5.52}, {"text": "content the parallel content so again if", "start": 1603.64, "duration": 5.0}, {"text": "we if we threw an infinite amount of", "start": 1606.799, "duration": 4.081}, {"text": "hardware at the problem we would get a", "start": 1608.64, "duration": 5.6}, {"text": "speed up of one over s where s is the", "start": 1610.88, "duration": 5.72}, {"text": "serial content so you might be thinking", "start": 1614.24, "duration": 4.36}, {"text": "that doesn't look so bad I'm going to", "start": 1616.6, "duration": 3.76}, {"text": "show in the next slide that it doesn't", "start": 1618.6, "duration": 4.36}, {"text": "take much serial content to quickly", "start": 1620.36, "duration": 4.319}, {"text": "impact the speed", "start": 1622.96, "duration": 4.04}, {"text": "up now so here are some plots that I've", "start": 1624.679, "duration": 4.681}, {"text": "put together um illustrating hum doll's", "start": 1627.0, "duration": 5.32}, {"text": "law for for various amounts of parallel", "start": 1629.36, "duration": 7.28}, {"text": "or or serial serial content um the", "start": 1632.32, "duration": 6.359}, {"text": "Jupiter notebooks for for generating", "start": 1636.64, "duration": 4.399}, {"text": "these plots are available in that link", "start": 1638.679, "duration": 3.88}, {"text": "that I showed earlier and again I'll", "start": 1641.039, "duration": 4.0}, {"text": "paste that paste that into the chat so", "start": 1642.559, "duration": 5.0}, {"text": "we could see that if we have um just a", "start": 1645.039, "duration": 4.24}, {"text": "little bit of just a little bit of", "start": 1647.559, "duration": 6.0}, {"text": "serial code say say 5% or 95% parallel", "start": 1649.279, "duration": 7.361}, {"text": "content the absolute maximum speed up", "start": 1653.559, "duration": 5.201}, {"text": "that we're going to be able to get is", "start": 1656.64, "duration": 4.32}, {"text": "20x so we may we may have a", "start": 1658.76, "duration": 3.919}, {"text": "supercomputer where with tens of", "start": 1660.96, "duration": 3.319}, {"text": "thousands or hundreds of thousands of", "start": 1662.679, "duration": 3.72}, {"text": "cores or you know maybe hundreds of", "start": 1664.279, "duration": 5.0}, {"text": "thousands of gpus but our speed up is", "start": 1666.399, "duration": 4.961}, {"text": "going to be absolutely Limited in this", "start": 1669.279, "duration": 4.28}, {"text": "case to to 20x so we can't use that", "start": 1671.36, "duration": 4.64}, {"text": "entire", "start": 1673.559, "duration": 2.441}, {"text": "system and even if if we take a code", "start": 1676.399, "duration": 6.52}, {"text": "with 99% parallel content there we're", "start": 1679.12, "duration": 6.559}, {"text": "only going to get a maximum speed up of", "start": 1682.919, "duration": 5.561}, {"text": "100x and let's say that this was a CPU", "start": 1685.679, "duration": 4.801}, {"text": "based", "start": 1688.48, "duration": 4.199}, {"text": "application if I look at this if I look", "start": 1690.48, "duration": 4.319}, {"text": "at this top um top scaling curve you", "start": 1692.679, "duration": 3.84}, {"text": "know speed up is number of", "start": 1694.799, "duration": 4.921}, {"text": "cores speed up versus number of cores", "start": 1696.519, "duration": 7.561}, {"text": "what we'll see that if I'm using all 128", "start": 1699.72, "duration": 9.24}, {"text": "cores on a single Noe even there um", "start": 1704.08, "duration": 7.0}, {"text": "I'm I'm I'm making terrible use of that", "start": 1708.96, "duration": 4.88}, {"text": "Hardware so not even looking way out", "start": 1711.08, "duration": 4.12}, {"text": "here you know at the tail where I'm", "start": 1713.84, "duration": 3.36}, {"text": "using an infinite amount of Hardware", "start": 1715.2, "duration": 4.16}, {"text": "even down here something very um very", "start": 1717.2, "duration": 4.92}, {"text": "reasonable just a single compute Noe we", "start": 1719.36, "duration": 4.319}, {"text": "could see that we're running at an", "start": 1722.12, "duration": 4.559}, {"text": "efficiency of about", "start": 1723.679, "duration": 3.0}, {"text": "50% right so om Del's law is going to", "start": 1728.159, "duration": 5.081}, {"text": "give you a theoretical upper limit and", "start": 1731.24, "duration": 4.039}, {"text": "speed up but there are other factors", "start": 1733.24, "duration": 4.319}, {"text": "that affect scalability and these are", "start": 1735.279, "duration": 4.64}, {"text": "these are going to have to keep in mind", "start": 1737.559, "duration": 5.321}, {"text": "if you're developing parallel machine Le", "start": 1739.919, "duration": 5.36}, {"text": "machine learning um workloads and", "start": 1742.88, "duration": 4.72}, {"text": "applications there's Communications", "start": 1745.279, "duration": 4.681}, {"text": "overhead you know there's it it takes", "start": 1747.6, "duration": 4.679}, {"text": "time to move data from from one process", "start": 1749.96, "duration": 3.12}, {"text": "to", "start": 1752.279, "duration": 2.801}, {"text": "another you're going to have limits for", "start": 1753.08, "duration": 4.36}, {"text": "it based on the problem size basically", "start": 1755.08, "duration": 5.479}, {"text": "if the problem is too small you can only", "start": 1757.44, "duration": 5.28}, {"text": "divide it down so far and still have a", "start": 1760.559, "duration": 3.801}, {"text": "reasonable amount of work for each", "start": 1762.72, "duration": 3.92}, {"text": "processor thread and there's going to be", "start": 1764.36, "duration": 5.12}, {"text": "uneven load Balan if you take this work", "start": 1766.64, "duration": 4.759}, {"text": "and you're trying to distribute it", "start": 1769.48, "duration": 4.919}, {"text": "across CPUs and gpus if they don't all", "start": 1771.399, "duration": 5.081}, {"text": "get the same amount of work that's going", "start": 1774.399, "duration": 4.681}, {"text": "to affect the um affect the scalability", "start": 1776.48, "duration": 3.72}, {"text": "of your", "start": 1779.08, "duration": 3.36}, {"text": "application and in real life", "start": 1780.2, "duration": 4.68}, {"text": "applications involving communication", "start": 1782.44, "duration": 5.04}, {"text": "synchronization ear regular problems the", "start": 1784.88, "duration": 4.6}, {"text": "speed up can be much less than predicted", "start": 1787.48, "duration": 4.0}, {"text": "by om doll's", "start": 1789.48, "duration": 4.88}, {"text": "law yeah so touching on the first one um", "start": 1791.48, "duration": 5.4}, {"text": "uneven load balancing you know here I", "start": 1794.36, "duration": 5.679}, {"text": "label it as CPU 0 through three this", "start": 1796.88, "duration": 5.639}, {"text": "could also be gpus it could be any any", "start": 1800.039, "duration": 5.48}, {"text": "type of Hardware but in this case I've", "start": 1802.519, "duration": 5.64}, {"text": "taken um I've taken work you know", "start": 1805.519, "duration": 5.201}, {"text": "divided into chunks distributed across", "start": 1808.159, "duration": 5.801}, {"text": "processes but if I don't have um an even", "start": 1810.72, "duration": 4.959}, {"text": "distribution then I'm going to have", "start": 1813.96, "duration": 4.92}, {"text": "processors or CPUs sitting idle waiting", "start": 1815.679, "duration": 5.0}, {"text": "for the longest chunk to", "start": 1818.88, "duration": 5.08}, {"text": "finish so let's say that the um area in", "start": 1820.679, "duration": 6.961}, {"text": "blue it is one one of the CPUs or gpus", "start": 1823.96, "duration": 7.64}, {"text": "is is busy and with the diagonal red", "start": 1827.64, "duration": 7.24}, {"text": "lines when it's idle so if I start off", "start": 1831.6, "duration": 5.28}, {"text": "if I give out chunks of work to to the", "start": 1834.88, "duration": 3.96}, {"text": "to these four processing", "start": 1836.88, "duration": 6.039}, {"text": "units CPU one is going to finish first", "start": 1838.84, "duration": 7.8}, {"text": "followed by cpu3 then cpu0 and then", "start": 1842.919, "duration": 4.88}, {"text": "finally", "start": 1846.64, "duration": 5.24}, {"text": "cpu2 given that cpu2 had more work to do", "start": 1847.799, "duration": 5.641}, {"text": "everybody else is going to be sitting", "start": 1851.88, "duration": 4.12}, {"text": "idle until we get to a synchronization", "start": 1853.44, "duration": 5.719}, {"text": "point so you know in a in a deep", "start": 1856.0, "duration": 6.76}, {"text": "learning um context that this might be", "start": 1859.159, "duration": 6.64}, {"text": "if you are", "start": 1862.76, "duration": 6.2}, {"text": "um doing the doing doing the training of", "start": 1865.799, "duration": 6.281}, {"text": "your the the the training of your model", "start": 1868.96, "duration": 7.199}, {"text": "and you need to wait until um each of", "start": 1872.08, "duration": 6.36}, {"text": "the CPUs or gpus has gotten through its", "start": 1876.159, "duration": 4.0}, {"text": "chunk of work before you could start", "start": 1878.44, "duration": 4.28}, {"text": "doing the back propagation now generally", "start": 1880.159, "duration": 3.88}, {"text": "this is something that you'll be able to", "start": 1882.72, "duration": 3.24}, {"text": "distribute pretty evenly but you do need", "start": 1884.039, "duration": 4.12}, {"text": "to be aware of this um and and then", "start": 1885.96, "duration": 4.199}, {"text": "after we hit that synchronization point", "start": 1888.159, "duration": 4.36}, {"text": "we hand out new chunks of work in this", "start": 1890.159, "duration": 5.041}, {"text": "case cpu2 finishes followed by zero", "start": 1892.519, "duration": 4.361}, {"text": "followed by three and they're all", "start": 1895.2, "duration": 5.88}, {"text": "sitting there idly waiting for a cp1 to", "start": 1896.88, "duration": 6.48}, {"text": "finish um this I'm going to go through", "start": 1901.08, "duration": 6.319}, {"text": "very very quickly um I I'm GNA guess", "start": 1903.36, "duration": 5.559}, {"text": "that that not many in this group are", "start": 1907.399, "duration": 4.76}, {"text": "really interested in solving systems", "start": 1908.919, "duration": 6.561}, {"text": "pdes um but if we have data on a grid", "start": 1912.159, "duration": 5.0}, {"text": "and we need to break it up across", "start": 1915.48, "duration": 4.36}, {"text": "multiple proc", "start": 1917.159, "duration": 2.681}, {"text": "processes and also assuming that that", "start": 1920.639, "duration": 5.28}, {"text": "each cell has to be updated on the value", "start": 1923.44, "duration": 4.76}, {"text": "using the values from its neighboring", "start": 1925.919, "duration": 4.48}, {"text": "cells we are going to run into a", "start": 1928.2, "duration": 4.359}, {"text": "situation where we have to um add", "start": 1930.399, "duration": 4.681}, {"text": "overhead for the communication we need", "start": 1932.559, "duration": 6.921}, {"text": "to have um have Halos of ghost cells to", "start": 1935.08, "duration": 8.16}, {"text": "to toore communicated data and we also", "start": 1939.48, "duration": 6.439}, {"text": "need to wait until until data has been", "start": 1943.24, "duration": 4.96}, {"text": "communicated in order to finish all the", "start": 1945.919, "duration": 5.321}, {"text": "calculations each", "start": 1948.2, "duration": 3.04}, {"text": "process all right so I'm going to say", "start": 1953.0, "duration": 4.279}, {"text": "though all is not lost you know we", "start": 1955.36, "duration": 3.72}, {"text": "looked at the Hard limits imposed by", "start": 1957.279, "duration": 4.801}, {"text": "amol's law we see that we can you know", "start": 1959.08, "duration": 4.88}, {"text": "it only takes a little bit of Serial", "start": 1962.08, "duration": 4.36}, {"text": "content to you know to kind of grind", "start": 1963.96, "duration": 5.64}, {"text": "things to Halt and limit what we can", "start": 1966.44, "duration": 5.32}, {"text": "accomplish with parallel Computing and", "start": 1969.6, "duration": 4.24}, {"text": "then we looked at the other factors that", "start": 1971.76, "duration": 3.32}, {"text": "affect", "start": 1973.84, "duration": 3.559}, {"text": "scalability um you know we might be", "start": 1975.08, "duration": 4.319}, {"text": "wondering how does anybody ever use all", "start": 1977.399, "duration": 4.24}, {"text": "the cores on a single modern compute", "start": 1979.399, "duration": 5.201}, {"text": "node let alone the full power of a large", "start": 1981.639, "duration": 5.721}, {"text": "supercomputer so the reality is that", "start": 1984.6, "duration": 4.919}, {"text": "most parallel applications including", "start": 1987.36, "duration": 4.48}, {"text": "your deep warning learning workloads are", "start": 1989.519, "duration": 4.28}, {"text": "not going to scale the thousands or even", "start": 1991.84, "duration": 3.48}, {"text": "hundreds of qus or", "start": 1993.799, "duration": 4.281}, {"text": "gpus um applications that achieve High", "start": 1995.32, "duration": 5.56}, {"text": "skillability often employ several", "start": 1998.08, "duration": 5.4}, {"text": "strategies so you can grow the problem", "start": 2000.88, "duration": 5.32}, {"text": "size with the number of cores or nodes", "start": 2003.48, "duration": 4.0}, {"text": "if you have big problems that you're", "start": 2006.2, "duration": 3.359}, {"text": "working I really want you to think about", "start": 2007.48, "duration": 4.84}, {"text": "um think about going big because as the", "start": 2009.559, "duration": 5.201}, {"text": "problem size grows as you have more data", "start": 2012.32, "duration": 4.079}, {"text": "that you're working with you're going to", "start": 2014.76, "duration": 4.519}, {"text": "have more opportunity to to run at", "start": 2016.399, "duration": 4.561}, {"text": "larger and larger", "start": 2019.279, "duration": 3.961}, {"text": "scale um you can overlap the", "start": 2020.96, "duration": 4.24}, {"text": "communications with computations that", "start": 2023.24, "duration": 4.24}, {"text": "means while you're communicating data", "start": 2025.2, "duration": 4.56}, {"text": "between nodes you can still have other", "start": 2027.48, "duration": 4.96}, {"text": "independent calculations going on you", "start": 2029.76, "duration": 5.24}, {"text": "could use Dynamic load balancing to", "start": 2032.44, "duration": 5.68}, {"text": "assign the work to cores or gpus as they", "start": 2035.0, "duration": 6.159}, {"text": "idle and you can increase the ratio of", "start": 2038.12, "duration": 5.279}, {"text": "computation to Communications basically", "start": 2041.159, "duration": 4.601}, {"text": "if you're doing more work with with the", "start": 2043.399, "duration": 5.321}, {"text": "data um before communicating data", "start": 2045.76, "duration": 6.68}, {"text": "between processes you can improve", "start": 2048.72, "duration": 3.72}, {"text": "scalability so we're going to move on", "start": 2052.879, "duration": 4.441}, {"text": "next to to running parallel applications", "start": 2054.879, "duration": 4.52}, {"text": "and doing scaling", "start": 2057.32, "duration": 4.0}, {"text": "studies I'm going to say this is where", "start": 2059.399, "duration": 3.76}, {"text": "it gets", "start": 2061.32, "duration": 4.48}, {"text": "um where things become really practical", "start": 2063.159, "duration": 4.24}, {"text": "you know we've covered the basics of", "start": 2065.8, "duration": 4.16}, {"text": "parall Computing hardware and threads", "start": 2067.399, "duration": 4.96}, {"text": "processes hybrid applications", "start": 2069.96, "duration": 5.84}, {"text": "implementing paralyzation MPI again", "start": 2072.359, "duration": 7.681}, {"text": "nickel and horod openmp omll law factors", "start": 2075.8, "duration": 5.599}, {"text": "that affect", "start": 2080.04, "duration": 3.72}, {"text": "scalability um so we got all this theory", "start": 2081.399, "duration": 4.2}, {"text": "in this background but we still need to", "start": 2083.76, "duration": 4.879}, {"text": "know how many CPUs and gpus to use when", "start": 2085.599, "duration": 5.841}, {"text": "running our parallel application and the", "start": 2088.639, "duration": 5.401}, {"text": "only way to definitively answer this is", "start": 2091.44, "duration": 5.679}, {"text": "to do what we call a scaling study and", "start": 2094.04, "duration": 4.36}, {"text": "this has to be done with a", "start": 2097.119, "duration": 3.681}, {"text": "representative problem that's run on", "start": 2098.4, "duration": 5.199}, {"text": "different number of processors or", "start": 2100.8, "duration": 6.319}, {"text": "gpus and a representative problem is one", "start": 2103.599, "duration": 6.081}, {"text": "that's the same size same grid", "start": 2107.119, "duration": 4.761}, {"text": "dimensions number of particles number of", "start": 2109.68, "duration": 6.76}, {"text": "images genomes um Etc and the same", "start": 2111.88, "duration": 6.84}, {"text": "complexity say if you're a computational", "start": 2116.44, "duration": 5.6}, {"text": "chemist the level of theory um the type", "start": 2118.72, "duration": 5.32}, {"text": "of analysis that that you're doing the", "start": 2122.04, "duration": 4.4}, {"text": "physics that's included", "start": 2124.04, "duration": 5.319}, {"text": "the B basically all of the all of the", "start": 2126.44, "duration": 5.88}, {"text": "details the science as the research", "start": 2129.359, "duration": 5.961}, {"text": "problem that you want to", "start": 2132.32, "duration": 3.0}, {"text": "solve so at some point you're probably", "start": 2135.839, "duration": 3.801}, {"text": "going to be asked if you're developing", "start": 2138.119, "duration": 3.401}, {"text": "these parallel developing and running", "start": 2139.64, "duration": 4.32}, {"text": "these parallel applications to present", "start": 2141.52, "duration": 4.88}, {"text": "scaling results", "start": 2143.96, "duration": 5.76}, {"text": "showing show showing the the runtime as", "start": 2146.4, "duration": 5.36}, {"text": "a function of the number of cores or", "start": 2149.72, "duration": 4.879}, {"text": "gpus that you're using and we see this", "start": 2151.76, "duration": 6.96}, {"text": "all the time and in allocation request", "start": 2154.599, "duration": 7.961}, {"text": "plots where it's time on the Y AIS cores", "start": 2158.72, "duration": 8.32}, {"text": "on the xais and it's plotted", "start": 2162.56, "duration": 7.12}, {"text": "using using linear", "start": 2167.04, "duration": 6.16}, {"text": "AIS so if you look at these two scaling", "start": 2169.68, "duration": 5.919}, {"text": "curves you know it's it's it's kind of", "start": 2173.2, "duration": 4.04}, {"text": "hard to tell which one of these", "start": 2175.599, "duration": 5.361}, {"text": "applications has better scaling", "start": 2177.24, "duration": 3.72}, {"text": "Behavior but if we do this the right way", "start": 2181.319, "duration": 5.641}, {"text": "and we plot the data on log axes we're", "start": 2184.52, "duration": 5.52}, {"text": "going to get lot more insight so the", "start": 2186.96, "duration": 5.639}, {"text": "plots that I'm showing here this is the", "start": 2190.04, "duration": 5.279}, {"text": "exact same data that I showed in the", "start": 2192.599, "duration": 6.161}, {"text": "previous slide except shown on shown on", "start": 2195.319, "duration": 4.601}, {"text": "log", "start": 2198.76, "duration": 3.44}, {"text": "axis when we do", "start": 2199.92, "duration": 5.159}, {"text": "this if we", "start": 2202.2, "duration": 5.72}, {"text": "um if we plot the runtime versus the", "start": 2205.079, "duration": 4.441}, {"text": "number of cords or", "start": 2207.92, "duration": 5.12}, {"text": "gpus if we have linear scaling it's", "start": 2209.52, "duration": 6.2}, {"text": "we're we're going to get a straight line", "start": 2213.04, "duration": 5.12}, {"text": "and what you can do to make your um", "start": 2215.72, "duration": 5.28}, {"text": "scaling plots even even more useful is", "start": 2218.16, "duration": 5.6}, {"text": "you can you can add um add a line that", "start": 2221.0, "duration": 5.68}, {"text": "shows the linear scaling and can also", "start": 2223.76, "duration": 5.839}, {"text": "plot the parallel efficiency so the", "start": 2226.68, "duration": 5.24}, {"text": "parallel efficiency is the speed up", "start": 2229.599, "duration": 5.52}, {"text": "divided by the number of cors so", "start": 2231.92, "duration": 4.96}, {"text": "essentially if you're getting perfect", "start": 2235.119, "duration": 4.761}, {"text": "scaling say every time you double the", "start": 2236.88, "duration": 6.36}, {"text": "number of cores you have the run time", "start": 2239.88, "duration": 6.0}, {"text": "your parallel efficiency is going to be", "start": 2243.24, "duration": 5.24}, {"text": "um is going to be maxed down out of", "start": 2245.88, "duration": 5.64}, {"text": "1.0 so we can see that this um", "start": 2248.48, "duration": 4.68}, {"text": "application", "start": 2251.52, "duration": 5.0}, {"text": "left perfect perfect parallel", "start": 2253.16, "duration": 6.52}, {"text": "efficiency and um linear scaling we", "start": 2256.52, "duration": 5.44}, {"text": "could see on the right and this is going", "start": 2259.68, "duration": 4.439}, {"text": "to be the more realistic situation is", "start": 2261.96, "duration": 5.359}, {"text": "that we start off as we go to two four", "start": 2264.119, "duration": 6.161}, {"text": "eight eight cores we're doing pretty", "start": 2267.319, "duration": 5.921}, {"text": "well we have nearly linear scalability", "start": 2270.28, "duration": 4.96}, {"text": "um we have a parallel efficiency that's", "start": 2273.24, "duration": 4.92}, {"text": "up around 90% or so but then after that", "start": 2275.24, "duration": 5.44}, {"text": "the performance really really drops off", "start": 2278.16, "duration": 4.8}, {"text": "so if you're taking your deep learning", "start": 2280.68, "duration": 4.8}, {"text": "or machine learning um workloads you're", "start": 2282.96, "duration": 4.76}, {"text": "scaling them up you really need to do", "start": 2285.48, "duration": 4.32}, {"text": "these scaling studies so that you know", "start": 2287.72, "duration": 3.96}, {"text": "you're running on the appropriate number", "start": 2289.8, "duration": 3.88}, {"text": "of cores or", "start": 2291.68, "duration": 4.399}, {"text": "gpus so the question now is where should", "start": 2293.68, "duration": 5.04}, {"text": "I be on that scaling curve so if your", "start": 2296.079, "duration": 5.0}, {"text": "work is not particularly sensitive to", "start": 2298.72, "duration": 4.44}, {"text": "the time to complete a single run", "start": 2301.079, "duration": 5.681}, {"text": "consider using a CPU or a GPU count", "start": 2303.16, "duration": 7.8}, {"text": "that's at we very close to 100% parallel", "start": 2306.76, "duration": 6.599}, {"text": "efficiency even if that means running on", "start": 2310.96, "duration": 3.8}, {"text": "a single core", "start": 2313.359, "duration": 4.121}, {"text": "GPU and this especially makes sense if", "start": 2314.76, "duration": 5.04}, {"text": "you're doing parameter sweep workloads", "start": 2317.48, "duration": 5.16}, {"text": "or um kind of work that could be done", "start": 2319.8, "duration": 4.68}, {"text": "independently the same sort of", "start": 2322.64, "duration": 4.88}, {"text": "calculation analysis run many times with", "start": 2324.48, "duration": 5.2}, {"text": "different sets of inputs and this is", "start": 2327.52, "duration": 5.079}, {"text": "what we like this is what we like to see", "start": 2329.68, "duration": 5.439}, {"text": "most applications out here parallel", "start": 2332.599, "duration": 6.121}, {"text": "efficiencies of better bet better than", "start": 2335.119, "duration": 5.841}, {"text": "80% now sometimes though you're going to", "start": 2338.72, "duration": 3.599}, {"text": "have to go a little bit further out on", "start": 2340.96, "duration": 3.72}, {"text": "the scaling curve if the job would have", "start": 2342.319, "duration": 5.161}, {"text": "taken an unreasonably long time to", "start": 2344.68, "duration": 6.24}, {"text": "complete at lower core or GPU counts or", "start": 2347.48, "duration": 5.32}, {"text": "if the shorter time the solution helps", "start": 2350.92, "duration": 4.52}, {"text": "you make progress in your research if", "start": 2352.8, "duration": 4.559}, {"text": "your code does not have checkpoint", "start": 2355.44, "duration": 3.84}, {"text": "resear", "start": 2357.359, "duration": 3.96}, {"text": "capabilities and basically this means", "start": 2359.28, "duration": 4.079}, {"text": "that you can you can St you can save the", "start": 2361.319, "duration": 4.241}, {"text": "state of the calculations write them to", "start": 2363.359, "duration": 4.601}, {"text": "dis read them in later and continue", "start": 2365.56, "duration": 4.799}, {"text": "continue your work if you don't have", "start": 2367.96, "duration": 4.879}, {"text": "those restart capabilities and the", "start": 2370.359, "duration": 5.121}, {"text": "runtime would exceed the Q limits", "start": 2372.839, "duration": 4.681}, {"text": "typically we only allow jobs to run for", "start": 2375.48, "duration": 4.56}, {"text": "up to 48 hours you'll have no choice but", "start": 2377.52, "duration": 5.76}, {"text": "to run at these higher cor", "start": 2380.04, "duration": 3.24}, {"text": "counts and then if time to solution is", "start": 2383.64, "duration": 6.199}, {"text": "absolutely critical it's okay to run at", "start": 2386.96, "duration": 5.32}, {"text": "lower efficiency I'm going to say this", "start": 2389.839, "duration": 4.561}, {"text": "is probably not going to apply to most", "start": 2392.28, "duration": 4.559}, {"text": "of you but do think about this so if", "start": 2394.4, "duration": 5.959}, {"text": "you're doing say operational work where", "start": 2396.839, "duration": 7.561}, {"text": "the results of a calculation or um you", "start": 2400.359, "duration": 6.601}, {"text": "know TR TR training model has to be done", "start": 2404.4, "duration": 5.28}, {"text": "every day on a regular schedule say data", "start": 2406.96, "duration": 4.56}, {"text": "is collected during the day it has to be", "start": 2409.68, "duration": 4.32}, {"text": "processed overnight available first in", "start": 2411.52, "duration": 4.319}, {"text": "morning or if you're doing severe", "start": 2414.0, "duration": 2.68}, {"text": "weather", "start": 2415.839, "duration": 2.841}, {"text": "forecasting you know I give the example", "start": 2416.68, "duration": 5.159}, {"text": "here of tornado or tsunami prediction um", "start": 2418.68, "duration": 4.679}, {"text": "then you can then you can run out", "start": 2421.839, "duration": 4.28}, {"text": "further um you know at lower power lower", "start": 2423.359, "duration": 4.72}, {"text": "efficiency just make make sure though at", "start": 2426.119, "duration": 4.361}, {"text": "the very least that your runtime is is", "start": 2428.079, "duration": 6.401}, {"text": "being reduced um I I don't I should add", "start": 2430.48, "duration": 6.24}, {"text": "this to the to the next version of the", "start": 2434.48, "duration": 4.599}, {"text": "presentation where there's sometimes", "start": 2436.72, "duration": 5.32}, {"text": "when you use more CPUs or gpus that the", "start": 2439.079, "duration": 5.76}, {"text": "code actually begins to run slower in", "start": 2442.04, "duration": 4.92}, {"text": "absolute terms it actually takes longer", "start": 2444.839, "duration": 5.681}, {"text": "to run so do these scaling studies be", "start": 2446.96, "duration": 6.159}, {"text": "aware of of where you are in the scaling", "start": 2450.52, "duration": 4.88}, {"text": "curves and and choose those points um", "start": 2453.119, "duration": 5.601}, {"text": "points appropriately", "start": 2455.4, "duration": 3.32}, {"text": "um I'm just going to skip over this you", "start": 2458.8, "duration": 3.799}, {"text": "know talk a bit about large memory", "start": 2461.0, "duration": 4.839}, {"text": "footprint applications um these are", "start": 2462.599, "duration": 6.24}, {"text": "cases where you say need need a large", "start": 2465.839, "duration": 3.961}, {"text": "amount of", "start": 2468.839, "duration": 5.721}, {"text": "memory um and you end up using", "start": 2469.8, "duration": 4.76}, {"text": "say", "start": 2474.68, "duration": 5.0}, {"text": "oh you need you need a lot of memory to", "start": 2476.68, "duration": 5.08}, {"text": "just store the problem but you're not", "start": 2479.68, "duration": 4.88}, {"text": "running at very um at very good parel", "start": 2481.76, "duration": 5.64}, {"text": "efficiency in that case since those CPUs", "start": 2484.56, "duration": 5.24}, {"text": "might otherwise go to waste it's it's", "start": 2487.4, "duration": 5.04}, {"text": "okay to to run a little bit to to run at", "start": 2489.8, "duration": 5.08}, {"text": "a little bit poor", "start": 2492.44, "duration": 5.08}, {"text": "scale okay so we're going to wrap up I'm", "start": 2494.88, "duration": 5.959}, {"text": "try to get us a little bit back on time", "start": 2497.52, "duration": 5.559}, {"text": "um we only Strat your surface parallel", "start": 2500.839, "duration": 5.52}, {"text": "Computing um exced and scsc have many", "start": 2503.079, "duration": 5.361}, {"text": "training resources covering a wide range", "start": 2506.359, "duration": 4.96}, {"text": "of topics um I'm sure a lot of you are", "start": 2508.44, "duration": 4.32}, {"text": "are probably familiar with these already", "start": 2511.319, "duration": 3.52}, {"text": "since you discovered the Discover the", "start": 2512.76, "duration": 5.2}, {"text": "simol summer Institute um user guides", "start": 2514.839, "duration": 4.921}, {"text": "for nationally allocated resources", "start": 2517.96, "duration": 3.879}, {"text": "contain practical information on job", "start": 2519.76, "duration": 4.28}, {"text": "submission accounting compilation and so", "start": 2521.839, "duration": 5.361}, {"text": "on um so you can get to that through the", "start": 2524.04, "duration": 5.559}, {"text": "exceed portal which will be active for", "start": 2527.2, "duration": 5.36}, {"text": "for the next few months be before exceed", "start": 2529.599, "duration": 6.961}, {"text": "is replaced by access the NSS follow on", "start": 2532.56, "duration": 8.36}, {"text": "program um and the ssse user", "start": 2536.56, "duration": 7.12}, {"text": "guide so with that this is my final", "start": 2540.92, "duration": 5.04}, {"text": "slide going say parallel Computing for", "start": 2543.68, "duration": 3.919}, {"text": "everybody who wants to accomplish more", "start": 2545.96, "duration": 3.359}, {"text": "research and solve more challenging", "start": 2547.599, "duration": 4.361}, {"text": "problems again I'm hoping that this", "start": 2549.319, "duration": 5.24}, {"text": "applies to applies to everybody here", "start": 2551.96, "duration": 4.639}, {"text": "that you're trying to go from doing", "start": 2554.559, "duration": 5.481}, {"text": "small scale on M machine learning to to", "start": 2556.599, "duration": 5.881}, {"text": "large scale for many of you you don't", "start": 2560.04, "duration": 4.96}, {"text": "need to be a programmer to use parallel", "start": 2562.48, "duration": 4.839}, {"text": "computers but you do need to understand", "start": 2565.0, "duration": 4.64}, {"text": "the fundamentals of processes and", "start": 2567.319, "duration": 3.961}, {"text": "threads and", "start": 2569.64, "duration": 4.719}, {"text": "scalability um minder processes you", "start": 2571.28, "duration": 4.68}, {"text": "could think of as an instance of a", "start": 2574.359, "duration": 4.081}, {"text": "program threads run within a process and", "start": 2575.96, "duration": 7.0}, {"text": "access shared data MPI nickel horod are", "start": 2578.44, "duration": 7.32}, {"text": "used to do Distributing memory on", "start": 2582.96, "duration": 5.52}, {"text": "parallelization openmp is used parallel", "start": 2585.76, "duration": 4.559}, {"text": "code with that's going to be running", "start": 2588.48, "duration": 5.28}, {"text": "within a node a shared memory model we", "start": 2590.319, "duration": 5.641}, {"text": "talked about om doll's law this gives", "start": 2593.76, "duration": 4.68}, {"text": "you an upper limit on scalability but", "start": 2595.96, "duration": 4.08}, {"text": "there are other factors that you need to", "start": 2598.44, "duration": 4.0}, {"text": "keep in mind such as load imbalance and", "start": 2600.04, "duration": 4.92}, {"text": "Communications overhead and then know", "start": 2602.44, "duration": 5.159}, {"text": "how to display and gener generate your", "start": 2604.96, "duration": 5.359}, {"text": "scaling data and choose your core counts", "start": 2607.599, "duration": 4.601}, {"text": "appropriately and then if you're", "start": 2610.319, "duration": 5.04}, {"text": "interested in um in generating your own", "start": 2612.2, "duration": 6.48}, {"text": "scaling plots these are available in my", "start": 2615.359, "duration": 5.641}, {"text": "my GitHub repo parallel Concepts and I", "start": 2618.68, "duration": 3.96}, {"text": "will paste that into the chat when we're", "start": 2621.0, "duration": 5.04}, {"text": "done here and with", "start": 2622.64, "duration": 7.88}, {"text": "that we are all done", "start": 2626.04, "duration": 4.48}]